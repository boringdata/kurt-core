# Kurt Core

Document intelligence CLI - Fetch web content and extract metadata using Trafilatura.

## Installation

```bash
# Install from source
cd kurt-core
uv sync
```

## Quick Start

```bash
# 1. Initialize a new Kurt project (creates .kurt/ directory and SQLite database)
kurt init

# 2. Map sitemap to discover URLs (fast, no downloads)
kurt ingest map https://example.com

# 3a. Fetch single document
kurt ingest fetch <document-id>

# 3b. Or batch fetch all discovered URLs (parallel, recommended)
kurt ingest fetch --url-prefix https://example.com/

# 4. List documents
kurt document list

# 5. Get document details
kurt document get <document-id>
```

## Configuration

Kurt stores project configuration in a `.kurt` file in your project directory:

```
KURT_PROJECT_PATH="."
KURT_DB=".kurt/kurt.sqlite"
```

This file is auto-generated by `kurt init`. You can customize:
- **KURT_PROJECT_PATH**: Root directory of your Kurt project
- **KURT_DB**: Path to SQLite database (relative to project path)

Example custom initialization:
```bash
# Use a custom database path
kurt init --db-path data/my-project.db

# Initialize in a specific directory
cd /path/to/my-project
kurt init
```

## Commands

### Content Ingestion

**Map-Then-Fetch Workflow** (recommended):
1. **Map**: Discover URLs from sitemap (fast, creates `NOT_FETCHED` records)
2. **Review**: List and examine discovered URLs
3. **Fetch**: Selectively download content (batch or single)

```bash
# Discover URLs from sitemap
kurt ingest map https://example.com
kurt ingest map https://example.com --limit 10      # Test with first 10 URLs
kurt ingest map https://example.com --fetch         # Map + fetch in one step

# Fetch content
kurt ingest fetch <doc-id>                          # Single document
kurt ingest fetch https://example.com/page          # By URL (creates if needed)
kurt ingest fetch --url-prefix https://example.com/ # Batch: all matching prefix
kurt ingest fetch --url-contains /blog/             # Batch: URLs containing string
kurt ingest fetch --all                             # Batch: all NOT_FETCHED docs
kurt ingest fetch --url-prefix https://example.com/ --max-concurrent 10  # 10 parallel downloads
kurt ingest fetch --url-prefix https://example.com/ --status ERROR       # Retry failed docs

# Add single URL without sitemap
kurt ingest add https://example.com/page
```

### Document Management

```bash
# List documents
kurt document list
kurt document list --status FETCHED --limit 20
kurt document list --status NOT_FETCHED

# Get document details
kurt document get <document-id>          # Full UUID
kurt document get 44ea066e               # Partial UUID (min 8 chars)

# Delete document
kurt document delete <document-id>
kurt document delete <document-id> --delete-content  # Also delete markdown file
kurt document delete <document-id> --yes             # Skip confirmation

# Document statistics
kurt document stats
```

### Project Management

```bash
# Initialize project
kurt init                                   # Current directory
kurt init --project-path /path/to/project  # Custom location
kurt init --db-path data/my-db.sqlite      # Custom database path
```

## Architecture

**Content Storage**:
- Metadata stored in SQLite (`Document` table)
- Content stored as markdown files in `sources/{domain}/{path}/`
- Metadata extracted with Trafilatura (title, author, date, categories, language)

**Batch Fetching**:
- Uses `httpx` with async/await for parallel downloads
- Semaphore-based concurrency control (default: 5 concurrent)
- Graceful error handling (continues on individual failures)

**Database Schema**:
```sql
CREATE TABLE documents (
    id TEXT PRIMARY KEY,              -- UUID
    title TEXT NOT NULL,
    source_type TEXT,                 -- URL, FILE_UPLOAD, API
    source_url TEXT UNIQUE,
    content_path TEXT,                -- Relative path to markdown file
    ingestion_status TEXT,            -- NOT_FETCHED, FETCHED, ERROR
    content_hash TEXT,                -- Trafilatura fingerprint (deduplication)
    description TEXT,                 -- Meta description
    author JSON,                      -- List of authors
    published_date DATETIME,
    categories JSON,                  -- Tags/categories
    language TEXT,                    -- ISO 639-1 code
    created_at DATETIME,
    updated_at DATETIME
);
```

## Development

```bash
# Install dev dependencies
uv sync --group dev

# Run tests
uv run pytest

# Run linter
uv run ruff check src/

# Format code
uv run ruff format src/
```

## Project Structure

```
kurt-core/
├── src/kurt/
│   ├── __init__.py
│   ├── cli.py              # Main CLI entry point
│   ├── ingest.py           # Content ingestion functions
│   ├── document.py         # Document CRUD operations
│   ├── models/
│   │   └── models.py       # SQLModel schemas
│   ├── commands/
│   │   ├── document.py     # Document CLI commands
│   │   └── ingest.py       # Ingestion CLI commands
│   └── database.py         # Database initialization
├── tests/                  # Test suite
└── pyproject.toml         # Dependencies
```

## Dependencies

Core:
- `trafilatura>=2.0.0` - Web scraping and metadata extraction
- `httpx>=0.27.0` - Async HTTP client for batch fetching
- `sqlmodel>=0.0.14` - SQLite ORM
- `click>=8.1.0` - CLI framework
- `rich>=13.0.0` - Terminal output formatting

## Use Cases

**Content Aggregation**:
```bash
# Ingest entire blog
kurt ingest map https://blog.example.com
kurt ingest fetch --url-prefix https://blog.example.com/
```

**Selective Ingestion**:
```bash
# Map all URLs, fetch only specific category
kurt ingest map https://example.com
kurt ingest fetch --url-contains /tutorials/
```

**Retry Failed Downloads**:
```bash
# Retry documents that failed to fetch
kurt ingest fetch --url-prefix https://example.com/ --status ERROR --max-concurrent 10
```

**Find Duplicate Content**:
```bash
# Trafilatura extracts content_hash (fingerprint) for deduplication
sqlite3 .kurt/kurt.sqlite "
  SELECT content_hash, GROUP_CONCAT(source_url)
  FROM documents
  WHERE content_hash IS NOT NULL
  GROUP BY content_hash
  HAVING COUNT(*) > 1
"
```

## License

MIT
