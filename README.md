# Kurt Core

Document intelligence CLI - Fetch web content and extract metadata using Trafilatura.

## Installation

```bash
# Install from source
cd kurt-core
uv sync
```

## Quick Start

```bash
# 1. Initialize a new Kurt project (creates .kurt/ directory and SQLite database)
kurt init

# 2. Map sitemap to discover URLs (fast, no downloads)
kurt ingest map https://example.com

# 3a. Fetch single document
kurt ingest fetch <document-id>

# 3b. Or batch fetch all discovered URLs (parallel, recommended)
kurt ingest fetch --url-prefix https://example.com/

# 4. List documents
kurt document list

# 5. Get document details
kurt document get <document-id>
```

## Configuration

Kurt stores project configuration in a `.kurt` file in your project directory:

```
KURT_PROJECT_PATH="."
KURT_DB=".kurt/kurt.sqlite"
```

This file is auto-generated by `kurt init`. You can customize:
- **KURT_PROJECT_PATH**: Root directory of your Kurt project
- **KURT_DB**: Path to SQLite database (relative to project path)

Example custom initialization:
```bash
# Use a custom database path
kurt init --db-path data/my-project.db

# Initialize in a specific directory
cd /path/to/my-project
kurt init
```

## Commands

### Content Ingestion

**Map-Then-Fetch Workflow** (recommended):
1. **Map**: Discover URLs from sitemap (fast, creates `NOT_FETCHED` records)
2. **Review**: List and examine discovered URLs
3. **Fetch**: Selectively download content (batch or single)

```bash
# Discover URLs from sitemap
kurt ingest map https://example.com
kurt ingest map https://example.com --limit 10      # Test with first 10 URLs
kurt ingest map https://example.com --fetch         # Map + fetch in one step

# Fetch content
kurt ingest fetch <doc-id>                          # Single document
kurt ingest fetch https://example.com/page          # By URL (creates if needed)
kurt ingest fetch --url-prefix https://example.com/ # Batch: all matching prefix
kurt ingest fetch --url-contains /blog/             # Batch: URLs containing string
kurt ingest fetch --all                             # Batch: all NOT_FETCHED docs
kurt ingest fetch --url-prefix https://example.com/ --max-concurrent 10  # 10 parallel downloads
kurt ingest fetch --url-prefix https://example.com/ --status ERROR       # Retry failed docs

# Add single URL without sitemap
kurt ingest add https://example.com/page
```

### Document Management

```bash
# List documents
kurt document list
kurt document list --status FETCHED --limit 20
kurt document list --status NOT_FETCHED

# Get document details
kurt document get <document-id>          # Full UUID
kurt document get 44ea066e               # Partial UUID (min 8 chars)

# Delete document
kurt document delete <document-id>
kurt document delete <document-id> --delete-content  # Also delete markdown file
kurt document delete <document-id> --yes             # Skip confirmation

# Document statistics
kurt document stats
```

### Project Management

```bash
# Initialize project
kurt init                                   # Current directory
kurt init --project-path /path/to/project  # Custom location
kurt init --db-path data/my-db.sqlite      # Custom database path
```

## Architecture

**Content Storage**:
- Metadata stored in SQLite (`Document` table)
- Content stored as markdown files in `sources/{domain}/{path}/`
- Metadata extracted with Trafilatura (title, author, date, categories, language)

**Batch Fetching**:
- Uses `httpx` with async/await for parallel downloads
- Semaphore-based concurrency control (default: 5 concurrent)
- Graceful error handling (continues on individual failures)

**Database Schema**:
```sql
CREATE TABLE documents (
    id TEXT PRIMARY KEY,              -- UUID
    title TEXT NOT NULL,
    source_type TEXT,                 -- URL, FILE_UPLOAD, API
    source_url TEXT UNIQUE,
    content_path TEXT,                -- Relative path to markdown file
    ingestion_status TEXT,            -- NOT_FETCHED, FETCHED, ERROR
    content_hash TEXT,                -- Trafilatura fingerprint (deduplication)
    description TEXT,                 -- Meta description
    author JSON,                      -- List of authors
    published_date DATETIME,
    categories JSON,                  -- Tags/categories
    language TEXT,                    -- ISO 639-1 code
    created_at DATETIME,
    updated_at DATETIME
);
```

## Development

```bash
# Install dev dependencies
uv sync --group dev

# Run tests
uv run pytest

# Run linter
uv run ruff check src/

# Format code
uv run ruff format src/
```

## Evaluation Framework

Kurt includes an evaluation framework for testing agent behavior with automated scenarios. This framework allows you to:

- Test real agent interactions with the Kurt CLI using the Claude Code SDK
- Run scenarios in isolated temporary workspaces
- Validate outcomes with assertions (file existence, database state, tool usage)
- Track metrics (tool calls, timing, conversation turns)
- Generate detailed results (JSON metrics + markdown transcripts)

### Installation

Install the `kurt-eval` CLI tool:

```bash
uv tool install -e .
```

### Quick Start

```bash
# List available scenarios
kurt-eval list

# Run a specific scenario
kurt-eval run 1                    # By number
kurt-eval run 03_project_no_sources  # By name

# Run all scenarios
kurt-eval run-all

# Run with options
kurt-eval run 3 --no-cleanup              # Preserve workspace
kurt-eval run 3 --llm-provider anthropic  # Use Anthropic for user agent
```

### Scenario Definition

Scenarios are defined in YAML format at [eval/scenarios/scenarios.yaml](eval/scenarios/scenarios.yaml). Each scenario specifies:

- **initial_prompt**: Task given to the agent
- **user_agent_prompt** (optional): Instructions for automated multi-turn conversations
- **setup_commands** (optional): Bash commands run before the scenario
- **assertions**: Validation conditions (file existence, database queries, tool usage)

Example scenario:

```yaml
scenarios:
  - name: 02_add_url
    description: Initialize Kurt and add content from a URL
    initial_prompt: >
      Initialize a Kurt project, then add content from
      https://docs.dagster.io/ using discovery only
    assertions:
      - type: FileExists
        path: kurt.config
      - type: SQLQueryAssertion
        query: SELECT COUNT(*) >= 500 FROM documents WHERE ingestion_status='NOT_FETCHED'
```

### Available Scenarios

The framework includes scenarios for:
- **01_basic_init**: Initialize a new Kurt project
- **02_add_url**: Initialize and discover content from a URL
- **03_project_no_sources**: Multi-turn project creation without sources
- **04_project_with_sources**: Project creation with content fetching
- **05_setup_foundations**: Complete foundation setup (init + discovery + rule extraction)
- **06_preconfigured_project**: Test with pre-configured project structure

See [eval/scenarios/scenarios.yaml](eval/scenarios/scenarios.yaml) for complete scenario definitions.

## Project Structure

```
kurt-core/
├── src/kurt/
│   ├── __init__.py
│   ├── cli.py              # Main CLI entry point
│   ├── ingest.py           # Content ingestion functions
│   ├── document.py         # Document CRUD operations
│   ├── models/
│   │   └── models.py       # SQLModel schemas
│   ├── commands/
│   │   ├── document.py     # Document CLI commands
│   │   └── ingest.py       # Ingestion CLI commands
│   └── database.py         # Database initialization
├── tests/                  # Test suite
└── pyproject.toml         # Dependencies
```

## Dependencies

Core:
- `trafilatura>=2.0.0` - Web scraping and metadata extraction
- `httpx>=0.27.0` - Async HTTP client for batch fetching
- `sqlmodel>=0.0.14` - SQLite ORM
- `click>=8.1.0` - CLI framework
- `rich>=13.0.0` - Terminal output formatting

## Use Cases

**Content Aggregation**:
```bash
# Ingest entire blog
kurt ingest map https://blog.example.com
kurt ingest fetch --url-prefix https://blog.example.com/
```

**Selective Ingestion**:
```bash
# Map all URLs, fetch only specific category
kurt ingest map https://example.com
kurt ingest fetch --url-contains /tutorials/
```

**Retry Failed Downloads**:
```bash
# Retry documents that failed to fetch
kurt ingest fetch --url-prefix https://example.com/ --status ERROR --max-concurrent 10
```

**Find Duplicate Content**:
```bash
# Trafilatura extracts content_hash (fingerprint) for deduplication
sqlite3 .kurt/kurt.sqlite "
  SELECT content_hash, GROUP_CONCAT(source_url)
  FROM documents
  WHERE content_hash IS NOT NULL
  GROUP BY content_hash
  HAVING COUNT(*) > 1
"
```

## Telemetry

Kurt collects anonymous usage analytics to help us understand how the tool is used and improve it. We take privacy seriously.

### What We Collect

- **Command usage**: Which commands are run (e.g., `kurt content add`)
- **Execution metrics**: Timing and success/failure rates
- **Environment**: OS, Python version, Kurt version
- **Machine ID**: Anonymous identifier (UUID, not tied to personal info)

### What We DON'T Collect

- Personal information (names, emails, etc.)
- File paths or URLs
- Command arguments or user data
- Any sensitive information

### How to Opt-Out

Disable telemetry using any of these methods:

```bash
# 1. Use the CLI command
kurt telemetry disable

# 2. Set environment variable (universal)
export DO_NOT_TRACK=1

# 3. Set Kurt-specific environment variable
export KURT_TELEMETRY_DISABLED=1
```

Check telemetry status:

```bash
kurt telemetry status
```

### Privacy

All telemetry is:
- **Anonymous**: No personal information collected
- **Transparent**: Clearly documented what we collect
- **Optional**: Easy to opt-out
- **Non-blocking**: Never slows down CLI commands
- **Secure**: Uses PostHog cloud (SOC 2 compliant)

## License

MIT
