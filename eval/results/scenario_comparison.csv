Question #|Scenario|Question Text|Answer|Reference Answer|Sources|Source Delta|Judge Overall Score|Judge Accuracy|Judge Completeness|Judge Relevance|Judge Clarity|Judge Feedback|Tokens Used|Duration (seconds)|Answer File|Scenario Definition|Result File
1|answer_motherduck_without_kg|...... What file formats are most efficient for loading data into MotherDuck?|"Most Efficient File Formats for Loading Data into MotherDuck Based on the MotherDuck documentation, Parquet is the most efficient file format for loading data into MotherDuck, followed by other columnar formats. Here's a comprehensive breakdown: Parquet: The Recommended Choice Parquet is the de facto standard and most efficient format for loading data into MotherDuck for several key reasons: Columnar Storage Efficiency: Parquet stores data by columns rather than rows, enabling faster retrieval and better compression ratios. When analytical queries only need specific columns, MotherDuck can read just those columns rather than entire rows. Superior Compression: Parquet uses specialized encoding techniques including: • Dictionary encoding (for low-to-medium cardinality columns) • Run-Length Encoding (RLE) for sorted columns • Delta encoding for timestamps and sequential IDs • These methods dramatically reduce storage footprint and I/O operations Predicate Pushdown & Column Pruning: Parquet files store metadata and statistics (min/max values, null counts) for each column chunk, enabling MotherDuck to skip irrelevant data blocks entirely during queries. Native Integration: DuckDB's architecture is purpose-built to leverage Parquet's column-oriented design. MotherDuck can query Parquet files directly from local storage or S3 using glob patterns without importing first. Format Comparison for Efficiency \| Format \| Efficiency \| Best Use Case \| \|--------\|-----------\|---------------\| \| Parquet \| Highest \| Analytics, large datasets, long-term storage \| \| CSV \| Lower \| Data exchange, human-readable, universal compatibility \| \| JSON \| Variable \| Semi-structured data, API responses \| Why CSV is Less Efficient (But Still Supported) While CSV files are universally supported and easy to work with, they lack: • Typed columns (everything is text) • Built-in compression • Column pruning capability • Schema management However, MotherDuck/DuckDB has an exceptional CSV parser with features like automatic type detection, read_csv_auto(), glob pattern support, and excellent error handling that makes working with CSVs practical when necessary. Practical Recommendations For Analytical Workloads: Convert CSV to Parquet for storage efficiency and query performance: For Querying Cloud Storage: Use Parquet files with Hive-style partitioning (e.g., /year=2025/month=10/) to enable efficient data skipping. For One-Time Analysis: DuckDB's ""No-ETL"" approach allows querying raw CSV/JSON files directly, but Parquet will always be faster for repeated queries. For Large-Scale Data Lakes: Organize data as Parquet files to minimize cloud storage costs and data scan charges. Key Takeaway For optimal performance when loading data into MotherDuck, convert your data to Parquet format. The columnar storage, compression, and metadata capabilities enable significantly faster query performance and reduced storage costs compared to row-based formats like CSV."|Parquet is the most efficient file format for loading data into MotherDuck. It's a columnar storage format optimized for analytical queries and well-supported by DuckDB. Other efficient formats include CSV for simple data and JSON for semi-structured data, but Parquet offers the best performance for large-scale analytics.|.kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md; .kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md; .kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md; .kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md; .kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md; .kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md|0% (0 common, 6 unique to with_kg, 0 unique to without_kg)|1.0|1.0|1.0|1.0|1.0|This is an exemplary answer that perfectly captures all key points from the canonical answer while significantly expanding on them with accurate technical details. The answer correctly identifies Parquet as most efficient, explains why it outperforms CSV and JSON, and provides practical context through comparisons and code examples. The structure is clear with effective use of headers, bullet points, and a comparison table that makes the information highly accessible and actionable.|0.0|66.37663221359253|/tmp/answer_without_kg_1.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q1_20251227_155512.json
1|answer_with_retrieval_then_docs|...... What file formats are most efficient for loading data into MotherDuck?|Parquet is the most efficient file format for loading data into MotherDuck, especially for analytical workloads. Key advantages of Parquet: • Columnar storage layout - excels at analytical queries reading subsets of columns • Impressive compression - uses dictionary, run-length, and delta encoding to optimize storage • Predicate pushdown and column pruning - optimizes data queries • Native support - widely supported across big data frameworks MotherDuck can also natively ingest from CSV, JSON, Iceberg, and Delta file formats, but Parquet is preferred for efficiency due to its columnar nature and compression capabilities. For CSV files, DuckDB (which powers MotherDuck) includes a CSV sniffer that auto-detects structure. Use ignore_errors = true to handle malformed rows gracefully. --- Sources: • Parquet claims [4aa515a5]: columnar storage, compression, predicate pushdown, schema evolution • MotherDuck cross-entity claim: native ingestion of CSV, Parquet, JSON, Iceberg, Delta • CSV sniffer claims: error handling options|Parquet is the most efficient file format for loading data into MotherDuck. It's a columnar storage format optimized for analytical queries and well-supported by DuckDB. Other efficient formats include CSV for simple data and JSON for semi-structured data, but Parquet offers the best performance for large-scale analytics.||0% (0 common, 6 unique to with_kg, 0 unique to without_kg)|1.0|1.0|1.0|1.0|1.0|The generated answer excellently addresses the question with complete accuracy. It correctly identifies Parquet as the most efficient format and provides detailed technical justification that aligns with the canonical answer. The additional information about other supported formats and practical CSV handling tips enhances the answer without detracting from the core message, making it both comprehensive and highly useful for the reader.|0.0|42.89682698249817|/tmp/answer_retrieval_docs_1.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_retrieval_then_docs.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_retrieval_then_docs/q1_20251228_082303.json
2|answer_motherduck_without_kg|...How does MotherDuck integrate with DuckDB?|"MotherDuck integrates with DuckDB through a sophisticated architecture that extends DuckDB's capabilities without forking the open source project. The integration is built on several key components: The MotherDuck Extension MotherDuck connects to DuckDB through a purpose-built MotherDuck extension that enables seamless integration with the MotherDuck cloud service. This extension allows users to connect their local DuckDB instance to MotherDuck's serverless analytics platform with a simple ATTACH command. Whether working in the CLI, a Python script, or a JDBC connection, this extension facilitates the connection between local DuckDB and the MotherDuck cloud. Importantly, MotherDuck leverages DuckDB's powerful extension system rather than maintaining a separate fork. This allows MotherDuck to add new capabilities at multiple layers—including the SQL parser, optimizer, and storage interface—while staying current with the latest open source DuckDB releases. Dual/Hybrid Execution Model The most innovative feature of the integration is the dual execution model (also called hybrid execution). This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense. In practice, this means: • When both tables reside in a MotherDuck database, the query executes entirely in the cloud • When a query involves both local and cloud data, the system creates a mix of local and remote operations • Local file scans and their associated processing happen locally on the user's machine • Cloud table scans happen remotely on MotherDuck's compute • The system efficiently transfers only the necessary intermediate data to complete operations like joins Users can even manually control execution location using the md_run parameter within scan functions (setting md_run = 'local' or md_run = 'remote'). Three-Layer Architecture MotherDuck's architecture is built on three main layers: The Client Extension: Integrates with the DuckDB ecosystem. This extension is even used to run DuckDB in the browser via WASM for the MotherDuck UI, enabling client-side processing to reduce latency and cloud compute costs. The Compute Layer: In the cloud, queries are processed by containerized DuckDB instances called ""ducklings."" These compute resources scale based on user needs, providing the necessary CPU and memory for complex analytical queries. The Storage Layer: MotherDuck implemented a differential storage system that maps a logical database file to append-only snapshot layers in cloud object storage. This design enables efficient in-place updates and forms the foundation for features like database sharing and time travel. Single-Engine Semantics A key benefit of the integration is single-engine semantics—MotherDuck uses the exact same DuckDB engine both locally and in the cloud. A query that works on your laptop is guaranteed to work in the cloud, eliminating the ""it worked on my machine"" problem and ensuring consistency between development and production environments. Connection Simplicity Any DuckDB instance running in Python or CLI can connect to MotherDuck with a single line of code. This ease of onboarding was made possible through close collaboration between MotherDuck and the creators of DuckDB. As MotherDuck describes it: ""Suddenly, by running this command your DuckDB magically becomes supercharged by MotherDuck."""|MotherDuck is built on top of DuckDB and provides a serverless cloud data warehouse. It uses DuckDB as its query engine while adding cloud storage, collaboration features, and managed infrastructure. MotherDuck maintains full compatibility with DuckDB's SQL dialect and allows users to run queries seamlessly across local and cloud data.|.kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md; .kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md; .kurt/sources/motherduck.com/learn-more/what-is-duckdb.md; .kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md; .kurt/sources/motherduck.com/glossary/MotherDuck|0% (0 common, 4 unique to with_kg, 0 unique to without_kg)|0.96|0.95|1.0|0.95|0.95|"The generated answer is highly accurate and comprehensive, covering all key integration points mentioned in the canonical answer while providing substantial additional technical depth. The explanation of the dual execution model, extension system, and three-layer architecture directly addresses how the integration works. The answer is well-organized with clear headings and concrete examples, though it is notably more detailed than the canonical answer, which could be seen as either a strength (thoroughness) or minor weakness (potential over-elaboration for a simple ""how does it integrate"" question)."|0.0|71.40647912025452|/tmp/answer_without_kg_2.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q2_20251228_210127.json
2|answer_with_retrieval_then_docs|...How does MotherDuck integrate with DuckDB?|"MotherDuck integrates with DuckDB by providing a serverless cloud analytics platform built on top of DuckDB. The key integration features are: Hybrid Execution Model: MotherDuck lets you analyze local data while still JOINing with data processed in the cloud. This dual execution approach allows queries to run both locally (using DuckDB's in-process engine) and in the cloud seamlessly. Native DuckDB Compatibility: Since MotherDuck is built on DuckDB, you can use the same DuckDB SQL syntax and features. DuckDB is an in-process database you can pip install and use immediately, and MotherDuck extends this to the cloud. Tool Integrations: Multiple tools connect to both DuckDB and MotherDuck: • The DuckDB/MotherDuck MCP server allows AI copilots to run queries against local DuckDB databases • Flyte has a DuckDB plugin with MotherDuck integration • PostgreSQL can integrate with DuckDB and MotherDuck to enhance analytical capabilities • Meltano connects to DuckDB-based data warehouses including MotherDuck Storage Efficiency: MotherDuck provides compression benefits (one customer reported 4 TB compressing to 1 TB) while maintaining DuckDB query performance. --- Sources: • Entity: MotherDuck (Company - serverless analytics built on DuckDB) • Entity: DuckDB (Technology - in-process SQL OLAP database) • Claim: ""MotherDuck lets you analyze local data while still JOINing with data processed in the cloud"" • Claim: ""DuckDB is an in-process database that you can literally pip install and start using immediately"" • Claim: ""The DuckDB/MotherDuck MCP server allows the AI copilot to run queries against local DuckDB databases"" • Claim: ""Meltano integrates with MotherDuck by utilizing its ability to connect to DuckDB-based data warehouses"""|MotherDuck is built on top of DuckDB and provides a serverless cloud data warehouse. It uses DuckDB as its query engine while adding cloud storage, collaboration features, and managed infrastructure. MotherDuck maintains full compatibility with DuckDB's SQL dialect and allows users to run queries seamlessly across local and cloud data.||0% (0 common, 4 unique to with_kg, 0 unique to without_kg)|0.86|0.85|0.8|0.9|0.95|"The generated answer is factually accurate and demonstrates strong understanding of the MotherDuck-DuckDB relationship. It correctly identifies the serverless cloud platform built on DuckDB and explains the hybrid execution model well. However, it misses explicitly mentioning some key aspects from the canonical answer such as ""collaboration features"" and ""managed infrastructure,"" though these concepts are somewhat implied. The answer excels in clarity with its numbered structure and concrete examples, though it provides more detail than the question strictly requires. The inclusion of specific tool integrations and storage benefits adds practical value but goes beyond the core integration question."|0.0|72.41940188407898|/tmp/answer_retrieval_docs_2.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_retrieval_then_docs.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_retrieval_then_docs/q2_20251228_082353.json
3|answer_motherduck_without_kg|...What SQL features from DuckDB are not yet supported in MotherDuck?|"Based on the MotherDuck documentation and guides, there are several SQL features from DuckDB that are not yet supported in MotherDuck: User-Defined Functions (UDFs) Custom Python/Native UDFs do not work on MotherDuck. This is a significant limitation for users who rely on extending DuckDB's functionality with custom Python code. As explicitly stated in the documentation: > ""User-Defined Functions (UDFs)... are a feature of the local, embedded DuckDB Python library and do not work on MotherDuck. The reason is that UDFs require a Python runtime to execute the function's code. MotherDuck is a serverless platform that provides SQL execution but does not run user-provided Python code on its servers."" This means you cannot: • Register custom Python functions to use in SQL queries on MotherDuck • Use con.create_function() for MotherDuck-executed queries • Run vectorized scalar Python UDFs on the cloud backend Workaround: Perform UDF-based transformations locally in DuckDB or upstream in your data pipeline before loading data into MotherDuck. Server-Side ATTACH to Other Databases MotherDuck does not support server-side ATTACH commands to other databases like PostgreSQL. While DuckDB locally can attach to external databases using extensions (e.g., ATTACH 'dbname=postgres...' AS pg (TYPE postgres)), this capability is not available for execution on MotherDuck's cloud servers. This limitation affects users who want to: • Directly query PostgreSQL, MySQL, or other databases from within MotherDuck • Create federated queries that join MotherDuck tables with external database tables on the server side Workaround: You can still use DuckDB's postgres extension locally and join local external database queries with MotherDuck data through the hybrid/dual execution model, or perform ETL to bring external data into MotherDuck. Custom Extensions Custom DuckDB extensions are not yet available on MotherDuck. While MotherDuck provides many built-in DuckDB extensions, users cannot install or run their own custom extensions on the MotherDuck cloud platform. Workaround: Perform operations requiring custom extensions on a local DuckDB instance and use MotherDuck for the operations it supports natively. Summary of Unsupported Features \| Feature \| Status on MotherDuck \| Workaround \| \|---------\|---------------------\|------------\| \| Python UDFs \| Not supported \| Run locally or transform data upstream \| \| Native UDFs \| Not supported \| Run locally or transform data upstream \| \| Server-side ATTACH to Postgres/other DBs \| Not supported \| Use local DuckDB federation or ETL \| \| Custom extensions \| Not yet available \| Use local DuckDB for custom extension operations \| These limitations stem from MotherDuck's serverless architecture, which executes SQL but does not provide a user-configurable runtime environment for arbitrary code execution. The recommended approach is to perform these specific transformations upstream before loading data into MotherDuck, or to leverage DuckDB's local execution capabilities in conjunction with MotherDuck's hybrid/dual execution model."|MotherDuck aims for high compatibility with DuckDB but may have some limitations with certain extensions, user-defined functions, or very recent DuckDB features that haven't been integrated yet. Check the MotherDuck documentation for the current list of unsupported features or limitations.|.kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md; .kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md; .kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md; .kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md; .kurt/sources/motherduck.com/glossary/MotherDuck|0% (0 common, 4 unique to with_kg, 0 unique to without_kg)|0.98|0.95|1.0|1.0|1.0|The generated answer excels in all dimensions, providing specific, well-documented examples of unsupported features (UDFs, ATTACH commands, custom extensions) that align with the canonical answer's general categories. The answer goes beyond the canonical response by offering detailed explanations, workarounds, and a summary table, making it highly practical and actionable. The only minor consideration is that it presents very specific details that may become outdated, whereas the canonical answer's approach of directing users to current documentation ensures ongoing accuracy - hence the slightly reduced accuracy score of 0.95 rather than perfect.|0.0|95.40004110336304|/tmp/answer_without_kg_3.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q3_20251228_134529.json
3|answer_with_retrieval_then_docs|...What SQL features from DuckDB are not yet supported in MotherDuck?|"DuckDB SQL Features Not Yet Supported in MotherDuck Unsupported Extensions The following DuckDB extensions are not available in any MotherDuck environment: • aws - AWS integration extension • jemalloc - Memory allocator extension • mysql - MySQL scanner extension • postgres - PostgreSQL scanner extension • ui - DuckDB UI extension • Community extensions - User-built extensions are not supported Partially Supported Extensions Some extensions work only in certain environments: • autocomplete - Web UI + CLI only (not in Cloud) • azure - Cloud + CLI only (not in Web UI) • encodings - Cloud + CLI only • excel - Web UI only • sqlite - Web UI + CLI only • vss - Web UI + CLI only SQL Command Limitations • COPY DATABASE - The full COPY FROM DATABASE command (copying both schema and data simultaneously) is not supported in the Web UI. Users must run separate commands for schema and data. • FTS Index Creation - The create_fts_index PRAGMA command will not work in the MotherDuck UI • PySpark Integration - Running PySpark code with the DuckDB engine is not yet supported on MotherDuck Environment Notes • The MotherDuck Web UI (Wasm) has the most restrictions - ""not all features of extensions in the MotherDuck UI (Wasm) are supported"" • The DuckDB CLI connecting to MotherDuck has the broadest compatibility - all DuckDB extensions are available locally --- Sources: MotherDuck documentation (https://motherduck.com/docs/concepts/duckdb-extensions/), MotherDuck FAQ, Kurt entity/claims retrieval (DuckDB, MotherDuck, Core Extensions, Community Extensions entities)"|MotherDuck aims for high compatibility with DuckDB but may have some limitations with certain extensions, user-defined functions, or very recent DuckDB features that haven't been integrated yet. Check the MotherDuck documentation for the current list of unsupported features or limitations.||0% (0 common, 4 unique to with_kg, 0 unique to without_kg)|0.9|0.85|0.95|0.9|0.95|The generated answer is highly detailed, well-structured, and factually accurate based on the documentation sources provided. It successfully identifies specific unsupported features with concrete examples (FTS, PySpark, Python UDFs, etc.) and provides helpful context about the dual execution model. However, it diverges from the canonical answer's more conservative approach, which emphasizes checking documentation rather than providing a comprehensive static list. The canonical answer suggests a more general response acknowledging potential limitations rather than enumerating specific ones, which may be intentional to avoid outdated information as MotherDuck evolves.|0.0|395.5072600841522|/tmp/answer_retrieval_docs_3.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_retrieval_then_docs.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_retrieval_then_docs/q3_20251228_072034.json
4|answer_motherduck_without_kg|...Why might a query run slower on MotherDuck than locally?|"A query might run slower on MotherDuck than locally due to several key factors related to the nature of cloud-based vs. local execution: Network Latency and Data Transfer Overhead When running queries on MotherDuck (in the cloud), there's inherent network latency involved in: • Sending queries from your local machine to the cloud • Downloading final result sets back to the client • Any intermediate data transfers between local and cloud compute Local DuckDB runs entirely in-process on your machine, eliminating all network round-trips. This means for smaller datasets or simple queries, local execution can feel instantaneous while cloud execution adds network overhead. Data Locality MotherDuck's dual execution model is optimized to push computation to where the data lives. However, performance characteristics differ: • Local file queries: When querying local files (CSV, Parquet), the query runs entirely on your local DuckDB instance with zero network latency • Cloud queries: Queries against MotherDuck tables are routed to cloud compute (""ducklings""), which adds request/response time • Hybrid queries: Joining local files with cloud tables requires coordination and data transfer between environments A direct comparison showed that querying a 2GB Parquet file on S3 took 36 seconds when executed locally (due to downloading the data first) vs 25 seconds when executed remotely on MotherDuck (which pushes computation closer to the data). However, for data that's already local, the reverse is true - local execution is faster. Small Dataset Optimization For ""small data"" workloads (which analysis shows covers 99% of queries on systems like Redshift and Snowflake), a single modern laptop with multiple cores and substantial RAM can process queries extremely fast. The overhead of cloud coordination may exceed the execution time itself for these smaller workloads. As noted in MotherDuck's architecture documentation: • Most queries under a gigabyte can fit and process efficiently on a single node • Modern hardware is significantly more powerful, making local execution highly capable Cold Start and Cloud Resource Allocation Cloud execution may involve: • Spinning up or allocating compute resources (""ducklings"") • Loading data into memory on remote instances • Potential queueing if resources are constrained Local DuckDB runs in-process and is always ""warm"" - ready to execute immediately. Use Case Optimization MotherDuck is specifically designed to excel when: • Data volumes exceed what fits comfortably on a single machine • Multiple users need concurrent access to shared data • Data lives in cloud storage (S3, etc.) and pushing compute to the data saves transfer time • Collaboration features like database sharing are needed For purely local, single-user scenarios with data that fits in memory, local DuckDB removes all the overhead that MotherDuck's cloud architecture introduces for collaboration and scale. Best Practices MotherDuck provides the md_run parameter to manually control execution location: • md_run = 'local' forces local execution • md_run = 'remote' forces cloud execution Understanding your data locality and query patterns helps you leverage dual execution for optimal performance - sometimes local is faster, sometimes cloud is faster, depending on where your data lives."|Queries may run slower on MotherDuck compared to local DuckDB due to network latency, data transfer overhead when moving data between local and cloud, cold start times for serverless compute, or suboptimal data partitioning. Local queries benefit from direct disk access and no network overhead.|.kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md; .kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md; .kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md; .kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md; .kurt/sources/motherduck.com/videos/a-duck-in-the-hand-is-worth-two-in-the-cloud.md|0% (0 common, 5 unique to with_kg, 0 unique to without_kg)|1.0|1.0|1.0|1.0|1.0|This is an exemplary answer that not only covers all required topics from the canonical answer (network latency, data transfer overhead, cold starts, and data partitioning) but significantly enriches them with concrete examples, architectural context, and practical guidance. The answer is well-structured with clear sections, maintains factual accuracy throughout, and provides actionable insights like the `md_run` parameter usage. The level of detail and clarity makes this answer highly valuable while remaining directly relevant to the question asked.|0.0|100.96247124671936|/tmp/answer_without_kg_4.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q4_20251228_210127.json
4|answer_with_retrieval_then_docs|...Why might a query run slower on MotherDuck than locally?|"A query might run slower on MotherDuck than locally for these key reasons: Network latency and data transfer: When querying data that needs to be transferred between your local machine and the cloud, network overhead adds latency. The Dual Execution model addresses this by minimizing data movement and reducing round trips, but queries involving significant data transfer between local and cloud will still be slower than purely local execution. Data location: If you're analyzing large local files while joining them with cloud data, data must traverse the network. MotherDuck's hybrid execution model provides ""local processing speed with cloud scalability,"" but queries that aren't optimized for dual execution may incur transfer penalties. Query planning overhead: The Dual Execution query planner determines where to execute each part of a query (locally vs. cloud). While optimizations reduce round trips ""from two to one,"" this planning adds some overhead compared to purely local DuckDB execution. In-process vs. cloud: DuckDB ""operates in-process, allowing us to iterate quickly since computation occurs locally within the same process."" This zero-overhead local execution is hard to beat for small queries. Best practice: For latency-sensitive queries on small local data, use DuckDB locally. Use MotherDuck when you need cloud-scale data, collaboration, or when data already resides in MotherDuck storage. --- Sources: • Dual Execution: ""reduces network traffic by orders of magnitude,"" ""reduces round trips from two to one"" • Dual Execution model: ""minimizes data movement and improves query performance"" • hybrid execution model: ""provides local processing speed with cloud scalability"" • DuckDB: ""operates in-process, allowing us to iterate quickly...computation occurs locally"""|Queries may run slower on MotherDuck compared to local DuckDB due to network latency, data transfer overhead when moving data between local and cloud, cold start times for serverless compute, or suboptimal data partitioning. Local queries benefit from direct disk access and no network overhead.||0% (0 common, 5 unique to with_kg, 0 unique to without_kg)|0.86|0.85|0.8|0.9|0.95|The answer accurately covers the main reasons for slower performance (network latency, data transfer, local vs cloud execution) and is exceptionally well-written with clear structure and examples. However, it misses two specific points from the canonical answer: cold start times for serverless compute and suboptimal data partitioning. The addition of query planning overhead and the Dual Execution context is valuable but slightly shifts focus from the direct question. The best practices section, while helpful, extends beyond the scope of the question asked.|0.0|127.93572926521301|/tmp/answer_retrieval_docs_4.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_retrieval_then_docs.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_retrieval_then_docs/q4_20251228_133302.json
5|answer_motherduck_without_kg|...How do I set up MotherDuck to work with dbt?|"Setting up MotherDuck to work with dbt involves installing the dbt-duckdb adapter and configuring your dbt profile to connect to MotherDuck. Here's a comprehensive guide: Prerequisites Sign up for a MotherDuck account at motherduck.com Get your Service Token from the MotherDuck UI (Settings > Service Token) Install the dbt-duckdb adapter created by Josh Wills Step-by-Step Setup Install Dependencies Create a Python virtual environment and install the required packages: Configure Your dbt Profile Edit your profiles.yml file to connect to MotherDuck. The key is using the md: prefix in the path to indicate a MotherDuck database: Set Environment Variables Export your MotherDuck token as an environment variable: If you're also working with S3 data, add those credentials too: Verify Connection Run dbt debug to verify dbt can connect to MotherDuck: Run Your dbt Project Execute your dbt models: Dual Execution Setup (Local Dev + Cloud Prod) MotherDuck supports a powerful ""dual execution"" pattern where you can develop locally with DuckDB while using MotherDuck for production. This can provide ~5x faster dbt runs during development. Configure your profile with multiple targets: This allows you to: • Run queries locally for faster iteration • Sample data when developing (e.g., using {% if target.name == 'local' %} using sample 1 % {% endif %}) • Push to production when ready Key Benefits • Speed: DuckDB's columnar design enables fast analytical queries • Cost efficiency: Local development is free, and MotherDuck enables cloud execution only when needed • Simplicity: Same technologies for dev and production • Dynamic execution: MotherDuck can intelligently run queries locally or in the cloud"|To use MotherDuck with dbt, you need to install the dbt-duckdb adapter and configure your profiles.yml with MotherDuck connection details including your token. The dbt-duckdb adapter supports MotherDuck's cloud features while maintaining compatibility with dbt's standard workflow.|.kurt/sources/motherduck.com/ecosystem/dbt.md; .kurt/sources/motherduck.com/glossary/data; .kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md; .kurt/sources/motherduck.com/blog/dual-execution-dbt.md; .kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md|0% (0 common, 4 unique to with_kg, 0 unique to without_kg)|1.0|1.0|1.0|1.0|1.0|The generated answer excellently addresses the question with complete accuracy and comprehensive detail. It covers all essential elements from the canonical answer (dbt-duckdb adapter installation, profiles.yml configuration, MotherDuck token) while providing valuable additional context including prerequisites, step-by-step instructions with code examples, and advanced patterns like dual execution. The answer is exceptionally well-organized with clear headings, properly formatted code blocks, and practical guidance that would enable successful implementation.|0.0|71.31297206878662|/tmp/answer_without_kg_5.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q5_20251228_210127.json
5|answer_with_retrieval_then_docs|...How do I set up MotherDuck to work with dbt?|"To set up MotherDuck to work with dbt: Install the dbt-duckdb adapter: Configure your profiles.yml (typically at ~/.dbt/profiles.yml): Set the MotherDuck token as an environment variable: You can get your token from the MotherDuck web UI. Initialize your dbt project (if new): Run dbt commands as usual: dbt Core projects run extremely fast with MotherDuck—one user reported dbt runs that ""dropped from 8 hours to 2 minutes"" after migrating from dbt Cloud with traditional warehouses. --- Sources: • Entity: dbt (Technology) - ""A command line tool that enables data analysts and engineers to transform data"" • Entity: dbt Core (Technology) - ""A tool for transforming data within MotherDuck"" • Entity: motherduck_token (Feature) - ""An environment variable token used for connecting to MotherDuck"" • Entity: profiles.yml (Feature) - ""A YAML file used by dbt to define connection configurations for different environments"" • Claim: ""dbt runs dropped from 8 hours to 2 minutes"" [29b4d13f] • Claim: ""dbt is a great and straightforward tool for building production-ready data pipelines with SQL"" [170e537b] • Document: duckdb-dbt-e2e-data-engineering-project-part-2 (170e537b) • Document: motherduck-dbt-pipelines (be16b3b5)"|To use MotherDuck with dbt, you need to install the dbt-duckdb adapter and configure your profiles.yml with MotherDuck connection details including your token. The dbt-duckdb adapter supports MotherDuck's cloud features while maintaining compatibility with dbt's standard workflow.||0% (0 common, 4 unique to with_kg, 0 unique to without_kg)|1.0|1.0|1.0|1.0|1.0|This is an excellent answer that not only covers all the essential setup requirements (dbt-duckdb adapter installation, profiles.yml configuration, and MotherDuck token setup) but enhances them with practical code examples and clear step-by-step instructions. The answer is well-organized, easy to follow, and provides actionable guidance that directly addresses the question. The additional context about performance benefits and proper source citations further strengthen the response quality.|0.0|465.0733251571655|/tmp/answer_retrieval_docs_5.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_retrieval_then_docs.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_retrieval_then_docs/q5_20251228_133519.json
6|answer_motherduck_without_kg|...How do I migrate data from a local DuckDB database to MotherDuck?|"Migrating data from a local DuckDB database to MotherDuck is a straightforward process thanks to the seamless integration between DuckDB and MotherDuck. Here's how to do it: Method 1: Using the ATTACH Command (Recommended) The simplest way to connect your local DuckDB to MotherDuck and migrate data is using the ATTACH command: Connect to MotherDuck: From your local DuckDB CLI or Python environment, run: This single command connects your local DuckDB instance to MotherDuck. Authentication requires a MotherDuck token, which can be set as an environment variable. Create a database in MotherDuck: Once connected, you can create a new database in the cloud: Copy tables from local to cloud: Use CREATE TABLE AS SELECT to copy your local data to MotherDuck: Method 2: Upload Files via MotherDuck Web UI MotherDuck provides a notebook-like web interface where you can: • Upload local CSV and Parquet files directly • Analyze and manage them alongside your other MotherDuck data • Use DuckDB-WASM in-browser for immediate data exploration Method 3: From Local Files (CSV, Parquet) If your data is in file format, you can create tables directly in MotherDuck: Key Concepts Dual Execution When connected to MotherDuck, queries can leverage dual execution - an intelligent hybrid query execution strategy where the optimizer determines whether to run parts of a query locally or in the cloud. This minimizes data movement and leverages compute where it makes the most sense. Hybrid Workflow MotherDuck creates a ""multiplayer"" experience from DuckDB's traditionally single-player architecture: • Your laptop and MotherDuck cloud become two nodes in a single distributed system • Both nodes execute queries in concert, automatically routing parts of queries to the optimal location • You can even JOIN local data with cloud data in a single query No Database Changes Needed One of the key benefits is that what works locally will work in the cloud - MotherDuck uses the exact same DuckDB engine. Simply add the md: prefix to your connection or database references to work with cloud data. After Migration Once your data is in MotherDuck, you can: • Share databases with team members using CREATE SHARE • Use MotherDuck's Secret Manager for secure credential management • Access your data from anywhere using Python, CLI, JDBC, or the web UI • Take advantage of MotherDuck's serverless infrastructure for scaling"|To migrate from local DuckDB to MotherDuck, you can attach to MotherDuck from your local DuckDB instance and use INSERT INTO or CREATE TABLE AS SELECT statements to copy data from local tables to MotherDuck tables. Alternatively, export data to Parquet files and load them into MotherDuck from cloud storage.|.kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md; .kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md; .kurt/sources/motherduck.com/product/duckdb-users.md; .kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md; .kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md; .kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md|0% (0 common, 6 unique to with_kg, 0 unique to without_kg)|0.95|0.95|1.0|0.9|0.95|The answer is highly accurate and complete, covering all methods mentioned in the canonical answer (ATTACH with INSERT/CREATE TABLE AS SELECT, and Parquet export via cloud storage) plus additional helpful approaches. The clarity is excellent with well-structured sections and code examples. The relevance score is slightly lower because while all information is useful, some sections (like dual execution details and hybrid workflow concepts) go beyond what was directly asked, making the answer somewhat verbose for the specific migration question. Overall, this is a high-quality, comprehensive response that would serve users well.|0.0|92.65185499191284|/tmp/answer_without_kg_6.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q6_20251228_210127.json
6|answer_with_retrieval_then_docs|...How do I migrate data from a local DuckDB database to MotherDuck?|"To migrate data from a local DuckDB database to MotherDuck, you can leverage MotherDuck's hybrid/dual execution model, which allows queries to execute across both local and cloud environments seamlessly. Migration Steps Connect to MotherDuck: Set your motherduck_token environment variable for authentication, then connect using the MotherDuck connection string Attach both databases: Use the attach functionality to connect your local DuckDB file alongside your MotherDuck cloud database Copy data using SQL: Transfer tables using standard SQL commands like CREATE TABLE AS SELECT or INSERT INTO to copy data from your local database to MotherDuck tables Key Benefits • The dual execution model minimizes data movement and improves query performance during migration • The dual execution planner reduces network traffic by orders of magnitude compared to traditional methods • MotherDuck's hybrid execution model provides local processing speed with cloud scalability • You can analyze local files and join them with cloud data in a unified data environment The hybrid architecture means you don't need to fully migrate immediately—you can work with both local and cloud data simultaneously, making the transition gradual and low-risk. --- Sources: • Entity: Dual Execution - ""The dual execution planner reduces network traffic by orders of magnitude compared to traditional methods"" • Entity: Dual Execution model - ""The dual execution model minimizes data movement and improves query performance"" • Entity: hybrid execution model - ""The hybrid execution model provides local processing speed with cloud scalability"" • Entity: Hybrid Analytics - ""a powerful approach that unifies your local machine and the cloud into a single, unified data environment"" • Entity: motherduck_token - ""An environment variable token used for connecting to MotherDuck"" • Entity: attach_mode - ""A feature that allows users to specify the mode of connection to a database"""|To migrate from local DuckDB to MotherDuck, you can attach to MotherDuck from your local DuckDB instance and use INSERT INTO or CREATE TABLE AS SELECT statements to copy data from local tables to MotherDuck tables. Alternatively, export data to Parquet files and load them into MotherDuck from cloud storage.||0% (0 common, 6 unique to with_kg, 0 unique to without_kg)|0.79|0.85|0.75|0.7|0.9|"The answer correctly describes the primary migration method using attach and SQL commands, demonstrating good technical accuracy. However, it misses the Parquet export alternative mentioned in the canonical answer, reducing completeness. The extensive discussion of hybrid execution benefits, while informative, shifts focus away from the practical ""how-to"" nature of the question, affecting relevance. The writing is well-structured and clear with good use of formatting and sections."|0.0|162.16168904304504|/tmp/answer_retrieval_docs_6.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_retrieval_then_docs.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_retrieval_then_docs/q6_20251228_134312.json
7|answer_motherduck_without_kg|...If I have a CSV on my laptop and a table in S3, what's the most efficient way to join them using MotherDuck?|"If you have a CSV on your laptop and a table in S3, the most efficient way to join them using MotherDuck is to leverage the Dual Execution feature, which enables hybrid queries that span local and cloud data sources with automatic optimization for minimal data transfer. The Approach: Dual Execution MotherDuck treats your laptop and the MotherDuck cloud as two nodes in a single distributed system. When you run a hybrid query, the query planner intelligently breaks it down and pushes computation to where the data lives, minimizing data transfer over the network. Here's how it works step-by-step: Local Scan: MotherDuck scans your local CSV file on your laptop Filter Locally: If your query has filters, they're applied on your local machine first Transfer Minimal Data: Only the filtered/necessary rows are sent to the cloud Join in the Cloud: The final join with the S3 table happens in the MotherDuck cloud Practical Implementation Step 1: Connect to MotherDuck and load required extensions Step 2: Attach the S3 data (if not already in a MotherDuck database) For Parquet files in S3, you can query them directly: Or attach a DuckDB database file stored on S3: Step 3: Write your join query You can directly join your local CSV with cloud data in a single query: Key Advantages of This Approach Automatic Optimization: MotherDuck's query planner automatically decides which parts run locally (marked as ""L"" in query plans) and which run remotely (marked as ""R"") No Manual ETL Required: You don't need to upload your entire CSV to the cloud or download the entire S3 table - the ""No-ETL"" approach lets you query files directly where they live Schema-on-Read: DuckDB's read_csv_auto function automatically infers column names and data types, so you don't need to define schemas Cost Efficiency: By leveraging free local compute on your laptop and minimizing data transfer, you significantly reduce cloud costs Consistent Engine: The same DuckDB engine runs both locally and in the cloud, ensuring query consistency Verifying Query Optimization You can use EXPLAIN to see how MotherDuck plans to execute your hybrid query: The output shows which parts run locally (L) and which run remotely (R), confirming that MotherDuck is optimizing data movement."|The most efficient approach is to create a temporary table from your local CSV, then use MotherDuck's ability to read directly from S3 in your join query. MotherDuck can push down filters and only transfer necessary data, minimizing data movement. Alternatively, if the CSV is small, you can upload it directly to MotherDuck.|.kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md; .kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md; .kurt/sources/motherduck.com/blog/dual-execution-dbt.md; .kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md; .kurt/sources/motherduck.com/blog/duckdb-the-great-federator.md; .kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md|0% (0 common, 6 unique to with_kg, 0 unique to without_kg)|0.81|0.75|0.8|0.85|0.95|"The generated answer is well-written, clear, and provides extensive technical detail about MotherDuck's Dual Execution feature. However, it doesn't explicitly mention the canonical answer's recommended approach of creating a temporary table from the local CSV first. While the direct query approach shown is valid, the canonical answer suggests a different ""most efficient"" method. The answer excels in clarity with code examples and step-by-step instructions, but could be more aligned with the specific recommendation in the canonical answer."|0.0|85.00172424316406|/tmp/answer_without_kg_7.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q7_20251228_210127.json
7|answer_with_retrieval_then_docs|...If I have a CSV on my laptop and a table in S3, what's the most efficient way to join them using MotherDuck?|The most efficient way to join a local CSV with an S3 table in MotherDuck is to use Dual Execution (also called hybrid execution). How it works: Connect to MotherDuck from DuckDB Use read_csv() to reference your local CSV file Query your S3 table through MotherDuck Join them in a single SQL query Example: Why this is efficient: • The dual execution planner reduces network traffic by orders of magnitude compared to traditional methods • The dual execution model minimizes data movement and improves query performance • The hybrid execution model provides local processing speed with cloud scalability • DuckDB automatically detects when extensions are needed (like httpfs for S3 access) and loads them transparently The query planner intelligently decides which parts run locally vs. in the cloud, and optimizations reduce round trips for many dual execution queries from two to one. --- Sources: Dual Execution, hybrid execution model, Dual Execution model, DuckDB (extension auto-loading claim)|The most efficient approach is to create a temporary table from your local CSV, then use MotherDuck's ability to read directly from S3 in your join query. MotherDuck can push down filters and only transfer necessary data, minimizing data movement. Alternatively, if the CSV is small, you can upload it directly to MotherDuck.||0% (0 common, 6 unique to with_kg, 0 unique to without_kg)|0.9|0.9|0.85|0.95|0.95|The answer is highly accurate and relevant, correctly explaining the dual execution approach with excellent technical detail and a practical code example. It effectively addresses the efficiency question by explaining how data movement is minimized. The completeness score is slightly reduced because it doesn't mention the alternative approach for small CSVs that could be uploaded directly to MotherDuck, which was included in the canonical answer as a valid option.|0.0|80.03302383422852|/tmp/answer_retrieval_docs_7.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_retrieval_then_docs.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_retrieval_then_docs/q7_20251228_134603.json
8|answer_motherduck_without_kg|...What's the difference between a MotherDuck database and a share?|"In MotherDuck, a database and a share are two distinct but related concepts that serve different purposes in the data warehousing workflow. Database A MotherDuck database is a cloud-hosted DuckDB database that you own and control. Key characteristics include: • Writable and Modifiable: You can create tables, insert data, update records, and modify the database structure • Full Ownership: The database belongs to your account, and you have complete control over its contents • Persistent Storage: Uses MotherDuck's Differential Storage system to store data as an ordered sequence of ""layers"" (snapshots), enabling features like time travel, branching, and efficient zero-copy operations • Cloud Catalog Management: Allows you to manage your DuckDB database catalog in the cloud for easy scaling and collaboration • Can be created from local databases: You can upload a local DuckDB database using CREATE DATABASE cloud_db FROM local_db Share A MotherDuck share is a read-only, shareable snapshot of a database that can be distributed to other users via a secret URL. Key characteristics include: • Read-Only Access: Recipients can query the data but cannot modify it • Snapshot-Based: A share represents a point-in-time snapshot of the source database that remains synchronized with updates from the original database owner • Zero-Copy Creation: Creating a share is a metadata-only operation (CREATE SHARE FROM <database>), meaning no data duplication occurs thanks to the Differential Storage system • URL-Based Sharing: Shares are distributed via a unique URL (e.g., md:_share/sample_data/...) that recipients can attach to their MotherDuck session • Access Control Options: Shares can be configured for different access levels - either ""anyone with the share link"" (including external users) or ""restricted to organization members with the share link"" • Discoverability: Org-level shares can optionally be made discoverable to all members of an organization, appearing in their ""Shared with me"" section Key Differences \| Aspect \| Database \| Share \| \|--------\|----------\|-------\| \| Ownership \| Owned by creator \| Derived from a database \| \| Permissions \| Read/Write \| Read-Only \| \| Purpose \| Primary data storage & manipulation \| Data distribution & collaboration \| \| Creation \| CREATE DATABASE \| CREATE SHARE FROM <database> \| \| Data Duplication \| Contains actual data \| Zero-copy reference to source \| \| Updates \| Direct modifications allowed \| Reflects source database updates \| In essence, a database is where you store and work with your data, while a share is how you distribute that data to others for querying without giving them write access or requiring data duplication."|A MotherDuck database is a collection of tables and schemas that you own and can modify. A share is a read-only view of another user's database that they've shared with you, allowing data collaboration without copying. Shares enable secure data sharing across MotherDuck accounts.|.kurt/sources/motherduck.com/blog/differential-storage-building-block-for-data-warehouse.md; .kurt/sources/motherduck.com/blog/new-collaboration-sharing-motherduck-data-warehouse-organization-auto-join.md; .kurt/sources/motherduck.com/blog/announcing-duckdb-snippet-sets-with-motherduck-sharing-databases.md; .kurt/sources/motherduck.com/product/duckdb-users.md; .kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md|0% (0 common, 5 unique to with_kg, 0 unique to without_kg)|0.99|1.0|1.0|0.95|1.0|The answer is factually accurate and comprehensive, correctly explaining that databases are owned/modifiable collections while shares are read-only views for collaboration. The extensive technical details (Differential Storage, time travel, zero-copy operations) go well beyond what was asked but remain accurate and relevant. The structured format with clear sections, bullet points, and a comparison table makes the information highly accessible, though the answer could be considered slightly verbose for the straightforward question asked.|0.0|118.9932427406311|/tmp/answer_without_kg_8.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q8_20251228_210127.json
8|answer_with_retrieval_then_docs|...What's the difference between a MotherDuck database and a share?|"A MotherDuck database is a standard database where you can read and write data. You have full control over the data, including the ability to create, modify, and delete tables. A share is a read-only database designed specifically for data collaboration and ad-hoc analytics. Key characteristics: • Read-only access: Recipients can query but not modify the data • Zero-copy cloning: Users access the same dataset without duplicating storage • Purpose-built for sharing: Ideal for providing data access to collaborators or external parties In essence: databases are for data you own and manage, while shares are for data you want others to consume without giving them write access. --- Sources: • Entity: Shares (""Read-only databases designed for collaboration and ad-hoc analytics"") • Claim: ""Shares allow users to access the same dataset as a zero-copy clone without duplicating data"" • Claim: ""Shares are read-only databases that are purpose-built for data collaboration and ad-hoc analytics"" • Claim: ""SHARES provide read-only access to specific databases"""|A MotherDuck database is a collection of tables and schemas that you own and can modify. A share is a read-only view of another user's database that they've shared with you, allowing data collaboration without copying. Shares enable secure data sharing across MotherDuck accounts.||0% (0 common, 5 unique to with_kg, 0 unique to without_kg)|0.86|0.85|0.8|0.95|0.9|The answer is factually accurate and clearly explains the key differences between databases and shares, with good technical detail about zero-copy cloning. However, it could be more complete by explicitly stating that shares are views of another user's database and emphasizing the cross-account collaboration aspect. The structure and clarity are excellent, making the distinction easy to understand.|0.0|64.72869610786438|/tmp/answer_retrieval_docs_8.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_retrieval_then_docs.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_retrieval_then_docs/q8_20251228_134733.json
9|answer_motherduck_without_kg|...What compute instance sizes does MotherDuck offer?|"MotherDuck offers five compute instance sizes (called ""Ducklings""), designed for different workload requirements: Instance Sizes Pulse • Description: The smallest instance, perfect for lightweight, fully serverless analytics • Use cases: Ad-hoc analytics tasks, read-only workloads with high volumes of concurrent users (like customer-facing analytics), situations where a larger dedicated instance is overkill • Billing: Per Compute Unit (CU) second, with 1 second minimum • Pricing: • Free plan: Up to 10 CU hours included • Lite plan: $0.25 per CU hour • Business plan: $0.40 per CU hour Standard • Description: Built to handle common data warehouse workloads including loads and transforms • Use cases: Typical data engineering tasks like data ingest and dbt transformations, running dbt jobs with multiple parallel transformations, building standard data pipelines before scaling up • Billing: Per second, with 60 second minimum • Pricing: • Lite plan: $1.20 per hour • Business plan: $1.80 per hour Jumbo • Description: For larger data warehouse workloads with many transformations or complex aggregations • Use cases: When query complexity, data volume, and transformations exceed Standard capacity; complex joins and aggregations on growing datasets; event stream data or nested JSON with changing schemas • Billing: Per second, with 60 second minimum • Pricing: Business plan only - $3.60 per hour Mega • Description: An extremely large instance for demanding jobs at an even larger scale • Use cases: Queries too complex or data volumes too high for Jumbo in crunch time; weekly jobs that rebuild all tables and must run in minutes, not hours; serving customers with 10x the data volume while maintaining the same performance • Billing: Per second, with 5 minute minimum • Pricing: Business plan only - $10.80 per hour • Availability: Self-serve on the Business plan Giga • Description: The largest instances that enable the toughest transformations to run faster • Use cases: Data workloads so complex that nothing else will work; one-time jobs (e.g., restating revenue for the last 10 years); growth path beyond Mega when data volume and complexity grow 10x • Billing: Per second, with 10 minute minimum • Pricing: Business plan only - Contact sales for pricing • Availability: Requires contacting MotherDuck (""quack with us"") to request access Plan Availability \| Instance Type \| Free Plan \| Lite Plan \| Business Plan \| \|---------------\|-----------\|-----------\|---------------\| \| Pulse \| ✓ \| ✓ \| ✓ \| \| Standard \| ✓ \| ✓ \| ✓ \| \| Jumbo \| - \| - \| ✓ \| \| Mega \| - \| - \| ✓ \| \| Giga \| - \| - \| ✓ (by request)\| Additional Scaling Options Beyond instance sizes, MotherDuck also offers: • Read Scaling: Up to 16 read-scaling replicas, each backed by its own instance, for BI dashboards and read-only use cases • Per-user Tenancy: Each user or customer gets their own isolated duckling, which can be individually sized • Dual Execution: Automatically leverages both local (laptop) compute and cloud compute for optimal performance"|MotherDuck uses serverless compute that automatically scales based on query demands. Instead of choosing fixed instance sizes, MotherDuck allocates iOS resources dynamically and charges based on usage. This provides flexibility without requiring manual instance sizing.|.kurt/sources/motherduck.com/product/pricing.md; .kurt/sources/motherduck.com/blog/announcing-mega-giga-instance-sizes-huge-scale.md; .kurt/sources/motherduck.com/blog/scaling-duckdb-with-ducklings.md; .kurt/sources/motherduck.com/blog/the-simple-joys-of-scaling-up.md|0% (0 common, 4 unique to with_kg, 0 unique to without_kg)|0.25|0.1|0.2|0.3|0.9|The generated answer is well-written and clear but contains a critical factual error. The canonical answer explicitly states that MotherDuck does NOT offer fixed instance sizes but instead uses serverless compute with dynamic resource allocation. The generated answer contradicts this by describing five specific instance types with fixed pricing, which appears to be incorrect information about MotherDuck's actual architecture. While the answer is comprehensive and well-structured, it fails on accuracy - the most important dimension for a factual question.|0.0|75.57650423049927|/tmp/answer_without_kg_9.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q9_20251228_210127.json
9|answer_with_retrieval_then_docs|...What compute instance sizes does MotherDuck offer?|"MotherDuck offers five compute instance sizes called ""Ducklings"": Pulse - The smallest size, designed for lightweight, on-demand analytics Standard - Designed for common data warehouse workloads Jumbo - Built for production-scale analytics with heavy concurrent queries Mega - For workloads that have outgrown Jumbo Giga - The largest size, providing maximum compute and memory for the toughest, most complex data transformations Users can choose from these instance types to match their job requirements, optimizing for either cost or performance. Ducklings can start almost instantly, shut down when not in use, and auto-scaling operates independently for each Duckling. --- Sources: • Claim: ""MotherDuck has three Duckling sizes: Pulse, Standard and Jumbo"" [190306a1] • Claim: ""Mega and Giga instances are built for the largest, toughest, most complex data transformations"" [90c8eca1] • Claim: ""Giga gives DuckDB an environment with maximum compute and memory"" [900d4d8f] • Entities: Pulse, Standard, Jumbo, Mega Ducklings, Giga Ducklings"|MotherDuck uses serverless compute that automatically scales based on query demands. Instead of choosing fixed instance sizes, MotherDuck allocates iOS resources dynamically and charges based on usage. This provides flexibility without requiring manual instance sizing.||0% (0 common, 4 unique to with_kg, 0 unique to without_kg)|0.21|0.0|0.2|0.3|0.9|The answer is factually incorrect and contradicts the canonical answer's core premise. While it's well-written and clearly structured, it fundamentally misrepresents MotherDuck's serverless compute model by describing fixed instance sizes instead of dynamic resource allocation. The answer appears to confuse MotherDuck with a different service or contains outdated/incorrect information about the product architecture.|0.0|59.90754008293152|/tmp/answer_retrieval_docs_9.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_retrieval_then_docs.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_retrieval_then_docs/q9_20251228_134846.json
10|answer_motherduck_without_kg|...What DuckDB versions does MotherDuck currently support?|Based on the MotherDuck documentation, MotherDuck currently supports DuckDB version 1.4.1, which is the latest version as of October 2025. MotherDuck has progressively supported each major DuckDB release: • DuckDB 1.0 - Supported since June 2024 (when DuckDB reached its 1.0 milestone) • DuckDB 1.2 - Supported since February 2025 • DuckDB 1.3 - Supported since June 2025 • DuckDB 1.4.1 - Currently supported (announced October 2025) A key feature that enables this is Version Independence, which MotherDuck introduced to decouple client versions from the server-side DuckDB version. This means: When DuckDB ships a new version, MotherDuck can upgrade all servers to run the latest version in the cloud Users automatically get access to improved performance and bug fixes without taking any action While users can continue using their current local DuckDB client version, MotherDuck encourages upgrading to 1.4.1 to take advantage of all the latest fixes and features You can check which version you're running with the simple query: SELECT version();|MotherDuck typically supports recent stable versions of DuckDB and updates regularly to incorporate new features and improvements. Check the MotherDuck documentation or release notes for the current supported DuckDB version, as this evolves over time.|.kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md; .kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md; .kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md; .kurt/sources/motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release.md|0% (0 common, 4 unique to with_kg, 0 unique to without_kg)|0.85|0.75|0.9|0.95|0.95|The generated answer is highly relevant, clear, and comprehensive, providing specific version information and helpful context about MotherDuck's Version Independence feature. However, the accuracy score is reduced because it provides specific version numbers and dates (DuckDB 1.4.1, October 2025) that cannot be verified and may become outdated, whereas the canonical answer wisely avoids such specifics and instead directs users to check current documentation. The answer would be stronger if it balanced specific information with guidance to verify current versions through official sources.|0.0|71.28517603874207|/tmp/answer_without_kg_10.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q10_20251228_210127.json
10|answer_with_retrieval_then_docs|...What DuckDB versions does MotherDuck currently support?|"Based on the indexed MotherDuck documentation, the knowledge graph shows the following DuckDB versions referenced in MotherDuck content: • DuckDB 1.4 - Described as ""the latest version of DuckDB"" and shown as integrating with MotherDuck • DuckDB 1.3 / 1.3.0 - Referenced as a supported version with performance improvements • DuckDB 1.1.0 and 1.1.1 - Historical versions mentioned in documentation Note: The specific ""support-policy"" document (https://motherduck.com/support-policy) that would contain official version support details is in an ERROR state and couldn't be retrieved from the indexed content. The version information above is inferred from entity metadata extracted from blog posts and documentation. For the most accurate and current version support information, refer to MotherDuck's official support policy page directly. --- Sources: • Entity: DuckDB 1.4 (Type: Product, ""The latest version of DuckDB with new features and improvements"") • Entity: DuckDB 1.3 (Type: Product, ""The latest version of DuckDB with performance improvements"") • Entity: DuckDB 1.1.0 (Type: Product, ""version 1.1.0 of DuckDB, codenamed 'Eatoni'"") • Entity: DuckDB 1.1.1 (Type: Product, ""includes minor bug fixes"") • Relationship: DuckDB 1.4 → integrates_with → MotherDuck (confidence: 0.85) • Document: announcing-series-seed-and-a (698c66a9)"|MotherDuck typically supports recent stable versions of DuckDB and updates regularly to incorporate new features and improvements. Check the MotherDuck documentation or release notes for the current supported DuckDB version, as this evolves over time.||0% (0 common, 4 unique to with_kg, 0 unique to without_kg)|0.89|0.85|0.9|0.95|0.9|The generated answer provides accurate, well-sourced information about specific DuckDB versions while appropriately acknowledging data limitations and directing users to official sources for current information. It successfully addresses the question with greater specificity than the canonical answer while maintaining the important caveat that version support evolves over time. The structured format with clear sourcing and the transparency about the ERROR state of the support-policy document demonstrates good information quality practices.|0.0|338.49254393577576|/tmp/answer_retrieval_docs_10.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_retrieval_then_docs.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_retrieval_then_docs/q10_20251228_134955.json
