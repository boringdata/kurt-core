Question #|Scenario|Question Text|Answer|Reference Answer|Sources|Source Delta|Judge Overall Score|Judge Accuracy|Judge Completeness|Judge Relevance|Judge Clarity|Judge Feedback|Tokens Used|Duration (seconds)|Answer File|Scenario Definition|Result File
1|answer_motherduck_without_kg|...... What file formats are most efficient for loading data into MotherDuck?|"The most efficient file formats for loading data into MotherDuck are, in order of efficiency: Apache Parquet (Most Efficient) Parquet is the recommended format for loading data into MotherDuck due to its columnar storage design that aligns perfectly with DuckDB's analytical engine. Key efficiency benefits include: • Superior compression: Parquet uses specialized encoding techniques (dictionary encoding, run-length encoding, delta encoding) that reduce storage footprint by storing similar data types together • Reduced I/O: Only reads columns needed for a query rather than entire rows - if your query needs 2 columns out of 100, a columnar system might only read 2% of the total data • Predicate pushdown and column pruning: MotherDuck's query engine pushes down filters to the file level, minimizing data read from cloud storage and reducing costs • Native integration: DuckDB's architecture is ""purpose-built to leverage the column-oriented design of Parquet,"" enabling seamless data processing with minimal overhead • Rich metadata: Parquet files store statistics (min/max values, null counts) for each column chunk, enabling data skipping optimizations CSV Files While less efficient than Parquet, CSV files are well-supported with DuckDB's exceptional CSV parser: • Auto-detection: DuckDB's read_csv_auto function automatically infers column names, data types, and file dialects • Schema-on-read: No need to write CREATE TABLE statements - query CSV files directly • Error handling: Options like ignore_errors, store_rejects, and null_padding handle messy data gracefully • Glob patterns: Query multiple files at once with wildcards (e.g., 's3://my-bucket/logs/2025//*.csv') • Benchmark performance: DuckDB ranked #1 on the Pollock benchmark for CSV parsing, correctly reading 99.61% of data across problematic test files When CSV makes sense: Universal compatibility, easy to edit/read, and still common for data exchange from CRMs, social media exports, and financial systems. JSON Files JSON is supported but less efficient than columnar formats: • Auto-detection: read_json_auto infers schema, representing nested objects as STRUCTs and arrays as LISTs • Nested data support: Use unnest function and dot notation to access nested values • Direct API queries: Can parse JSON directly from web APIs • Best for: Semi-structured data with varying schemas, API responses, and log files Efficiency Comparison Summary \| Format \| Compression \| Query Performance \| Best Use Case \| \|--------\|-------------\|-------------------\|---------------\| \| Parquet \| Excellent (columnar) \| Fastest (column pruning, predicate pushdown) \| Analytics, large datasets, cloud storage \| \| CSV \| None \| Good with auto-detection \| Data exchange, quick imports, universal compatibility \| \| JSON \| None \| Moderate (nested parsing overhead) \| Semi-structured data, API data, logs \| Additional Table Formats For advanced use cases, MotherDuck also supports: • Apache Iceberg: Provides transactional guarantees, time travel, and schema evolution on top of Parquet files • Delta Lake: Similar to Iceberg with ACID transactions and data versioning • DuckLake: MotherDuck's native table format with modern architecture Practical Recommendations Convert CSV to Parquet when possible for repeated analytical queries - DuckDB makes this trivial with COPY TO statements Use glob patterns to query multiple files as a single table: read_parquet('s3://bucket//*.parquet') Leverage Hive-style partitioning (e.g., /year=2025/month=10/) to enable partition pruning and reduce scan costs For cloud storage: Parquet's compression and columnar design significantly reduces network transfer costs"|Parquet is the most efficient file format for loading data into MotherDuck. It's a columnar storage format optimized for analytical queries and well-supported by DuckDB. Other efficient formats include CSV for simple data and JSON for semi-structured data, but Parquet offers the best performance for large-scale analytics.|.kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md; .kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md; .kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md; .kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md; .kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md; .kurt/sources/motherduck.com/blog/analyze-json-data-using-sql.md|0% (0 common, 6 unique to with_kg, 0 unique to without_kg)|1.0|1.0|1.0|1.0|1.0|This is an exemplary answer that not only matches the canonical answer's core message but significantly enhances it with technical depth and practical guidance. The structured format with clear headings, comparison tables, and actionable recommendations makes complex information highly accessible. The answer maintains perfect accuracy while providing comprehensive coverage of file formats, their trade-offs, and real-world usage patterns that would be valuable to users at different skill levels.|0.0|73.37604594230652|/tmp/answer_without_kg_1.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q1_20260105_141302.json
1|answer_with_yaml_metadata|...... What file formats are most efficient for loading data into MotherDuck?|"--- sources: • path: .kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md claims_used: • ""Apache Parquet is an open-source columnar storage file format designed for efficient data processing"" • ""Parquet efficiently manages large datasets while ensuring high performance"" • ""Parquet has superior compression and faster query performance"" • ""CSV files are a straightforward, text-based format... lacks advanced functionalities like compression and schema management, which limits its efficiency in handling large-scale datasets"" • path: .kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md claims_used: • ""CSV is a file format used for data storage"" • ""The COPY command allows for filtering and selecting specific columns"" --- Most Efficient File Formats for Loading Data into MotherDuck Parquet is the most efficient file format for loading data into MotherDuck. Why Parquet is Optimal Columnar Storage: Unlike row-based formats like CSV, Parquet stores data by columns, enabling faster query performance and superior compression for large-scale analytical workloads. Efficient Compression: Parquet leverages specialized encoding techniques such as dictionary, run-length, and delta encoding to optimize storage. These methods reduce the data footprint, translating into cost savings and improved access speeds. Predicate Pushdown and Column Pruning: Parquet supports techniques that bring filtering and aggregation operations closer to the storage layer, reducing the amount of data that needs processing. Schema Evolution: Parquet handles evolving data structures with ease, supporting seamless schema modifications without disrupting existing workflows. Comparison with Alternatives \| Format \| Type \| Best For \| Efficiency for MotherDuck \| \|--------\|------\|----------\|--------------------------\| \| Parquet \| Columnar \| Analytical workloads \| Highest \| \| Apache ORC \| Columnar \| Hadoop/Hive ecosystems \| High \| \| CSV \| Row-based \| Simple data exchange \| Low (lacks compression, schema management) \| \| Apache Avro \| Row-based \| Schema evolution, streaming \| Moderate \| Recommendation For loading data into MotherDuck: • Use Parquet for optimal performance on large datasets • Convert CSV files to Parquet using DuckDB's COPY command if you receive data in CSV format • Parquet's efficient compression minimizes I/O operations, which is especially important for cloud-based analytics"|Parquet is the most efficient file format for loading data into MotherDuck. It's a columnar storage format optimized for analytical queries and well-supported by DuckDB. Other efficient formats include CSV for simple data and JSON for semi-structured data, but Parquet offers the best performance for large-scale analytics.||0% (0 common, 6 unique to with_kg, 0 unique to without_kg)|0.96|0.95|1.0|0.95|0.95|The answer is highly accurate and complete, correctly identifying Parquet as the most efficient format with strong technical justification. It exceeds expectations by providing detailed comparisons and technical explanations of why Parquet is optimal. The clarity is excellent with well-organized sections and a helpful comparison table. Minor deduction in relevance for not mentioning JSON (which was in the canonical answer) and for being more technical than strictly necessary, though this additional detail is valuable rather than problematic.|0.0|73.21867990493774|/tmp/answer_yaml_metadata_1.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_yaml_metadata.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_yaml_metadata/q1_20260105_135010.json
2|answer_motherduck_without_kg|...How does MotherDuck integrate with DuckDB?|"MotherDuck integrates with DuckDB through a sophisticated architecture that extends DuckDB's capabilities without forking the open-source project. Here's how the integration works: Extension-Based Integration MotherDuck uses DuckDB's powerful extension system to add cloud capabilities at multiple layers, including the SQL parser, optimizer, and storage interface. The MotherDuck extension is built directly into DuckDB and allows users to connect their local DuckDB instance to the MotherDuck cloud service with a simple ATTACH command. This approach means users automatically benefit from the rapid innovation of the DuckDB community almost immediately. Three-Layer Architecture The integration is built on three main components: The Client Extension: This is how MotherDuck integrates with the DuckDB ecosystem. Whether a user is working in the CLI, a Python script, or a JDBC connection, a simple command connects their local DuckDB instance to the MotherDuck cloud. This extension even enables running DuckDB in the browser via WASM for the MotherDuck UI. The Compute Layer: In the cloud, queries are processed by containerized DuckDB instances (called ""ducklings""). These compute resources scale based on user needs, providing necessary CPU and memory for complex analytical queries. The Storage Layer: MotherDuck implemented a differential storage system that maps a logical database file to append-only snapshot layers in cloud object storage. This enables features like database sharing and time travel. Dual Execution Model The most innovative feature is dual execution (also called hybrid execution). This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense. For example, when a query joins local data with cloud data: • Local files are scanned and processed on the user's machine • Cloud tables are scanned remotely on MotherDuck's compute • The system efficiently transfers only the necessary intermediate data to complete the join • Users can override execution location with the md_run parameter ('local' or 'remote') Key Benefits of the Integration • Seamless Connection: Any DuckDB instance running in Python or CLI can connect to MotherDuck with a single line of code • Same Engine: MotherDuck uses the exact same DuckDB engine both locally and in the cloud, ensuring query consistency • Database Sharing: Teams can share databases and collaborate on a single source of truth • Centralized Secret Manager: Secure credential management for accessing external data sources like AWS S3, Google Cloud Storage, or Delta Lake • Browser-Based Access: DuckDB runs via WASM in the MotherDuck web UI, enabling client-side processing Symbiotic Relationship MotherDuck is the largest production user of DuckDB and continuously pushes the engine to its limits, uncovering opportunities for improvement and contributing enhancements back to the open-source project. This collaboration ensures both DuckDB and MotherDuck continue to evolve together."|MotherDuck is built on top of DuckDB and provides a serverless cloud data warehouse. It uses DuckDB as its query engine while adding cloud storage, collaboration features, and managed infrastructure. MotherDuck maintains full compatibility with DuckDB's SQL dialect and allows users to run queries seamlessly across local and cloud data.|.kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md; .kurt/sources/motherduck.com/learn-more/what-is-duckdb.md; .kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md; .kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md; .kurt/sources/motherduck.com/glossary/MotherDuck|0% (0 common, 4 unique to with_kg, 0 unique to without_kg)|0.97|0.95|1.0|1.0|0.95|The generated answer is excellent, providing comprehensive and accurate information about MotherDuck's integration with DuckDB. It covers all essential points from the canonical answer while adding valuable technical depth through the extension system, three-layer architecture, and dual execution model explanations. The answer is well-organized with clear headings and concrete examples. Minor deduction in accuracy for some implementation details that, while likely correct, go beyond what can be verified against the canonical answer. The clarity is outstanding with logical flow and accessible technical explanations.|0.0|55.09143090248108|/tmp/answer_without_kg_2.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q2_20260105_141425.json
2|answer_with_yaml_metadata|...How does MotherDuck integrate with DuckDB?|"--- sources: • path: motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md claims_used: • ""MotherDuck, a modern cloud data warehouse, is built on DuckDB to solve these challenges. It extends DuckDB's capabilities without forking the open source project, creating a hybrid system that intelligently balances local and cloud computing."" • ""Dual execution is a feature of DuckDB."" • ""The most innovative feature of MotherDuck's architecture is its dual execution model. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud."" • path: motherduck.com/learn-more/hybrid-analytics-guide.md claims_used: • ""MotherDuck solves this with an innovative query planner called Dual Execution. It treats your laptop and the MotherDuck cloud as two nodes in a single distributed system."" • ""MotherDuck eliminates this by using the exact same DuckDB engine both locally and in the cloud."" • ""The dual execution planner reduces network traffic by orders of magnitude compared to traditional methods."" --- How MotherDuck Integrates with DuckDB MotherDuck is a modern cloud data warehouse built on DuckDB, extending DuckDB's capabilities without forking the open source project. This creates a hybrid system that intelligently balances local and cloud computing. Integration Architecture MotherDuck integrates with DuckDB through three key components: Client Extension • Uses DuckDB's powerful extension system to add capabilities at multiple layers (SQL parser, optimizer, storage interface) • A simple ATTACH command connects any local DuckDB instance (CLI, Python, JDBC) to MotherDuck cloud • Also runs DuckDB in the browser via WASM for the MotherDuck UI Compute Layer • Cloud queries processed by containerized DuckDB instances called ""ducklings"" • Resources scale based on user needs Storage Layer • Implements a differential storage system mapping logical database files to append-only snapshot layers in cloud object storage • Enables concurrent multi-user read/write access (unlike DuckDB's local single-writer model) Dual Execution: The Core Innovation The most distinctive integration feature is dual execution, a hybrid query execution strategy where: • The optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud • Your laptop and MotherDuck cloud are treated as two nodes in a single distributed system • The same DuckDB engine runs both locally and in the cloud, guaranteeing consistent behavior How Dual Execution Works When joining local and cloud data: Local Scan: Scans local files (CSV, Parquet) on your laptop Filter Locally: Applies filters on your machine Transfer Minimal Data: Sends only filtered/necessary data to cloud Join in Cloud: Performs final operations against cloud data This reduces network traffic by orders of magnitude compared to traditional methods. Benefits • Faster Development: Zero-latency local iteration before scaling to cloud • Lower Costs: Leverages free local compute and minimizes data transfer • Better Collaboration: Transforms ""single-player"" DuckDB into a ""multiplayer"" platform with database sharing Staying Current Because MotherDuck uses extensions rather than forking DuckDB, users benefit from rapid DuckDB community innovation almost immediately. MotherDuck, as the largest production user of DuckDB, also contributes improvements back to the open source project."|MotherDuck is built on top of DuckDB and provides a serverless cloud data warehouse. It uses DuckDB as its query engine while adding cloud storage, collaboration features, and managed infrastructure. MotherDuck maintains full compatibility with DuckDB's SQL dialect and allows users to run queries seamlessly across local and cloud data.||0% (0 common, 4 unique to with_kg, 0 unique to without_kg)|0.97|0.95|1.0|1.0|0.95|The generated answer is highly accurate and comprehensive, covering all key points from the canonical answer while providing valuable technical depth. It correctly explains MotherDuck's foundation on DuckDB, the compatibility maintenance, and the seamless local-cloud integration. The structure is clear with well-organized sections, though some technical details might be slightly dense for general audiences. The answer excels in relevance by directly addressing the integration question with concrete architectural explanations and practical examples.|0.0|59.5308358669281|/tmp/answer_yaml_metadata_2.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_yaml_metadata.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_yaml_metadata/q2_20260105_135133.json
3|answer_motherduck_without_kg|...What SQL features from DuckDB are not yet supported in MotherDuck?|"Based on the MotherDuck documentation, there are several SQL features from DuckDB that are not yet supported in MotherDuck: User-Defined Functions (UDFs) Custom Python User-Defined Functions (UDFs) do not work on MotherDuck. The documentation explicitly states: ""UDFs, as described in this section, are a feature of the local, embedded DuckDB Python library and do not work on MotherDuck. The reason is that UDFs require a Python runtime to execute the function's code. MotherDuck is a serverless platform that provides SQL execution but does not run user-provided Python code on its servers."" This means that while you can create and use Python UDFs when running DuckDB locally (using con.create_function()), these functions cannot be executed on MotherDuck's cloud infrastructure. Server-Side ATTACH to External Databases MotherDuck does not yet support server-side ATTACH to other databases like PostgreSQL. This means you cannot directly connect to external database systems from within MotherDuck's cloud environment. Custom Extensions Custom DuckDB extensions are not yet available on MotherDuck. While DuckDB has a rich extension ecosystem that allows adding capabilities at multiple layers (SQL parser, optimizer, storage interface), custom or third-party extensions cannot be installed on MotherDuck's serverless platform. Workarounds The documentation suggests a common workaround for these limitations: ""perform these specific transformations upstream before loading data into MotherDuck."" This means handling UDF-based transformations, external database connections, and extension-dependent operations in your local DuckDB environment or ETL pipeline before loading the processed data into MotherDuck. Additional DuckDB Limitations (Present in Both DuckDB and MotherDuck) It's worth noting that some SQL features are not supported in DuckDB itself (and therefore also unavailable in MotherDuck): • ALTER TABLE limitations: DuckDB does not support ADD CONSTRAINT and DROP CONSTRAINT in ALTER TABLE statements. Constraints must be defined during initial table creation. • ALTER TABLE column type changes: Unlike PostgreSQL, DuckDB's ALTER TABLE doesn't support changing column data types, adding or removing constraints, or modifying default values of existing columns directly. • SERIAL/AUTO_INCREMENT: DuckDB does not support the SERIAL type or simple AUTOINCREMENT keywords. Instead, you must use sequences with CREATE SEQUENCE and DEFAULT nextval('sequence_name')."|MotherDuck aims for high compatibility with DuckDB but may have some limitations with certain extensions, user-defined functions, or very recent DuckDB features that haven't been integrated yet. Check the MotherDuck documentation for the current list of unsupported features or limitations.|.kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md; .kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md; .kurt/sources/motherduck.com/glossary/ALTER; .kurt/sources/motherduck.com/learn-more/acid-transactions-sql.md; .kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md; .kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md|0% (0 common, 5 unique to with_kg, 0 unique to without_kg)|0.82|0.85|0.8|0.75|0.9|The answer accurately identifies specific unsupported features (UDFs, ATTACH, custom extensions) with good detail and clear explanations. However, it loses some relevance by including a lengthy section on DuckDB's own limitations rather than focusing solely on the gap between DuckDB and MotherDuck capabilities. The writing is clear and well-organized with helpful formatting, though it could be more concise by removing the off-topic section about general DuckDB limitations.|0.0|87.99311685562134|/tmp/answer_without_kg_3.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q3_20260105_141529.json
3|answer_with_yaml_metadata|...What SQL features from DuckDB are not yet supported in MotherDuck?|"--- title: SQL Features from DuckDB Not Yet Supported in MotherDuck sources: • path: .kurt/sources/motherduck.com/blog/duckdb-excel-extension.md lines: 70-72 claim: ""MotherDuck UI does not allow importing of Excel extension files"" • path: .kurt/sources/motherduck.com/.metadata/claims.txt lines: 2183 claim: ""MotherDuck does not allow importing of Excel extension files through its UI"" --- SQL Features from DuckDB Not Yet Supported in MotherDuck Based on the MotherDuck documentation, the following DuckDB SQL features have limitations when used in MotherDuck: Excel Extension File Imports via UI Limitation: The MotherDuck UI does not allow importing of Excel extension files. Users must use the DuckDB CLI to accomplish Excel file integration. From the source documentation: > ""It should be noted that as of this writing, the MotherDuck UI does not allow importing of Excel extension files, so you need to use the DuckDB CLI to accomplish this integration. While this is fine for data pipeline work, it is fairly annoying for ad-hoc data exploration; we are aware of this and working on it."" Workaround: Use the DuckDB CLI to load Excel files into MotherDuck: Summary The primary documented limitation between DuckDB and MotherDuck is the inability to import Excel extension files through the MotherDuck web UI. The MotherDuck team has acknowledged this limitation and is working on addressing it. For now, users can work around this by using the DuckDB CLI to handle Excel file operations before syncing data to MotherDuck. Note: MotherDuck and DuckDB share core SQL compatibility, and MotherDuck's dual execution model allows most DuckDB SQL features to work seamlessly across local and cloud environments."|MotherDuck aims for high compatibility with DuckDB but may have some limitations with certain extensions, user-defined functions, or very recent DuckDB features that haven't been integrated yet. Check the MotherDuck documentation for the current list of unsupported features or limitations.||0% (0 common, 5 unique to with_kg, 0 unique to without_kg)|0.61|0.7|0.4|0.6|0.9|The answer is well-written and provides accurate, detailed information about the Excel extension limitation with proper documentation and workarounds. However, it significantly undersells the scope of unsupported features by focusing on only one specific UI limitation rather than addressing the broader categories mentioned in the canonical answer (extensions, UDFs, recent features). The answer would benefit from acknowledging that this is just one example of potential limitations and directing users to comprehensive documentation for a complete list, as the canonical answer does.|0.0|131.94327521324158|/tmp/answer_yaml_metadata_3.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_yaml_metadata.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_yaml_metadata/q3_20260105_135243.json
4|answer_motherduck_without_kg|...Why might a query run slower on MotherDuck than locally?|"A query might run slower on MotherDuck than locally for several key reasons, all stemming from the fundamental differences between local (in-process) and cloud-based execution: Network Latency and Data Transfer Overhead When running queries on MotherDuck (the cloud), there is inherent network latency involved in: • Sending the query from your client to MotherDuck's cloud compute • Transferring intermediate data and final results back to the client • Any ""shuffling"" of data between cloud nodes if distributed processing is involved As demonstrated in MotherDuck's documentation, a direct comparison showed that querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds). However, this comparison is for remote data - if your data is already local, the opposite can be true. Elimination of Distributed System Overhead When Running Locally DuckDB running locally is an ""in-process"" engine that eliminates network overhead entirely. The ""duckfooding at MotherDuck"" case study illustrates this dramatically: > ""I ran a MapReduce job that took 42 seconds. But when I ran the same computation locally on my machine, it completed in just 4 seconds. I wondered: where were the other 38 seconds going?"" The answer is the overhead of distributed computing: • Data must be serialized, sent over the network, deserialized, and reassembled • Each ""shuffle"" operation causes network storms where nodes must communicate • You're exposed to long-tail latency because query plans cannot proceed until all data partitions have been received Local Data Access is Inherently Faster When querying local data, DuckDB can: • Use the local operating system's page cache for low-latency I/O • Access data from fast local SSDs (orders of magnitude faster than fetching from remote object storage) • Perform joins and aggregations entirely in-memory without network serialization/deserialization • Avoid the ""shuffle tax"" that distributed systems impose The ""I/O Hierarchy"" - Cloud Storage vs Local Storage According to MotherDuck's performance documentation, query bottlenecks occur in a predictable order: I/O first, then Network, then CPU. Reading data from cloud object storage (like S3) is inherently the slowest part of any cloud query - each request to S3 can take 100-200 milliseconds of latency. In contrast, local SSDs offer dramatically lower latency for data access. Small Data Scenarios Favor Local Execution MotherDuck's hybrid architecture (Dual Execution) is designed to handle this by intelligently routing queries: • Queries on local files run entirely on your local DuckDB instance • The system pushes computation to where the data lives • Small datasets that fit in memory see near-instantaneous results locally Amazon's analysis of Redshift users found that approximately 95% of queries scan less than 10GB of data and 90% scan less than 1GB - workloads that can often be handled faster locally without cloud overhead. When MotherDuck Is Actually Faster MotherDuck will be faster than local execution when: • Data is stored in the cloud (avoiding download time) • Multiple concurrent users need to run queries (local machines get bottlenecked) • Data volume exceeds local memory and disk capacity • You need to join cloud-hosted data with local data (Dual Execution optimizes this) Summary The core reason queries can run slower on MotherDuck than locally is physics: network latency, data serialization/deserialization overhead, and the inherent cost of moving data over networks versus accessing it directly on a local disk. Modern hardware (dozens of cores, hundreds of GBs of RAM, fast SSDs) has made single-machine processing remarkably powerful for most analytical workloads. MotherDuck's Dual Execution model attempts to optimize this by running computation where data lives, but for purely local data, running locally will generally be faster."|Queries may run slower on MotherDuck compared to local DuckDB due to network latency, data transfer overhead when moving data between local and cloud, cold start times for serverless compute, or suboptimal data partitioning. Local queries benefit from direct disk access and no network overhead.|.kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md; .kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md; .kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md; .kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md; .kurt/sources/motherduck.com/blog/faster-ducks.md; .kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md|0% (0 common, 6 unique to with_kg, 0 unique to without_kg)|0.95|0.95|1.0|0.9|0.92|The answer is highly accurate and comprehensive, correctly identifying all key factors mentioned in the canonical answer (network latency, data transfer overhead, local disk access advantages) plus additional valid considerations like cold start times and data partitioning. The extensive detail and examples strengthen understanding but make the response longer than necessary for the straightforward question asked. The writing is clear and well-organized with effective use of headers and examples, though some sections could be more concise to improve focus on the core question.|0.0|92.72700595855713|/tmp/answer_without_kg_4.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q4_20260105_141706.json
4|answer_with_yaml_metadata|...Why might a query run slower on MotherDuck than locally?|"--- sources: • path: motherduck.com/learn-more/diagnose-fix-slow-queries.md lines: 18-35 claim: ""Query performance bottlenecks occur in order: I/O (storage), Network (shuffle), then CPU (compute). Reading data from cloud object storage like Amazon S3 is the slowest part of any query."" • path: motherduck.com/learn-more/diagnose-fix-slow-queries.md lines: 318-330 claim: ""Lakehouse architectures often suffer from high latency because they query data directly on cloud object storage like S3, which is inherently slow for I/O."" • path: motherduck.com/.metadata/claims.txt lines: 772 claim: ""Distributed systems create significant latency due to overhead."" • path: motherduck.com/.metadata/claims.txt lines: 710-711 claim: ""The dual execution planner reduces network traffic by orders of magnitude; the dual execution model minimizes data movement and improves query performance."" --- Why a Query Might Run Slower on MotherDuck Than Locally There are several reasons why a query might run slower on MotherDuck (or any cloud data warehouse) compared to running it locally with DuckDB: Network Latency and Data Transfer Overhead When running queries in the cloud, data must travel over the network between: • Your client and the MotherDuck service • MotherDuck and cloud object storage (like S3) As the documentation states: ""Distributed systems create significant latency due to overhead."" Network latency creates a ""slow feedback loop"" that doesn't exist when processing data locally in memory. I/O Bottlenecks from Cloud Object Storage Reading data from cloud object storage like Amazon S3 is ""the slowest part of any query."" Locally, data can be read from fast local SSDs or even cached in memory, which is orders of magnitude faster than cloud storage I/O. The bottleneck hierarchy is: I/O (storage) - Reading from cloud object storage is slowest Network (shuffle) - Moving data between nodes CPU (compute) - Actual processing Data Layout Not Optimized for Your Query If your cloud data isn't properly partitioned or clustered to match your query patterns, the engine must scan far more data than necessary. Locally, with smaller datasets, this overhead is less pronounced. When Local is Genuinely Better For smaller datasets (under 10 TB), cloud overhead can make queries ""potentially slower than vanilla DuckDB."" Local DuckDB eliminates: • Network round-trips • Cloud storage I/O latency • Serialization overhead Mitigation: Dual Execution Model MotherDuck's dual execution model is designed to address this by intelligently minimizing data transfer and allowing queries to leverage both local compute and cloud resources. This ""reduces network traffic by orders of magnitude compared to traditional methods."" For interactive development, consider using DuckDB locally and MotherDuck for production workloads that benefit from cloud scalability."|Queries may run slower on MotherDuck compared to local DuckDB due to network latency, data transfer overhead when moving data between local and cloud, cold start times for serverless compute, or suboptimal data partitioning. Local queries benefit from direct disk access and no network overhead.||0% (0 common, 6 unique to with_kg, 0 unique to without_kg)|0.9|0.9|0.85|0.95|0.95|"The answer is highly accurate and well-structured, correctly identifying network latency, data transfer overhead, and I/O bottlenecks as primary reasons for slower cloud performance. It provides excellent clarity with organized sections and concrete examples. The completeness score is slightly reduced because it doesn't explicitly mention ""cold start times for serverless compute"" from the canonical answer, though it covers most other key concepts thoroughly. The answer goes beyond the canonical response with helpful additional context about MotherDuck's dual execution model and mitigation strategies."|0.0|56.550999879837036|/tmp/answer_yaml_metadata_4.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_yaml_metadata.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_yaml_metadata/q4_20260105_135503.json
5|answer_motherduck_without_kg|...How do I set up MotherDuck to work with dbt?|Setting up MotherDuck to work with dbt involves installing the dbt-duckdb adapter and configuring your profiles.yml to connect to MotherDuck. Here's a comprehensive guide: Install the dbt-duckdb Adapter Install the dbt adapter with MotherDuck support: Get Your MotherDuck Service Token Sign up for a MotherDuck account at [motherduck.com](https://motherduck.com) Sign in to the MotherDuck UI Click on settings in the upper right corner Copy your Service Token to the clipboard Export the Token as an Environment Variable Configure Your dbt Profile Create or edit your profiles.yml file. The key difference from local DuckDB is the path setting - use md: prefix to connect to MotherDuck: Basic Configuration Local Dev + Cloud Prod Configuration (Recommended) For a development workflow where you iterate locally and deploy to MotherDuck: With S3 Integration If you're reading data from S3: Initialize and Test the Connection Key Benefits of MotherDuck + dbt • Fast Development: Both dbt and DuckDB can run locally, so you can develop and test using the same technologies you run in production • Dual Execution: MotherDuck uses your local computer when queries make more sense to run locally, providing dynamic execution out of the box • Seamless Cloud Scaling: MotherDuck runs DuckDB on your behalf in the cloud while maintaining local development capabilities • No Schema Configuration Needed: DuckDB/MotherDuck doesn't require you to define schemas or tedious configurations to make it work|To use MotherDuck with dbt, you need to install the dbt-duckdb adapter and configure your profiles.yml with MotherDuck connection details including your token. The dbt-duckdb adapter supports MotherDuck's cloud features while maintaining compatibility with dbt's standard workflow.|.kurt/sources/motherduck.com/ecosystem/dbt.md; .kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md; .kurt/sources/motherduck.com/glossary/data; .kurt/sources/motherduck.com/blog/dual-execution-dbt.md; .kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md|0% (0 common, 4 unique to with_kg, 0 unique to without_kg)|1.0|1.0|1.0|1.0|1.0|The generated answer excellently addresses the question with complete accuracy and comprehensive detail. It covers all essential elements from the canonical answer (dbt-duckdb adapter installation, profiles.yml configuration, MotherDuck token) while providing valuable additional context including prerequisites, step-by-step instructions with code examples, and advanced patterns like dual execution. The answer is exceptionally well-organized with clear headings, properly formatted code blocks, and practical guidance that would enable successful implementation.|0.0|71.31297206878662|/tmp/answer_without_kg_5.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q5_20251228_210127.json
5|answer_with_yaml_metadata|...How do I set up MotherDuck to work with dbt?|"--- sources: • claim: ""dbt is a great and straightforward tool for building production-ready data pipelines with SQL"" doc: motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md lines: 27-37 • claim: ""Since DuckDB is an embedded database and just another Python library to install, we can run the computation within the same Python dbt process"" doc: motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md lines: 97-100 • claim: ""profiles.yml configuration shows dev uses local path and prod uses md: for MotherDuck"" doc: motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md lines: 306-318 • claim: ""Using md: triggers authentication with MotherDuck, which checks for a token in the motherduck_token environment variable"" doc: motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md lines: 320-321 --- Setting Up MotherDuck with dbt Installation Install the dbt-duckdb adapter with MotherDuck support using pip or Poetry: Initialize a dbt Project This creates the standard dbt project structure with folders for models, tests, macros, etc. Configure profiles.yml The key to connecting dbt to MotherDuck is in your profiles.yml configuration. You can set up two targets: one for local development and one for MotherDuck (production): Key Configuration Details • type: Always set to duckdb for the dbt-duckdb adapter • path: • For local development: Use a local file path (e.g., /tmp/dbt.duckdb) • For MotherDuck: Use md: to connect to MotherDuck cloud • You can also specify a database: md:my_database Authentication MotherDuck authentication requires a token. Set the motherduck_token environment variable: You can obtain this token from your MotherDuck account settings page. Running dbt with MotherDuck Benefits of This Setup Seamless dev-to-prod transition: Since MotherDuck is DuckDB in the cloud, SQL is fully compatible between environments Fast local iteration: DuckDB operates in-process, allowing quick iteration since computation occurs locally within the same dbt process Zero cloud costs for development: Using DuckDB locally eliminates cloud compute costs during development Easy testing: You can run dbt unit tests locally without any cloud dependencies Working with External Data (S3) For pipelines that read from S3, you can configure sources in sources.yml: Use AWS credentials by calling load_aws_credentials() as a pre-hook in your dbt_project.yml."|To use MotherDuck with dbt, you need to install the dbt-duckdb adapter and configure your profiles.yml with MotherDuck connection details including your token. The dbt-duckdb adapter supports MotherDuck's cloud features while maintaining compatibility with dbt's standard workflow.||0% (0 common, 4 unique to with_kg, 0 unique to without_kg)|1.0|1.0|1.0|1.0|1.0|This is an excellent answer that fully addresses the question with accurate, complete information. It covers all essential topics from the canonical answer (adapter installation, profiles.yml configuration, token authentication) while providing valuable additional context like code examples, dual environment setup, and practical usage commands. The answer is exceptionally well-organized with clear headings, properly formatted code blocks, and logical flow from installation through execution.|0.0|60.957098960876465|/tmp/answer_yaml_metadata_5.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_yaml_metadata.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_yaml_metadata/q5_20260105_135609.json
6|answer_motherduck_without_kg|...How do I migrate data from a local DuckDB database to MotherDuck?|"Migrating data from a local DuckDB database to MotherDuck is a straightforward process thanks to the seamless integration between DuckDB and MotherDuck. Here's how to do it: Method 1: Using the ATTACH Command (Recommended) The simplest way to connect your local DuckDB to MotherDuck and migrate data is using the ATTACH command: Connect to MotherDuck: From your local DuckDB CLI or Python environment, run: This single command connects your local DuckDB instance to MotherDuck. Authentication requires a MotherDuck token, which can be set as an environment variable. Create a database in MotherDuck: Once connected, you can create a new database in the cloud: Copy tables from local to cloud: Use CREATE TABLE AS SELECT to copy your local data to MotherDuck: Method 2: Upload Files via MotherDuck Web UI MotherDuck provides a notebook-like web interface where you can: • Upload local CSV and Parquet files directly • Analyze and manage them alongside your other MotherDuck data • Use DuckDB-WASM in-browser for immediate data exploration Method 3: From Local Files (CSV, Parquet) If your data is in file format, you can create tables directly in MotherDuck: Key Concepts Dual Execution When connected to MotherDuck, queries can leverage dual execution - an intelligent hybrid query execution strategy where the optimizer determines whether to run parts of a query locally or in the cloud. This minimizes data movement and leverages compute where it makes the most sense. Hybrid Workflow MotherDuck creates a ""multiplayer"" experience from DuckDB's traditionally single-player architecture: • Your laptop and MotherDuck cloud become two nodes in a single distributed system • Both nodes execute queries in concert, automatically routing parts of queries to the optimal location • You can even JOIN local data with cloud data in a single query No Database Changes Needed One of the key benefits is that what works locally will work in the cloud - MotherDuck uses the exact same DuckDB engine. Simply add the md: prefix to your connection or database references to work with cloud data. After Migration Once your data is in MotherDuck, you can: • Share databases with team members using CREATE SHARE • Use MotherDuck's Secret Manager for secure credential management • Access your data from anywhere using Python, CLI, JDBC, or the web UI • Take advantage of MotherDuck's serverless infrastructure for scaling"|To migrate from local DuckDB to MotherDuck, you can attach to MotherDuck from your local DuckDB instance and use INSERT INTO or CREATE TABLE AS SELECT statements to copy data from local tables to MotherDuck tables. Alternatively, export data to Parquet files and load them into MotherDuck from cloud storage.||N/A|0.95|0.95|1.0|0.9|0.95|The answer is highly accurate and complete, covering all methods mentioned in the canonical answer (ATTACH with INSERT/CREATE TABLE AS SELECT, and Parquet export via cloud storage) plus additional helpful approaches. The clarity is excellent with well-structured sections and code examples. The relevance score is slightly lower because while all information is useful, some sections (like dual execution details and hybrid workflow concepts) go beyond what was directly asked, making the answer somewhat verbose for the specific migration question. Overall, this is a high-quality, comprehensive response that would serve users well.|0.0|92.65185499191284|/tmp/answer_without_kg_6.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q6_20251228_210127.json
6|answer_with_yaml_metadata|...How do I migrate data from a local DuckDB database to MotherDuck?|"--- sources: • path: .kurt/sources/motherduck.com/blog/differential-storage-building-block-for-data-warehouse.md lines: 158-168 claim: ""If you're using DuckDB currently, just run attach md:, and your DuckDB instance suddenly becomes MotherDuck-supercharged"" • path: .kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md lines: 207-244 claim: ""ATTACH 'md:' connects to MotherDuck, CREATE TABLE AS copies data to cloud databases"" • path: .kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2025.md lines: 69-72 claim: ""Pushing data from a local DuckDB table to a remote MotherDuck table is just another COPY command"" --- How to Migrate Data from Local DuckDB to MotherDuck Migrating data from a local DuckDB database to MotherDuck is straightforward using the ATTACH command and standard SQL statements. Basic Migration Steps Connect to MotherDuck From your local DuckDB CLI or Python session, connect to MotherDuck with a single command: Authentication requires a MotherDuck token, which can be set as an environment variable (MOTHERDUCK_TOKEN). Create a Cloud Database (if needed) Copy Tables to MotherDuck Use CREATE TABLE AS to copy local tables to MotherDuck: Or use the COPY command for more control: Verify the Migration After copying, you can query your data in the cloud: Key Points • Seamless Integration: Once you run ATTACH 'md:', your local DuckDB instance becomes ""MotherDuck-supercharged"" with access to both local and cloud data • Dual Execution: MotherDuck intelligently determines whether to run computations locally or in the cloud based on data location • Simple Authentication: Just set your MotherDuck token as an environment variable • Standard SQL: Pushing data from local to remote uses familiar SQL commands like CREATE TABLE AS and COPY"|To migrate from local DuckDB to MotherDuck, you can attach to MotherDuck from your local DuckDB instance and use INSERT INTO or CREATE TABLE AS SELECT statements to copy data from local tables to MotherDuck tables. Alternatively, export data to Parquet files and load them into MotherDuck from cloud storage.||N/A|0.93|0.95|0.85|1.0|0.95|The generated answer is highly accurate and relevant, providing correct technical details about the ATTACH command, CREATE TABLE AS, and COPY methods for migration. It excels in clarity with well-organized sections, code examples, and step-by-step instructions that make it very practical. The completeness score is slightly lower because it doesn't mention the alternative Parquet export method described in the canonical answer, though it does cover the primary migration approach comprehensively.|0.0|69.50934028625488|/tmp/answer_yaml_metadata_6.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_yaml_metadata.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_yaml_metadata/q6_20260105_135718.json
7|answer_motherduck_without_kg|...If I have a CSV on my laptop and a table in S3, what's the most efficient way to join them using MotherDuck?|"If you have a CSV on your laptop and a table in S3, the most efficient way to join them using MotherDuck is to leverage the Dual Execution feature, which enables hybrid queries that span local and cloud data sources with automatic optimization for minimal data transfer. The Approach: Dual Execution MotherDuck treats your laptop and the MotherDuck cloud as two nodes in a single distributed system. When you run a hybrid query, the query planner intelligently breaks it down and pushes computation to where the data lives, minimizing data transfer over the network. Here's how it works step-by-step: Local Scan: MotherDuck scans your local CSV file on your laptop Filter Locally: If your query has filters, they're applied on your local machine first Transfer Minimal Data: Only the filtered/necessary rows are sent to the cloud Join in the Cloud: The final join with the S3 table happens in the MotherDuck cloud Practical Implementation Step 1: Connect to MotherDuck and load required extensions Step 2: Attach the S3 data (if not already in a MotherDuck database) For Parquet files in S3, you can query them directly: Or attach a DuckDB database file stored on S3: Step 3: Write your join query You can directly join your local CSV with cloud data in a single query: Key Advantages of This Approach Automatic Optimization: MotherDuck's query planner automatically decides which parts run locally (marked as ""L"" in query plans) and which run remotely (marked as ""R"") No Manual ETL Required: You don't need to upload your entire CSV to the cloud or download the entire S3 table - the ""No-ETL"" approach lets you query files directly where they live Schema-on-Read: DuckDB's read_csv_auto function automatically infers column names and data types, so you don't need to define schemas Cost Efficiency: By leveraging free local compute on your laptop and minimizing data transfer, you significantly reduce cloud costs Consistent Engine: The same DuckDB engine runs both locally and in the cloud, ensuring query consistency Verifying Query Optimization You can use EXPLAIN to see how MotherDuck plans to execute your hybrid query: The output shows which parts run locally (L) and which run remotely (R), confirming that MotherDuck is optimizing data movement."|The most efficient approach is to create a temporary table from your local CSV, then use MotherDuck's ability to read directly from S3 in your join query. MotherDuck can push down filters and only transfer necessary data, minimizing data movement. Alternatively, if the CSV is small, you can upload it directly to MotherDuck.||N/A|0.81|0.75|0.8|0.85|0.95|"The generated answer is well-written, clear, and provides extensive technical detail about MotherDuck's Dual Execution feature. However, it doesn't explicitly mention the canonical answer's recommended approach of creating a temporary table from the local CSV first. While the direct query approach shown is valid, the canonical answer suggests a different ""most efficient"" method. The answer excels in clarity with code examples and step-by-step instructions, but could be more aligned with the specific recommendation in the canonical answer."|0.0|85.00172424316406|/tmp/answer_without_kg_7.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q7_20251228_210127.json
7|answer_with_yaml_metadata|...If I have a CSV on my laptop and a table in S3, what's the most efficient way to join them using MotherDuck?|"--- sources: • doc: motherduck.com/learn-more/hybrid-analytics-guide.md lines: 79-94 claim: ""How to join local CSVs with cloud data using Dual Execution"" • doc: motherduck.com/learn-more/modern-data-warehouse-playbook.md lines: 375-377 claim: ""Write a single SQL query that seamlessly joins a local CSV or Parquet file on your laptop with a large dataset stored in the MotherDuck cloud"" • doc: motherduck.com/blog/duckdb-tutorial-for-beginners.md lines: 408-418 claim: ""DuckDB supports accessing private S3 buckets by using CREATE SECRET to specify and store your credentials"" • doc: motherduck.com/blog/geospatial-for-beginner-duckdb-spatial-motherduck.md lines: 1-50 claim: ""The httpfs extension allows reading/writing remote files over HTTPS and S3"" --- Joining a Local CSV with an S3 Table Using MotherDuck The most efficient way to join a local CSV on your laptop with a table in S3 is to use MotherDuck's Dual Execution feature, which intelligently minimizes data transfer by pushing computation to where the data lives. How It Works MotherDuck treats your laptop and the MotherDuck cloud as two nodes in a single distributed system. When you run a hybrid query, the planner: Scans locally: It scans the small CSV on your laptop Filters locally: Applies any filters on your local machine first Transfers minimal data: Sends only the filtered/needed rows to the cloud Joins in the cloud: Performs the final join against the large cloud table Example Approach Key Benefits • Eliminates complex ETL: No need to upload your entire CSV to S3 first • Minimizes data transfer: Only filtered/necessary data crosses the network • Single query: Write one SQL statement that seamlessly joins local and cloud data • Interactive performance: What would be a difficult data movement problem becomes an interactive query Accessing Private S3 Buckets For private S3 data, use CREATE SECRET to configure authentication: Verifying Execution Plan Use EXPLAIN to see which parts run locally (L) vs remotely (R): This shows MotherDuck's intelligent query planning in action, proving minimal data is being transferred."|The most efficient approach is to create a temporary table from your local CSV, then use MotherDuck's ability to read directly from S3 in your join query. MotherDuck can push down filters and only transfer necessary data, minimizing data movement. Alternatively, if the CSV is small, you can upload it directly to MotherDuck.||N/A|0.86|0.85|0.8|0.95|0.95|The generated answer is highly accurate and provides excellent detail about MotherDuck's Dual Execution approach for joining local and cloud data. It correctly explains the efficiency mechanisms (filter pushdown, minimal data transfer) mentioned in the canonical answer. However, it misses explicitly mentioning two alternatives from the canonical answer: creating a temporary table from the CSV first, and the option to upload small CSVs directly to MotherDuck. Despite these minor omissions, the answer is comprehensive, well-written, and highly practical with clear code examples.|0.0|70.92887997627258|/tmp/answer_yaml_metadata_7.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_yaml_metadata.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_yaml_metadata/q7_20260105_135836.json
8|answer_motherduck_without_kg|...What's the difference between a MotherDuck database and a share?|"In MotherDuck, a database and a share are two distinct but related concepts that serve different purposes in the data warehousing workflow. Database A MotherDuck database is a cloud-hosted DuckDB database that you own and control. Key characteristics include: • Writable and Modifiable: You can create tables, insert data, update records, and modify the database structure • Full Ownership: The database belongs to your account, and you have complete control over its contents • Persistent Storage: Uses MotherDuck's Differential Storage system to store data as an ordered sequence of ""layers"" (snapshots), enabling features like time travel, branching, and efficient zero-copy operations • Cloud Catalog Management: Allows you to manage your DuckDB database catalog in the cloud for easy scaling and collaboration • Can be created from local databases: You can upload a local DuckDB database using CREATE DATABASE cloud_db FROM local_db Share A MotherDuck share is a read-only, shareable snapshot of a database that can be distributed to other users via a secret URL. Key characteristics include: • Read-Only Access: Recipients can query the data but cannot modify it • Snapshot-Based: A share represents a point-in-time snapshot of the source database that remains synchronized with updates from the original database owner • Zero-Copy Creation: Creating a share is a metadata-only operation (CREATE SHARE FROM <database>), meaning no data duplication occurs thanks to the Differential Storage system • URL-Based Sharing: Shares are distributed via a unique URL (e.g., md:_share/sample_data/...) that recipients can attach to their MotherDuck session • Access Control Options: Shares can be configured for different access levels - either ""anyone with the share link"" (including external users) or ""restricted to organization members with the share link"" • Discoverability: Org-level shares can optionally be made discoverable to all members of an organization, appearing in their ""Shared with me"" section Key Differences \| Aspect \| Database \| Share \| \|--------\|----------\|-------\| \| Ownership \| Owned by creator \| Derived from a database \| \| Permissions \| Read/Write \| Read-Only \| \| Purpose \| Primary data storage & manipulation \| Data distribution & collaboration \| \| Creation \| CREATE DATABASE \| CREATE SHARE FROM <database> \| \| Data Duplication \| Contains actual data \| Zero-copy reference to source \| \| Updates \| Direct modifications allowed \| Reflects source database updates \| In essence, a database is where you store and work with your data, while a share is how you distribute that data to others for querying without giving them write access or requiring data duplication."|A MotherDuck database is a collection of tables and schemas that you own and can modify. A share is a read-only view of another user's database that they've shared with you, allowing data collaboration without copying. Shares enable secure data sharing across MotherDuck accounts.||N/A|0.99|1.0|1.0|0.95|1.0|The answer is factually accurate and comprehensive, correctly explaining that databases are owned/modifiable collections while shares are read-only views for collaboration. The extensive technical details (Differential Storage, time travel, zero-copy operations) go well beyond what was asked but remain accurate and relevant. The structured format with clear sections, bullet points, and a comparison table makes the information highly accessible, though the answer could be considered slightly verbose for the straightforward question asked.|0.0|118.9932427406311|/tmp/answer_without_kg_8.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q8_20251228_210127.json
8|answer_with_yaml_metadata|...What's the difference between a MotherDuck database and a share?|"--- sources: • claim: ""Shares are read-only databases that are purpose-built for data collaboration and ad-hoc analytics."" doc: motherduck.com/blog/duckdb-110-hidden-gems.md • claim: ""SHARES provide read-only access to specific databases."" doc: motherduck.com/learn-more/hybrid-analytics-guide.md • claim: ""Shares allow users to access the same dataset as a zero-copy clone without duplicating data."" doc: motherduck.com/blog/duckdb-cognee-sql-analytics-graph-rag.md • claim: ""Auto Update allows database shares to automatically sync with the latest changes within five minutes."" doc: motherduck.com/blog/data-warehouse-feature-roundup-oct-2024.md • claim: ""MotherDuck allows you to upload your data and share a named snapshot with your colleagues in two lines of SQL."" doc: motherduck.com/blog/duckdb-ecosystem-newsletter-december-2023.md • claim: ""MotherDuck supports two modes for users to publish changes to the data they have shared."" doc: motherduck.com/blog/motherduck-reflections-sigmod-pods-2024.md --- Difference Between a MotherDuck Database and a Share Database A database in MotherDuck is your primary data storage unit where you create, modify, and manage your data. Databases are: • Read-write: You can perform full CRUD operations (Create, Read, Update, Delete) • Owned by you: Full control over the data and schema • Attachable: Can be connected to DuckDB sessions for querying • Persistent: Stores your data in the MotherDuck cloud Share A share is a read-only view of a database designed specifically for data collaboration. Shares are: • Read-only: Purpose-built for data collaboration and ad-hoc analytics - users can query but not modify the data • Zero-copy clones: Allow users to access the same dataset without duplicating data, meaning no additional storage costs • Snapshot-based: By default, shares capture a point-in-time snapshot of your database • Automatically syncable: With Auto Update enabled, shares can automatically sync with the latest changes within five minutes of any completed writes to the source database Key Differences \| Aspect \| Database \| Share \| \|--------\|----------\|-------\| \| Access \| Full read-write \| Read-only \| \| Purpose \| Primary data storage and manipulation \| Data collaboration and distribution \| \| Data duplication \| Original data \| Zero-copy clone (no duplication) \| \| Updates \| Direct modifications \| Snapshots; can auto-sync with Auto Update \| \| Use case \| Your working data environment \| Sharing data with colleagues or external users \| When to Use Each • Use a database when you need to store, transform, and manage your data • Use a share when you want to give others access to query your data without giving them write permissions or duplicating the underlying data"|A MotherDuck database is a collection of tables and schemas that you own and can modify. A share is a read-only view of another user's database that they've shared with you, allowing data collaboration without copying. Shares enable secure data sharing across MotherDuck accounts.||N/A|0.97|0.95|0.98|1.0|1.0|"The generated answer excellently addresses the question with accurate information that aligns well with the canonical answer. It goes beyond the basic requirements by providing a structured comparison table, additional technical details about zero-copy clones and auto-sync features, and practical guidance on use cases. The only minor gap is not explicitly mentioning ""schemas"" as part of database structure, though this is a very minor omission that doesn't significantly impact the overall quality of the response."|0.0|65.96209478378296|/tmp/answer_yaml_metadata_8.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_yaml_metadata.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_yaml_metadata/q8_20260105_135959.json
9|answer_motherduck_without_kg|...What compute instance sizes does MotherDuck offer?|"MotherDuck offers five compute instance sizes (called ""Ducklings""), designed for different workload requirements: Instance Sizes Pulse • Description: The smallest instance, perfect for lightweight, fully serverless analytics • Use cases: Ad-hoc analytics tasks, read-only workloads with high volumes of concurrent users (like customer-facing analytics), situations where a larger dedicated instance is overkill • Billing: Per Compute Unit (CU) second, with 1 second minimum • Pricing: • Free plan: Up to 10 CU hours included • Lite plan: $0.25 per CU hour • Business plan: $0.40 per CU hour Standard • Description: Built to handle common data warehouse workloads including loads and transforms • Use cases: Typical data engineering tasks like data ingest and dbt transformations, running dbt jobs with multiple parallel transformations, building standard data pipelines before scaling up • Billing: Per second, with 60 second minimum • Pricing: • Lite plan: $1.20 per hour • Business plan: $1.80 per hour Jumbo • Description: For larger data warehouse workloads with many transformations or complex aggregations • Use cases: When query complexity, data volume, and transformations exceed Standard capacity; complex joins and aggregations on growing datasets; event stream data or nested JSON with changing schemas • Billing: Per second, with 60 second minimum • Pricing: Business plan only - $3.60 per hour Mega • Description: An extremely large instance for demanding jobs at an even larger scale • Use cases: Queries too complex or data volumes too high for Jumbo in crunch time; weekly jobs that rebuild all tables and must run in minutes, not hours; serving customers with 10x the data volume while maintaining the same performance • Billing: Per second, with 5 minute minimum • Pricing: Business plan only - $10.80 per hour • Availability: Self-serve on the Business plan Giga • Description: The largest instances that enable the toughest transformations to run faster • Use cases: Data workloads so complex that nothing else will work; one-time jobs (e.g., restating revenue for the last 10 years); growth path beyond Mega when data volume and complexity grow 10x • Billing: Per second, with 10 minute minimum • Pricing: Business plan only - Contact sales for pricing • Availability: Requires contacting MotherDuck (""quack with us"") to request access Plan Availability \| Instance Type \| Free Plan \| Lite Plan \| Business Plan \| \|---------------\|-----------\|-----------\|---------------\| \| Pulse \| ✓ \| ✓ \| ✓ \| \| Standard \| ✓ \| ✓ \| ✓ \| \| Jumbo \| - \| - \| ✓ \| \| Mega \| - \| - \| ✓ \| \| Giga \| - \| - \| ✓ (by request)\| Additional Scaling Options Beyond instance sizes, MotherDuck also offers: • Read Scaling: Up to 16 read-scaling replicas, each backed by its own instance, for BI dashboards and read-only use cases • Per-user Tenancy: Each user or customer gets their own isolated duckling, which can be individually sized • Dual Execution: Automatically leverages both local (laptop) compute and cloud compute for optimal performance"|MotherDuck uses serverless compute that automatically scales based on query demands. Instead of choosing fixed instance sizes, MotherDuck allocates iOS resources dynamically and charges based on usage. This provides flexibility without requiring manual instance sizing.||N/A|0.25|0.1|0.2|0.3|0.9|The generated answer is well-written and clear but contains a critical factual error. The canonical answer explicitly states that MotherDuck does NOT offer fixed instance sizes but instead uses serverless compute with dynamic resource allocation. The generated answer contradicts this by describing five specific instance types with fixed pricing, which appears to be incorrect information about MotherDuck's actual architecture. While the answer is comprehensive and well-structured, it fails on accuracy - the most important dimension for a factual question.|0.0|75.57650423049927|/tmp/answer_without_kg_9.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q9_20251228_210127.json
9|answer_with_yaml_metadata|...What compute instance sizes does MotherDuck offer?|"--- sources: • path: .kurt/sources/motherduck.com/blog/announcing-mega-giga-instance-sizes-huge-scale.md claims_used: • ""Mega and Giga instances are built for the largest, toughest, most complex data transformations DuckDB can handle"" • ""Mega ducklings are designed for when your workloads have outgrown Jumbo"" • ""Giga ducklings are our largest instance sizes, purpose-built for the toughest of transformations"" • ""MotherDuck offers configurable instances for users"" • ""Megas and Gigas will help scale up compute needs"" --- MotherDuck Compute Instance Sizes MotherDuck offers four compute instance sizes (called ""ducklings""): Standard - Default instance size for most users Jumbo - Larger instance, sufficient for the vast majority of customers Mega - For demanding workloads at larger scale Giga - Largest instances for the toughest transformations Instance Size Details Mega Ducklings • Designed for when workloads have outgrown Jumbo • Unlocks more in-memory execution • Handles larger joins and aggregations without spilling • Use cases: • Queries too complex or data volume too high for Jumbo • Weekly jobs that rebuild tables and need to run in minutes, not hours • Customers with 10x the data who need subsecond response times • Availability: Self-serve on the Business Plan Giga Ducklings • Largest instance sizes available • Purpose-built for the toughest of transformations • Provides maximum compute and memory environment • Use cases: • Data workloads so complex or massive that nothing else will work • One-time jobs like restating 10 years of revenue data • Growth path beyond Mega when data volume/complexity grows 10x • Very complex joins, deeply nested CTEs, long-range analytical backfills • Availability: Available on request through Business Plan (contact sales) Architecture Note MotherDuck uses a per-user tenancy model where each user gets their own configurable duckling. This allows mixing instance sizes - for example, using Mega for complex data pipeline transformations while using Standard instances to serve most users. Each instance is provisioned on demand and fully managed."|MotherDuck uses serverless compute that automatically scales based on query demands. Instead of choosing fixed instance sizes, MotherDuck allocates iOS resources dynamically and charges based on usage. This provides flexibility without requiring manual instance sizing.||N/A|0.25|0.1|0.2|0.3|0.9|The generated answer is well-written and clear but fundamentally contradicts the canonical answer. While the canonical answer describes a serverless, automatically-scaling architecture without fixed instance sizes, the generated answer details four specific instance types (Standard, Jumbo, Mega, Giga) that users can configure. This represents a major factual error about MotherDuck's core compute model. The answer would mislead users about how MotherDuck actually works, despite being clearly structured and easy to understand.|0.0|45.14459180831909|/tmp/answer_yaml_metadata_9.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_yaml_metadata.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_yaml_metadata/q9_20260105_140115.json
10|answer_motherduck_without_kg|...What DuckDB versions does MotherDuck currently support?|Based on the MotherDuck documentation, MotherDuck currently supports DuckDB version 1.4.1, which is the latest version as of October 2025. MotherDuck has progressively supported each major DuckDB release: • DuckDB 1.0 - Supported since June 2024 (when DuckDB reached its 1.0 milestone) • DuckDB 1.2 - Supported since February 2025 • DuckDB 1.3 - Supported since June 2025 • DuckDB 1.4.1 - Currently supported (announced October 2025) A key feature that enables this is Version Independence, which MotherDuck introduced to decouple client versions from the server-side DuckDB version. This means: When DuckDB ships a new version, MotherDuck can upgrade all servers to run the latest version in the cloud Users automatically get access to improved performance and bug fixes without taking any action While users can continue using their current local DuckDB client version, MotherDuck encourages upgrading to 1.4.1 to take advantage of all the latest fixes and features You can check which version you're running with the simple query: SELECT version();|MotherDuck typically supports recent stable versions of DuckDB and updates regularly to incorporate new features and improvements. Check the MotherDuck documentation or release notes for the current supported DuckDB version, as this evolves over time.||N/A|0.85|0.75|0.9|0.95|0.95|The generated answer is highly relevant, clear, and comprehensive, providing specific version information and helpful context about MotherDuck's Version Independence feature. However, the accuracy score is reduced because it provides specific version numbers and dates (DuckDB 1.4.1, October 2025) that cannot be verified and may become outdated, whereas the canonical answer wisely avoids such specifics and instead directs users to check current documentation. The answer would be stronger if it balanced specific information with guidance to verify current versions through official sources.|0.0|71.28517603874207|/tmp/answer_without_kg_10.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_motherduck_without_kg.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_motherduck_without_kg/q10_20251228_210127.json
10|answer_with_yaml_metadata|...What DuckDB versions does MotherDuck currently support?|"--- sources: • path: motherduck.com/blog/announcing-duckdb-141-motherduck.md claim: ""MotherDuck now supports DuckDB version 1.4.1 and DuckLake version 0.3"" • path: motherduck.com/blog/duckdb-ecosystem-newsletter-august-2024.md claim: ""As of now, MotherDuck supports only one version of DuckDB"" • path: motherduck.com/blog/duckdb-ecosystem-newsletter-june-2025.md claim: ""DuckDB establishes its first Long Term Support (LTS) model with one year of community support"" --- What DuckDB Versions Does MotherDuck Currently Support? MotherDuck currently supports DuckDB version 1.4.1. As of October 2025, MotherDuck upgraded to support DuckDB 1.4.1, which builds on the foundation of DuckDB 1.4.0 with important bugfixes and additional improvements. Key Points Single Version Support: MotherDuck supports only one version of DuckDB at a time. Users are encouraged to upgrade their local DuckDB clients to match MotherDuck's supported version. DuckDB 1.4.x Features: The 1.4.0 release introduced landmark features including: • MERGE statement • VARIANT type • Completely rewritten sorting engine • Database encryption using AES-256-GCM DuckLake Support: Alongside DuckDB 1.4.1, MotherDuck now supports DuckLake version 0.3, which includes: • Iceberg interoperability • CHECKPOINT function for automatic table maintenance • Native support for spatial geometry types • Full MERGE INTO statement support Long Term Support (LTS): DuckDB has established its first Long Term Support model with one year of community support, starting with version 1.4.0. Recommendation While users can continue using their current version of DuckDB, MotherDuck encourages upgrading DuckDB clients to 1.4.1 as soon as possible to ensure compatibility."|MotherDuck typically supports recent stable versions of DuckDB and updates regularly to incorporate new features and improvements. Check the MotherDuck documentation or release notes for the current supported DuckDB version, as this evolves over time.||N/A|0.97|0.95|1.0|1.0|0.95|"The generated answer excellently addresses the question with specific, sourced information about DuckDB 1.4.1 support. It goes beyond the canonical answer by providing concrete version details, feature highlights, and the single-version support policy. The answer is well-structured, clear, and includes helpful context about DuckLake and LTS support. Minor deduction in accuracy reflects that the canonical answer's emphasis on ""checking documentation"" suggests version information may change, while the generated answer provides a point-in-time snapshot."|0.0|66.97643494606018|/tmp/answer_yaml_metadata_10.md|https://github.com/anthropics/kurt-core/blob/main/eval/scenarios/answer_with_yaml_metadata.yaml|https://github.com/anthropics/kurt-core/blob/main/eval/results/answer_with_yaml_metadata/q10_20260105_140212.json
