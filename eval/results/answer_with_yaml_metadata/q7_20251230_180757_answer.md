---
sources:
  - doc: motherduck.com/learn-more/modern-data-warehouse-playbook.md
    claim: "You can write a single SQL query that seamlessly joins a local CSV or Parquet file on your laptop with a large dataset stored in the MotherDuck cloud."
  - doc: motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md
    claim: "MotherDuck lets you analyze local data while still JOINing with data processed in the cloud."
  - doc: motherduck.com/blog/geospatial-for-beginner-duckdb-spatial-motherduck.md
    claim: "The httpfs extension allows reading/writing remote files over HTTPS and S3."
  - doc: motherduck.com/blog/vibe-coding-sql-cursor.md
    claim: "DuckDB allows joining local and remote data."
  - doc: motherduck.com/videos/ducklake-big-data-small-coalesce-2025.md
    claim: "A modern hybrid platform with a dual-execution engine solves the challenge of data gravity by intelligently minimizing data transfer."
---

# Joining a Local CSV with an S3 Table Using MotherDuck

The most efficient way to join a CSV file on your laptop with a table stored in S3 is to use **MotherDuck's hybrid/dual execution model**. Here's how:

## The Approach

**Write a single SQL query that seamlessly joins your local CSV with the remote S3 data** — MotherDuck and DuckDB handle the complexity of where to execute each part of the query.

```sql
-- Example: Join local CSV with S3 table via MotherDuck
SELECT
    local.customer_id,
    local.order_date,
    cloud.customer_name
FROM read_csv('~/data/orders.csv') AS local
JOIN my_database.customers AS cloud
    ON local.customer_id = cloud.id;
```

## Why This is Efficient

1. **Dual Execution Model**: MotherDuck's hybrid execution model minimizes data movement by intelligently deciding what to process locally vs. in the cloud. Data stays where it makes sense.

2. **No Data Upload Required**: You don't need to upload your CSV to the cloud first — DuckDB reads the local file directly while MotherDuck handles the cloud-side join.

3. **httpfs Extension**: For direct S3 access, DuckDB's httpfs extension allows reading/writing remote files directly from S3, so you can also query the S3 table directly if needed.

## Steps

1. **Connect to MotherDuck**: Run `ATTACH 'md:'` to connect your local DuckDB to MotherDuck
2. **Query directly**: Reference your local CSV with `read_csv()` and your S3/cloud table by its MotherDuck database path
3. **Execute**: The dual execution engine automatically optimizes where each part runs

This approach eliminates the need to move all your data to one location before querying — you get the speed of local processing combined with cloud scalability in a single SQL statement.
