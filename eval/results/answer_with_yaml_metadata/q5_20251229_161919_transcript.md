# Scenario: answer_with_yaml_metadata

**Status**: âœ… PASSED
**Timestamp**: 20251229_161919

**Duration**: 88.10 seconds
**Tokens Used**: 0

---

## Conversation


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ SCENARIO: answer_with_yaml_metadata
   Answer questions using pre-extracted YAML metadata (entities + claims) for fast retrieval
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


ğŸ” Verifying .claude installation...
   âœ“ .claude folder exists at /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_ou1wtb8o/.claude
   âœ“ Skills: 0 found
   âœ“ Commands: 0 found

ğŸ” Filtering to questions: [5]

ğŸ§ª Running 1 of 10 question(s) defined in questions_motherduck.yaml


======================================================================
â“ Question 5/10
...How do I set up MotherDuck to work with dbt?
======================================================================


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 5
â”‚ Answer this question using the pre-extracted metadata files in .kurt/sources/motherduck.com/.metadata/:

...How do I set up MotherDuck to work with dbt?

## Available Metadata Files

You have access to two YAML metadata files that index all the source documents:

1. `.kurt/sources/motherduck.com/.metadata/entities.yaml` - Index of 2000+ entities with:
   - name, type, description, keywords, aliases
   - sources: list of documents where this entity is discussed

2. `.kurt/sources/motherduck.com/.metadata/claims.yaml` - 3000+ pre-extracted factual claims with:
   - statement: the actual fact
   - type: definition, instruction, capability, limitation, etc.
   - keywords: searchable terms
   - source: document path and quote

## Efficient Workflow (IMPORTANT - follow this order!)

1. **First, grep claims.yaml** for keywords from the question (e.g., "dbt", "version", "not supported")
   - This gives you pre-extracted facts that may directly answer the question
   - Note the source documents mentioned in relevant claims

2. **If needed, grep entities.yaml** to find related entities and their source documents
   - This helps you find additional relevant documents

3. **Read specific source documents** to verify claims and get additional context
   - Only read documents that were referenced in the metadata
   - Aim to read 2-3 most relevant documents (not 5-6!)

4. **Write your answer** to: /tmp/answer_yaml_metadata_5.md

## Example Grep Commands

```bash
# Search for claims about dbt
grep -i "dbt" .kurt/sources/motherduck.com/.metadata/claims.yaml

# Search for limitation claims
grep -i "limitation\|not supported" .kurt/sources/motherduck.com/.metadata/claims.yaml

# Search for entity sources
grep -A5 "name: dbt" .kurt/sources/motherduck.com/.metadata/entities.yaml
```

## Answer Format

```markdown
# Answer

[Your comprehensive answer here]

## Sources

List each source file you read:
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description
```

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [96mğŸ”§ TOOL:[0m Bash â†’ grep -i "dbt" /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_ou1wtb8o/.kurt/sources/motherduck.com/.metadata/claims.yaml | head -100
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m     doc: motherduck.com/blog/solving-advent-code-duckdb-dbt.md
  [2m  â”‚[0m     doc: motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md
  [2m  â”‚[0m     orchestration, dbt for transformation, and Fivetran for data extraction.
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m       dbt for transformation, and Fivetran for data extraction.
  [2m  â”‚[0m     doc: motherduck.com/blog/motherduck-dbt-pipelines.md
  [2m  â”‚[0m     doc: motherduck.com/blog/llm-data-pipelines-prompt-motherduck-dbt.md
  [2m  â”‚[0m     doc: motherduck.com/blog/motherduck-duckdb-dbt.md
  [2m  â”‚[0m     doc: motherduck.com/blog/solving-advent-code-duckdb-dbt.md
  [2m  â”‚[0m     doc: motherduck.com/blog/motherduck-duckdb-dbt.md
  [2m  â”‚[0m     doc: motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md
  [2m  â”‚[0m - statement: Coalesce by dbt labs is happening in multiple locations.
  [2m  â”‚[0m   entity: Coalesce by dbt labs
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m     quote: Coalesce by dbt labs is happening in multiple locations.
  [2m  â”‚[0m     doc: motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md
  [2m  â”‚[0m     doc: motherduck.com/blog/motherduck-duckdb-dbt.md
  [2m  â”‚[0m     doc: motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md
  [2m  â”‚[0m - statement: dbt can output data in CSV format.
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m     quote: dbt can output data in CSV format.
  [2m  â”‚[0m - statement: The switch from Airflow to Dagster was driven by seamless dbt integration.
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m     quote: The switch from Airflow to Dagster was driven by seamless dbt integration.
  [2m  â”‚[0m     doc: motherduck.com/blog/solving-advent-code-duckdb-dbt.md
  [2m  â”‚[0m     doc: motherduck.com/blog/llm-data-pipelines-prompt-motherduck-dbt.md
  [2m  â”‚[0m     doc: motherduck.com/blog/motherduck-duckdb-dbt.md
  [2m  â”‚[0m - statement: A modern dev-to-prod workflow combines this with tools like dbt.
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m     quote: A modern dev-to-prod workflow combines this with tools like dbt (data build
  [2m  â”‚[0m - statement: dbt can read data from S3.
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m     quote: dbt can read data from S3.
  [2m  â”‚[0m - statement: dbt can work with Iceberg for data modeling.
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m     quote: dbt can work with Iceberg for data modeling.
  [2m  â”‚[0m - statement: dbt doesn't support incremental loading when writing to an external source
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m     quote: dbt doesn't support incremental loading when writing to an external source
  [2m  â”‚[0m - statement: dbt enables data analysts and engineers to transform data in their warehouse
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m - statement: dbt is a great and straightforward tool for building production-ready
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m     doc: motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md
  [2m  â”‚[0m     quote: dbt is a great and straightforward tool for building production-ready data
  [2m  â”‚[0m - statement: dbt is improving unit tests in its April 1.8 release.
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m     doc: motherduck.com/blog/dual-execution-dbt.md
  [2m  â”‚[0m     quote: dbt is improving unit tests in its April 1.8 release.
  [2m  â”‚[0m - statement: dbt is particularly useful for implementing the ELT (Extract, Load, Transform)
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m - statement: dbt is used for transforming data in DuckDB.
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m - statement: dbt is used for transforming data in the cloud.
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m     quote: dual execution in data build tools (dbt)
  [2m  â”‚[0m - statement: dbt profiles can specify duckdb as a database type.
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m     quote: In the example dbt profile below, `prod` runs entirely in the cloud, while
  [2m  â”‚[0m - statement: dbt runs dropped from 8 hours to 2 minutes.
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m     quote: With dbt runs dropping from 8 hours to 2 minutes and queries returning
  [2m  â”‚[0m - statement: dbt runs full refreshes by default, which recreates the table each time
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m     quote: By default, dbt runs full refreshes, which recreates the table each time
  [2m  â”‚[0m - statement: dbt supports a wide range of data platforms, including Snowflake, BigQuery,
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m     with dbt and DuckDB.
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m - statement: I use dbt daily during my job as an analytics engineer.
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m     quote: I use dbt daily during my job as an analytics engineer...
  [2m  â”‚[0m - statement: I was able to build a dbt project based on the TPC-DS dataset in MotherDuck
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m     quote: I was able to build a dbt project based on the TPC-DS dataset (about 40GB
  [2m  â”‚[0m - statement: Nightly dbt Cloud jobs that took 8 hours were replaced with a dbt Core
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m     quote: Nightly dbt Cloud jobs that took 8 hours were replaced with a dbt Core
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m       starts to be a reality through the usage of dbt.
  [2m  â”‚[0m - statement: The next blog will dive into the transformation layer using dbt duckdb.
  [2m  â”‚[0m   entity: dbt
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ grep -i -B2 -A10 "setup\|install\|configure\|connect" /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_ou1wtb8o/.kurt/sources/motherduck.com/.metadata/claims.yaml | grep -i "dbt" -A10 -B2 | head -80
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m   - immediately
  [2m  â”‚[0m   source:
  [2m  â”‚[0m     doc: motherduck.com/blog/llm-data-pipelines-prompt-motherduck-dbt.md
  [2m  â”‚[0m     quote: It's an in-process database that you can literally pip install duckdb and
  [2m  â”‚[0m       start using immediately.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 98
  [2m  â”‚[0m - statement: DuckDB is being selected for more projects due to its developer productivity.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: DuckDB
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - duckdb
  [2m  â”‚[0m --
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 175
  [2m  â”‚[0m - statement: The dual execution setup can lead to a 5X increase in dbt run speeds.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: DuckDB
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - dual
  [2m  â”‚[0m   - execution
  [2m  â”‚[0m   - setup
  [2m  â”‚[0m   - lead
  [2m  â”‚[0m   - 5x
  [2m  â”‚[0m   - increase
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m   - run
  [2m  â”‚[0m   - speeds
  [2m  â”‚[0m   source:
  [2m  â”‚[0m     doc: motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md
  [2m  â”‚[0m     quote: with an approximate 5X increase in dbt run speeds by minimizing local data
  [2m  â”‚[0m       size and optimizing compute resources.
  [2m  â”‚[0m --
  [2m  â”‚[0m     doc: motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md
  [2m  â”‚[0m     quote: The DuckDB Postgres Extension offers the most straightforward approach,
  [2m  â”‚[0m       requiring minimal setup...
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 98
  [2m  â”‚[0m - statement: The duckdb-docker repository provides a small Docker image that exposes
  [2m  â”‚[0m     the DuckDB CLI.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m --
  [2m  â”‚[0m   source:
  [2m  â”‚[0m     doc: motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md
  [2m  â”‚[0m     quote: Speed up dbt development using local dev and cloud prod setups with DuckDB
  [2m  â”‚[0m       and MotherDuck.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 90
  [2m  â”‚[0m - statement: Using DuckDB significantly speeds up the pipeline compared to Pyspark.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: DuckDB
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - using
  [2m  â”‚[0m   - duckdb
  [2m  â”‚[0m --
  [2m  â”‚[0m     char_end: 111
  [2m  â”‚[0m - statement: Datacoves and MotherDuck make it possible to connect directly to S3,
  [2m  â”‚[0m     build dbt models in DuckLake, and visualize the results.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: DuckLake
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - datacoves
  [2m  â”‚[0m   - motherduck
  [2m  â”‚[0m   - make
  [2m  â”‚[0m   - possible
  [2m  â”‚[0m   - connect
  [2m  â”‚[0m   - directly
  [2m  â”‚[0m   - s3
  [2m  â”‚[0m   - build
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m   - models
  [2m  â”‚[0m   - ducklake
  [2m  â”‚[0m   - visualize
  [2m  â”‚[0m   - results
  [2m  â”‚[0m   source:
  [2m  â”‚[0m     doc: motherduck.com/videos/what-if-sql-queries-returned-results-instantly.md
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ grep -i -A5 "name: dbt" /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_ou1wtb8o/.kurt/sources/motherduck.com/.metadata/entities.yaml
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m - name: dbt
  [2m  â”‚[0m   type: Technology
  [2m  â”‚[0m   description: A command line tool that enables data analysts and engineers to transform
  [2m  â”‚[0m     data in their warehouse more effectively.
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m --
  [2m  â”‚[0m - name: dbt Coalesce
  [2m  â”‚[0m   type: Topic
  [2m  â”‚[0m   description: A conference focused on data transformation and analytics using dbt.
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m   - coalesce
  [2m  â”‚[0m --
  [2m  â”‚[0m - name: dbt Core
  [2m  â”‚[0m   type: Technology
  [2m  â”‚[0m   description: A tool for transforming data within MotherDuck.
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m   - core
  [2m  â”‚[0m --
  [2m  â”‚[0m - name: dbt MCP
  [2m  â”‚[0m   type: Product
  [2m  â”‚[0m   description: A MCP server provides tools to interact with dbt autonomously.
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m   - mcp
  [2m  â”‚[0m --
  [2m  â”‚[0m - name: dbt Semantic Layer
  [2m  â”‚[0m   type: Integration
  [2m  â”‚[0m   description: A tool for transforming data in the warehouse.
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m   - semantic
  [2m  â”‚[0m --
  [2m  â”‚[0m - name: dbt-unit-testing
  [2m  â”‚[0m   type: Product
  [2m  â”‚[0m   description: A dbt package for unit testing dbt models.
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - dbt
  [2m  â”‚[0m   - unit
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ grep -i "dbt-duckdb\|dbt adapter\|pip install\|profiles.yml\|profile" /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_ou1wtb8o/.kurt/sources/motherduck.com/.metadata/claims.yaml | head -50
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m - statement: dbt profiles can specify duckdb as a database type.
  [2m  â”‚[0m   - profiles
  [2m  â”‚[0m     quote: In the example dbt profile below, `prod` runs entirely in the cloud, while
  [2m  â”‚[0m - statement: DuckDB can load AWS credentials based on a profile name.
  [2m  â”‚[0m   - profile
  [2m  â”‚[0m     quote: This function will load AWS credentials based on a profile name.
  [2m  â”‚[0m     quote: dbt-duckdb creates a dev.duckdb file in your project directory during a
  [2m  â”‚[0m - statement: DuckDB is an in-process database that you can literally pip install and
  [2m  â”‚[0m     quote: It's an in-process database that you can literally pip install duckdb and
  [2m  â”‚[0m - statement: Migrating a Spark pipeline to dbt-duckdb can run in under a second.
  [2m  â”‚[0m     quote: Hereâ€™s how we can replace it with a simple, elegant `dbt-duckdb` project
  [2m  â”‚[0m     quote: Jake Thomas providing the first R2 catalog, see [his post](https://bsky.app/profile/jakthom.bsky.social/post/3lb4y65z24k2q)
  [2m  â”‚[0m - statement: Josh Wills is the author of dbt-duckdb.
  [2m  â”‚[0m     quote: Josh Wills, author of dbt-duckdb.
  [2m  â”‚[0m - statement: The feature 'profile' is used by 373 distinct users.
  [2m  â”‚[0m   entity: profile
  [2m  â”‚[0m   - profile
  [2m  â”‚[0m - statement: Niels contributed to the dbt adapter for DuckDB.
  [2m  â”‚[0m     quote: He recently contributed to the dbt adapter for DuckDB.
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me search for more specific setup instructions and find the relevant source documents:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ grep -i -B2 -A15 "dbt-duckdb\|motherduck.*dbt\|dbt.*motherduck" /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_ou1wtb8o/.kurt/sources/motherduck.com/.metadata/claims.yaml | head -100
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m   - problems
  [2m  â”‚[0m   source:
  [2m  â”‚[0m     doc: motherduck.com/blog/solving-advent-code-duckdb-dbt.md
  [2m  â”‚[0m     quote: Recursive CTEs are essential for many of the more challenging AoC problems.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 75
  [2m  â”‚[0m - statement: The author decided to use SQL for AoC.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: Advent of Code
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - author
  [2m  â”‚[0m   - decided
  [2m  â”‚[0m   - use
  [2m  â”‚[0m   - sql
  [2m  â”‚[0m   - aoc
  [2m  â”‚[0m   source:
  [2m  â”‚[0m     doc: motherduck.com/blog/getting-started-ducklake-table-format.md
  [2m  â”‚[0m --
  [2m  â”‚[0m   - work
  [2m  â”‚[0m   source:
  [2m  â”‚[0m     doc: motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md
  [2m  â”‚[0m     quote: The original pipeline was built on Hadoop. It was slow and incredibly difficult
  [2m  â”‚[0m       to work with.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 93
  [2m  â”‚[0m - statement: The struct_pack function is used to create STRUCTs in DuckDB.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: Amazon S3
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - struct
  [2m  â”‚[0m   - pack
  [2m  â”‚[0m   - function
  [2m  â”‚[0m   - used
  [2m  â”‚[0m   - create
  [2m  â”‚[0m   - structs
  [2m  â”‚[0m --
  [2m  â”‚[0m   - travel
  [2m  â”‚[0m   source:
  [2m  â”‚[0m     doc: motherduck.com/blog/motherduck-dbt-pipelines.md
  [2m  â”‚[0m     quote: Now that everyone seems to have stopped squabbling about whether to use
  [2m  â”‚[0m       Iceberg or Hudi.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 88
  [2m  â”‚[0m - statement: Iceberg provides built-in time travel capabilities.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: Apache Iceberg
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - iceberg
  [2m  â”‚[0m   - provides
  [2m  â”‚[0m   - built
  [2m  â”‚[0m   - time
  [2m  â”‚[0m   - travel
  [2m  â”‚[0m   - capabilities
  [2m  â”‚[0m --
  [2m  â”‚[0m   - methods
  [2m  â”‚[0m   source:
  [2m  â”‚[0m     doc: motherduck.com/blog/llm-data-pipelines-prompt-motherduck-dbt.md
  [2m  â”‚[0m     quote: my guess is that if someone from the BigQuery team updated the method of
  [2m  â”‚[0m       running the benchmark slightly, the results would look a lot better.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 141
  [2m  â”‚[0m - statement: BigQuery's pricing model can be costly for typical startup data volumes.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: BigQuery
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - bigquery
  [2m  â”‚[0m   - pricing
  [2m  â”‚[0m   - model
  [2m  â”‚[0m   - costly
  [2m  â”‚[0m   - typical
  [2m  â”‚[0m   - startup
  [2m  â”‚[0m --
  [2m  â”‚[0m   - capabilities
  [2m  â”‚[0m   source:
  [2m  â”‚[0m     doc: motherduck.com/blog/motherduck-duckdb-dbt.md
  [2m  â”‚[0m     quote: 'Bokeh: Interactive visualization library with geospatial capabilities.'
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 70
  [2m  â”‚[0m - statement: The Boring Semantic Layer uses Ibis for the execution layer.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: Boring Semantic Layer
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - boring
  [2m  â”‚[0m   - semantic
  [2m  â”‚[0m   - layer
  [2m  â”‚[0m   - uses
  [2m  â”‚[0m   - ibis
  [2m  â”‚[0m   - execution
  [2m  â”‚[0m   source:
  [2m  â”‚[0m     doc: motherduck.com/blog/solving-advent-code-duckdb-dbt.md
  [2m  â”‚[0m     quote: We use DuckDB as the query engine and Python with Ibis for the execution
  [2m  â”‚[0m       layer.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 79
  [2m  â”‚[0m - statement: The BSL includes some visualization capabilities with a built-in wrapper
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_ou1wtb8o/.kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_ou1wtb8o/.kurt/sources/motherduck.com/blog/dual-execution-dbt.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_ou1wtb8o/.kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     "content": "---\ntitle: motherduck-duckdb-dbt\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/motherduck-duckdb-dbt\nindexed_at: '2025-11-25T19:58:31.660396'\ncontent_hash: 4233f76fbdf40552\nhas_code_examples: true\nhas_step_by_step: true\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# MotherDuck + dbt: Better Together\n\n2023/09/07 - 5 min read\n\nBY\n\n[Sung Won Chung](https://motherduck.com/authors/sung-won-chung--/)\n\n## My Personal DuckDB Story\n\nDuckDB has been charming to me ever since I wrote [about it a year ago](https://roundup.getdbt.com/p/dbt-learning-to-love-software-engineers).\n\nIt gave me the glimmers of something I\u2019ve been begging for a long time: fast should be measured in seconds, not minutes.\n\n[I kicked the tires a lot when working at dbt Labs](https://github.com/dbt-labs/jaffle_shop_duckdb).\n\n- [And here](https://www.loom.com/share/ed4a6f59957e43158837eb4ba0c5ed67)\n\n- [And most recently here](https://www.loom.com/share/e213768457094a3187663a6cff76a61d?sid=29d6d696-0581-4b50-af45-7132dfb65f80)\n\n\nAnd in all the tire kicking, it has remained true to the glimmers it gave me and so much more. It\u2019s fast, easy, and cheap. And if it\u2019s running on your local computer, it\u2019s free.\n\nI\u2019ve had incredibly asymmetric expectations of how much money, time, and work it takes to make data fast and easy that I think to myself, \u201cOh, of course you\u2019re supposed to pay lots of dollars to run queries on millions/billions of rows per month.\u201d This has pleasantly disrupted that inner anchoring point. I see something more charming at play. Data teams can be productive with data bigger and work faster and save more money than they could have dreamed of 5 years ago. Heck! Even a year ago. So let\u2019s get into it.\n\n## Why use MotherDuck + dbt?\n\nWell, DuckDB and Motherduck\u2019s primary use case is solving analytical problems fast. Because of its columnar design, it\u2019s able to do just that. Even more so, the creators were smart about making integrations with adjacent data tools a first class experience. We see this with reading S3 files without copying them over and querying postgres directly without needing to extract and load it into DuckDB. And you don\u2019t need to define schemas or tedious configurations to make it work! Motherduck enables the multiplayer experience that having a single file on your machine is too tedious to pass around and synchronize with your teammates. Motherduck runs DuckDB on your behalf AND uses your local computer if the query you\u2019re running makes more sense to run there. You get dynamic execution out of the box. And that\u2019s pretty sweet.\n\nBut more than platitudes, let\u2019s get hands-on with working code so you can taste and see for yourself!\n\n## Get Started\n\nYou can follow along with [this repo](https://github.com/sungchun12/jaffle_shop_duckdb/tree/blog-guide):\n\n1. Signup for a [MotherDuck account!](https://motherduck.com/)![signup](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmotherduck_signup_33d4e9cf54.png%3Fupdated_at%3D2023-09-06T12%3A58%3A42.172Z&w=3840&q=75)\nNote : MotherDuck is still under private beta, but I heard you could get an invite if you join their [community slack](https://slack.motherduck.com/) with a good duck pun.\n\n2. Sign in and your screen should look like this minus some of the stuff you\u2019ll be building in the rest of this guide.\n![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fsignin_14a92fa2b0.png%3Fupdated_at%3D2023-09-06T12%3A58%3A41.340Z&w=3840&q=75)\n\n3. Click on the settings in the upper right hand corner and copy your Service Token to the clipboard.\n![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fservice_token_8fec7f0642.png%3Fupdated_at%3D2023-09-06T12%3A58%3A39.304Z&w=3840&q=75)\n\n4. Clone the repo and change directories into it.\n\n\n```bash\nCopy code\n\ngit clone -b blog-guide https://github.com/sungchun12/jaffle_shop_duckdb.git\ncd jaffle_shop_duckdb\n```\n\n5. Follow the detailed instructions to setup your [free AWS account and use S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/GetStartedWithS3.html):\n\n_Note: Feel free to skip this step if you already have an AWS account with S3 setup! Plus, MotherDuck has these data under their public S3 bucket at s3://us-prd-motherduck-open-datasets/jaffle\\_shop/csv/_\n\n6. Take the csv files stored in the git repo [here](https://github.com/sungchun12/jaffle_shop_duckdb/tree/blog-guide/seeds) and upload them into S3:\n![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fseeds_9b3753fd5d.png%3Fupdated_at%3D2023-09-06T12%3A58%3A41.504Z&w=3840&q=75)![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fs3_seeds_adf0454153.png%3Fupdated_at%3D2023-09-06T12%3A58%3A41.673Z&w=3840&q=75)\n\n7. [Copy the AWS S3 access keys to authenticate](https://docs.aws.amazon.com/powershell/latest/userguide/pstools-appendix-sign-up.html) your dbt project for later.\n\n\n## Configure Your dbt project\n\n_Note: Huge thanks to Josh Wills for creating the dbt-duckdb adapter and it works great with both DuckDB and MotherDuck: [https://github.com/jwills/dbt-duckdb](https://github.com/jwills/dbt-duckdb). This demo only works with DuckDB version 0.8.1: [https://motherduck.com/docs/intro](https://motherduck.com/docs/intro)_\n\n1. Adjust your `profiles.yml` for the naming conventions that make sense to you. Specifically, focus on schema.\n\n```yaml\nCopy code\n\njaffle_shop:\n\n  target: dev\n  outputs:\n    dev:\n      type: duckdb\n      schema: dev_sung\n      path: 'md:jaffle_shop'\n      threads: 16\n      extensions:\n        - httpfs\n      settings:\n        s3_region: \"{{ env_var('S3_REGION', 'us-west-1') }}\"\n        s3_access_key_id: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n        s3_secret_access_key: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n\n    dev_public_s3:\n      type: duckdb\n      schema: dev_sung\n      path: 'md:jaffle_shop'\n      threads: 16\n      extensions:\n        - httpfs\n      settings:\n        s3_region: \"{{ env_var('S3_REGION', 'us-east-1') }}\" # default region to make hello_public_s3.sql work correctly!\n        s3_access_key_id: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n        s3_secret_access_key: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n\n    prod:\n      type: duckdb\n      schema: prod_sung\n      path: 'md:jaffle_shop'\n      threads: 16\n      extensions:\n        - httpfs\n      settings:\n        s3_region: us-west-1\n        s3_access_key_id: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n        s3_secret_access_key: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n```\n\n2. Export your motherduck and S3 credentials to the terminal session, so your dbt project can authenticate to both\n\n```shell\nCopy code\n\n# all examples are fake\nexport motherduck_token=<your motherduck token> # aouiweh98229g193g1rb9u1\nexport S3_REGION=<your region> # us-west-1\nexport S3_ACCESS_KEY_ID=<your access key id> # haoiwehfpoiahpwohf\nexport S3_SECRET_ACCESS_KEY=<your secret access key> # jiaowhefa998333\n```\n\n3. Create a python virtual environment and install the packages to run this dbt project\n\n```shell\nCopy code\n\npython3 -m venv venv\nsource venv/bin/activate\npython3 -m pip install --upgrade pip\npython3 -m pip install -r requirements.txt\n```\n\n4. Run `dbt debug` to verify dbt can connect to motherduck and S3\n\n```shell\nCopy code\n\ndbt debug\n```\n\n5. Run `dbt build` to run and test the project!\n\n```shell\nCopy code\n\ndbt build\n```\n\n6. If you're feeling adventurous, run the below to reference a public s3 bucket provided by MotherDuck!\n\nImpacted dbt model\n\n```sql\nCopy code\n\n--filename: hello_public_s3.sql\n{% if target.name == 'dev_public_s3' %}\n\nSELECT * FROM 's3://us-prd-motherduck-open-datasets/jaffle_shop/csv/raw_customers.csv'\n\n{% else %}\n\nselect 1 as id\n\n{% endif %}\n```\n\n```shell\nCopy code\n\ndbt build --target dev_public_s3\n```\n\n7. Now, you should see everything ran with green font everywhere and you should see this in the UI! Including the S3 data you built a dbt model on top of!\n\n![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgreen_logs_ad3ec33dd1.png%3Fupdated_at%3D2023-09-06T12%3A58%3A39.718Z&w=3840&q=75)![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmotherduck_success_5f718670be.png%3Fupdated_at%3D2023-09-06T12%3A58%3A42.460Z&w=3840&q=75)\n\nThat\u2019s it! Ruffle up those feathers and start quacking and slapping those juicy SQL queries together to solve your analytics problems faster and cheaper than ever before!\n\n## Conclusion\n\nWe\u2019re at a really cool place where all I had to give you was a couple instructions to get you up and running with MotherDuck. I really hope the data industry gets to a place where we brag about the things we do NOT have to do vs. pride ourselves on complexity for its own sake. What matters is that we solve problems and spend time, money, and energy doing it where it\u2019s actually worth it to solve those problems. I\u2019m excited to see you all build MotherDuck guides far superior to mine. That\u2019s why this is so fun. We get to sharpen each other!\n\n_Want to know more about MotherDuck and dbt ? Checkout [MotherDuck & dbt documentation](https://motherduck.com/docs/integrations/transformation/dbt/) and have a look at their YouTube tutorial about DuckDB & dbt \ud83d\udc47_\n\nUnleashing DuckDB & dbt for local analytics triumphs - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Unleashing DuckDB & dbt for local analytics triumphs](https://www.youtube.com/watch?v=asxGh2TrNyI)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nFull screen is unavailable. [Learn More](https://support.google.com/youtube/answer/6276924)\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nMore videos\n\n## More videos\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=asxGh2TrNyI&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 8:54\n\n\u2022Live\n\n\u2022\n\n### TABLE OF CONTENTS\n\n[My Personal DuckDB Story](https://motherduck.com/blog/motherduck-duckdb-dbt/#my-personal-duckdb-story)\n\n[Why use MotherDuck + dbt?](https://motherduck.com/blog/motherduck-duckdb-dbt/#why-use-motherduck-dbt)\n\n[Get Started](https://motherduck.com/blog/motherduck-duckdb-dbt/#get-started)\n\n[Configure Your dbt project](https://motherduck.com/blog/motherduck-duckdb-dbt/#configure-your-dbt-project)\n\n[Conclusion](https://motherduck.com/blog/motherduck-duckdb-dbt/#conclusion)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![This Month in the DuckDB Ecosystem: August 2023](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fthumbnail_duckdb_newsletter_1_feb51165aa.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-nine/)\n\n[2023/08/21 - Marcos Ortiz](https://motherduck.com/blog/duckdb-ecosystem-newsletter-nine/)\n\n### [This Month in the DuckDB Ecosystem: August 2023](https://motherduck.com/blog/duckdb-ecosystem-newsletter-nine)\n\nThis Month in the DuckDB Ecosystem: August 2023\n\n[![DuckDB Tutorial For Beginners](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgetting_started_duckdb_thumbnail_70b197b1ab.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n[2024/10/31 - Mehdi Ouazza, Ryan Boyd](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n### [DuckDB Tutorial For Beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners)\n\nGet up to speed quickly with DuckDB, including installation, VSCode workflow integration and your first SQL analytics project.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 314,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 314
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_ou1wtb8o/.kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     "content": "---\ntitle: dual-execution-dbt\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/dual-execution-dbt\nindexed_at: '2025-11-25T19:57:41.215232'\ncontent_hash: 2b6c5ae395798479\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Local dev and cloud prod for faster dbt development\n\n2025/01/16 - 8 min read\n\nBY\n\n[Jacob Matson](https://motherduck.com/authors/jacob-matson/)\n\n## Introducktion\n\nI hate waiting for slow pipelines to run, so I am delighted to share some strategies to iterate on your data problems at maximum speed - MotherDuck even gave a talk on this concept at [dbt Coalesce in 2024](https://www.youtube.com/watch?v=oqwIHvSfOVQ). By harnessing the capabilities of DuckDB locally, backed by MotherDuck in the cloud, we can unlock an incredibly fast and efficient development cycle. We'll explore how to configure your dbt profile for dual execution and share some tips on how much data to bring local. By implementing these techniques, you can significantly accelerate your data pipeline development and iterate even faster to solve business problems.\n\nLooking to following along in the code?\n\nCheck out the [example repo](https://github.com/motherduckdb/motherduck-examples/tree/main/dbt-dual-execution)!\n\n![Instant feedback loop](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FInstant_feedback_loop_b99078a679.png&w=3840&q=75)\n\n## Setting up your Profile\n\nIn order to take advantage of these capabilities, we need to configure our dbt profile to execute in the correct place, as well as define the behavior that we want in our sources. In the example dbt profile below, `prod` runs entirely in the cloud, while `local` runs mostly on local but is also linked to MotherDuck for reading data into your local database.\n\n```yml\nCopy code\n\ndual_execution:\n  outputs:\n    local:\n      type: duckdb\n      path: local.db\n      attach:\n        - path: \"md:\"\t# attaches all MotherDuck databases\n    prod:\n      type: duckdb\n      path: \"md:jdw\"\n  target: local\n```\n\n## Sources & Models\n\nWith your sources, you need to define which ones to replicate entirely, which ones are ok as views, and which ones to sample. Keep in mind for sampling, you need to think about your data model and make sure that related samples are hydrated (i.e. if you only bring in 100 customers, you need to make sure you also bring in their orders too).\n\nIn my example project using TPC-DS as the source data, I am sampling 1% of the data when running locally on the large tables. In general, I am aiming to keep the datasets less than a million rows per table, although there is no hard limit. For the remaining tables, I am replicating the entire data set locally since they are so small.\n\nThe way that we conditionally sample our models is by using the [\u2018target\u2019 variable](https://docs.getdbt.com/reference/dbt-jinja-functions/target). You can add this parameter by checking your `target` and running it conditionally on your model.\n\nAn example sql snippet is below (using jinja).\n\n```sql\nCopy code\n\nfrom {{ source(\"tpc-ds\", \"catalog_sales\") }}\n{% if target.name == 'local' %} using sample 1 % {% endif %}\n```\n\nAs an example of a simple \u201ccreate local table from cloud\u201d, consider the following query plan. The \u201cL\u201d indicates Local and the \u201cR\u201d indicates Remote (i.e. MotherDuck).\n\n```bash\nCopy code\n\n\ud83e\udd86 explain create table\n        \"local\".\"main\".\"call_center\"\n      as (\n        from \"jdw_dev\".\"jdw_tpcds\".\"call_center\"\n      );\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502\u2502       Physical Plan       \u2502\u2502\n\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 BATCH_CREATE_TABLE_AS (L) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    DOWNLOAD_SOURCE (L)    \u2502\n\u2502    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n\u2502        bridge_id: 1       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  BATCH_DOWNLOAD_SINK (R)  \u2502\n\u2502    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n\u2502        bridge_id: 1       \u2502\n\u2502       parallel: true      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       SEQ_SCAN  (R)       \u2502\n\u2502    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n\u2502        call_center        \u2502\n\u2502                           \u2502\n\u2502        Projections:       \u2502\n\u2502     cc_call_center_sk     \u2502\n\u2502     cc_call_center_id     \u2502\n\u2502     cc_rec_start_date     \u2502\n\u2502      cc_rec_end_date      \u2502\n\u2502     cc_closed_date_sk     \u2502\n\u2502      cc_open_date_sk      \u2502\n\u2502          cc_name          \u2502\n\u2502          cc_class         \u2502\n\u2502        cc_employees       \u2502\n\u2502          cc_sq_ft         \u2502\n\u2502          cc_hours         \u2502\n\u2502         cc_manager        \u2502\n\u2502         cc_mkt_id         \u2502\n\u2502        cc_mkt_class       \u2502\n\u2502        cc_mkt_desc        \u2502\n\u2502     cc_market_manager     \u2502\n\u2502        cc_division        \u2502\n\u2502      cc_division_name     \u2502\n\u2502         cc_company        \u2502\n\u2502      cc_company_name      \u2502\n\u2502      cc_street_number     \u2502\n\u2502       cc_street_name      \u2502\n\u2502       cc_street_type      \u2502\n\u2502      cc_suite_number      \u2502\n\u2502          cc_city          \u2502\n\u2502         cc_county         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nThis can also be extended to your `sources.yml` if necessary for testing local datasets (i.e. json or parquet on experimental pipelines that have not yet made it to your data lake). Configuring these is similar:\n\n```ini\nCopy code\n\n{%if- target.name == 'local' -%}\n   meta:\n      external_location:\n        data/tpcds/{name}.parquet\n{%- endif -%}\n```\n\n## Running your pipeline\n\nOnce you have this configuration in place, you can simply run your pipeline as normal, although for ease of use, you may want to add tags to the models that you are working on so you can avoid going back to the cloud data set too often. This can be set simply in the `dbt_project.yml` like this:\n\n```yml\nCopy code\n\nmodels:\n  dual_execution:\n    tpcds:\n      raw:\n        +tags: ['raw']\n        +materialized: table\n      queries:\n        +materialized: view\n        +tags: ['queries']\n```\n\nFrom there, it is as simple as running `dbt build -s tag:raw` to load your raw data and then for subsequent query iteration, run `dbt build -s tag:queries` in the CLI. The subsequent runs can be visualized like this:\n\n![data flow cloud to local](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdata_flow_cloud_to_local_792a293673.png&w=3840&q=75)\n\n## Shipping dev to the cloud\n\nCertain tables may need to be available in your cloud data warehouse for testing even in the local workflow. This may be something like a BI tool, that is connected to your cloud instance and is difficult to run locally. This can be accomplished by setting the database attribute in your model, so that after the model is run, it is available in the cloud as well.\n\n```yml\nCopy code\n\n{{ config(\n    database=\"jdw_dev\",\n    schema=\"local_to_prod\"\n    materialized=\"table\"\n) }}\n```\n\nIt should be noted that this is a static configuration that is best used for testing. If you don\u2019t want to manually flip models between dev / prod destinations, you can define the database as an attribute of a specific model in your `dbt_project.yml` file.\n\n## Wrapping up\n\nAs you can see from this example, using MotherDuck\u2019s dual execution allows us to leverage the unique value proposition of DuckDB to run an accelerated development cycle on your local machine. With some basic optimization, we can get ~5x faster dbt runs by making the data smaller and using local compute. This is a very powerful combination for rapidly iterating on your pipeline and then pushing a high quality change back into your production environment.\n\nWant to learn more? Join our webinar about Local Dev & Cloud Prod on [February 13th, 2025](https://lu.ma/0die8ual?utm_source=blog).\n\n### TABLE OF CONTENTS\n\n[Introducktion](https://motherduck.com/blog/dual-execution-dbt/#introducktion)\n\n[Setting up your Profile](https://motherduck.com/blog/dual-execution-dbt/#setting-up-your-profile)\n\n[Sources & Models](https://motherduck.com/blog/dual-execution-dbt/#sources-models)\n\n[Running your pipeline](https://motherduck.com/blog/dual-execution-dbt/#running-your-pipeline)\n\n[Shipping dev to the cloud](https://motherduck.com/blog/dual-execution-dbt/#shipping-dev-to-the-cloud)\n\n[Wrapping up](https://motherduck.com/blog/dual-execution-dbt/#wrapping-up)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![What\u2019s New: Streamlined User Management, Metadata, and UI Enhancements](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FMother_Duck_Feature_Roundup_2_47f5d902c0.png&w=3840&q=75)](https://motherduck.com/blog/data-warehouse-feature-roundup-dec-2024/)\n\n[2024/12/21 - Sheila Sitaram](https://motherduck.com/blog/data-warehouse-feature-roundup-dec-2024/)\n\n### [What\u2019s New: Streamlined User Management, Metadata, and UI Enhancements](https://motherduck.com/blog/data-warehouse-feature-roundup-dec-2024)\n\nDecember\u2019s feature roundup is focused on improving the user experience on multiple fronts. Introducing the User Management REST API, the Table Summary, and a read-only MD\\_INFORMATION\\_SCHEMA for metadata.\n\n[![DuckDB Ecosystem: January 2025](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fnewsletter_a65cff5430.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025/)\n\n[2025/01/10 - Simon Sp\u00e4ti](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025/)\n\n### [DuckDB Ecosystem: January 2025](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025)\n\nDuckDB Monthly #25: PyIceberg, 0$ data distribution and more!\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 245,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 245
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now I have the information I need. Let me write the answer:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_yaml_metadata_5.md

  [96mğŸ”§ TOOL:[0m Bash â†’ cat > /tmp/answer_yaml_metadata_5.md << 'EOF'
# Answer

## How to Set Up MotherDuck to Work with dbt

Setting up MotherDuck with dbt involves installing the dbt-duckdb adapter, configuring your profiles.yml, and authenticating with your MotherDuck token. Here's the complete setup process:

### 1. Prerequisites

- Sign up for a MotherDuck account at [motherduck.com](https://motherduck.com)
- Get your Service Token from the MotherDuck UI (Settings > Service Token)

### 2. Install the dbt-duckdb Adapter

The dbt-duckdb adapter (created by Josh Wills) works with both DuckDB and MotherDuck:

```bash
python3 -m venv venv
source venv/bin/activate
python3 -m pip install dbt-duckdb
```

### 3. Configure Your profiles.yml

Create or edit your `~/.dbt/profiles.yml` file. The key is using the `md:` prefix in the `path` parameter to connect to MotherDuck:

**Basic Configuration:**
```yaml
your_project:
  target: dev
  outputs:
    dev:
      type: duckdb
      schema: dev_yourname
      path: 'md:your_database_name'
      threads: 16
```

**Dual Execution (Local Dev + Cloud Prod):**
```yaml
dual_execution:
  outputs:
    local:
      type: duckdb
      path: local.db
      attach:
        - path: "md:"  # attaches all MotherDuck databases
    prod:
      type: duckdb
      path: "md:your_database"
  target: local
```

**With S3 Integration:**
```yaml
your_project:
  target: dev
  outputs:
    dev:
      type: duckdb
      schema: dev_yourname
      path: 'md:your_database'
      threads: 16
      extensions:
        - httpfs
      settings:
        s3_region: "{{ env_var('S3_REGION', 'us-west-1') }}"
        s3_access_key_id: "{{ env_var('S3_ACCESS_KEY_ID') }}"
        s3_secret_access_key: "{{ env_var('S3_SECRET_ACCESS_KEY') }}"
```

### 4. Set Environment Variables

Export your MotherDuck token (and S3 credentials if needed):

```bash
export motherduck_token=<your_motherduck_token>
export S3_REGION=<your_region>  # if using S3
export S3_ACCESS_KEY_ID=<your_access_key>  # if using S3
export S3_SECRET_ACCESS_KEY=<your_secret>  # if using S3
```

### 5. Verify Connection

```bash
dbt debug
```

### 6. Run Your Project

```bash
dbt build
```

### Advanced: Conditional Sampling for Local Development

You can use the `target` variable in your models to sample data when running locally:

```sql
from {{ source("your_source", "large_table") }}
{% if target.name == 'local' %} using sample 1 % {% endif %}
```

### Key Benefits

- **Fast iterations**: dbt runs can drop from hours to minutes using local DuckDB for development
- **Dual execution**: Run development locally while production runs in the cloud
- **S3 integration**: Query data directly from S3 without copying it
- **Approximately 5x faster** dbt runs by minimizing local data size and optimizing compute resources

## Sources

- .kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md - Comprehensive tutorial on setting up MotherDuck with dbt, including profiles.yml configuration, environment variables, and S3 integration
- .kurt/sources/motherduck.com/blog/dual-execution-dbt.md - Guide for dual execution setup (local dev + cloud prod) with profile configuration and sampling strategies
EOF
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m 
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I've written the answer to `/tmp/answer_yaml_metadata_5.md`. 
  [94mâ”‚[0m 
  [94mâ”‚[0m Here's a summary of what I found:
  [94mâ”‚[0m 
  [94mâ”‚[0m **Setting up MotherDuck with dbt requires:**
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. **Install dbt-duckdb adapter** - `pip install dbt-duckdb` (created by Josh Wills)
  [94mâ”‚[0m 
  [94mâ”‚[0m 2. **Configure profiles.yml** with key settings:
  [94mâ”‚[0m    - `type: duckdb`
  [94mâ”‚[0m    - `path: 'md:your_database_name'` (the `md:` prefix connects to MotherDuck)
  [94mâ”‚[0m    - Set your schema and threads
  [94mâ”‚[0m 
  [94mâ”‚[0m 3. **Set environment variable**: `export motherduck_token=<your_token>`
  [94mâ”‚[0m 
  [94mâ”‚[0m 4. **Verify with**: `dbt debug` then `dbt build`
  [94mâ”‚[0m 
  [94mâ”‚[0m The sources also covered advanced topics like dual execution (local dev + cloud prod) and S3 integration for reading data directly from cloud storage.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 3,421 | Cost: $0.4248
     Cumulative: 3,421 tokens | $0.4248
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent has presented information in a structured format without posing any questions, signaling that it has completed the task.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 9 | Duration: 73.2s
â•‘    Tokens: 3,421 | Cost: $0.4248
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  Running LLM judge evaluation...
   ğŸ§  LLM Judge score: 1.00
