# Scenario: answer_with_yaml_metadata

**Status**: âœ… PASSED
**Timestamp**: 20251229_162306

**Duration**: 77.27 seconds
**Tokens Used**: 0

---

## Conversation


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ SCENARIO: answer_with_yaml_metadata
   Answer questions using pre-extracted YAML metadata (entities + claims) for fast retrieval
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


ğŸ” Verifying .claude installation...
   âœ“ .claude folder exists at /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_237prssm/.claude
   âœ“ Skills: 0 found
   âœ“ Commands: 0 found

ğŸ” Filtering to questions: [7]

ğŸ§ª Running 1 of 10 question(s) defined in questions_motherduck.yaml


======================================================================
â“ Question 7/10
...If I have a CSV on my laptop and a table in S3, what's the most efficient way to join them using MotherDuck?
======================================================================


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 7
â”‚ Answer this question using the pre-extracted metadata files in .kurt/sources/motherduck.com/.metadata/:

...If I have a CSV on my laptop and a table in S3, what's the most efficient way to join them using MotherDuck?

## Available Metadata Files

You have access to two YAML metadata files that index all the source documents:

1. `.kurt/sources/motherduck.com/.metadata/entities.yaml` - Index of 2000+ entities with:
   - name, type, description, keywords, aliases
   - sources: list of documents where this entity is discussed

2. `.kurt/sources/motherduck.com/.metadata/claims.yaml` - 3000+ pre-extracted factual claims with:
   - statement: the actual fact
   - type: definition, instruction, capability, limitation, etc.
   - keywords: searchable terms
   - source: document path and quote

## Efficient Workflow (IMPORTANT - follow this order!)

1. **First, grep claims.yaml** for keywords from the question (e.g., "dbt", "version", "not supported")
   - This gives you pre-extracted facts that may directly answer the question
   - Note the source documents mentioned in relevant claims

2. **If needed, grep entities.yaml** to find related entities and their source documents
   - This helps you find additional relevant documents

3. **Read specific source documents** to verify claims and get additional context
   - Only read documents that were referenced in the metadata
   - Aim to read 2-3 most relevant documents (not 5-6!)

4. **Write your answer** to: /tmp/answer_yaml_metadata_7.md

## Example Grep Commands

```bash
# Search for claims about dbt
grep -i "dbt" .kurt/sources/motherduck.com/.metadata/claims.yaml

# Search for limitation claims
grep -i "limitation\|not supported" .kurt/sources/motherduck.com/.metadata/claims.yaml

# Search for entity sources
grep -A5 "name: dbt" .kurt/sources/motherduck.com/.metadata/entities.yaml
```

## Answer Format

```markdown
# Answer

[Your comprehensive answer here]

## Sources

List each source file you read:
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description
```

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [96mğŸ”§ TOOL:[0m Bash â†’ grep -i -A3 "csv\|local file\|laptop" /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_237prssm/.kurt/sources/motherduck.com/.metadata/claims.yaml | head -100

  [96mğŸ”§ TOOL:[0m Bash â†’ grep -i -A3 "s3\|join\|attach" /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_237prssm/.kurt/sources/motherduck.com/.metadata/claims.yaml | head -100
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m     storage like AWS S3.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: Amazon S3
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - practical
  [2m  â”‚[0m --
  [2m  â”‚[0m   - s3
  [2m  â”‚[0m   source:
  [2m  â”‚[0m     doc: motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md
  [2m  â”‚[0m     quote: A practical and cost-effective DR plan can be built by using cloud object
  [2m  â”‚[0m       storage like AWS S3.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 94
  [2m  â”‚[0m - statement: A single Graviton3 instance delivers 300 gigabytes per second of aggregate
  [2m  â”‚[0m --
  [2m  â”‚[0m   entity: Amazon S3
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - single
  [2m  â”‚[0m --
  [2m  â”‚[0m - statement: AWS S3 is used as a staging bucket for MotherDuck.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: Amazon S3
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - aws
  [2m  â”‚[0m   - s3
  [2m  â”‚[0m   - used
  [2m  â”‚[0m   - staging
  [2m  â”‚[0m   - bucket
  [2m  â”‚[0m --
  [2m  â”‚[0m     quote: 'S3 Staging Bucket: The name of your AWS S3 bucket.'
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 50
  [2m  â”‚[0m - statement: Keeping a warehouse warm for related analytical queries can be beneficial.
  [2m  â”‚[0m --
  [2m  â”‚[0m   entity: Amazon S3
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - keeping
  [2m  â”‚[0m --
  [2m  â”‚[0m   entity: Amazon S3
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - network
  [2m  â”‚[0m --
  [2m  â”‚[0m   entity: Amazon S3
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - pandas
  [2m  â”‚[0m --
  [2m  â”‚[0m - statement: Popular cloud storage providers include Amazon S3, Google Cloud Storage,
  [2m  â”‚[0m     and Microsoft Azure Blob Storage.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: Amazon S3
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - popular
  [2m  â”‚[0m --
  [2m  â”‚[0m   - s3
  [2m  â”‚[0m   - google
  [2m  â”‚[0m   - microsoft
  [2m  â”‚[0m   - azure
  [2m  â”‚[0m --
  [2m  â”‚[0m     quote: Popular cloud storage providers include Amazon S3, Google Cloud Storage,
  [2m  â”‚[0m       and Microsoft Azure Blob Storage.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 106
  [2m  â”‚[0m --
  [2m  â”‚[0m   entity: Amazon S3
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - postgresql
  [2m  â”‚[0m --
  [2m  â”‚[0m - statement: Reading data from cloud object storage like Amazon S3 is the slowest
  [2m  â”‚[0m     part of any query.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: Amazon S3
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - reading
  [2m  â”‚[0m --
  [2m  â”‚[0m   - s3
  [2m  â”‚[0m   - slowest
  [2m  â”‚[0m   - part
  [2m  â”‚[0m   - any
  [2m  â”‚[0m --
  [2m  â”‚[0m     quote: Reading data from cloud object storage like Amazon S3 is the slowest part
  [2m  â”‚[0m       of any query.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 87
  [2m  â”‚[0m - statement: S3 organizes data into 'buckets' which act as containers for objects.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: Amazon S3
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - s3
  [2m  â”‚[0m   - organizes
  [2m  â”‚[0m   - data
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m     doc: motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md
  [2m  â”‚[0m     quote: Popular cloud storage providers include Amazon S3, Google Cloud Storage,
  [2m  â”‚[0m       and Microsoft Azure Blob Storage.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m --
  [2m  â”‚[0m     doc: motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md
  [2m  â”‚[0m     quote: Ascend
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 6
  [2m  â”‚[0m --
  [2m  â”‚[0m     doc: motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md
  [2m  â”‚[0m     quote: 'BigQuery''s on-demand pricing model presents a different kind of financial
  [2m  â”‚[0m       risk: the ''scan-based pricing trap.'''
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m --
  [2m  â”‚[0m     doc: motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md
  [2m  â”‚[0m     quote: Costs can be controlled by enforcing query best practices, such as always
  [2m  â”‚[0m       using a WHERE clause.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m --
  [2m  â”‚[0m     doc: motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md
  [2m  â”‚[0m     quote: Organizations can implement hard financial guardrails by setting project-level
  [2m  â”‚[0m       and user-level quotas.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m --
  [2m  â”‚[0m     doc: motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md
  [2m  â”‚[0m     quote: Unlike traditional row-based storage systems, columnar storage groups data
  [2m  â”‚[0m       by column rather than by row, allowing for faster data retrieval and compression.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m --
  [2m  â”‚[0m     doc: motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md
  [2m  â”‚[0m     quote: The SELECT statement is your playground. You can perform filtering, transformations,
  [2m  â”‚[0m       and even simple aggregations as part of your conversion pipeline.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m --
  [2m  â”‚[0m   entity: CSV
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - astrodata
  [2m  â”‚[0m --
  [2m  â”‚[0m   entity: CSV
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - bigquery
  [2m  â”‚[0m --
  [2m  â”‚[0m - statement: Common alternatives to Parquet include row-based formats like CSV and
  [2m  â”‚[0m     Avro, and other columnar formats like Apache ORC.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: CSV
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - common
  [2m  â”‚[0m --
  [2m  â”‚[0m   - csv
  [2m  â”‚[0m   - avro
  [2m  â”‚[0m   - columnar
  [2m  â”‚[0m   - apache
  [2m  â”‚[0m --
  [2m  â”‚[0m     quote: Common alternatives to Parquet include row-based formats like CSV and Avro,
  [2m  â”‚[0m       and other columnar formats like Apache ORC.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 119
  [2m  â”‚[0m - statement: 'CSV Error on Line: 35365 indicates a conversion issue.'
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: CSV
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - csv
  [2m  â”‚[0m   - error
  [2m  â”‚[0m   - line
  [2m  â”‚[0m   - '35365'
  [2m  â”‚[0m --
  [2m  â”‚[0m     doc: motherduck.com/blog/csv-files-persist-duckdb-solution.md
  [2m  â”‚[0m     quote: 'Conversion Error: CSV Error on Line: 35365'
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 42
  [2m  â”‚[0m - statement: CSV files are a straightforward, text-based format that organizes data
  [2m  â”‚[0m     in a tabular form.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: CSV
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - csv
  [2m  â”‚[0m   - files
  [2m  â”‚[0m   - straightforward
  [2m  â”‚[0m   - text
  [2m  â”‚[0m --
  [2m  â”‚[0m     quote: CSV files are a straightforward, text-based format that organizes data
  [2m  â”‚[0m       in a tabular form.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 89
  [2m  â”‚[0m - statement: CSV files are still relevant in data management.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: CSV
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - csv
  [2m  â”‚[0m   - files
  [2m  â”‚[0m   - still
  [2m  â”‚[0m   - relevant
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ grep -i -A3 "hybrid\|local.*cloud\|read_csv\|dual" /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_237prssm/.kurt/sources/motherduck.com/.metadata/claims.yaml | head -100

  [96mğŸ”§ TOOL:[0m Bash â†’ grep -i -A5 "name: hybrid execution\|name: dual execution" /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_237prssm/.kurt/sources/motherduck.com/.metadata/entities.yaml | head -50
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m - name: Dual Execution
  [2m  â”‚[0m   type: Feature
  [2m  â”‚[0m   description: A feature of MotherDuck that allows local data analysis while joining
  [2m  â”‚[0m     with cloud data.
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - dual
  [2m  â”‚[0m --
  [2m  â”‚[0m - name: Dual Execution model
  [2m  â”‚[0m   type: Feature
  [2m  â”‚[0m   description: A feature of MotherDuck that allows applications to write to both old
  [2m  â”‚[0m     and new databases simultaneously.
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - dual
  [2m  â”‚[0m --
  [2m  â”‚[0m - name: Dual Execution query planner
  [2m  â”‚[0m   type: Feature
  [2m  â”‚[0m   description: A feature of MotherDuck that allows for efficient data querying by
  [2m  â”‚[0m     minimizing data transfer.
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - dual
  [2m  â”‚[0m --
  [2m  â”‚[0m - name: hybrid execution
  [2m  â”‚[0m   type: Feature
  [2m  â”‚[0m   description: A model that allows queries to execute across local and cloud environments.
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - hybrid
  [2m  â”‚[0m   - execution
  [2m  â”‚[0m --
  [2m  â”‚[0m - name: hybrid execution model
  [2m  â”‚[0m   type: Feature
  [2m  â”‚[0m   description: An architecture that processes queries both locally and in the cloud.
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - hybrid
  [2m  â”‚[0m   - execution
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m     doc: motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md
  [2m  â”‚[0m     quote: Artificial intelligence systems that can perform tasks autonomously.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 68
  [2m  â”‚[0m --
  [2m  â”‚[0m     doc: motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md
  [2m  â”‚[0m     quote: Apache Iceberg is a middle ground, but its catalog is a hurdle.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 63
  [2m  â”‚[0m --
  [2m  â”‚[0m     doc: motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md
  [2m  â”‚[0m     quote: This transformation promises to make data analytics more accessible while
  [2m  â”‚[0m       maintaining the precision and version control benefits.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m --
  [2m  â”‚[0m     doc: motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md
  [2m  â”‚[0m     quote: Learn from Streamlit Developer Advocate Chanin Natasenamat
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 58
  [2m  â”‚[0m --
  [2m  â”‚[0m     quote: Meet Christophe, a dedicated individual who has been thriving in the field
  [2m  â”‚[0m       of data engineering for the past 8 years.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 116
  [2m  â”‚[0m --
  [2m  â”‚[0m     doc: motherduck.com/learn-more/hybrid-analytics-guide.md
  [2m  â”‚[0m     quote: These contracts specify the schema, quality standards, and ownership of
  [2m  â”‚[0m       a dataset.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m --
  [2m  â”‚[0m - statement: Data Fabric creates a unified data management layer that spans hybrid
  [2m  â”‚[0m     and multi-cloud environments.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: Data Fabric
  [2m  â”‚[0m --
  [2m  â”‚[0m   - hybrid
  [2m  â”‚[0m   - multi
  [2m  â”‚[0m   - cloud
  [2m  â”‚[0m   - environments
  [2m  â”‚[0m --
  [2m  â”‚[0m     quote: Data Fabric Creating a unified data management layer that spans hybrid
  [2m  â”‚[0m       and multi-cloud environments.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 100
  [2m  â”‚[0m --
  [2m  â”‚[0m     doc: motherduck.com/learn-more/hybrid-analytics-guide.md
  [2m  â”‚[0m     quote: allowing you to systematically clean data or mask PII before exposing it
  [2m  â”‚[0m       to a wider audience.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m --
  [2m  â”‚[0m - statement: 'David Neal will speak about ''Hybrid Queries: the Future of Data Analytics''.'
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: DataTune conference
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m --
  [2m  â”‚[0m   - hybrid
  [2m  â”‚[0m   - queries
  [2m  â”‚[0m   - future
  [2m  â”‚[0m   - data
  [2m  â”‚[0m --
  [2m  â”‚[0m     quote: 'David Neal will speak about â€œHybrid Queries: the Future of Data Analyticsâ€'
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 74
  [2m  â”‚[0m - statement: Datawisp is a no code analytics platform that lets anyone analyze data
  [2m  â”‚[0m --
  [2m  â”‚[0m     doc: motherduck.com/blog/dual-execution-dbt.md
  [2m  â”‚[0m     quote: dbt is improving unit tests in its April 1.8 release.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 53
  [2m  â”‚[0m --
  [2m  â”‚[0m     quote: dual execution in data build tools (dbt)
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 40
  [2m  â”‚[0m - statement: dbt profiles can specify duckdb as a database type.
  [2m  â”‚[0m --
  [2m  â”‚[0m - statement: Optimizations reduce the round trips needed for many Dual Execution queries
  [2m  â”‚[0m     from two to one.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: Dual Execution
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - optimizations
  [2m  â”‚[0m --
  [2m  â”‚[0m   - dual
  [2m  â”‚[0m   - execution
  [2m  â”‚[0m   - queries
  [2m  â”‚[0m   - two
  [2m  â”‚[0m --
  [2m  â”‚[0m     quote: Weâ€™ve made optimizations to reduce the round trips needed for many Dual
  [2m  â”‚[0m       Execution queries from two to one.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 106
  [2m  â”‚[0m - statement: The dual execution planner reduces network traffic by orders of magnitude
  [2m  â”‚[0m     compared to traditional methods.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: Dual Execution
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - dual
  [2m  â”‚[0m   - execution
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ grep -i -B2 -A5 "local data\|local file\|local.*join\|join.*cloud" /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_237prssm/.kurt/sources/motherduck.com/.metadata/claims.yaml | head -80

  [96mğŸ”§ TOOL:[0m Bash â†’ grep -i "dual execution\|hybrid" /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_237prssm/.kurt/sources/motherduck.com/.metadata/entities.yaml | head -30
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m   - doc: motherduck.com/learn-more/hybrid-analytics-guide.md
  [2m  â”‚[0m   - doc: motherduck.com/learn-more/hybrid-analytics-guide.md
  [2m  â”‚[0m   description: Creating a unified data management layer that spans hybrid and multi-cloud
  [2m  â”‚[0m   - hybrid
  [2m  â”‚[0m   - doc: motherduck.com/learn-more/hybrid-analytics-guide.md
  [2m  â”‚[0m   - doc: motherduck.com/learn-more/hybrid-analytics-guide.md
  [2m  â”‚[0m - name: Dual Execution
  [2m  â”‚[0m - name: Dual Execution model
  [2m  â”‚[0m - name: Dual Execution query planner
  [2m  â”‚[0m   description: A hands-on course teaching hybrid data workflows using DuckDB and MotherDuck.
  [2m  â”‚[0m   - hybrid
  [2m  â”‚[0m   - doc: motherduck.com/learn-more/hybrid-analytics-guide.md
  [2m  â”‚[0m   - doc: motherduck.com/learn-more/hybrid-analytics-guide.md
  [2m  â”‚[0m   description: A repository containing example code for implementing dual execution
  [2m  â”‚[0m - name: Hybrid Analytics
  [2m  â”‚[0m   - hybrid
  [2m  â”‚[0m - name: Hybrid Architecture
  [2m  â”‚[0m   - hybrid
  [2m  â”‚[0m - name: hybrid execution
  [2m  â”‚[0m   - hybrid
  [2m  â”‚[0m - name: hybrid execution model
  [2m  â”‚[0m   - hybrid
  [2m  â”‚[0m - name: Hybrid Search
  [2m  â”‚[0m   - hybrid
  [2m  â”‚[0m   - doc: motherduck.com/learn-more/hybrid-analytics-guide.md
  [2m  â”‚[0m   - doc: motherduck.com/learn-more/hybrid-analytics-guide.md
  [2m  â”‚[0m   - doc: motherduck.com/learn-more/hybrid-analytics-guide.md
  [2m  â”‚[0m   - doc: motherduck.com/learn-more/hybrid-analytics-guide.md
  [2m  â”‚[0m   - doc: motherduck.com/learn-more/hybrid-analytics-guide.md
  [2m  â”‚[0m   - doc: motherduck.com/learn-more/hybrid-analytics-guide.md
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m     quote: In the example dbt profile below, `prod` runs entirely in the cloud, while
  [2m  â”‚[0m       `local` runs mostly on local but is also linked to MotherDuck for reading data
  [2m  â”‚[0m       into your local database.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 179
  [2m  â”‚[0m - statement: dbt runs dropped from 8 hours to 2 minutes.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: dbt
  [2m  â”‚[0m --
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 133
  [2m  â”‚[0m - statement: A query on a local file runs entirely on your machine's resources.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: DuckDB
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - query
  [2m  â”‚[0m --
  [2m  â”‚[0m   source:
  [2m  â”‚[0m     doc: motherduck.com/learn-more/pandas-dataframes-guide.md
  [2m  â”‚[0m     quote: A query on a local file runs entirely on your machine's resources.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 66
  [2m  â”‚[0m - statement: A query scanning a 5 GB Apache Iceberg table took 5.093 seconds to run.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: DuckDB
  [2m  â”‚[0m --
  [2m  â”‚[0m   source:
  [2m  â”‚[0m     doc: motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md
  [2m  â”‚[0m     quote: with an approximate 5X increase in dbt run speeds by minimizing local data
  [2m  â”‚[0m       size and optimizing compute resources.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 113
  [2m  â”‚[0m - statement: The DuckDB CLI gets a fresh upgrade with syntax highlighting and thousands-separator
  [2m  â”‚[0m     support.
  [2m  â”‚[0m --
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 91
  [2m  â”‚[0m - statement: The new UI allows users to work with local data and data hosted in cloud
  [2m  â”‚[0m     object stores like S3.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: DuckDB
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m --
  [2m  â”‚[0m   source:
  [2m  â”‚[0m     doc: motherduck.com/blog/perf-is-not-enough.md
  [2m  â”‚[0m     quote: You can work with local data, data hosted in cloud object stores like S3.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 73
  [2m  â”‚[0m - statement: The new UI provides a notebook interface tailored for SQL and DuckDB
  [2m  â”‚[0m     workflows.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m --
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 125
  [2m  â”‚[0m - statement: You can write a single SQL query that joins local files with data in
  [2m  â”‚[0m     cloud storage.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: DuckLake
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m --
  [2m  â”‚[0m     doc: motherduck.com/videos/ducklake-big-data-small-coalesce-2025.md
  [2m  â”‚[0m     quote: Yes. With a hybrid analytics platform like MotherDuck, you can write a
  [2m  â”‚[0m       single SQL query that joins local files (e.g., CSVs) with data in cloud storage
  [2m  â”‚[0m       (e.g., Parquet files in S3).
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 179
  [2m  â”‚[0m - statement: You can write a single SQL query that seamlessly joins a local CSV or
  [2m  â”‚[0m     Parquet file on your laptop with a large dataset stored in the MotherDuck cloud.
  [2m  â”‚[0m --
  [2m  â”‚[0m   source:
  [2m  â”‚[0m     doc: motherduck.com/quacking-around-the-web.md
  [2m  â”‚[0m     quote: You could analyze a new batch of log files locally on your laptop and join
  [2m  â”‚[0m       them against a massive historical log table stored in MotherDuck to find trends.
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 155
  [2m  â”‚[0m - statement: Your own workload is the only benchmark that truly matters.
  [2m  â”‚[0m   type: definition
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I found excellent claims. Let me now read the most relevant source documents to get the full context:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_237prssm/.kurt/sources/motherduck.com/quacking-around-the-web.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_237prssm/.kurt/sources/motherduck.com/videos/ducklake-big-data-small-coalesce-2025.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_237prssm/.kurt/sources/motherduck.com/quacking-around-the-web.md",
  [2m  â”‚[0m     "content": "---\ntitle: quacking-around-the-web\ncontent_type: blog\nsource_url: https://motherduck.com/quacking-around-the-web\nindexed_at: '2025-11-25T20:38:11.864473'\ncontent_hash: 9458f0f2a47a011e\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n# Quacking Around the Web\n\nMotherDuck-related news, blog posts, podcasts, videos and more\n\nSearch and filter\n\nSearch and filter\n\nCLEAR ALL\n\nSearch\n\nType\n\nCLEAR ALL\n\n[![Tech Review: DuckLake - From Parquet to Powerhouse](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Fducklake_30495fc993.png&w=3840&q=75)](https://thefulldatastack.substack.com/p/tech-review-ducklake-from-parquet)\n\n[2025/10/09 - Hoyt Emerson](https://thefulldatastack.substack.com/p/tech-review-ducklake-from-parquet)\n\n## [Tech Review: DuckLake - From Parquet to Powerhouse](https://thefulldatastack.substack.com/p/tech-review-ducklake-from-parquet)\n\nHoyt Emerson, The Full Data Stack, explores how this sleek lakehouse solution delivers enterprise grade features with startling simplicity\n\n[![[Future Data] DuckLake: Learning from Cloud Data Warehouses to Build a Robust \u201cLakehouse\u201d](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Fcarnegie_mellon_03a8bba5a4.png&w=3840&q=75)](https://db.cs.cmu.edu/events/future-data-ducklake-learning-from-cloud-data-warehouses-to-build-a-robust-lakehouse/)\n\n[2025/10/06 - Carnegie Mellon Database Group](https://db.cs.cmu.edu/events/future-data-ducklake-learning-from-cloud-data-warehouses-to-build-a-robust-lakehouse/)\n\n## [\\[Future Data\\] DuckLake: Learning from Cloud Data Warehouses to Build a Robust \u201cLakehouse\u201d](https://db.cs.cmu.edu/events/future-data-ducklake-learning-from-cloud-data-warehouses-to-build-a-robust-lakehouse/)\n\nJordan spoke at the Carnegie Mellon Database Group about how metadata storage enables query performance and helps provide transactional semantics in modern data warehouses. He also talked about how the metadata story in popular open data formats take us several steps backwards. He sahred about how DuckLake makes metadata access work more closely to a traditional data warehouse, which solves a lot of problems\n\n[![ Is MotherDuck with DuckDB the FASTEST Lake House Solution Ever?](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Fthe_average_engineer_1a43cf3d09.png&w=3840&q=75)](https://www.youtube.com/watch?v=0HIKn8aDGOU)\n\n[2025/10/06 - The Average Engineer](https://www.youtube.com/watch?v=0HIKn8aDGOU)\n\n## [Is MotherDuck with DuckDB the FASTEST Lake House Solution Ever?](https://www.youtube.com/watch?v=0HIKn8aDGOU)\n\nWanting to learn about DuckDB and MotherDuck, is it the fastest and best Lake House option on the market? Can DuckDB and MotherDuck process large datasets? Find Out!\n\n[![Finally Trying MotherDuck (DuckDB Data Warehouse)\u2026I Did Not Expect This! \ud83e\udd86](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Fhoyt_emerson_10_25_fb71d47539.png&w=3840&q=75)](https://www.youtube.com/watch?v=TeKXwuYjAHs)\n\n[2025/10/04 - Hoyt Emerson](https://www.youtube.com/watch?v=TeKXwuYjAHs)\n\n## [Finally Trying MotherDuck (DuckDB Data Warehouse)\u2026I Did Not Expect This! \ud83e\udd86](https://www.youtube.com/watch?v=TeKXwuYjAHs)\n\nI just gave MotherDuck (DuckDB Data Warehouse) the attention it deserved and wow, I wasn\u2019t ready for what happened. This is a full reaction video where I explore it live, test how it works, and share my honest surprises along the way.\n\nIf you\u2019re into data engineering, analytics, or just curious about the next wave of lightweight cloud data tools, you\u2019ll want to stick around for this one.\n\n[![MotherDuck Explained: How the Next-Gen AI & Analytics Solution Fits Into Your Data Stack](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Fartifact_blog_10_25_dcddb3b9d7.png&w=3840&q=75)](https://www.artefact.com/blog/motherduck-explained-how-it-fits-into-your-data-stack-with-benchmarks/)\n\n[2025/10/02 - Paul Galotto](https://www.artefact.com/blog/motherduck-explained-how-it-fits-into-your-data-stack-with-benchmarks/)\n\n## [MotherDuck Explained: How the Next-Gen AI & Analytics Solution Fits Into Your Data Stack](https://www.artefact.com/blog/motherduck-explained-how-it-fits-into-your-data-stack-with-benchmarks/)\n\nAt Artefact, we constantly explore emerging technologies to separate real innovation from passing hype. In today\u2019s crowded data landscape, spotting the hidden gems that can truly make a difference is both a challenge and a thrill. MotherDuck is one of those promising technologies. When its arrival on the European market was announced at Big Data London and our partnership as of September 24th, we knew what we had to do: put it to the test, evaluate its potential, and share our insights.\n\n[![Your First Data Analysis with MotherDuck and DuckDB: From CSV to Insights in 5 Minutes](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Fcodecentric_blog_9_25_04c8bbdb43.png&w=3840&q=75)](https://www.codecentric.de/en/knowledge-hub/blog/your-first-data-analysis-with-duckdb-from-csv-to-insights-in-5-minutes)\n\n[2025/09/30 - Codecentric](https://www.codecentric.de/en/knowledge-hub/blog/your-first-data-analysis-with-duckdb-from-csv-to-insights-in-5-minutes)\n\n## [Your First Data Analysis with MotherDuck and DuckDB: From CSV to Insights in 5 Minutes](https://www.codecentric.de/en/knowledge-hub/blog/your-first-data-analysis-with-duckdb-from-csv-to-insights-in-5-minutes)\n\nIn this post, we'll explore how MotherDuck, powered by DuckDB, revolutionizes the way you interact with your data, particularly when dealing with CSV files. You'll learn how to quickly parse and filter even large datasets directly from your local machine, using familiar SQL syntax, and without the need for a dedicated server or extensive configuration. Get ready to transform your raw CSV data into valuable insights in a matter of minutes!\n\n[![Honest review of MotherDuck](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Fan_honest_review_of_motherduck_80064eaa34.png&w=3840&q=75)](https://dataengineeringcentral.substack.com/p/honest-review-of-motherduck)\n\n[2025/09/22 - Daniel Beach](https://dataengineeringcentral.substack.com/p/honest-review-of-motherduck)\n\n## [Honest review of MotherDuck](https://dataengineeringcentral.substack.com/p/honest-review-of-motherduck)\n\n...the MotherDuck signup flow is incredible.\n\nIt\u2019s a beautiful UI, step by step, snappy. Everything you would expect from something built in this century, and bodes well for the things we have yet to do.\n\nOne of the first things you will notice is the nice clean layout, one that doesn\u2019t leave you wondering where to click next or what to explore.\n\n[![DuckDB benchmarked against Spark](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FDuckdb_vs_spark_72a8e0d11a.png&w=3840&q=75)](https://blog.dataexpert.io/p/duckdb-can-be-100x-faster-than-spark)\n\n[2025/09/22 - Zach Wilson](https://blog.dataexpert.io/p/duckdb-can-be-100x-faster-than-spark)\n\n## [DuckDB benchmarked against Spark](https://blog.dataexpert.io/p/duckdb-can-be-100x-faster-than-spark)\n\nInstead of guessing whether Spark or DuckDB is faster, we built out some tests datasets with increasing number of rows and then time query benchmarks between both DuckDB and Spark to illustrate just how fast DuckDB is\n\n[![Introducing the Stacksync connector for MotherDuck](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fstacksync_image_5918379698.png&w=3840&q=75)](https://www.stacksync.com/blog/introducing-the-motherduck-connector)\n\n[2024/10/28 - Ruben @ Stacksync](https://www.stacksync.com/blog/introducing-the-motherduck-connector)\n\n## [Introducing the Stacksync connector for MotherDuck](https://www.stacksync.com/blog/introducing-the-motherduck-connector)\n\nData workflows can get messy with constant back-and-forth between local analytics and cloud solutions. But what if you could have the best of both worlds: lightweight, in-process analytics and scalable cloud power?\n\nThat's where Stacksync's new MotherDuck connector comes in, so you can quit ducking around with complex setups.\n\n[![It Depends Podcast: Jordan Tigani on how DuckDB and MotherDuck power Small Data](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2024_08_16_at_1_17_07_PM_2ca715fe97.png&w=3840&q=75)](https://www.youtube.com/watch?v=tip96F_u8lc)\n\n[2024/07/27 - Sanjeev Mohan](https://www.youtube.com/watch?v=tip96F_u8lc)\n\n## [It Depends Podcast: Jordan Tigani on how DuckDB and MotherDuck power Small Data](https://www.youtube.com/watch?v=tip96F_u8lc)\n\nJordan Tigani shares why he chose DuckDB as the foundation for MotherDuck. As a founding member of Google\u2019s BigQuery, Jordan\u2019s transition from Big Data to Small Data is particularly fascinating. It's quite the plot twist!\n\nJordan takes us on a geeky joyride through the database's internals and its unique differentiators. Beyond the technical deep dive, we also explore the intriguing backstory behind the quirky name, MotherDuck.\n\n[![SFGATE: Ducks Wandering San Francisco ](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fsfgate_ducks_v3_b097e51349.jpg&w=3840&q=75)](https://www.sfgate.com/local/article/sf-moscone-center-duck-costumes-19520869.php)\n\n[2024/06/18 - Greta Reich](https://www.sfgate.com/local/article/sf-moscone-center-duck-costumes-19520869.php)\n\n## [SFGATE: Ducks Wandering San Francisco](https://www.sfgate.com/local/article/sf-moscone-center-duck-costumes-19520869.php)\n\nThe SFGATE local news team writes about our duck mascots walking around Moscone in celebration of the Snowflake Summit, Data and AI Summit and the MotherDuck General Availability (GA) launch.\n\n[![PR Newswire: MotherDuck Announces General Availability](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmotherduck_goes_ga_160f03c1bb.jpg&w=3840&q=75)](https://www.prnewswire.com/news-releases/motherduck-announces-general-availability-brings-simplicity-and-power-of-duckdb-in-a-serverless-data-warehouse-302168749.html)\n\n[2024/06/11 - MotherDuck](https://www.prnewswire.com/news-releases/motherduck-announces-general-availability-brings-simplicity-and-power-of-duckdb-in-a-serverless-data-warehouse-302168749.html)\n\n## [PR Newswire: MotherDuck Announces General Availability](https://www.prnewswire.com/news-releases/motherduck-announces-general-availability-brings-simplicity-and-power-of-duckdb-in-a-serverless-data-warehouse-302168749.html)\n\nMotherDuck, builder of a serverless, easy to use data analytics platform based on open source database DuckDB, today announced the general availability of MotherDuck, providing the 99% of users who do not need a complex data infrastructure with the ability to use DuckDB's simplicity at scale. DuckDB 1.0, which was recently announced by DuckDB Foundation and Labs, provides a scale-up approach to analytics that is faster, cheaper and more user-friendly than distributed architectures.\n\n[![Office Hours with Tomasz Tunguz: Modern Data Architectures](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FOffice_Hours_with_Jordan_Tigani_Modern_Data_Architectures_0_1_screenshot_56b7bb8dbb.png&w=3840&q=75)](https://www.youtube.com/watch?v=j5dcfEVnfdI)\n\n[2024/05/06 - Tomasz Tunguz](https://www.youtube.com/watch?v=j5dcfEVnfdI)\n\n## [Office Hours with Tomasz Tunguz: Modern Data Architectures](https://www.youtube.com/watch?v=j5dcfEVnfdI)\n\nTom Tunguz & Lauren DeMeuse from Theory Ventures join Jordan Tigani, CEO and Co-Founder of MotherDuck. In this discussion, we cover the evolution of enterprise data needs and database architectures, the importance of ease of use, and the value of levity and personality in your brand.\n\n[![Compass Tech Summit: Data Analytics in the Post Big-Data Era](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2024_08_16_at_1_18_31_PM_3ed8fc6ecf.png&w=3840&q=75)](https://www.youtube.com/watch?v=hZiTg_995e4)\n\n[2024/04/23 - Jordan Tigani](https://www.youtube.com/watch?v=hZiTg_995e4)\n\n## [Compass Tech Summit: Data Analytics in the Post Big-Data Era](https://www.youtube.com/watch?v=hZiTg_995e4)\n\nJordan Tigani, CEO of MotherDuck, speaks about data analytics and MotherDuck in an era where Big Data is Dead.\n\n[![The Coolest Data Analytics Companies Of The 2024 Big Data 100](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2024_08_16_at_1_04_15_PM_c786b571e1.png&w=3840&q=75)](https://www.crn.com/news/software/2024/the-coolest-data-analytics-companies-of-the-2024-big-data-100)\n\n[2024/04/22 - CRN](https://www.crn.com/news/software/2024/the-coolest-data-analytics-companies-of-the-2024-big-data-100)\n\n## [The Coolest Data Analytics Companies Of The 2024 Big Data 100](https://www.crn.com/news/software/2024/the-coolest-data-analytics-companies-of-the-2024-big-data-100)\n\nCRN, the Channel Company, names MotherDuck one of the coolest data analytics companies of the year.\n\n[![Next Tech Titan: MotherDuck, Rec Room, Statsig, Submittable, Zuper vie for pivotal GeekWire Award](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2F20160712_Geek_Wire_Awards_2013_3bf240dca1.jpg&w=3840&q=75)](https://www.geekwire.com/2024/next-tech-titan-motherduck-rec-room-statsig-submittable-zuper-vie-for-pivotal-geekwire-award/)\n\n[2024/03/26 - GeekWire](https://www.geekwire.com/2024/next-tech-titan-motherduck-rec-room-statsig-submittable-zuper-vie-for-pivotal-geekwire-award/)\n\n## [Next Tech Titan: MotherDuck, Rec Room, Statsig, Submittable, Zuper vie for pivotal GeekWire Award](https://www.geekwire.com/2024/next-tech-titan-motherduck-rec-room-statsig-submittable-zuper-vie-for-pivotal-geekwire-award/)\n\nMotherDuck is chosen as a finalist in the GeekWire \"Next Tech Titan\" awards. This is an annual event recognizing the top innovators and companies in Seattle and the Pacific Northwest.\n\n[![Data App Monetization Success](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FMD_Cure_large_44ee46711c.jpeg&w=3840&q=75)](https://drive.google.com/file/d/14m-P2-6ho8o3dXCHxxmsrBbXl-kwGZD9/view?usp=sharing)\n\n[2024/03/20 - Brian @ Cube](https://drive.google.com/file/d/14m-P2-6ho8o3dXCHxxmsrBbXl-kwGZD9/view?usp=sharing)\n\n## [Data App Monetization Success](https://drive.google.com/file/d/14m-P2-6ho8o3dXCHxxmsrBbXl-kwGZD9/view?usp=sharing)\n\nExplore how to use Astrodata, MotherDuck, and Cube\u2019s combined data stack to build embedded analytics. The speakers in this webinar underscored the importance of several key characteristics for successful data monetization strategies: analytics, monetary value, user convenience, interactivity, and real-time capabilities. The presentation of the MDCuRe stack with MotherDuck, Cube, and React illustrated how developers stand to benefit from reduced unit costs and better ease of use.\n\n[![Building Local and Serverless ELT(ETL) pipelines with CloudQuery and MotherDuck](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fcloudquery_blog_8c6cef78fe.webp&w=3840&q=75)](https://www.cloudquery.io/blog/building-local-and-serverless-elt-cq-and-md)\n\n[2024/03/19 - Yevgeny @ CloudQuery](https://www.cloudquery.io/blog/building-local-and-serverless-elt-cq-and-md)\n\n## [Building Local and Serverless ELT(ETL) pipelines with CloudQuery and MotherDuck](https://www.cloudquery.io/blog/building-local-and-serverless-elt-cq-and-md)\n\nIn this blog post we will walk through building an ELT pipeline using CloudQuery and DuckDB locally and then how to run the same configuration and pipeline with CloudQuery Cloud and MotherDuck (DuckDB Cloud version).\n\nOne cool thing about CloudQuery and DuckDB is that both of them are very easy to run locally which makes it easy to develop and debug before running in the cloud which can save time on feedback loops and surging cloud costs of compute, data, and egress - especially for development purposes.\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 187,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 187
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_237prssm/.kurt/sources/motherduck.com/videos/ducklake-big-data-small-coalesce-2025.md",
  [2m  â”‚[0m     "content": "---\ntitle: ducklake-big-data-small-coalesce-2025\ncontent_type: event\nsource_url: https://motherduck.com/videos/ducklake-big-data-small-coalesce-2025\nindexed_at: '2025-11-25T20:45:11.624296'\ncontent_hash: 1b0e4bb0ed63f8a1\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nDuckLake: Making BIG DATA feel small (Coalesce) - YouTube\n\n[Photo image of dbt Labs](https://www.youtube.com/channel/UCVpBwKK-ecMEV75y1dYLE5w?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\ndbt Labs\n\n15.1K subscribers\n\n[DuckLake: Making BIG DATA feel small (Coalesce)](https://www.youtube.com/watch?v=qEYqz5n2xFc)\n\ndbt Labs\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=qEYqz5n2xFc&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 30:23\n\n\u2022Live\n\n\u2022\n\nTalk\n\n# DuckLake: Making BIG DATA feel small (Coalesce 2025)\n\n2025/10/14\n\nMotherDuck\u2019s managed DuckLake data lakehouse blends the cost efficiency, scale, and openness of a lakehouse with the speed of a warehouse for truly joyful dbt pipelines. They will show you how!\n\n## Related Videos\n\n[![\"What can Postgres learn from DuckDB? (PGConf.dev 2025)\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_06_13_at_3_52_19_PM_470b0f71b1.png&w=3840&q=75)\\\\\n\\\\\n20:44](https://motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025/)\n\n[2025-06-13](https://motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025/)\n\n### [What can Postgres learn from DuckDB? (PGConf.dev 2025)](https://motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025)\n\nDuckDB an open source SQL analytics engine that is quickly growing in popularity. This begs the question: What can Postgres learn from DuckDB?\n\nYouTube\n\nEcosystem\n\nTalk\n\n[![\" pg_duckdb: Ducking awesome analytics in Postgres\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2F0kc_W5o0tcg_HD_03880f61fb.jpg&w=3840&q=75)](https://motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres/)\n\n[2025-06-12](https://motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres/)\n\n### [pg\\_duckdb: Ducking awesome analytics in Postgres](https://motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres)\n\nSupercharge your Postgres analytics! This talk shows how the pg\\_duckdb extension accelerates your slowest queries instantly, often with zero code changes. Learn practical tips and how to use remote columnar storage for even more speed.\n\nTalk\n\nSources\n\n[![\"Taming file zoos: Data science with DuckDB database files\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FYq_hxgys_WLQ_HD_dd1df87caa.jpg&w=3840&q=75)](https://motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files/)\n\n[2025-06-02](https://motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files/)\n\n### [Taming file zoos: Data science with DuckDB database files](https://motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files)\n\nTame \"file zoos\" with DuckDB! This talk reveals how DuckDB seamlessly integrates with Python to read/write diverse files (CSV, Parquet), organize data efficiently, enable larger-than-memory analysis, and streamline your workflows.\n\nPython\n\nTalk\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 123,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 123
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search for more specific documents about hybrid/dual execution that explain how to join local and cloud data:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_237prssm/.kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md

  [96mğŸ”§ TOOL:[0m Bash â†’ grep -i -B2 -A8 "single sql query\|join local\|seamlessly join" /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_237prssm/.kurt/sources/motherduck.com/.metadata/claims.yaml
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_237prssm/.kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     "content": "---\ntitle: hybrid-analytics-guide\ncontent_type: tutorial\nsource_url: https://motherduck.com/learn-more/hybrid-analytics-guide\nindexed_at: '2025-11-25T09:57:23.352209'\ncontent_hash: 91344eaa7ee54322\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO LEARN](https://motherduck.com/learn-more/)\n\n# Hybrid Analytics: Query Local & Cloud Data Instantly\n\n12 min readBY\n\n[Aditya Somani](https://motherduck.com/authors/aditya-aomani/)\n\n![Hybrid Analytics: Query Local & Cloud Data Instantly](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FZero_latency_e8da68f055.png&w=3840&q=75)\n\nAre your analytics queries timing out in Postgres? Does your laptop grind to a halt when you try to analyze a large CSV file? If you feel caught between the memory limits of local tools and the spiraling costs of cloud data warehouses, you're not alone. The traditional divide between local development and cloud-scale analytics creates friction, slows down projects, and drains budgets.\n\nBut there is a modern, more efficient way. Imagine running complex SQL queries on a 20 GB Parquet file directly on your laptop, without a flicker of memory strain. Picture joining a local spreadsheet with a massive table in Amazon S3 using a single, elegant command. This is the reality of the **hybrid analytics workflow**, a powerful approach that unifies your local machine and the cloud into a single, unified data environment.\n\nThis guide is your definitive resource for understanding and implementing this new workflow. We'll break down the core concepts, provide practical examples, and show you how to solve the most common data bottlenecks for good. You will learn why traditional databases fail for analytics, how to analyze datasets larger than your RAM, and how the power of \"Dual Execution\" lets you instantly join local files with cloud data, all without the cost and complexity of managing a server cluster.\n\n## Why Are My Analytics Queries on Postgres & MySQL Timing Out?\n\nIf your analytical queries on a database like PostgreSQL or MySQL are consistently slow or timing out, you've hit a fundamental architectural limit. These databases are masterpieces of engineering for **[Online Transaction Processing (OLTP)](https://estuary.dev/blog/postgres-to-motherduck/)**. These are the small, fast operations that power applications, like creating a user or updating an order. However, this same design becomes a major bottleneck for **[Online Analytical Processing (OLAP)](https://en.wikipedia.org/wiki/Online_analytical_processing)**, which involves complex queries that scan and aggregate vast amounts of data.\n\nThe root of the problem is their **[row-oriented](https://en.wikipedia.org/wiki/Data_orientation)** storage model. A row-store keeps all data for a single record together on disk, which is efficient for retrieving an entire user profile. But for an analytical query that only needs to sum the `sale_amount` column across millions of rows, a row-store is forced to read every single column for every single row. This wastes an enormous amount of I/O and CPU cycles on data it doesn't need, which is the primary reason your dashboards are slow and your queries fail.\n\nThe next logical step is to move analytical workloads to a system built on a **[columnar architecture](https://motherduck.com/learn-more/columnar-storage-guide/)**. A columnar database organizes data by column, storing all values for `sale_amount` together. When your query asks for the sum of `sale_amount`, the database reads _only_ that column, greatly reducing I/O and speeding up performance by orders of magnitude. Modern analytical engines like DuckDB couple this with **[vectorized query execution](https://15721.courses.cs.cmu.edu/spring2024/notes/06-vectorization.pdf)**, a technique that processes data in large batches or \"vectors\" instead of row-by-row. This method fully utilizes modern CPU capabilities to perform a single instruction on multiple data points simultaneously, leading to large gains in computational efficiency.\n\n| Architecture | Row-Oriented (OLTP) | Column-Oriented (OLAP) |\n| --- | --- | --- |\n| **Primary Use Case** | Application backends, frequent small updates. | Business intelligence, data analytics, large scans. |\n| **Data Storage** | All data for a single record is stored together. | All data for a single column is stored together. |\n| **Query Example** | `SELECT * FROM users WHERE user_id = 123;` (Fast) | `SELECT SUM(sale_amount) FROM sales;` (Slow) |\n| **Best For** | Fast writes and single-record lookups. | Fast aggregations and filtering on specific columns. |\n\nBy offloading analytics to a purpose-built columnar engine, you let your transactional database continue to excel at what it does best while your analytical queries run in a [high-speed, optimized environment](https://motherduck.com/blog/postgres-duckdb-options/).\n\n## How Can I Analyze a 20 GB Parquet File on My Laptop Without It Crashing?\n\nThe second major bottleneck for data professionals is local memory. Trying to load a 20 GB file into a traditional in-memory library like Pandas on a 16 GB laptop will almost certainly result in a `MemoryError`. This happens because these tools must load the entire dataset into your computer's RAM before they can begin processing.\n\nThe modern solution is **out-of-core processing**, a strategy where data is processed in manageable chunks directly from disk without ever being fully loaded into RAM. This is the key feature of engines like DuckDB, which uses a [streaming execution model to handle datasets far larger than your available memory](https://duckdb.org/2024/07/09/memory-management.html). If a complex operation requires more memory than is available (by default, 80% of your system's RAM), DuckDB can \"spill\" intermediate results to a temporary file on disk, ensuring your query completes without crashing.\n\nThis capability turns your laptop into a surprisingly powerful analytics workstation. With an out-of-core engine, analyzing that 20 GB file becomes straightforward. You can simply use SQL to query it directly from a Python script or Jupyter notebook.\n\n```python\nCopy code\n\nimport duckdb\n\n# This query runs out-of-core, never loading the full 20GB file into RAM.\n# DuckDB's streaming engine processes the file in chunks.\nresult = duckdb.sql(\"\"\"\n  SELECT\n      product_category,\n      AVG(sale_price) as avg_price\n  FROM read_parquet('large_sales_dataset.parquet')\n  WHERE region = 'North America'\n  GROUP BY product_category\n  ORDER BY avg_price DESC;\n\"\"\").arrow()\n\nprint(result)\n```\n\nThis simple, powerful approach allows you to perform heavy-duty data analysis on your local machine, iterating quickly without waiting for a cloud cluster to spin up or worrying about memory crashes.\n\n## How Can I Join Local CSVs with Cloud Data in a Single Query?\n\nThis is where the hybrid workflow really shows its strength. Joining a local CSV of fresh sales data with a historical customer table in an S3 bucket has traditionally been a major challenge. The problem is \"data gravity,\" since it's [slow and expensive to move large amounts of data](https://motherduck.com/learn-more/no-etl-query-raw-files/). A naive query would have to either upload your entire local file to the cloud or download the entire cloud table to your laptop, both of which are very inefficient.\n\nMotherDuck solves this with an innovative query planner called **Dual Execution**. It treats your laptop and the MotherDuck cloud as two nodes in a single distributed system. When you run a hybrid query, the planner intelligently breaks it down and pushes computation to where the data lives, minimizing data transfer.\n\n> \"The most innovative feature of MotherDuck's architecture is its dual execution model. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\" - [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained/)\n\nImagine you want to join a local CSV of new product pricing with a massive sales table in MotherDuck, but only for a specific product. Instead of moving entire tables, the Dual Execution planner does the following:\n\n1. **Local Scan:** It scans the small pricing CSV on your laptop.\n2. **Filter Locally:** It applies the filter for the specific product on your machine.\n3. **Transfer Minimal Data:** It sends only the single, filtered pricing row (a few bytes of data) to the cloud.\n4. **Join in the Cloud:** It performs the final join against the massive sales table in the MotherDuck cloud.\n\nThis process can reduce network traffic by orders of magnitude compared to traditional methods, turning a difficult query into an interactive one. You can see this in action with a simple `EXPLAIN` statement, which [shows which parts of the query run locally `(L)` and which run remotely `(R)`](https://motherduck.com/docs/key-tasks/running-hybrid-queries/).\n\n## Why Are Data Engineers So Excited About Dual-Execution Engines?\n\nData engineers are excited because dual-execution engines solve one of their biggest challenges: the \"it worked on my machine\" problem. In traditional setups, the tools and data used for local development are often completely different from the production cloud environment, leading to bugs and deployment failures.\n\nMotherDuck eliminates this by using the **exact same DuckDB engine** both locally and in the cloud. A query that works on your laptop is guaranteed to work in the cloud, creating a direct path from development to production. This greatly improves developer experience (DX) and accelerates iteration cycles.\n\nThis architecture provides three key benefits:\n\n1. **Faster Development:** Engineers can build and test pipelines with the zero-latency feedback of local development before scaling to the cloud.\n2. **Lower Costs:** By using the free, powerful compute on users' laptops and minimizing data transfer, this model significantly reduces cloud bills.\n3. **Better Collaboration:** It transforms the traditionally \"single-player\" DuckDB into a [\"multiplayer\" platform](https://motherduck.com/videos/339/bringing-duckdb-to-the-cloud-dual-execution-explained/) where teams can share databases and work from a single source of truth.\n\nThis powerful combination of local speed and cloud scale is backed by real-world success.\n\n> **[Case Study: Finqore's 60x Pipeline Acceleration](https://motherduck.com/case-studies/)**\n> Finqore, a financial technology company, was struggling with an 8-hour data pipeline built on Postgres. By migrating to a hybrid workflow with MotherDuck, they were able to use DuckDB's performance and MotherDuck's serverless scale to transform that pipeline into an **8-minute workflow**, a 60x improvement that unlocked real-time data exploration for their team.\n\n## How Can I Run SQL Analytics Without Managing Clusters?\n\nThe operational overhead of provisioning, scaling, and maintaining server clusters is a major drain on data teams. **[Serverless SQL analytics](https://motherduck.com/docs/concepts/architecture-and-capabilities/)** removes this burden entirely. Instead of managing infrastructure, you simply write and run your queries, and the platform handles the rest.\n\nThis model is not only simpler but also much more cost-effective for the \"spiky\" workloads typical of analytics. You pay only for the compute you actually use, avoiding the massive costs of idle clusters.\n\n| Feature / Scenario | Traditional Cloud Warehouse (e.g., Snowflake/BigQuery) | Hybrid Approach (Local + Serverless) |\n| --- | --- | --- |\n| **Development & Prototyping** | Billed for active compute time, even for small test queries. | **Free.** Uses local machine resources with no cloud costs. |\n| **Pricing Model** | Complex credits or per-TB scanned, which is hard to predict. | Simple, usage-based billing; pay only for queries you run. |\n| **Idle Compute** | Billed for provisioned clusters, even when idle, which can create a costly [idle tax on short-running queries](https://motherduck.com/learn-more/reduce-snowflake-costs-duckdb). | **No cost.** Serverless architecture has no idle compute. |\n| **Hardware Requirement** | Entirely reliant on expensive, provisioned cloud hardware. | Leverages powerful, existing local hardware (laptops, servers). |\n\nBy adopting a serverless, hybrid approach, you can reduce your cloud data warehouse costs while empowering your team with a faster, more flexible workflow.\n\n## What Lightweight SQL Solutions Can Be Integrated with Jupyter Notebooks?\n\nFor data scientists who work primarily in Jupyter, several excellent tools bring the power of SQL directly into the notebook environment. The most powerful option for serious analytics is the **DuckDB engine** itself. It can query Pandas DataFrames, Arrow tables, and large local files directly with full SQL support. Its performance on analytical queries is excellent in the embedded space.\n\nFor comparison, while Python's standard library includes **SQLite**, it's a row-based OLTP engine and is significantly slower than DuckDB for analytical queries. An independent benchmark found DuckDB to be **[12-35 times faster](https://motherduck.com/learn-more/duckdb-vs-sqlite-databases/)** for aggregations on a 100-million-row dataset. For any analytical task in a notebook, DuckDB is the clear choice for performance and scalability.\n\nAnother popular tool is **[JupySQL](https://jupysql.ploomber.io/)**, which provides convenient SQL \"magics\" (`%sql`, `%%sql`) that turn a notebook cell into a SQL editor. It connects to various backends, including DuckDB, making it great for quick, ad-hoc exploration.\n\n## How Can I Keep Queries on My Laptop in Sync With the Cloud?\n\nThe key to keeping local and cloud queries synchronized is **single-engine semantics**. Because MotherDuck uses the same DuckDB engine on your laptop and in the cloud, a query validated locally is guaranteed to behave identically in production. This eliminates the most common source of dev-prod drift.\n\nA modern dev-to-prod workflow combines this with tools like dbt (data build tool). An analyst can develop and test a dbt model on their laptop against local data for rapid iteration. Once validated, they can promote the model to run in the cloud against the full production dataset with a [single configuration change](https://motherduck.com/blog/dual-execution-dbt/), ensuring complete consistency. This workflow combines the speed of local development with the scale and reliability of a production cloud environment.\n\n## When Should I Not Use This Hybrid Approach?\n\nWhile the hybrid, in-process model is very useful for analytics, it's not a solution for every problem. It's crucial to understand its limitations to know when a different architecture is needed.\n\n1. **High-Concurrency Applications:** An in-process engine like DuckDB is not designed to be a client-server database serving hundreds of simultaneous application users. For powering a public-facing web application, a traditional OLTP database like PostgreSQL or a distributed SQL database is the right choice.\n2. **Real-Time, High-Volume Ingestion:** If your use case involves ingesting thousands of events per second in real-time, a specialized streaming platform or real-time OLAP database would be more suitable.\n3. **Truly Massive, Multi-Terabyte Workloads:** While DuckDB can handle surprisingly large datasets on a single node, if your active working set is in the tens or hundreds of terabytes, you've reached the scale where a distributed cloud data warehouse becomes necessary to parallelize work across a large cluster.\n\nThe beauty of the MotherDuck ecosystem is that it provides a smooth way to scale up. You can start with a local-first, hybrid workflow and, as your concurrency or data scale needs grow, easily push more of the workload to the MotherDuck cloud without changing your core tools or SQL logic.\n\n### TABLE OF CONTENTS\n\n[Why Are My Analytics Queries on Postgres & MySQL Timing Out?](https://motherduck.com/learn-more/hybrid-analytics-guide/#why-are-my-analytics-queries-on-postgres-mysql-timing-out)\n\n[How Can I Analyze a 20 GB Parquet File on My Laptop Without It Crashing?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-analyze-a-20-gb-parquet-file-on-my-laptop-without-it-crashing)\n\n[How Can I Join Local CSVs with Cloud Data in a Single Query?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-join-local-csvs-with-cloud-data-in-a-single-query)\n\n[Why Are Data Engineers So Excited About Dual-Execution Engines?](https://motherduck.com/learn-more/hybrid-analytics-guide/#why-are-data-engineers-so-excited-about-dual-execution-engines)\n\n[How Can I Run SQL Analytics Without Managing Clusters?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-run-sql-analytics-without-managing-clusters)\n\n[What Lightweight SQL Solutions Can Be Integrated with Jupyter Notebooks?](https://motherduck.com/learn-more/hybrid-analytics-guide/#what-lightweight-sql-solutions-can-be-integrated-with-jupyter-notebooks)\n\n[How Can I Keep Queries on My Laptop in Sync With the Cloud?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-keep-queries-on-my-laptop-in-sync-with-the-cloud)\n\n[When Should I Not Use This Hybrid Approach?](https://motherduck.com/learn-more/hybrid-analytics-guide/#when-should-i-not-use-this-hybrid-approach)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\n## FAQS\n\n### My analytics queries on Postgres / MySQL are timing out. What\u2019s the next step up?\n\nThe next step is to move your analytical workloads from a row-oriented (OLTP) database like Postgres to a purpose-built columnar (OLAP) engine like DuckDB. Columnar engines are designed for analytics and are orders of magnitude faster for large scans and aggregations because they only read the specific columns needed for a query.\n\n### How can I analyze a 20 GB Parquet file on my laptop without it crashing?\n\nUse a query engine that supports \"out-of-core\" processing, like DuckDB. It processes data in manageable chunks directly from your disk instead of loading the entire file into RAM. This allows you to analyze datasets that are significantly larger than your computer's available memory.\n\n### Why is it so hard to join local CSVs with cloud data?\n\nThe main challenge is \"data gravity\"\u2014it's slow and expensive to move large datasets across a network. Traditional tools force you to either upload your entire local file or download the entire cloud table. A modern hybrid platform with a dual-execution engine solves this by intelligently minimizing data transfer.\n\n### Can I mix local datasets and S3 data in a single query?\n\nYes. With a hybrid analytics platform like MotherDuck, you can write a single SQL query that joins local files (e.g., CSVs) with data in cloud storage (e.g., Parquet files in S3). The system treats them as if they exist in one unified environment, abstracting away their physical location.\n\n### How can I run SQL analytics without managing clusters?\n\nAdopt a serverless SQL analytics platform. These services handle all infrastructure provisioning, scaling, and maintenance for you. You simply run your queries and pay only for the compute you use, which eliminates the high cost and operational overhead of managing idle clusters.\n\n### Why are data engineers excited about dual-execution engines?\n\nDual-execution engines solve the \"it worked on my machine\" problem by using the exact same query engine on the developer's laptop and in the cloud. This guarantees consistency, speeds up development cycles, lowers costs by leveraging local compute, and enables better team collaboration on a single source of truth.\n\n### What lightweight SQL solutions can be integrated with Jupyter notebooks?\n\nDuckDB is the leading lightweight SQL solution for Jupyter. It can be installed via pip and can directly query Pandas DataFrames, Arrow tables, and large local files with full SQL support. It is significantly faster for analytical queries than other embedded options like SQLite.\n\n### How can I keep queries on my laptop in sync with the cloud?\n\nUse a platform that offers single-engine semantics, like MotherDuck. Because it runs the same DuckDB engine locally and in the cloud, a query validated on your laptop is guaranteed to behave identically in production. This eliminates dev-prod drift and ensures consistency.\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 212,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 212
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 125
  [2m  â”‚[0m - statement: You can write a single SQL query that joins local files with data in
  [2m  â”‚[0m     cloud storage.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: DuckLake
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - write
  [2m  â”‚[0m   - single
  [2m  â”‚[0m   - sql
  [2m  â”‚[0m --
  [2m  â”‚[0m     doc: motherduck.com/videos/ducklake-big-data-small-coalesce-2025.md
  [2m  â”‚[0m     quote: Yes. With a hybrid analytics platform like MotherDuck, you can write a
  [2m  â”‚[0m       single SQL query that joins local files (e.g., CSVs) with data in cloud storage
  [2m  â”‚[0m       (e.g., Parquet files in S3).
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 179
  [2m  â”‚[0m - statement: You can write a single SQL query that seamlessly joins a local CSV or
  [2m  â”‚[0m     Parquet file on your laptop with a large dataset stored in the MotherDuck cloud.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: DuckLake
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - write
  [2m  â”‚[0m   - single
  [2m  â”‚[0m   - sql
  [2m  â”‚[0m --
  [2m  â”‚[0m     doc: motherduck.com/learn-more/modern-data-warehouse-playbook.md
  [2m  â”‚[0m     quote: Yes. This is a key innovation of the modern, hybrid data stack. With DuckDB
  [2m  â”‚[0m       and MotherDuck, you can write a single SQL query that seamlessly joins a local
  [2m  â”‚[0m       CSV or Parquet file on your laptop with a lar
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 242
  [2m  â”‚[0m - statement: You could analyze a new batch of log files locally on your laptop and
  [2m  â”‚[0m     join them against a massive historical log table stored in MotherDuck to find
  [2m  â”‚[0m     trends.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: DuckLake
  [2m  â”‚[0m --
  [2m  â”‚[0m     char_start: 0
  [2m  â”‚[0m     char_end: 89
  [2m  â”‚[0m - statement: PostgreSQL can join local data with remote data lake files in a single
  [2m  â”‚[0m     query.
  [2m  â”‚[0m   type: definition
  [2m  â”‚[0m   entity: Postgres
  [2m  â”‚[0m   keywords:
  [2m  â”‚[0m   - definition
  [2m  â”‚[0m   - postgresql
  [2m  â”‚[0m   - join
  [2m  â”‚[0m   - local
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Excellent! I have all the information I need. Let me now write the answer:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_yaml_metadata_7.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "create",
  [2m  â”‚[0m   "filePath": "/tmp/answer_yaml_metadata_7.md",
  [2m  â”‚[0m   "content": "<2360 chars>",
  [2m  â”‚[0m   "structuredPatch": [],
  [2m  â”‚[0m   "originalFile": null
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I've written the answer to `/tmp/answer_yaml_metadata_7.md`. 
  [94mâ”‚[0m 
  [94mâ”‚[0m **Summary:** The most efficient way to join a local CSV with an S3 table in MotherDuck is through **Dual Execution** - MotherDuck's hybrid query planner that treats your laptop and the cloud as a single distributed system. It minimizes data transfer by:
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. Scanning and filtering your local CSV on your laptop
  [94mâ”‚[0m 2. Sending only the necessary filtered rows to the cloud
  [94mâ”‚[0m 3. Performing the final join against the S3 table in MotherDuck
  [94mâ”‚[0m 
  [94mâ”‚[0m This can reduce network traffic by orders of magnitude compared to traditional approaches. You simply write a single SQL query referencing both local files (via `read_csv()`) and cloud tables, and MotherDuck handles the optimization automatically.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 3,650 | Cost: $0.3957
     Cumulative: 3,650 tokens | $0.3957
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent has provided a detailed summary and indicated that it has written the answer, signaling task completion rather than asking for user input.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 11 | Duration: 61.5s
â•‘    Tokens: 3,650 | Cost: $0.3957
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  Running LLM judge evaluation...
   ğŸ§  LLM Judge score: 0.90
