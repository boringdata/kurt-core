# Answer

If you have a CSV on your laptop and a table in S3, the most efficient way to join them using MotherDuck is to leverage its **Dual Execution model** with the `ATTACH` command. Here's the approach:

## The Most Efficient Method

1. **Connect to MotherDuck with dual execution**: Configure your connection to use both local DuckDB and MotherDuck cloud resources:

```sql
-- Attach to MotherDuck while keeping local database
ATTACH 'md:' AS remote;
```

2. **Read your local CSV directly** into a local DuckDB table or query it inline:

```sql
-- Create local table from CSV
CREATE TABLE local_data AS SELECT * FROM read_csv('path/to/your/file.csv');
```

3. **Query the S3 table through MotherDuck** and join with local data:

```sql
-- Join local CSV with S3 data in a single query
SELECT *
FROM local_data l
JOIN remote.my_database.s3_table r ON l.key = r.key;
```

## Why This Is Efficient

The **Dual Execution model** is key to efficiency because it:

- **Minimizes data movement**: The dual execution planner reduces network traffic by orders of magnitude compared to traditional methods. It intelligently determines which parts of the query run locally vs. in the cloud.

- **Optimizes based on data locality**: The dual-engine execution model optimizes query performance based on where the data resides, reducing round trips needed for queries.

- **Balances local and cloud compute**: You can "analyze a new batch of log files locally on your laptop and join them against a massive historical log table stored in MotherDuck to find trends."

## Additional Considerations

- **httpfs extension**: MotherDuck/DuckDB's httpfs extension allows reading/writing remote files over HTTPS and S3, so you can query S3 directly without pre-loading data.

- **Data sampling**: If the S3 table is massive, consider sampling when running locally to keep datasets manageable (e.g., `USING SAMPLE 1%` for initial development).

- **COPY command**: You can also push data between local and remote using the COPY command - "Pushing data from a local DuckDB table to a remote MotherDuck table is just another COPY command."

## Sources

- .kurt/sources/motherduck.com/blog/dual-execution-dbt.md (lines 1-80) - Detailed guide on configuring dual execution with local and cloud resources
- .kurt/sources/motherduck.com/.metadata/claims.txt (line 794) - "DuckDB allows joining local and remote data"
- .kurt/sources/motherduck.com/.metadata/claims.txt (line 711) - "The dual execution model minimizes data movement and improves query performance"
- .kurt/sources/motherduck.com/.metadata/claims.txt (line 710) - "The dual execution planner reduces network traffic by orders of magnitude"
- .kurt/sources/motherduck.com/.metadata/claims.txt (line 1499) - "You could analyze a new batch of log files locally on your laptop and join them against a massive historical log table stored in MotherDuck"
- .kurt/sources/motherduck.com/.metadata/claims.txt (line 2319) - "Pushing data from a local DuckDB table to a remote MotherDuck table is just another COPY command"
- .kurt/sources/motherduck.com/.metadata/claims.txt (line 1828) - "The httpfs extension allows reading/writing remote files over HTTPS and S3"
