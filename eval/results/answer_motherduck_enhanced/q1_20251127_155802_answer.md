# What file formats are most efficient for loading data into MotherDuck?

The most efficient file formats for loading data into MotherDuck are **columnar storage formats**, with **Parquet** being the top choice, followed by **ORC**. These formats are optimized for analytical queries and leverage DuckDB's (which underlies MotherDuck) powerful columnar processing capabilities.

## Primary Recommendations

### 1. Parquet (Highly Recommended)

**Parquet is the gold standard for MotherDuck** due to its exceptional performance characteristics:

- **Columnar Storage**: Organizes data by columns rather than rows, allowing DuckDB to read only the columns needed for a query
- **Superior Compression**: Utilizes specialized encoding techniques (dictionary, run-length, delta encoding) that can reduce data size by 10x or more compared to CSV
- **Built-in Statistics**: Contains min/max values and metadata for each column chunk (row group), enabling DuckDB to skip irrelevant data without reading it
- **Predicate Pushdown**: Filters can be applied at the storage layer, dramatically reducing I/O
- **Column Pruning**: Only requested columns are read from disk, minimizing data transfer
- **Schema Evolution**: Supports adding or removing columns without rewriting existing files
- **Native DuckDB Integration**: DuckDB can query Parquet files directly without importing them first

**Performance Advantage**: Parquet's columnar format combined with DuckDB's optimized Parquet reader makes it 2-10x faster than row-based formats for analytical workloads.

### 2. ORC (Apache Optimized Row Columnar)

ORC is another columnar format with similar benefits to Parquet:

- **Efficient compression and encoding** optimized for large-scale data processing
- **ACID transaction support** for data integrity
- **Columnar storage benefits** comparable to Parquet
- **Strong integration with Hadoop ecosystems**, particularly Hive

**When to use ORC**: If your data originates from Hadoop/Hive environments or you need specific Hive-optimized features. Otherwise, Parquet is generally preferred for MotherDuck workflows.

### 3. DuckLake (MotherDuck's Native Table Format)

For advanced use cases, **DuckLake** represents MotherDuck's modern table format:

- **Database-Backed Metadata**: Stores metadata in a relational database (PostgreSQL, MySQL, or DuckDB itself) rather than files, dramatically speeding up metadata operations
- **Uses Parquet Underneath**: Data files are stored as Parquet in object storage (S3, GCS, Azure)
- **ACID Transactions**: Full transactional support across multiple tables
- **Time Travel**: Query historical states of data via snapshot isolation
- **Automatic Partitioning**: Intelligently partitions data while managing partition metadata in the catalog
- **Built-in Encryption**: Automatically encrypts Parquet files with keys stored securely in the metadata catalog
- **2-10x Faster Queries**: Compared to standard Parquet when using MotherDuck's native storage

**When to use DuckLake**: For production data lakes requiring transactional guarantees, data governance, and maximum query performance within the MotherDuck ecosystem.

## Alternative Formats (Less Efficient)

### CSV (Comma-Separated Values)

**CSV remains widely used but is significantly less efficient**:

- **Pros**:
  - Universal compatibility and human readability
  - Simple to generate and debug
  - DuckDB includes a highly optimized CSV parser
  - Easy data exchange between systems

- **Cons**:
  - No compression (though GZIP compression can be applied)
  - No columnar optimization—entire rows must be scanned
  - No built-in statistics for query optimization
  - No schema enforcement
  - 5-10x larger file sizes compared to Parquet
  - Significantly slower query performance

**When to use CSV**: For small datasets, initial data exploration, or when exchanging data with systems that don't support Parquet. DuckDB makes it trivial to convert CSV to Parquet with a single command.

### JSON

JSON files are supported but have similar limitations to CSV:

- Human-readable and flexible for nested/hierarchical data
- No columnar optimization
- Poor compression compared to Parquet
- Slower parsing and query performance

**When to use JSON**: When data has deeply nested structures that don't fit well in tabular formats, or for small configuration/metadata files.

### Apache Avro

Avro is a row-based binary format:

- **Schema evolution support** with backward/forward compatibility
- **Efficient serialization** for streaming and data exchange
- **Row-based storage** makes it less optimal for analytical queries
- Better suited for write-heavy or streaming workloads

**When to use Avro**: For data streaming pipelines or when you need robust schema evolution in event-driven systems. Convert to Parquet for analytical queries in MotherDuck.

## Comparison: Modern Table Formats

Beyond basic file formats, modern table formats add metadata layers on top of Parquet:

### Delta Lake

- **Built by Databricks**, tightly integrated with Spark
- Uses Parquet files with JSON/Avro metadata
- ACID transactions and time travel
- Strong for batch and streaming workloads

**MotherDuck Compatibility**: DuckDB can query Delta Lake tables via extensions, though performance may not match native Parquet or DuckLake.

### Apache Iceberg

- **Open table format** with broad ecosystem support
- Uses Parquet (or ORC/Avro) with metadata tracking
- Schema evolution and partition evolution
- Designed for massive petabyte-scale data lakes

**MotherDuck Compatibility**: Supported through DuckDB extensions, but adds complexity compared to direct Parquet access.

## Practical Recommendations

### For Best Performance

1. **Use Parquet** for all production data loading into MotherDuck
2. **Enable compression** (ZSTD or GZIP) when writing Parquet files
3. **Partition large datasets** using Hive-style partitioning (`year=2024/month=01/`) for efficient query pruning
4. **Use DuckLake** when you need transactional guarantees, encryption, or are building a complete data lakehouse

### File Size Considerations

- **Aim for 100MB-1GB per Parquet file** for optimal performance
- Avoid thousands of tiny files (< 10MB) which slow down file listing and metadata operations
- Use DuckDB's `COPY TO ... PARTITION_BY` to create properly structured Parquet exports

### Quick Conversion Example

DuckDB makes format conversion trivial:

```bash
# Convert CSV to Parquet
duckdb -c "COPY (SELECT * FROM 'data.csv') TO 'data.parquet' (FORMAT PARQUET, COMPRESSION ZSTD);"

# Convert Parquet to CSV (for compatibility)
duckdb -c "COPY (SELECT * FROM 'data.parquet') TO 'data.csv' (HEADER, DELIMITER ',');"
```

### Loading Data into MotherDuck

Once your data is in Parquet format:

```sql
-- Query Parquet directly from S3
SELECT * FROM 's3://my-bucket/data/*.parquet' WHERE year = 2024;

-- Create a MotherDuck table from Parquet
CREATE TABLE my_table AS SELECT * FROM 'data.parquet';

-- Or use DuckLake for advanced features
ATTACH 'ducklake:my_lake.ducklake' AS lake;
CREATE TABLE lake.my_table AS SELECT * FROM 'data.parquet';
```

## Summary Matrix

| Format | Efficiency | Compression | Query Speed | Use Case |
|--------|-----------|-------------|-------------|----------|
| **Parquet** | ⭐⭐⭐⭐⭐ | Excellent (10x) | Very Fast | **Primary choice for analytics** |
| **DuckLake** | ⭐⭐⭐⭐⭐ | Excellent | Fastest | Production data lakehouses |
| **ORC** | ⭐⭐⭐⭐ | Excellent | Fast | Hadoop/Hive environments |
| **CSV** | ⭐⭐ | Poor | Slow | Data exchange, small files |
| **JSON** | ⭐⭐ | Poor | Slow | Nested data, configs |
| **Avro** | ⭐⭐⭐ | Good | Medium | Streaming, schema evolution |
| **Delta Lake** | ⭐⭐⭐⭐ | Excellent | Fast | Databricks ecosystems |
| **Iceberg** | ⭐⭐⭐⭐ | Excellent | Fast | Multi-petabyte data lakes |

## Sources

### Knowledge Graph Entities Used
- Data Warehouse
- Data Engineering
- Data Lakehouse
- Data Analytics
- Data Lake

### Documents Referenced

**Primary Sources:**
- [Parquet File Format: What It Is, Benefits, and Alternatives](https://motherduck.com/learn-more/why-choose-parquet-table-file-format) - Comprehensive guide to Parquet benefits, compression, and DuckDB integration
- [Fast Data Exports with DuckDB's Partitioned Writes and DuckLake](https://motherduck.com/learn-more/partitioned-writes-parquet-ducklake) - Detailed coverage of partitioning strategies and DuckLake features
- [Understanding DuckLake: A Table Format with a Modern Architecture](https://motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture) - DuckLake architecture and comparison to Iceberg/Delta Lake

**Supplementary Sources:**
- [Simple way to convert CSV and Parquet files](https://motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files) - Practical file conversion workflows
- [Why CSVs Still Matter: The Indispensable File Format](https://motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format) - CSV use cases and limitations
- [How to Efficiently Load Data into DuckLake with Estuary](https://motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary) - Real-time data loading patterns and performance considerations
- [Leveraging DuckDB and Delta Lake Together](https://motherduck.com/videos/leveraging-duckdb-and-delta-lake-together) - Delta Lake integration with DuckDB

**GraphRAG Answer Sources:**
- Pandas DataFrames: A Practical Guide for Beginners (relevance: 0.95)
- cloud-data-warehouse-startup-guide (relevance: 0.95)
- data-lake-vs-data-warehouse-vs-lakehouse (relevance: 0.95)
- is-bi-too-big-for-small-data (relevance: 0.95)
- duckdb-ecosystem-newsletter-february-2025 (relevance: 0.90)

**Confidence Level**: High (0.90) - Based on extensive MotherDuck documentation and technical guides
