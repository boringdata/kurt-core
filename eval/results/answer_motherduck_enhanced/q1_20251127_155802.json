{
  "question": "...... What file formats are most efficient for loading data into MotherDuck?",
  "answer": "# What file formats are most efficient for loading data into MotherDuck?\n\nThe most efficient file formats for loading data into MotherDuck are **columnar storage formats**, with **Parquet** being the top choice, followed by **ORC**. These formats are optimized for analytical queries and leverage DuckDB's (which underlies MotherDuck) powerful columnar processing capabilities.\n\n## Primary Recommendations\n\n### 1. Parquet (Highly Recommended)\n\n**Parquet is the gold standard for MotherDuck** due to its exceptional performance characteristics:\n\n- **Columnar Storage**: Organizes data by columns rather than rows, allowing DuckDB to read only the columns needed for a query\n- **Superior Compression**: Utilizes specialized encoding techniques (dictionary, run-length, delta encoding) that can reduce data size by 10x or more compared to CSV\n- **Built-in Statistics**: Contains min/max values and metadata for each column chunk (row group), enabling DuckDB to skip irrelevant data without reading it\n- **Predicate Pushdown**: Filters can be applied at the storage layer, dramatically reducing I/O\n- **Column Pruning**: Only requested columns are read from disk, minimizing data transfer\n- **Schema Evolution**: Supports adding or removing columns without rewriting existing files\n- **Native DuckDB Integration**: DuckDB can query Parquet files directly without importing them first\n\n**Performance Advantage**: Parquet's columnar format combined with DuckDB's optimized Parquet reader makes it 2-10x faster than row-based formats for analytical workloads.\n\n### 2. ORC (Apache Optimized Row Columnar)\n\nORC is another columnar format with similar benefits to Parquet:\n\n- **Efficient compression and encoding** optimized for large-scale data processing\n- **ACID transaction support** for data integrity\n- **Columnar storage benefits** comparable to Parquet\n- **Strong integration with Hadoop ecosystems**, particularly Hive\n\n**When to use ORC**: If your data originates from Hadoop/Hive environments or you need specific Hive-optimized features. Otherwise, Parquet is generally preferred for MotherDuck workflows.\n\n### 3. DuckLake (MotherDuck's Native Table Format)\n\nFor advanced use cases, **DuckLake** represents MotherDuck's modern table format:\n\n- **Database-Backed Metadata**: Stores metadata in a relational database (PostgreSQL, MySQL, or DuckDB itself) rather than files, dramatically speeding up metadata operations\n- **Uses Parquet Underneath**: Data files are stored as Parquet in object storage (S3, GCS, Azure)\n- **ACID Transactions**: Full transactional support across multiple tables\n- **Time Travel**: Query historical states of data via snapshot isolation\n- **Automatic Partitioning**: Intelligently partitions data while managing partition metadata in the catalog\n- **Built-in Encryption**: Automatically encrypts Parquet files with keys stored securely in the metadata catalog\n- **2-10x Faster Queries**: Compared to standard Parquet when using MotherDuck's native storage\n\n**When to use DuckLake**: For production data lakes requiring transactional guarantees, data governance, and maximum query performance within the MotherDuck ecosystem.\n\n## Alternative Formats (Less Efficient)\n\n### CSV (Comma-Separated Values)\n\n**CSV remains widely used but is significantly less efficient**:\n\n- **Pros**:\n  - Universal compatibility and human readability\n  - Simple to generate and debug\n  - DuckDB includes a highly optimized CSV parser\n  - Easy data exchange between systems\n\n- **Cons**:\n  - No compression (though GZIP compression can be applied)\n  - No columnar optimization\u2014entire rows must be scanned\n  - No built-in statistics for query optimization\n  - No schema enforcement\n  - 5-10x larger file sizes compared to Parquet\n  - Significantly slower query performance\n\n**When to use CSV**: For small datasets, initial data exploration, or when exchanging data with systems that don't support Parquet. DuckDB makes it trivial to convert CSV to Parquet with a single command.\n\n### JSON\n\nJSON files are supported but have similar limitations to CSV:\n\n- Human-readable and flexible for nested/hierarchical data\n- No columnar optimization\n- Poor compression compared to Parquet\n- Slower parsing and query performance\n\n**When to use JSON**: When data has deeply nested structures that don't fit well in tabular formats, or for small configuration/metadata files.\n\n### Apache Avro\n\nAvro is a row-based binary format:\n\n- **Schema evolution support** with backward/forward compatibility\n- **Efficient serialization** for streaming and data exchange\n- **Row-based storage** makes it less optimal for analytical queries\n- Better suited for write-heavy or streaming workloads\n\n**When to use Avro**: For data streaming pipelines or when you need robust schema evolution in event-driven systems. Convert to Parquet for analytical queries in MotherDuck.\n\n## Comparison: Modern Table Formats\n\nBeyond basic file formats, modern table formats add metadata layers on top of Parquet:\n\n### Delta Lake\n\n- **Built by Databricks**, tightly integrated with Spark\n- Uses Parquet files with JSON/Avro metadata\n- ACID transactions and time travel\n- Strong for batch and streaming workloads\n\n**MotherDuck Compatibility**: DuckDB can query Delta Lake tables via extensions, though performance may not match native Parquet or DuckLake.\n\n### Apache Iceberg\n\n- **Open table format** with broad ecosystem support\n- Uses Parquet (or ORC/Avro) with metadata tracking\n- Schema evolution and partition evolution\n- Designed for massive petabyte-scale data lakes\n\n**MotherDuck Compatibility**: Supported through DuckDB extensions, but adds complexity compared to direct Parquet access.\n\n## Practical Recommendations\n\n### For Best Performance\n\n1. **Use Parquet** for all production data loading into MotherDuck\n2. **Enable compression** (ZSTD or GZIP) when writing Parquet files\n3. **Partition large datasets** using Hive-style partitioning (`year=2024/month=01/`) for efficient query pruning\n4. **Use DuckLake** when you need transactional guarantees, encryption, or are building a complete data lakehouse\n\n### File Size Considerations\n\n- **Aim for 100MB-1GB per Parquet file** for optimal performance\n- Avoid thousands of tiny files (< 10MB) which slow down file listing and metadata operations\n- Use DuckDB's `COPY TO ... PARTITION_BY` to create properly structured Parquet exports\n\n### Quick Conversion Example\n\nDuckDB makes format conversion trivial:\n\n```bash\n# Convert CSV to Parquet\nduckdb -c \"COPY (SELECT * FROM 'data.csv') TO 'data.parquet' (FORMAT PARQUET, COMPRESSION ZSTD);\"\n\n# Convert Parquet to CSV (for compatibility)\nduckdb -c \"COPY (SELECT * FROM 'data.parquet') TO 'data.csv' (HEADER, DELIMITER ',');\"\n```\n\n### Loading Data into MotherDuck\n\nOnce your data is in Parquet format:\n\n```sql\n-- Query Parquet directly from S3\nSELECT * FROM 's3://my-bucket/data/*.parquet' WHERE year = 2024;\n\n-- Create a MotherDuck table from Parquet\nCREATE TABLE my_table AS SELECT * FROM 'data.parquet';\n\n-- Or use DuckLake for advanced features\nATTACH 'ducklake:my_lake.ducklake' AS lake;\nCREATE TABLE lake.my_table AS SELECT * FROM 'data.parquet';\n```\n\n## Summary Matrix\n\n| Format | Efficiency | Compression | Query Speed | Use Case |\n|--------|-----------|-------------|-------------|----------|\n| **Parquet** | \u2b50\u2b50\u2b50\u2b50\u2b50 | Excellent (10x) | Very Fast | **Primary choice for analytics** |\n| **DuckLake** | \u2b50\u2b50\u2b50\u2b50\u2b50 | Excellent | Fastest | Production data lakehouses |\n| **ORC** | \u2b50\u2b50\u2b50\u2b50 | Excellent | Fast | Hadoop/Hive environments |\n| **CSV** | \u2b50\u2b50 | Poor | Slow | Data exchange, small files |\n| **JSON** | \u2b50\u2b50 | Poor | Slow | Nested data, configs |\n| **Avro** | \u2b50\u2b50\u2b50 | Good | Medium | Streaming, schema evolution |\n| **Delta Lake** | \u2b50\u2b50\u2b50\u2b50 | Excellent | Fast | Databricks ecosystems |\n| **Iceberg** | \u2b50\u2b50\u2b50\u2b50 | Excellent | Fast | Multi-petabyte data lakes |",
  "answer_file": "/tmp/answer_enhanced_1.md",
  "token_usage": {
    "total_tokens": 5716,
    "duration_seconds": 113.07991194725037
  },
  "cached_response": false,
  "llm_judge": {
    "overall_score": 0.83,
    "component_scores": {
      "accuracy": 0.9,
      "completeness": 0.8,
      "relevance": 0.7,
      "clarity": 0.9
    },
    "feedback": "The generated answer is mostly accurate and provides a wealth of information about various file formats, particularly highlighting Parquet's advantages. However, it includes excessive detail that may detract from the directness of the response to the specific question. While the clarity is high, the relevance could be improved by focusing more on the primary question without delving into less relevant formats."
  },
  "command": "question:q1",
  "returncode": 0
}