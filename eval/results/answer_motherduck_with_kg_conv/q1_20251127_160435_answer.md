# What file formats are most efficient for loading data into MotherDuck?

The most efficient file formats for loading data into MotherDuck are **columnar storage formats**, particularly **Parquet**, which is optimized for analytical queries and native to DuckDB (the engine underlying MotherDuck). However, the optimal format depends on your specific use case, data pipeline, and storage architecture.

## Columnar Formats: The Performance Leaders

### Parquet (Most Efficient)
Parquet is the most efficient format for loading data into MotherDuck because:

- **Native support**: DuckDB can query Parquet files directly without intermediate conversion
- **Columnar storage**: Optimized for analytical queries with column-based compression and efficient scanning
- **Compression**: Significantly smaller file sizes compared to row-based formats like CSV
- **Direct querying**: MotherDuck can read Parquet files from cloud storage (S3, GCS, Azure) as if they were database tables

**Example usage:**
```sql
-- Query Parquet directly from S3
SELECT * FROM 's3://bucket/data.parquet';

-- Load Parquet into MotherDuck
CREATE TABLE my_table AS SELECT * FROM 'data.parquet';
```

### ORC (Also Efficient)
ORC (Optimized Row Columnar) is another columnar format well-supported by DuckDB, offering similar performance characteristics to Parquet. It's particularly common in Hadoop/Spark ecosystems.

## CSV and JSON: Versatile but Less Efficient

### CSV
While less efficient than Parquet, CSV files remain widely supported and useful:

- **Ubiquity**: Universal compatibility across tools and systems
- **Human-readable**: Easy to inspect and debug
- **Overhead**: Larger file sizes and slower query performance compared to columnar formats
- **DuckDB optimization**: DuckDB has a sophisticated CSV parser that makes loading reasonably fast

**Conversion tip**: DuckDB makes it easy to convert CSV to Parquet using a simple command:
```bash
duckdb -c "COPY (SELECT * FROM 'data.csv') TO 'data.parquet';"
```

This conversion significantly improves subsequent query performance while maintaining data in an open format.

### JSON
JSON is supported but typically less efficient due to:
- Verbose text format leading to larger files
- More complex parsing requirements
- Better suited for semi-structured or nested data

## Modern Table Formats: Best for Large-Scale Data Lakes

For enterprise-scale deployments, modern table formats provide additional features beyond raw file formats:

### DuckLake
DuckLake is MotherDuck's native open table format that offers:

- **Database-backed metadata**: Metadata stored in a relational database (PostgreSQL, MySQL, DuckDB) rather than files, enabling faster metadata operations
- **ACID transactions**: Full transactional guarantees across tables
- **Schema evolution**: Add or modify columns without rewriting entire datasets
- **Time travel**: Query historical states of data
- **Parquet underneath**: Data files stored as Parquet on object storage for performance

**When to use DuckLake:**
- Large-scale data lakes (terabytes to petabytes)
- Need for concurrent writes and transactions
- Complex schemas that evolve over time
- Requirement for open formats compatible with multiple compute engines

### Delta Lake and Apache Iceberg
These established table formats are also compatible with MotherDuck:
- Provide similar features to DuckLake (ACID, schema evolution, time travel)
- Widely adopted in existing data lake architectures
- Can be queried directly by MotherDuck/DuckDB

## Direct Loading Methods

MotherDuck supports multiple efficient loading patterns:

### 1. Direct File Querying
Query files directly from cloud storage without explicit loading:
```sql
SELECT * FROM 's3://bucket/path/*.parquet' WHERE date > '2024-01-01';
```

### 2. COPY Command
Use the COPY statement for efficient bulk loading:
```sql
COPY my_table FROM 'data.parquet';
```

### 3. Streaming with Estuary
For real-time data pipelines, tools like Estuary can:
- Stream data continuously into MotherDuck/DuckLake
- Use Parquet as the underlying storage format
- Provide automatic schema evolution and reliability

### 4. ATTACH for Shared Databases
Attach remote MotherDuck databases for instant access:
```sql
ATTACH 'md:my_database';
```

## Storage Performance Considerations

### MotherDuck Native Storage vs. DuckLake

**MotherDuck Native Storage:**
- 2-10x faster query performance
- Optimized for read/write throughput
- Best for: frequently queried data, production workloads, smaller to medium datasets

**DuckLake (on S3/GCS/Azure):**
- Open format for ecosystem compatibility
- Better for: large-scale storage, multi-engine access, cost optimization
- Consider for: bronze/raw data layers in medallion architectures

**Hybrid approach**: Store raw data in DuckLake for openness, then materialize frequently-queried aggregations in MotherDuck native storage for performance.

## Practical Recommendations

### For Maximum Efficiency:
1. **Use Parquet** for analytical workloads
2. **Convert CSV to Parquet** if you receive CSV files regularly
3. **Consider DuckLake** for large-scale data lakes requiring ACID properties
4. **Store in MotherDuck native storage** for hot, frequently-queried data
5. **Use object storage** (S3/GCS) with Parquet for cold storage and archive

### File Size Optimization:
- Target Parquet file sizes of 100-1000 MB for optimal performance
- Avoid extremely small files (< 10 MB) which create metadata overhead
- Avoid extremely large files (> 5 GB) which reduce parallelism

### Quick Conversion Utility:
Create a reusable DuckDB file converter:
```bash
# Bash script for format conversion
duckdb -c "COPY (SELECT * FROM '${INPUT_FILE}') TO '${OUTPUT_FILE}';"
```

This works for any supported format (CSV, JSON, Parquet, etc.) and can include SQL transformations in the SELECT statement.

## Sources

### GraphRAG Knowledge Graph Entities:
- Data Warehouse
- Data Engineering
- Data Lakehouse
- Data Analytics
- Data Lake

### Documents Referenced:
1. **Simple way to convert CSV and Parquet files** (motherduck.com/videos) - Tutorial on DuckDB file conversion utilities
2. **How to Efficiently Load Data into DuckLake with Estuary** (motherduck.com/videos) - Real-time data loading and DuckLake architecture
3. **Understanding DuckLake: A Table Format with a Modern Architecture** (motherduck.com/videos) - DuckLake design and implementation details
4. **Why CSVs Still Matter: The Indispensable File Format** (motherduck.com/videos) - Discussion of CSV parsing and compatibility
5. **Pandas DataFrames: A Practical Guide for Beginners** (relevance: 0.95)
6. **cloud-data-warehouse-startup-guide** (relevance: 0.95)
7. **data-lake-vs-data-warehouse-vs-lakehouse** (relevance: 0.95)
8. **is-bi-too-big-for-small-data** (relevance: 0.95)
9. **duckdb-ecosystem-newsletter-february-2025** (relevance: 0.90)
