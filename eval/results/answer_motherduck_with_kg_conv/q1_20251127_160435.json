{
  "question": "...... What file formats are most efficient for loading data into MotherDuck?",
  "answer": "# What file formats are most efficient for loading data into MotherDuck?\n\nThe most efficient file formats for loading data into MotherDuck are **columnar storage formats**, particularly **Parquet**, which is optimized for analytical queries and native to DuckDB (the engine underlying MotherDuck). However, the optimal format depends on your specific use case, data pipeline, and storage architecture.\n\n## Columnar Formats: The Performance Leaders\n\n### Parquet (Most Efficient)\nParquet is the most efficient format for loading data into MotherDuck because:\n\n- **Native support**: DuckDB can query Parquet files directly without intermediate conversion\n- **Columnar storage**: Optimized for analytical queries with column-based compression and efficient scanning\n- **Compression**: Significantly smaller file sizes compared to row-based formats like CSV\n- **Direct querying**: MotherDuck can read Parquet files from cloud storage (S3, GCS, Azure) as if they were database tables\n\n**Example usage:**\n```sql\n-- Query Parquet directly from S3\nSELECT * FROM 's3://bucket/data.parquet';\n\n-- Load Parquet into MotherDuck\nCREATE TABLE my_table AS SELECT * FROM 'data.parquet';\n```\n\n### ORC (Also Efficient)\nORC (Optimized Row Columnar) is another columnar format well-supported by DuckDB, offering similar performance characteristics to Parquet. It's particularly common in Hadoop/Spark ecosystems.\n\n## CSV and JSON: Versatile but Less Efficient\n\n### CSV\nWhile less efficient than Parquet, CSV files remain widely supported and useful:\n\n- **Ubiquity**: Universal compatibility across tools and systems\n- **Human-readable**: Easy to inspect and debug\n- **Overhead**: Larger file sizes and slower query performance compared to columnar formats\n- **DuckDB optimization**: DuckDB has a sophisticated CSV parser that makes loading reasonably fast\n\n**Conversion tip**: DuckDB makes it easy to convert CSV to Parquet using a simple command:\n```bash\nduckdb -c \"COPY (SELECT * FROM 'data.csv') TO 'data.parquet';\"\n```\n\nThis conversion significantly improves subsequent query performance while maintaining data in an open format.\n\n### JSON\nJSON is supported but typically less efficient due to:\n- Verbose text format leading to larger files\n- More complex parsing requirements\n- Better suited for semi-structured or nested data\n\n## Modern Table Formats: Best for Large-Scale Data Lakes\n\nFor enterprise-scale deployments, modern table formats provide additional features beyond raw file formats:\n\n### DuckLake\nDuckLake is MotherDuck's native open table format that offers:\n\n- **Database-backed metadata**: Metadata stored in a relational database (PostgreSQL, MySQL, DuckDB) rather than files, enabling faster metadata operations\n- **ACID transactions**: Full transactional guarantees across tables\n- **Schema evolution**: Add or modify columns without rewriting entire datasets\n- **Time travel**: Query historical states of data\n- **Parquet underneath**: Data files stored as Parquet on object storage for performance\n\n**When to use DuckLake:**\n- Large-scale data lakes (terabytes to petabytes)\n- Need for concurrent writes and transactions\n- Complex schemas that evolve over time\n- Requirement for open formats compatible with multiple compute engines\n\n### Delta Lake and Apache Iceberg\nThese established table formats are also compatible with MotherDuck:\n- Provide similar features to DuckLake (ACID, schema evolution, time travel)\n- Widely adopted in existing data lake architectures\n- Can be queried directly by MotherDuck/DuckDB\n\n## Direct Loading Methods\n\nMotherDuck supports multiple efficient loading patterns:\n\n### 1. Direct File Querying\nQuery files directly from cloud storage without explicit loading:\n```sql\nSELECT * FROM 's3://bucket/path/*.parquet' WHERE date > '2024-01-01';\n```\n\n### 2. COPY Command\nUse the COPY statement for efficient bulk loading:\n```sql\nCOPY my_table FROM 'data.parquet';\n```\n\n### 3. Streaming with Estuary\nFor real-time data pipelines, tools like Estuary can:\n- Stream data continuously into MotherDuck/DuckLake\n- Use Parquet as the underlying storage format\n- Provide automatic schema evolution and reliability\n\n### 4. ATTACH for Shared Databases\nAttach remote MotherDuck databases for instant access:\n```sql\nATTACH 'md:my_database';\n```\n\n## Storage Performance Considerations\n\n### MotherDuck Native Storage vs. DuckLake\n\n**MotherDuck Native Storage:**\n- 2-10x faster query performance\n- Optimized for read/write throughput\n- Best for: frequently queried data, production workloads, smaller to medium datasets\n\n**DuckLake (on S3/GCS/Azure):**\n- Open format for ecosystem compatibility\n- Better for: large-scale storage, multi-engine access, cost optimization\n- Consider for: bronze/raw data layers in medallion architectures\n\n**Hybrid approach**: Store raw data in DuckLake for openness, then materialize frequently-queried aggregations in MotherDuck native storage for performance.\n\n## Practical Recommendations\n\n### For Maximum Efficiency:\n1. **Use Parquet** for analytical workloads\n2. **Convert CSV to Parquet** if you receive CSV files regularly\n3. **Consider DuckLake** for large-scale data lakes requiring ACID properties\n4. **Store in MotherDuck native storage** for hot, frequently-queried data\n5. **Use object storage** (S3/GCS) with Parquet for cold storage and archive\n\n### File Size Optimization:\n- Target Parquet file sizes of 100-1000 MB for optimal performance\n- Avoid extremely small files (< 10 MB) which create metadata overhead\n- Avoid extremely large files (> 5 GB) which reduce parallelism\n\n### Quick Conversion Utility:\nCreate a reusable DuckDB file converter:\n```bash\n# Bash script for format conversion\nduckdb -c \"COPY (SELECT * FROM '${INPUT_FILE}') TO '${OUTPUT_FILE}';\"\n```\n\nThis works for any supported format (CSV, JSON, Parquet, etc.) and can include SQL transformations in the SELECT statement.",
  "answer_file": "/tmp/answer_with_kg_conv_1.md",
  "token_usage": {
    "total_tokens": 3730,
    "duration_seconds": 116.82125329971313
  },
  "cached_response": false,
  "llm_judge": {
    "overall_score": 1.0,
    "component_scores": {
      "accuracy": 1.0,
      "completeness": 1.0,
      "relevance": 1.0,
      "clarity": 1.0
    },
    "feedback": "The generated answer is highly accurate, complete, relevant, and clear. It effectively covers the required topic of efficient file formats for MotherDuck, providing detailed insights into each format's advantages and use cases. The clarity of the writing makes it easy to understand, and the structured format enhances readability."
  },
  "command": "question:q1",
  "returncode": 0
}