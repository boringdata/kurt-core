# Answer

The most efficient file formats for loading data into MotherDuck are, in order of efficiency:

## 1. Apache Parquet (Most Efficient)

**Parquet is the recommended format** for loading data into MotherDuck due to its columnar storage design that aligns perfectly with DuckDB's analytical engine. Key efficiency benefits include:

- **Superior compression**: Parquet uses specialized encoding techniques (dictionary encoding, run-length encoding, delta encoding) that reduce storage footprint by storing similar data types together
- **Reduced I/O**: Only reads columns needed for a query rather than entire rows - if your query needs 2 columns out of 100, a columnar system might only read 2% of the total data
- **Predicate pushdown and column pruning**: MotherDuck's query engine pushes down filters to the file level, minimizing data read from cloud storage and reducing costs
- **Native integration**: DuckDB's architecture is "purpose-built to leverage the column-oriented design of Parquet," enabling seamless data processing with minimal overhead
- **Rich metadata**: Parquet files store statistics (min/max values, null counts) for each column chunk, enabling data skipping optimizations

## 2. CSV Files

While less efficient than Parquet, CSV files are well-supported with DuckDB's exceptional CSV parser:

- **Auto-detection**: DuckDB's `read_csv_auto` function automatically infers column names, data types, and file dialects
- **Schema-on-read**: No need to write `CREATE TABLE` statements - query CSV files directly
- **Error handling**: Options like `ignore_errors`, `store_rejects`, and `null_padding` handle messy data gracefully
- **Glob patterns**: Query multiple files at once with wildcards (e.g., `'s3://my-bucket/logs/2025/**/*.csv'`)
- **Benchmark performance**: DuckDB ranked #1 on the Pollock benchmark for CSV parsing, correctly reading 99.61% of data across problematic test files

**When CSV makes sense**: Universal compatibility, easy to edit/read, and still common for data exchange from CRMs, social media exports, and financial systems.

## 3. JSON Files

JSON is supported but less efficient than columnar formats:

- **Auto-detection**: `read_json_auto` infers schema, representing nested objects as `STRUCT`s and arrays as `LIST`s
- **Nested data support**: Use `unnest` function and dot notation to access nested values
- **Direct API queries**: Can parse JSON directly from web APIs
- **Best for**: Semi-structured data with varying schemas, API responses, and log files

## Efficiency Comparison Summary

| Format | Compression | Query Performance | Best Use Case |
|--------|-------------|-------------------|---------------|
| **Parquet** | Excellent (columnar) | Fastest (column pruning, predicate pushdown) | Analytics, large datasets, cloud storage |
| **CSV** | None | Good with auto-detection | Data exchange, quick imports, universal compatibility |
| **JSON** | None | Moderate (nested parsing overhead) | Semi-structured data, API data, logs |

## Additional Table Formats

For advanced use cases, MotherDuck also supports:

- **Apache Iceberg**: Provides transactional guarantees, time travel, and schema evolution on top of Parquet files
- **Delta Lake**: Similar to Iceberg with ACID transactions and data versioning
- **DuckLake**: MotherDuck's native table format with modern architecture

## Practical Recommendations

1. **Convert CSV to Parquet** when possible for repeated analytical queries - DuckDB makes this trivial with `COPY TO` statements
2. **Use glob patterns** to query multiple files as a single table: `read_parquet('s3://bucket/**/*.parquet')`
3. **Leverage Hive-style partitioning** (e.g., `/year=2025/month=10/`) to enable partition pruning and reduce scan costs
4. **For cloud storage**: Parquet's compression and columnar design significantly reduces network transfer costs

## Sources

List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:
- .kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md - Comprehensive guide on Parquet format benefits, columnar storage advantages, compression techniques, and DuckDB integration
- .kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md - Details on DuckDB's CSV parser capabilities, loading multiple CSVs, and practical examples with Stack Overflow data
- .kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md - No-ETL approach for querying CSV, JSON, and Parquet files directly; glob patterns; handling messy data; cloud storage integration
- .kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md - In-depth explanation of columnar vs row-oriented storage, compression methods, vectorized execution, and Parquet as the standard format
- .kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md - Advanced CSV handling techniques including sniff_csv, error handling options, union_by_name for inconsistent schemas, and Pollock benchmark results
- .kurt/sources/motherduck.com/blog/analyze-json-data-using-sql.md - Guide on querying JSON files with DuckDB, handling nested data, using unnest function, and importing JSON into tables
