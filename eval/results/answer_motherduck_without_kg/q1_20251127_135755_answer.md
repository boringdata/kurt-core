# Most Efficient File Formats for Loading Data into MotherDuck

Based on MotherDuck's documentation and content, **Parquet** is the most efficient file format for loading data into MotherDuck, followed by other columnar formats and the native DuckDB format.

## Parquet: The Optimal Choice

Parquet is the recommended format for loading data into MotherDuck for several key reasons:

### Superior Compression and Storage Efficiency
- Parquet uses specialized encoding techniques (dictionary, run-length, and delta encoding) to optimize storage
- The columnar format achieves higher compression ratios compared to row-based formats like CSV
- Reduced storage footprint translates to cost savings and improved access speeds
- Minimizes I/O operations, which enhances query performance

### Performance Optimization
- **Column pruning**: Only reads the specific columns needed for a query, not entire rows
- **Predicate pushdown**: Filters data at the storage layer before processing, reducing data movement
- Columnar storage enables faster retrieval when queries focus on specific columns rather than entire rows
- DuckDB can automatically infer schema, parallelize downloads from S3, and load Parquet files efficiently

### Direct Integration with DuckDB/MotherDuck
- DuckDB's architecture is purpose-built to leverage Parquet's column-oriented design
- Seamless data processing with minimal overhead
- Can query Parquet files directly without importing them first
- Simple SQL-based loading: `CREATE TABLE my_table AS SELECT * FROM 's3://bucket/data_*.parquet';`

## Alternative Efficient Formats

### DuckDB Native Format
- Highly compressed columnar file format
- Can persist many large tables in the same file
- Provides ACID transactional safety and parallel processing
- Supports storing processing logic in views and functions
- Ideal for organizing and managing multiple tables together

### MotherDuck Native Storage
According to MotherDuck's documentation on loading data with Estuary:
- **2-10x faster query performance** compared to DuckLake/Parquet
- Optimized specifically for read/write throughput
- Better caching and regional performance
- Best for silver/gold layer data where maximum query speed is needed

### Delta Lake and Apache Iceberg
- Build on Parquet's strengths while adding advanced table management features
- Provide ACID transactions and data versioning
- Delta Lake noted as providing "a better format than Parquet" for data lakehouse scenarios
- Support schema evolution and time travel capabilities

## Less Efficient Formats

### CSV Files
- Simple and portable but lacks compression and schema management
- Less efficient for large-scale datasets
- DuckDB has a "world-class" CSV reader that can handle even messy CSV files
- Useful for compatibility but should be converted to Parquet for better performance
- Can easily convert: `duckdb -c "COPY (SELECT * FROM 'data.csv') TO 'data.parquet';"`

### JSON Files
- Can be read by DuckDB but less efficient than columnar formats
- Better suited for semi-structured data requirements
- Consider converting to Parquet for analytical workloads

## Best Practices for Loading Data

1. **Export to Parquet first**: When migrating from systems like BigQuery, export to Parquet format in object storage (S3, GCS)
2. **Use object storage**: Store Parquet files in S3 or compatible cloud storage for efficient parallel loading
3. **Leverage DuckDB's native capabilities**: Let DuckDB automatically infer schemas and parallelize downloads
4. **Consider storage location**: Keep raw data in DuckLake/Parquet for openness; move processed data to MotherDuck native storage for maximum query performance
5. **For large CSVs**: Convert to Parquet format to benefit from compression and faster processing

## Sources

- [Parquet File Format: What It Is, Benefits, and Alternatives](https://motherduck.com/learn-more/why-choose-parquet-table-file-format)
- [Simple way to convert CSV and Parquet files](https://motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files)
- [How to Efficiently Load Data into DuckLake with Estuary](https://motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary)
- [BigQuery Alternative: MotherDuck](https://motherduck.com/learn-more/bigquery-alternative-motherduck)
- [Taming file zoos: Data science with DuckDB database files](https://motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files)
- [Leveraging DuckDB and Delta Lake Together](https://motherduck.com/videos/leveraging-duckdb-and-delta-lake-together)
