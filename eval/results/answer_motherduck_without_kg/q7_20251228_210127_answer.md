# Answer

If you have a CSV on your laptop and a table in S3, the most efficient way to join them using MotherDuck is to leverage the **Dual Execution** feature, which enables hybrid queries that span local and cloud data sources with automatic optimization for minimal data transfer.

## The Approach: Dual Execution

MotherDuck treats your laptop and the MotherDuck cloud as two nodes in a single distributed system. When you run a hybrid query, the query planner intelligently breaks it down and pushes computation to where the data lives, minimizing data transfer over the network.

Here's how it works step-by-step:

1. **Local Scan**: MotherDuck scans your local CSV file on your laptop
2. **Filter Locally**: If your query has filters, they're applied on your local machine first
3. **Transfer Minimal Data**: Only the filtered/necessary rows are sent to the cloud
4. **Join in the Cloud**: The final join with the S3 table happens in the MotherDuck cloud

## Practical Implementation

### Step 1: Connect to MotherDuck and load required extensions

```sql
INSTALL httpfs;
LOAD httpfs;
CALL load_aws_credentials();
```

### Step 2: Attach the S3 data (if not already in a MotherDuck database)

For Parquet files in S3, you can query them directly:
```sql
SELECT * FROM read_parquet('s3://my-bucket/data/*.parquet')
```

Or attach a DuckDB database file stored on S3:
```sql
ATTACH 's3://<bucket>/my_data.db' (READ_ONLY);
```

### Step 3: Write your join query

You can directly join your local CSV with cloud data in a single query:
```sql
SELECT
    local_data.id,
    local_data.name,
    cloud_table.metric
FROM 'local_file.csv' AS local_data
JOIN my_cloud_db.cloud_table ON local_data.id = cloud_table.id
WHERE local_data.category = 'specific_value';
```

## Key Advantages of This Approach

1. **Automatic Optimization**: MotherDuck's query planner automatically decides which parts run locally (marked as "L" in query plans) and which run remotely (marked as "R")

2. **No Manual ETL Required**: You don't need to upload your entire CSV to the cloud or download the entire S3 table - the "No-ETL" approach lets you query files directly where they live

3. **Schema-on-Read**: DuckDB's `read_csv_auto` function automatically infers column names and data types, so you don't need to define schemas

4. **Cost Efficiency**: By leveraging free local compute on your laptop and minimizing data transfer, you significantly reduce cloud costs

5. **Consistent Engine**: The same DuckDB engine runs both locally and in the cloud, ensuring query consistency

## Verifying Query Optimization

You can use `EXPLAIN` to see how MotherDuck plans to execute your hybrid query:
```sql
EXPLAIN SELECT ...
```

The output shows which parts run locally `(L)` and which run remotely `(R)`, confirming that MotherDuck is optimizing data movement.

## Sources

List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:
- .kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md - Core explanation of hybrid analytics, Dual Execution architecture, and how joins between local CSVs and cloud data work with automatic optimization
- .kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md - Details on querying raw CSV files directly with SQL, the No-ETL approach, and joining local files with cloud data using Dual Execution
- .kurt/sources/motherduck.com/blog/dual-execution-dbt.md - Technical details on dual execution configuration, query plan visualization showing L (local) and R (remote) operators
- .kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md - Overview of hybrid query processing architecture and its academic foundations
- .kurt/sources/motherduck.com/blog/duckdb-the-great-federator.md - Details on DuckDB extensions including httpfs for S3 access, attach syntax for remote databases, and federated query patterns
- .kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md - Reference to querying S3 data lakes with DuckDB
