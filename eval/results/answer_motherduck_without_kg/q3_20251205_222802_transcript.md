# Scenario: answer_motherduck_without_kg

**Status**: âœ… PASSED
**Timestamp**: 20251205_222802

**Duration**: 275.36 seconds
**Tokens Used**: 0

---

## Conversation


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ SCENARIO: answer_motherduck_without_kg
   Answer MotherDuck questions by searching sources directly (no KG)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


ğŸ” Verifying .claude installation...
   âœ“ .claude folder exists at /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.claude
   âœ“ Skills: 0 found
   âœ“ Commands: 1 found

ğŸ§ª Running 3 question(s) defined in questions_motherduck.yaml


======================================================================
â“ Question 1/3
...... What file formats are most efficient for loading data into MotherDuck?
======================================================================


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 1
â”‚ Answer this question by searching through the .kurt/sources/ directory:

...... What file formats are most efficient for loading data into MotherDuck?

Required Steps:
1. Search for relevant markdown files in .kurt/sources/ using glob and grep
2. Read the most relevant files (aim for 3-5 most relevant sources)
3. As you read each file, keep track of:
   - The file path
   - The file title/name
   - Key information extracted from that file
4. Synthesize a comprehensive answer based on the information found
5. Write your answer to: /tmp/answer_without_kg_1.md

IMPORTANT - Your answer MUST have this structure:
```markdown
# Answer

[Your comprehensive answer here, drawing from the sources you read]

## Sources

List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
[etc.]

DO NOT use URLs or document titles as links. Show the actual file path you read with the Read tool.
```

The Sources section is MANDATORY - you must list every file you read to answer the question.

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I'll help you find information about file formats for loading data into MotherDuck. Let me search through the sources directory.
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Glob â†’ **/*.md

  [96mğŸ”§ TOOL:[0m Grep â†’ file format|data format|loading data|import data|CSV|Parquet|JSON
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/spatial-data-management-with-duckdb-ft-mattforrest.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/leveraging-duckdb-and-delta-lake-together.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/going-beyond-the-dataframe.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ibis-one-library-to-query-any-backend.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/about-us.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/acceptable-use-policy.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/adithya-krishnan.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/aditya-aomani.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/alex-monahan.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/amrutha-gujjar.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/andrew-mcewen.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/andrew-witten.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/anna-geller.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/antony-courtney.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/boaz-leskes.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/brett-griffin.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/christophe-oudar.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/daniel-palma.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/diptanu-gon-choudhury.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/doug-raymond.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/emily-lucek.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/faraz-hameed.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/garrett-obrien.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/graham-wetzler.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/hamilton-ulmer.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/jacob-matson.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/jelte-fennema-nio.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/jessica-libman.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/jordan-tigani.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/joseph-hwang.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/leslie-thomas.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/luciano-galvao-filho.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/manveer-chawla.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/marc-d.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/marcos-ortiz.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/mehdi-ouazza.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/michael-hunger.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/mike-spaulding.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/mike-xu.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/nick-ursa.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/nouras-haddad.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/oli-dinov.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/peter-boncz.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/pranav-aurora.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/ryan-boyd.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/sahil-gupta.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/saqib-rasul.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/sean-m-tracey.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/sheila-sitaram.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/simon-spati.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/stephanie-wang.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/sung-won-chung.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/thomas-tfmv-mcgeehan.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/till-dohmen.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/tino-tereshko.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/vasilije-markovic.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/yannick-welsch.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/yves-le-maout.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/zhenni-wu.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/MotherDuck-Visualize-Embeddings-Marimo.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/analyze-data-in-azure-with-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/analyze-json-data-using-sql.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/analyze-sqlite-databases-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/analyze-x-data-nodejs-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-duckdb-snippet-sets-with-motherduck-sharing-databases.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-mega-giga-instance-sizes-huge-scale.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-series-seed-and-a.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-small-data-sf-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/big-data-is-dead.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/bigquery-to-duckdb-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/building-data-applications-with-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/building-motherduck-partner-ecosystem.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/data-app-generator.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/data-engineer-highlights-PyConDE-2023.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/data-engineering-toolkit-essential-tools.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/data-engineering-toolkit-infrastructure-devops.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/data-engineers-answer-10-top-reddit-questions.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-dec-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-nov-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-oct-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/differential-storage-building-block-for-data-warehouse.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/dosomething-motherduck-data-warehouse-ROI.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-110-hidden-gems.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-cognee-sql-analytics-graph-rag.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2023.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eight.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "durationMs": 519,
  [2m  â”‚[0m   "numFiles": 100,
  [2m  â”‚[0m   "truncated": true
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search more specifically for information about file formats and data loading:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ Parquet|parquet|efficient|performance|columnar

  [96mğŸ”§ TOOL:[0m Grep â†’ load.*data|insert.*data|COPY|importing
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-power-of-wasm-for-analytics-duckdb-in-the-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-surprising-birth-of-duckdb-ft-co-creator-hannes-muhleisen.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-if-sql-queries-returned-results-instantly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/spatial-data-management-with-duckdb-ft-mattforrest.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/speed-querying-stackoverflow-data-with-duckdb-ft-michael-hunger.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/sql-ide-safari-harlequin-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/take-flight-with-dbt-and-duckdb-dropping-dev-warehouse-costs-to-zero.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-bad-the-good-and-the-ugly-navigating-the-new-waves-of-machine-learning-ft-adi-polak.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-future-of-bi-exploring-the-impact-of-bi-as-code-tools-with-duckdb.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 30,
  [2m  â”‚[0m   "appliedLimit": 30
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/take-flight-with-the-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-surprising-birth-of-duckdb-ft-co-creator-hannes-muhleisen.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-if-sql-queries-returned-results-instantly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/spatial-data-management-with-duckdb-ft-mattforrest.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/speed-querying-stackoverflow-data-with-duckdb-ft-michael-hunger.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-bad-the-good-and-the-ugly-navigating-the-new-waves-of-machine-learning-ft-adi-polak.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/panel-fixing-the-data-engineering-lifecycle-coalesce-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 30,
  [2m  â”‚[0m   "appliedLimit": 30
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me look for documentation files that might have specific information about data loading:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ docs.*load|docs.*import|docs.*ingest|ingestion

  [96mğŸ”§ TOOL:[0m Glob â†’ **/docs/**/*.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ibis-one-library-to-query-any-backend.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/cultivating-growth-how-gardyn-scaled-its-data-operations-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-dbt-accelerating-the-developer-experience-with-local-power.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "durationMs": 519,
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Great! I found a promising file about Parquet. Let me read several relevant files to get comprehensive information:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     "content": "---\ntitle: why-choose-parquet-table-file-format\ncontent_type: tutorial\nsource_url: https://motherduck.com/learn-more/why-choose-parquet-table-file-format\nindexed_at: '2025-11-25T09:56:57.618325'\ncontent_hash: 2fb6399ae13aa7e1\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO LEARN](https://motherduck.com/learn-more/)\n\n# Parquet File Format: What It Is, Benefits, and Alternatives\n\n11 min readBY\n\n[Aditya Somani](https://motherduck.com/authors/aditya-aomani/)\n\n![Parquet File Format: What It Is, Benefits, and Alternatives](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdataframe_3_3e66a00165.png&w=3840&q=75)\n\nIn data engineering, efficient storage and processing of massive datasets is often very important. As data volumes continue to grow, organizations are turning to innovative file formats and table structures to optimize their data warehousing and analytics workflows. Among these, the Apache Parquet file format has emerged as a popular choice for its columnar storage layout and impressive compression capabilities.\n\nParquet, designed for use with large-scale data processing frameworks like Apache Spark and Hadoop, has gained significant traction in both the \"big data\" and \"small data\" communities. Its ability to enable efficient data storage and retrieval, coupled with its support for complex data types and schema evolution, makes it a compelling option for a wide range of data-intensive applications.\n\nIn this article, we will dive deep into the world of Parquet, exploring its key features, benefits, and use cases. We will also compare Parquet to other popular file formats and table structures, such as Apache Iceberg and Delta Lake, to help you make informed decisions when designing your data architecture.\n\n## Key Takeaways\n\n- **What is Parquet**: An open-source columnar storage file format for efficient analytics.\n- **Core Benefits**: Superior compression, faster query performance (via column pruning and predicate pushdown), and schema evolution support.\n- **Common Alternatives**: Compared to row-based (CSV, Avro), columnar (ORC), and table formats (Iceberg, Delta Lake).\n- **DuckDB & MotherDuck**: Parquet integrates seamlessly with DuckDB for high-performance SQL queries directly on Parquet files.\n\n## What is Parquet?\n\nParquet is a [columnar storage file format](https://motherduck.com/learn-more/columnar-storage-guide/). When data engineers ask 'what is a Parquet file?', the simple answer is that it's a file that stores data in columns, not rows. This Parquet data format is designed for efficient data processing, particularly in the context of big data applications. Developed as part of the Apache Hadoop ecosystem, Parquet has gained widespread adoption due to its ability to optimize storage and query performance.\n\n### Columnar Storage Format\n\nOne of the key characteristics of Parquet is its columnar storage layout. Unlike traditional row-based formats, Parquet organizes data by columns rather than rows. This means that all values for a particular column are stored contiguously on disk, enabling faster retrieval and better compression ratios.\n\nBy storing data in a columnar fashion, Parquet excels at analytical queries that involve reading a subset of columns from a large dataset. This is particularly beneficial for data warehousing and business intelligence scenarios, where queries often focus on specific columns rather than entire rows.\n\nThe columnar storage format also allows for more efficient compression techniques. Since values within a column tend to be of the same data type and exhibit similar patterns, Parquet can apply appropriate compression algorithms to achieve higher compression ratios compared to row-based formats. This reduction in storage footprint translates to lower storage costs and faster query execution, as less data needs to be read from disk.\n\n![Post Image](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fstorage_comparison_1_5c87b9f5c1.svg&w=3840&q=75)\n\n### Open Source and Widely Supported\n\nParquet is an open-source project governed by the Apache Software Foundation, ensuring its transparency, community-driven development, and long-term sustainability. The open-source nature of Parquet has fostered a vibrant ecosystem, with contributions from industry leaders and a growing user base.\n\nOne of the key advantages of Parquet's open-source status is its wide support across various big data processing frameworks. Major platforms like Apache Spark, Apache Hadoop, and Presto have native support for reading and writing Parquet files, making it easy to integrate Parquet into existing data pipelines.\n\nThis broad compatibility ensures interoperability between different tools and systems, allowing organizations to leverage Parquet across their data ecosystem. It also mitigates the risk of vendor lock-in, as Parquet can be used with a variety of open-source and commercial solutions.\n\n## Why Use Parquet?\n\nParquet stands out in data processing environments for its ability to efficiently manage large datasets while ensuring high performance. Its unique approach to data storage enhances retrieval speeds and optimizes storage efficiency. This is achieved through intelligent data organization and compression strategies that minimize unnecessary data handling.\n\n### Efficient Compression and Encoding\n\nParquet's architecture benefits from exceptional data compression capabilities, essential for managing extensive datasets. Instead of treating data uniformly, Parquet leverages specialized encoding techniques such as dictionary, run-length, and delta encoding to optimize storage. These methods reduce the data footprint, translating into cost savings and improved access speeds. By minimizing I/O operations, Parquet enhances query performance, making it ideal for data-intensive tasks.\n\n![Post Image](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fencoding_compression_e1bd66680f.svg&w=3840&q=75)\n\n### Schema Evolution and Nested Data Types\n\nParquet is designed to handle evolving data structures with ease, supporting seamless schema modifications. This flexibility allows for the addition or alteration of columns without disrupting existing workflows, ensuring continuous data integrity. Parquet's proficiency with complex data structures\u2014like nested fields\u2014facilitates versatile data modeling. Its adaptability is vital for accommodating dynamic business requirements and integrating diverse datasets.\n\n### Predicate Pushdown and Column Pruning\n\nParquet effectively supports techniques like predicate pushdown and column pruning, which are crucial for optimizing data queries. By bringing filtering and aggregation operations closer to the storage layer, Parquet reduces the amount of data that needs processing. This approach not only speeds up queries but also lowers computational demands, enabling swift responses. Consequently, Parquet allows data professionals to execute complex queries on extensive datasets efficiently, providing timely and actionable insights.\n\n## Parquet Alternatives and Comparisons\n\nNavigating the landscape of data storage solutions reveals a variety of formats, each offering unique strengths tailored to specific needs. While Parquet remains a prominent choice, exploring its alternatives can provide valuable insights into selecting the best fit for particular data environments.\n\n### CSV Files\n\nCSV files are a straightforward, text-based format that organizes data in a tabular form with rows and columns separated by delimiters. This simplicity makes CSV highly portable and easy to use for data exchange across different systems. However, it lacks advanced functionalities like compression and schema management, which limits its efficiency in handling large-scale datasets.\n\n### Apache ORC\n\nApache ORC stands out with its columnar storage capabilities, optimized for high-performance data processing tasks. It excels in compressing and managing large datasets efficiently, offering features like ACID transactions that ensure data integrity during updates and queries. ORC's tight integration with Hive-specific functionalities makes it an appealing choice for Hadoop ecosystems, enabling seamless operations within Hive data warehouses.\n\n### Apache Avro\n\nApache Avro offers a flexible, row-based format that emphasizes efficient data serialization and schema evolution. Avro provides robust support for evolving schemas, allowing changes without requiring data rewrites\u2014ideal for applications needing frequent schema updates and cross-system data exchange. Its compact binary serialization format enhances data transmission efficiency across distributed systems.\n\n### Delta Lake and Apache Iceberg\n\nDelta Lake and Apache Iceberg build on the strengths of Parquet, introducing advanced table management features. Delta Lake, with its seamless Spark integration, offers capabilities like ACID transactions and data versioning, supporting both batch and streaming data processing. This makes it suitable for environments requiring consistent data updates and real-time analytics.\n\nApache Iceberg is crafted to optimize large-scale data lake operations, providing comprehensive support for multiple file formats, including Parquet. It facilitates complex data management with features like schema evolution and time travel, ensuring data consistency and adaptability across diverse processing engines and storage solutions.\n\n## Choosing the Right Format\n\nSelecting the optimal data format necessitates a keen understanding of your data access patterns. For workloads requiring extensive data scans with a focus on specific data attributes, leveraging formats designed for efficient data retrieval can significantly enhance performance. In contrast, for scenarios where frequent updates or point-specific data access is essential, a format that facilitates rapid row-level operations may be more advantageous.\n\n### Ecosystem Compatibility\n\nThe integration of a data format with existing systems is paramount for operational efficiency. Formats that align well with current data processing tools and frameworks simplify the implementation process and reduce potential disruptions. Evaluating the collective expertise of your team with a given format can inform the decision-making process, ensuring a smooth transition and effective utilization of the chosen technology.\n\n### Data Volume and Scalability\n\nAnticipating the trajectory of data growth and volume is critical in format selection. For environments managing substantial datasets, selecting a format that balances storage efficiency with retrieval speed is crucial. Formats equipped with advanced scalability features, such as those designed for extensive data management, offer robust solutions to handle burgeoning data lakes. These options ensure data integrity while maintaining high performance as data scales.\n\n## How you read and write parquet files in DuckDB\n\nDuckDB provides extensive support for Parquet files, seamlessly integrating them into workflows that demand high-performance data analysis. Users can take advantage of DuckDB's capabilities while preserving the structural and performance benefits of Parquet.\n\n### Reading Parquet Files\n\nWith DuckDB, accessing Parquet files becomes a streamlined process, eliminating the need for data importation. This approach offers:\n\n- **Direct File Access**: DuckDB enables SQL queries directly on Parquet files, allowing immediate data exploration without additional data loading steps.\n- **Optimized Column Retrieval**: By leveraging Parquet's columnar nature, DuckDB efficiently processes column-specific queries, ensuring rapid data retrieval and minimizing unnecessary data scans.\n\nThese features facilitate efficient and effective data analysis, maximizing the performance advantages inherent in Parquet's design.\n\n### Writing Parquet Files\n\nExporting data to Parquet format in DuckDB is straightforward, ensuring that the advantages of Parquet's compression and structuring are retained:\n\n- **SQL-Driven Export**: Users can export results to Parquet with simple SQL commands, preserving the data's integrity and organization.\n- **Advanced Compression Options**: DuckDB supports the use of Parquet's compression methods during export, optimizing file size for storage efficiency.\n\nThese functionalities enable seamless data management from analysis to storage, ensuring that DuckDB users can fully leverage the benefits of both technologies in their data workflows.\n\n## How the support for Parquet files in DuckDB differs from the support for other open file formats like Iceberg and Delta Lake\n\nDuckDB excels in its support for Parquet files, offering direct integration that maximizes the potential of Parquet\u2019s columnar storage benefits. This integration underscores DuckDB\u2019s focus on performance and efficiency. In comparison, its handling of other formats like Iceberg and Delta Lake involves additional considerations due to their distinct capabilities in data management and transactional features.\n\n### Parquet Integration in DuckDB\n\nDuckDB\u2019s architecture is purpose-built to leverage the column-oriented design of Parquet. This allows for seamless data processing and minimizes overhead, enhancing query performance. The optimized handling of Parquet files means that DuckDB can efficiently execute analytical tasks by taking full advantage of Parquet\u2019s compression and encoded structures, making it an ideal choice for applications requiring swift data interrogation.\n\n### Handling Iceberg and Delta Lake\n\nWhen it comes to supporting Iceberg and Delta Lake, DuckDB must navigate the complexities introduced by these formats\u2019 advanced features. Both formats provide robust table management functionalities that extend beyond simple storage solutions and may not be fully supported.\n\n### Performance Considerations\n\nThe performance dynamics between DuckDB and these formats are influenced by the specific use cases they address. Parquet\u2019s design aligns well with DuckDB\u2019s strengths, facilitating efficient data retrieval processes. In contrast, utilizing Iceberg or Delta Lake may necessitate additional performance considerations, particularly when dealing with their advanced metadata and transaction management capabilities, which may impact processing efficiency based on workload demands.\n\nDuckDB\u2019s approach to these formats showcases its flexibility and adaptability, offering robust support for Parquet while accommodating the advanced features of Iceberg and Delta Lake for more complex data management needs.\n\nThe ongoing transformation in data storage and management highlights Parquet's role as a cornerstone of [modern data strategies](https://motherduck.com/learn-more/modern-data-warehouse-playbook/). Its architecture aligns with the demands of large-scale data operations, offering efficient solutions that meet the needs of complex analytics. This adaptability ensures Parquet remains integral across diverse computing environments, valued for its performance and reliability.\n\nEmerging table formats, including Delta Lake and Apache Iceberg, introduce enhancements that build on Parquet's capabilities. These formats offer advanced features like transactional support and time travel, catering to sophisticated data management requirements. Their ability to efficiently handle massive datasets makes them indispensable for organizations seeking comprehensive data governance solutions.\n\nDuckDB's integration of Parquet reflects its commitment to high-performance data processing, optimizing Parquet's strengths for analytical workflows. The platform navigates the complexities of newer formats like Iceberg and Delta Lake, ensuring robust support while maintaining efficient data operations. This synergy exemplifies how DuckDB leverages Parquet's strengths alongside the advanced capabilities of modern table formats to deliver effective data solutions.\n\nAs the data landscape continues to evolve, understanding the intricacies of Parquet and its alternatives is crucial for making informed decisions about your data architecture. By leveraging the strengths of Parquet and integrating it with powerful tools like DuckDB, you can unlock the full potential of your data and drive meaningful insights. If you're ready to experience the benefits of Parquet and explore a collaborative data warehousing solution, [join us at MotherDuck and get started today](https://app.motherduck.com/?auth_flow=signup).\n\n### TABLE OF CONTENTS\n\n[Key Takeaways](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#key-takeaways)\n\n[What is Parquet?](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#what-is-parquet)\n\n[Why Use Parquet?](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#why-use-parquet)\n\n[Parquet Alternatives and Comparisons](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#parquet-alternatives-and-comparisons)\n\n[Choosing the Right Format](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#choosing-the-right-format)\n\n[How you read and write parquet files in DuckDB](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#how-you-read-and-write-parquet-files-in-duckdb)\n\n[How the support for Parquet files in DuckDB differs from the support for other open file formats like Iceberg and Delta Lake](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#how-the-support-for-parquet-files-in-duckdb-differs-from-the-support-for-other-open-file-formats-like-iceberg-and-delta-lake)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\n## FAQS\n\n### What is the Parquet file format?\n\nApache Parquet is an open-source columnar storage file format designed for efficient data processing. Unlike row-based formats like CSV, it stores data by columns, enabling faster query performance and superior compression for large-scale analytical workloads.\n\n### What are the main benefits of using Parquet files?\n\nParquet's main benefits include efficient data compression and encoding, schema evolution support, and performance optimization. It uses techniques like predicate pushdown and column pruning to speed up analytical queries by only reading the data that is needed.\n\n### What are the alternatives to Parquet?\n\nCommon alternatives to Parquet include row-based formats like CSV and Avro, and other columnar formats like Apache ORC. Newer table formats like Delta Lake and Apache Iceberg often use Parquet as their underlying file storage format while adding transactional guarantees.\n\n## Additional Resources\n\n[Docs\\\\\n\\\\\nWhat is a Data Warehouse?](https://motherduck.com/learn-more/what-is-a-data-warehouse/) [Docs\\\\\n\\\\\nWhat is OLAP?](https://motherduck.com/learn-more/what-is-OLAP/) [Docs\\\\\n\\\\\nLoading Data into MotherDuck with Parquet](https://motherduck.com/docs/key-tasks/loading-data-into-motherduck/loading-data-md-python/) [Docs\\\\\n\\\\\nUse DuckDB to Convert CSV to Parquet](https://duckdbsnippets.com/snippets/6/quickly-convert-a-csv-to-parquet-bash-function) [Video\\\\\n\\\\\nQuerying Parquet Files on S3 with DuckDB](https://www.youtube.com/watch?v=fZj6kTwXN1U)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 212,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 212
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     "content": "---\ntitle: simple-way-to-convert-csv-and-parquet-files\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files\nindexed_at: '2025-11-25T20:44:27.252956'\ncontent_hash: 0215f394eabfe4ec\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nSimple way to convert CSV - Parquet files - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Simple way to convert CSV - Parquet files](https://www.youtube.com/watch?v=Y_GXdbet9Gk)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=Y_GXdbet9Gk&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 1:00\n\n\u2022Live\n\n\u2022\n\nYouTubeShortTutorial\n\n# Simple way to convert CSV and Parquet files\n\n2024/01/17\n\nAs developers, we live in the terminal. It\u2019s our command center for everything from Git commits to running applications. But we all have those small, annoying tasks that break our flow\u2014the little papercuts of the development cycle. One of the most common? File format conversion.\n\nYou have a Parquet file, but a legacy tool or a colleague needs it in CSV format. Or maybe you have a massive CSV that you want to compress into the more efficient Parquet format. What's your go-to move? Do you spin up a Jupyter notebook and `import pandas`? Do you write a quick, one-off Python script? Or do you resort to a sketchy online file converter?\n\nThese interruptions, while small, add up. But what if the perfect tool for the job was already on your machine, ready to go?\n\nEnter DuckDB. While you might know it as a powerful embedded analytical database, it's also a versatile Swiss army knife for your data. Because it's a lightweight, serverless tool that speaks fluent SQL and natively understands formats like Parquet, CSV, and JSON, it's the perfect utility for lightning-fast file conversions directly from your command line.\n\nLet's build a powerful, reusable conversion utility in just a few minutes.\n\n### Instant File Conversion with a DuckDB One-Liner\n\nFirst things first, you need the `duckdb` CLI. If you're on macOS, installation is a single command with Homebrew.\n\n```bash\nCopy code\n\nbrew install duckdb\n```\n\nFor other operating systems, check out the [official installation documentation](https://duckdb.org/docs/installation/index).\n\nOnce installed, you have everything you need. Let's say you have a file named `data.parquet` and you want to convert it to `data.csv`. The magic is a single command that leverages DuckDB's powerful `COPY` statement.\n\n```bash\nCopy code\n\nduckdb -c \"COPY (SELECT * FROM 'data.parquet') TO 'data.csv' (HEADER, DELIMITER ',');\"\n```\n\nLet's break down this command to see what's happening:\n\n- `duckdb -c \"...\"`: This is the key to using DuckDB as a scripting tool. The `-c` flag tells DuckDB to execute the SQL command that follows and then immediately exit. No interactive shell, no fuss\u2014just pure, scriptable execution.\n- `COPY (...) TO 'data.csv'`: This is the workhorse. The `COPY` command is incredibly efficient at moving data into and out of DuckDB.\n- `(SELECT * FROM 'data.parquet')`: Instead of copying from a table, we're telling DuckDB to copy the result of a query. The magic here is that DuckDB can query files like Parquet or CSV directly, _as if they were database tables_. It automatically infers the file type and schema from the file extension.\n- `(HEADER, DELIMITER ',')`: These are options specific to the output format. Here, we're telling DuckDB to include a header row in our final CSV file.\n\nAnd that's it. In the time it would have taken you to open a new editor tab, you've converted your file.\n\n### Building a Reusable Conversion Script\n\nThat one-liner is great, but we can make it even better. The real power of the command line comes from creating reusable, generic tools.\n\nLet's wrap this logic into a simple Bash script. Create a file named `file-converter.sh` somewhere convenient, like `~/scripts/`.\n\n```bash\nCopy code\n\n#!/bin/bash\n# file-converter.sh\n\n# Check if two arguments are provided\nif [ \"$#\" -ne 2 ]; then\n    echo \"Usage: $0 <input_file> <output_file>\"\n    exit 1\nfi\n\nINPUT_FILE=$1\nOUTPUT_FILE=$2\n\nduckdb -c \"COPY (SELECT * FROM '${INPUT_FILE}') TO '${OUTPUT_FILE}';\"\n\necho \"Successfully converted ${INPUT_FILE} to ${OUTPUT_FILE}\"\n```\n\nMake the script executable:\n\n```bash\nCopy code\n\nchmod +x ~/scripts/file-converter.sh\n```\n\nNow, you have a generic script that takes an input file and an output file as arguments. The final step is to create a shell alias for ultimate convenience. Open your `.zshrc`, `.bashrc`, or equivalent shell configuration file and add this line:\n\n```bash\nCopy code\n\n# Add to your .zshrc or .bashrc\nalias dconvert='~/scripts/file-converter.sh'\n```\n\nRestart your terminal or run `source ~/.zshrc` to apply the changes. Now, witness your new superpower. You can convert files back and forth with a simple, memorable command.\n\n**Convert Parquet to CSV:**\n\n```bash\nCopy code\n\ndconvert data.parquet data.csv\n# Successfully converted data.parquet to data.csv\n```\n\n**Convert CSV to Parquet:**\n\n```bash\nCopy code\n\ndconvert data.csv data.parquet\n# Successfully converted data.csv to data.parquet\n```\n\nIt doesn't get simpler than that. You've just built a universal file conversion utility that is faster and more reliable than a custom script and safer than any online tool.\n\n### Conclusion: More Than Just Conversion\n\nWe started with a simple problem and ended with an elegant, reusable solution. With one small script, you've added a powerful tool to your developer toolkit, powered by DuckDB.\n\nBut don't forget what's happening inside that command. You aren't just copying bytes; you're running a full-fledged SQL query. This opens up a world of possibilities that go far beyond the simple 1-to-1 conversion our `dconvert` alias handles.\n\nWhat if you only wanted a subset of the data? For more complex tasks, you can bypass the alias and use the `duckdb -c` command directly to run a more powerful query.\n\n```bash\nCopy code\n\n# Filter for specific rows before converting\nduckdb -c \"COPY (SELECT * FROM 'data.parquet' WHERE category = 'A') TO 'filtered_data.csv' (HEADER, DELIMITER ',');\"\n```\n\nWhat if you only needed a few columns?\n\n```bash\nCopy code\n\n# Select specific columns\nduckdb -c \"COPY (SELECT user_id, event_timestamp FROM 'logs.parquet') TO 'events.csv' (HEADER, DELIMITER ',');\"\n```\n\nThe `SELECT` statement is your playground. You can perform filtering, transformations, and even simple aggregations as part of your conversion pipeline, all within that single command.\n\n* * *\n\n### Get Started Today\n\n- Try this out and share your favorite DuckDB one-liners with us on [Twitter](https://twitter.com/motherduckcorp)!\n- For more details on the options available, check out the official DuckDB documentation for the [`COPY` command](https://duckdb.org/docs/sql/data_loading/copy).\n- When your challenges go beyond local files, see how [MotherDuck](https://motherduck.com/) brings the power of DuckDB to the cloud for serverless, collaborative analytics.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Is DuckDB the Secret to Unlocking Your GIS Potential?\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmaxresdefault_1_c988e40ed0.jpg&w=3840&q=75)\\\\\n\\\\\n14:49](https://motherduck.com/videos/is-duckdb-the-secret-to-unlocking-your-gis-potential/)\n\n[2024-08-29](https://motherduck.com/videos/is-duckdb-the-secret-to-unlocking-your-gis-potential/)\n\n### [Is DuckDB the Secret to Unlocking Your GIS Potential?](https://motherduck.com/videos/is-duckdb-the-secret-to-unlocking-your-gis-potential)\n\nIn this video, \u202aMehdi walks you through the basics of working with geospatial data and introduces the DuckDB spatial extension. By the end, you will create your own heatmap using DuckDB, Python, and MotherDuck for sharing and scalability.\n\nYouTube\n\nTutorial\n\n[![\"DuckDB & dataviz | End-To-End Data Engineering Project (3/3)\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fvideo_ta_Pzc2_EE_Eo_23e0b0a9d0.jpg&w=3840&q=75)\\\\\n\\\\\n0:21:46](https://motherduck.com/videos/duckdb-dataviz-end-to-end-data-engineering-project-33/)\n\n[2024-06-27](https://motherduck.com/videos/duckdb-dataviz-end-to-end-data-engineering-project-33/)\n\n### [DuckDB & dataviz \\| End-To-End Data Engineering Project (3/3)](https://motherduck.com/videos/duckdb-dataviz-end-to-end-data-engineering-project-33)\n\nIn this part 3 of the project, @mehdio explores how to build a Dashboard with Evidence using MotherDuck/DuckDb as a data source.\n\nYouTube\n\nBI & Visualization\n\nTutorial\n\n[![\"One data tool with all its dependencies: DuckDB and extensions\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fvideo_we_RMT_Aj_Udic_9e9f5c6d41.jpg&w=3840&q=75)\\\\\n\\\\\n0:00:55](https://motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions/)\n\n[2024-06-21](https://motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions/)\n\n### [One data tool with all its dependencies: DuckDB and extensions](https://motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions)\n\nLearn about DuckDB extensions, including the ability to query data in your AWS S3-powered data lake.\n\nYouTube\n\nShort\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 267,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 267
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     "content": "---\ntitle: why-csvs-still-matter-the-indispensable-file-format\ncontent_type: event\nsource_url: https://motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format\nindexed_at: '2025-11-25T20:44:48.426444'\ncontent_hash: 3788e1cc125b23f9\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nWhy CSVs Still Matter: The Indispensable File Format - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Why CSVs Still Matter: The Indispensable File Format](https://www.youtube.com/watch?v=I07qV2hij4E)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=I07qV2hij4E&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 58:06\n\n\u2022Live\n\n\u2022\n\nYouTubeSources\n\n# Why CSVs Still Matter: The Indispensable File Format\n\n2024/05/07\n\nIn this discussion with Mehdi Ouazza and Pedro Holanda from DuckDB, they talk about all things CSVs and dive into a pragmatic example of how the CSV parser works in DuckDB. Enjoy!\n\n## Related Videos\n\n[![\" pg_duckdb: Ducking awesome analytics in Postgres\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2F0kc_W5o0tcg_HD_03880f61fb.jpg&w=3840&q=75)](https://motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres/)\n\n[2025-06-12](https://motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres/)\n\n### [pg\\_duckdb: Ducking awesome analytics in Postgres](https://motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres)\n\nSupercharge your Postgres analytics! This talk shows how the pg\\_duckdb extension accelerates your slowest queries instantly, often with zero code changes. Learn practical tips and how to use remote columnar storage for even more speed.\n\nTalk\n\nSources\n\n[![\"Can DuckDB revolutionize the data lake experience?\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmaxresdefault_2074c1c09e.jpg&w=3840&q=75)\\\\\n\\\\\n16:37](https://motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience/)\n\n[2024-11-22](https://motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience/)\n\n### [Can DuckDB revolutionize the data lake experience?](https://motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience)\n\nMehdi explores DuckDB as a catalog for Data Lake and Lakehouse pattern. He'll define what we mean by \"data catalog\", gives clear examples on how they work and dive into a pragmatic use case with DuckDB & MotherDuck.\n\nYouTube\n\nData Pipelines\n\nSources\n\n[![\"Leveraging DuckDB and Delta Lake Together\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmaxresdefault_2_426f95538d.jpg&w=3840&q=75)\\\\\n\\\\\n1:02:10](https://motherduck.com/videos/leveraging-duckdb-and-delta-lake-together/)\n\n[2024-07-24](https://motherduck.com/videos/leveraging-duckdb-and-delta-lake-together/)\n\n### [Leveraging DuckDB and Delta Lake Together](https://motherduck.com/videos/leveraging-duckdb-and-delta-lake-together)\n\nQuack & Code on table formats and Delta Lake with Holly Smith from Databricks. Can you use DuckDB and MotherDuck to query your data lakehouse based on Delta Lake? Yes! And Delta Lake provides a better format than Parquet.\n\nYouTube\n\nSources\n\nQuack & Code\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 127,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 127
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     "content": "---\ntitle: how-to-efficiently-load-data-into-ducklake-with-estuary\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary\nindexed_at: '2025-11-25T20:44:50.287804'\ncontent_hash: 54211f956cd60bed\nhas_step_by_step: true\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nHow to Efficiently Load Data into DuckLake with Estuary - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[How to Efficiently Load Data into DuckLake with Estuary](https://www.youtube.com/watch?v=8uce9V9VnjY)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Why am I seeing this?](https://support.google.com/youtube/answer/9004474?hl=en)\n\n[Watch on](https://www.youtube.com/watch?v=8uce9V9VnjY&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 50:08\n\n\u2022Live\n\n\u2022\n\nYouTube\n\n# How to Efficiently Load Data into DuckLake with Estuary\n\n2025/07/26\n\n## Introduction to DuckLake and Real-Time Data Integration\n\nDuckLake represents a new open table format created by the developers of DuckDB. Unlike traditional data lake formats that store metadata in JSON or Avro files within blob storage, DuckLake takes a different approach by storing metadata in a relational database. This architectural decision significantly speeds up metadata transactions, making operations faster and more efficient.\n\n## Understanding MotherDuck's Cloud Data Warehouse\n\nMotherDuck is a cloud data warehouse designed to make big data feel small. Built on top of DuckDB, it focuses on three key principles:\n\n- **Eliminating complexity** from traditional distributed data warehouses\n- **Enabling fast insights** while keeping developers in their workflow\n- **Leveraging DuckDB's performance** for cost-efficient operations\n\nThe platform integrates seamlessly with existing data stack tools and supports standard SQL queries with enhanced features.\n\n## Estuary's Real-Time Data Integration Platform\n\nEstuary provides a real-time data integration platform that supports both streaming and batch data movement. The platform features:\n\n### Architecture Components\n\n- **Capture connectors** that extract data from source systems using change data capture (CDC)\n- **Collections** stored in object storage (S3, GCS, or compatible systems)\n- **Materialization connectors** that load data into destinations\n\n### Key Capabilities\n\n- Support for hundreds of source and destination systems\n- Native CDC for databases like PostgreSQL, MySQL, and Oracle\n- Real-time extraction from SaaS applications like Salesforce and HubSpot\n- No-code setup with managed service\n\n## Setting Up a Real-Time Pipeline to DuckLake\n\nThe process of loading data into DuckLake involves several straightforward steps:\n\n### Source Configuration\n\n1. Connect to your source database (PostgreSQL, MySQL, etc.)\n2. Enable change data capture to track real-time changes\n3. Configure schema evolution settings for automatic handling of schema changes\n\n### DuckLake Setup\n\n1. Create a DuckLake database in MotherDuck\n2. Configure access to your S3 bucket where data will be stored\n3. Set up appropriate access tokens for read/write permissions\n\n### Pipeline Configuration\n\n- Choose sync frequency (from real-time to scheduled batches)\n- Select specific fields to materialize\n- Configure merge queries for maintaining latest state\n\n## Performance Optimization with MotherDuck\n\n### Instant SQL Feature\n\nMotherDuck introduces Instant SQL, which provides query results at the speed of typing by:\n\n- Pre-caching data for immediate feedback\n- Validating SQL syntax in real-time\n- Enabling rapid iteration on complex queries\n\n### Storage Trade-offs\n\nWhen deciding between DuckLake and MotherDuck native storage:\n\n**DuckLake advantages:**\n\n- Open format with broader ecosystem compatibility\n- Support for Spark and other compute engines\n- Better suited for petabyte-scale workloads\n\n**MotherDuck storage advantages:**\n\n- 2-10x faster query performance\n- Optimized for read/write throughput\n- Better caching and regional performance\n\n## Scaling Considerations\n\nMotherDuck now offers larger instance sizes (Mega and Giga) to support intensive data lake operations. These instances are comparable to Snowflake 3XL configurations and enable:\n\n- Terabyte to petabyte-scale operations\n- Complex aggregations and sorting\n- Efficient medallion architecture implementations\n\n## Best Practices for Implementation\n\n### Data Architecture\n\n- Keep raw data in DuckLake for openness and flexibility\n- Move silver/gold layer data to MotherDuck storage for performance\n- Use Estuary's intermediate storage for reliability and replay capabilities\n\n### Partitioning Strategy\n\nWhile Estuary doesn't natively configure partitions, you can:\n\n1. Allow Estuary to create initial tables\n2. Use `ALTER TABLE` commands to add partitions\n3. Subsequent writes will respect partition configuration\n\n### Error Handling and Reliability\n\nThe architecture prevents common streaming issues:\n\n- Intermediate storage prevents message loss\n- Automatic handling of destination unavailability\n- Support for backfills without re-querying sources\n\n## Integration with Modern Data Stack\n\nThe combination of Estuary and MotherDuck integrates with existing tools:\n\n- dbt support through DuckDB adapter\n- AI-powered SQL writing assistance\n- Automatic error detection and fixing\n- Support for multiple materialization targets from single source\n\nThis architecture enables organizations to implement real-time data pipelines without the traditional complexity of streaming systems, making the difference between batch and streaming simply a configuration toggle rather than an architectural decision.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Data-based: Going Beyond the Dataframe\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FData_based_f32745b461.png&w=3840&q=75)](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n[2025-11-20](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n### [Data-based: Going Beyond the Dataframe](https://motherduck.com/videos/going-beyond-the-dataframe)\n\nLearn how to turbocharge your Python data work using DuckDB and MotherDuck with Pandas. We walk through performance comparisons, exploratory data analysis on bigger datasets, and an end-to-end ML feature engineering pipeline.\n\nWebinar\n\nPython\n\nAI, ML and LLMs\n\n[![\"Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FHex_Webinar_778e3959e4.png&w=3840&q=75)](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n[2025-11-14](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n### [Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck)\n\nAI isn't here to replace data work, it's here to make it better. Watch this webinar to see how Hex and MotherDuck build AI workflows that prioritize context, iteration, and real-world impact.\n\nWebinar\n\nAI, ML and LLMs\n\n[![\"Lies, Damn Lies, and Benchmarks\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FLies_Damn_Lies_and_Benchmarks_Thumbnail_404db1bf46.png&w=3840&q=75)](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n[2025-10-31](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n### [Lies, Damn Lies, and Benchmarks](https://motherduck.com/videos/lies-damn-lies-and-benchmarks)\n\nWhy do database benchmarks so often mislead? MotherDuck CEO Jordan Tigani discusses the pitfalls of performance benchmarking, lessons from BigQuery, and why your own workload is the only benchmark that truly matters.\n\nStream\n\nInterview\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 247,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 247
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search for more specific information about data loading and documentation:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ loading.*data.*motherduck|insert.*performance|bulk.*load|COPY.*INTO

  [96mğŸ”§ TOOL:[0m Glob â†’ **/blog/*load*.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "durationMs": 383,
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/4-lightning-talks-on-practical-ai-workflows-from-notion-1password-motherduck-evidence.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/pandas-dataframes-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/product-analytics-motherduck-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/airbyte.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/layers-multi-tenant-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vector-technologies-ai-data-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/spark-ducklake-getting-started.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/quacktosql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-duckdb-release.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me read one more relevant file on partitioned writes:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     "content": "---\ntitle: Fast Data Exports with DuckDB's Partitioned Writes and DuckLake\ncontent_type: tutorial\ndescription: This post dives into the practical art of exporting data from DuckDB\n  with structure, security, and speed in mind. You\u2019ll get a crash course in Hive-style\n  partitioning, learn how to use COPY TO ... PARTITION_BY effectively, and explore\n  how DuckLake builds on these concepts with catalog-aware writes and encryption support.\n  Packed with hands-on code examples, file format tips, and performance advice, this\n  guide helps you turn raw data into a streamlined, query-friendly lakehouse\u2014no quackery\n  involved.\npublished_date: '2025-06-12T00:00:00'\nsource_url: https://motherduck.com/learn-more/partitioned-writes-parquet-ducklake\nindexed_at: '2025-11-25T20:37:04.208683'\ncontent_hash: 0d0993bbc0c4f2d8\nhas_code_examples: true\nhas_step_by_step: true\n---\n\n# Fast Data Exports with DuckDB's Partitioned Writes and DuckLake\n\n14 min readBYEver caught yourself staring at a terminal watching a data export process crawl along, wondering if there's a better way? Or maybe you've opened a folder containing thousands of tiny files from your data lake, each needing to be scanned for a simple query? I've been right there with you in the data engineering trenches.\n\nAs someone who's spent countless hours optimizing data pipelines, I've found that efficient data organization isn't just about storage\u2014it's about making your future self (and your queries) much happier. Today, I want to share a technique that's become an essential part of my toolkit: **DuckDB's partitioned write capability using the **.\n\n`COPY TO`\n\ncommandWhile `COPY TO ... PARTITION_BY`\n\nis a useful tool for exporting data into a structured format, it's also worth looking at **DuckLake**, an integrated data lake and catalog format built on DuckDB, which offers more advanced data management features including its own approach to partitioned writes and capabilities like data encryption.\n\nIn this post, we'll wade through:\n\n-\n**What Hive Partitioning is**(just enough to be dangerous) -\n**Using**in DuckDB to create logically structured data directories`COPY TO ... PARTITION_BY`\n\n-\n**Partitioned Writes with DuckLake**: An alternative approach leveraging catalog metadata -\n**Encrypted Data with DuckLake**: Adding a layer of security to your data lake -\n**Practical code examples**covering different file formats, overwriting strategies, compression options, and custom filenames using`COPY TO`\n\n-\n**Performance considerations**that could save you hours of debugging\n\nBy the time we finish, you'll have a clear understanding of how to export data from DuckDB in a way that makes downstream processing significantly more efficient, and how DuckLake extends these concepts for a more complete lakehouse experience. No quack solutions here\u2014just solid, proven techniques.\n\n## What's This \"Hive Partitioning\" Thing Anyway?\n\nBefore we dive into the code, let's quickly understand *what* we're trying to create.\n\nHive Partitioning is essentially an organizational pattern that structures data files into directories based on column values from specific fields. Imagine you have sales data with `year`\n\nand `month`\n\ncolumns\u2014instead of one massive file, you'd get a directory structure like:\n\nCopy code\n\n```\nsales_data/\n\u251c\u2500\u2500 year=2023/\n\u2502 \u251c\u2500\u2500 month=11/\n\u2502 \u2502 \u2514\u2500\u2500 data_0.parquet\n\u2502 \u2514\u2500\u2500 month=12/\n\u2502 \u251c\u2500\u2500 data_0.parquet\n\u2502 \u2514\u2500\u2500 data_1.parquet\n\u2514\u2500\u2500 year=2024/\n\u2514\u2500\u2500 month=01/\n\u2514\u2500\u2500 data_0.parquet\n```\n\n\nNotice the pattern? `column_name=value`\n\nis used for each directory level. This structure is particularly useful because query engines (including our feathered friend DuckDB) can use these directory names to skip over data that's irrelevant to your query.\n\nIf you run a query like `WHERE year = 2024 AND month = 1`\n\n, the engine knows it only needs to look inside the `year=2024/month=01/`\n\ndirectory, completely ignoring everything else. This capability, known as filter pushdown or partition pruning, can improve query performance on large datasets, especially on slower storage systems like S3 or HDFS.\n\nWhile `COPY TO ... PARTITION_BY`\n\ndirectly creates this physical directory structure based on the partition columns, DuckLake also utilizes partitioning, albeit with a different underlying mechanism. In DuckLake, partitioning keys are defined on a table, and new data is split into separate files based on these keys - the partition keys for a file are stored in the DuckLake metadata catalog as well as the file paths. This allows DuckLake to use the catalog database in addition to the file paths for efficient query planning and pruning.\n\n## Writing Partitioned Data with COPY TO\n\nDuckDB makes [writing data](https://duckdb.org/docs/stable/data/partitioning/partitioned_writes.html) in this Hive-partitioned format straightforward using the `PARTITION_BY`\n\nclause within the `COPY TO`\n\nstatement. The basic concept is telling DuckDB which columns to use for creating the directory structure.\n\nLet's assume we have an `orders`\n\ntable in DuckDB that looks like this:\n\n| order_id | customer_id | order_date | amount |\n|---|---|---|---|\n| 1 | 101 | 2023-11-15 | 50.00 |\n| 2 | 102 | 2023-12-01 | 75.50 |\n| 3 | 101 | 2024-01-20 | 120.00 |\n\nFor the examples that follow, assume we've added `year`\n\nand `month`\n\ncolumns derived from `order_date`\n\n.\n\n### Example 1: Basic Partitioned Write to Parquet\n\nLet's start with a simple example, writing our table to Parquet files, partitioned by year and month:\n\nCopy code\n\n```\nCOPY orders TO 'output_orders_parquet' (\nFORMAT PARQUET,\nPARTITION_BY (year, month)\n);\n```\n\n\nHere's what's happening: `COPY orders TO 'output_orders_parquet'`\n\nexports data from the orders table to a directory named output_orders_parquet. DuckDB will create this directory if it doesn't exist. `FORMAT PARQUET`\n\nspecifies Parquet as our output format, which is generally a wise choice for analytics due to its columnar structure and efficient compression. `PARTITION_BY (year, month)`\n\nis the key part\u2014DuckDB uses the year and month columns to create a nested directory structure, automatically extracting the distinct values from these columns to create the `year=YYYY/month=MM/`\n\npaths.\n\nAfter running this command, you'll find files organized just like in our example directory structure above, with each partition containing its own Parquet file(s). If this looks something like what DuckLake does internally, that is because DuckLake handles in a similar way (as previously discussed).\n\n### Example 2: Writing Partitioned CSV with Overwriting\n\nPerhaps Parquet isn't your target format, or you need to overwrite existing data during a pipeline rerun:\n\nCopy code\n\n```\nCOPY orders TO 'output_orders_csv' (\nFORMAT CSV,\nHEADER, -- Include a header row in the CSV files\nPARTITION_BY (year, month),\nOVERWRITE_OR_IGNORE TRUE -- Use with caution!\n);\n```\n\n\nHere we're writing CSV files this time with `FORMAT CSV, HEADER`\n\nto include header rows. `OVERWRITE_OR_IGNORE TRUE`\n\ntells DuckDB to remove the entire target directory and its contents before writing the new partitioned data. Handle with care, especially in production! Some remote file systems like S3 might not support overwriting in the same way as local storage.\n\n### Example 3: Compression and Custom File Extensions\n\nLet's say you want to save storage space by adding compression:\n\nCopy code\n\n```\nCOPY orders TO 'output_orders_compressed_csv' (\nFORMAT CSV,\nHEADER,\nPARTITION_BY (year, month),\nCOMPRESSION GZIP\n);\n```\n\n\n`COMPRESSION GZIP`\n\nspecifies the compression codec. DuckDB supports others like ZSTD too.\n\n### Example 4: Appending Data to Existing Partitions\n\nOften in data pipelines, you don't want to overwrite\u2014you just want to add new data to an existing partitioned structure:\n\nCopy code\n\n```\nCOPY new_orders TO 'output_orders_parquet' (\nFORMAT PARQUET,\nPARTITION_BY (year, month),\nAPPEND TRUE\n);\n```\n\n\n`APPEND TRUE`\n\nallows DuckDB to add new files to the existing directory structure without deleting what's already there. It's perfect for incremental data loads.\n\n### Example 5: Custom Filenames\n\nNot a fan of the default `data_0.parquet`\n\nnaming? You can customize filenames using patterns:\n\nCopy code\n\n```\n-- Using an index\nCOPY orders TO 'output_orders_indexed' (\nFORMAT PARQUET,\nPARTITION_BY (year, month),\nOVERWRITE_OR_IGNORE TRUE,\nFILENAME_PATTERN 'order_data_{i}' -- {i} gets replaced by an index\n);\n-- Using a UUID for guaranteed unique names\nCOPY orders TO 'output_orders_uuid' (\nFORMAT PARQUET,\nPARTITION_BY (year, month),\nOVERWRITE_OR_IGNORE TRUE,\nFILENAME_PATTERN 'file_{uuid}' -- {uuid} gets replaced by a unique ID\n);\n```\n\n\n`FILENAME_PATTERN`\n\nallows you to define a template for the output data files within each partition. `{i}`\n\nis replaced by a sequential index (0, 1, 2...) while `{uuid}`\n\nis replaced by a unique 128-bit identifier, which is useful for preventing conflicts if multiple processes might write to the same partition directory when using APPEND.\n\n### Example 6: Creating Partition Columns On-the-Fly\n\nWhat if your source table doesn't have year and month columns directly, but has a timestamp? You can generate them within the COPY statement:\n\nCopy code\n\n```\nCOPY (\nSELECT\n*, -- Select all original columns\nyear(order_timestamp) AS year, -- Extract year\nmonth(order_timestamp) AS month -- Extract month\nFROM raw_orders -- Assuming raw_orders has order_timestamp\n)\nTO 'output_orders_generated_partitions' (\nFORMAT PARQUET,\nPARTITION_BY (year, month), -- Use the newly created columns\nOVERWRITE_OR_IGNORE TRUE\n);\n```\n\n\nInstead of copying directly from a table name, we use `COPY (SELECT ... FROM ...)`\n\n. Inside the SELECT, we extract the year and month from `order_timestamp`\n\nand alias them, then use these newly created aliases in the `PARTITION_BY`\n\nclause. This is a flexible way to partition based on derived values.\n\n[as discussed here](https://stackoverflow.com/questions/78849044/avoid-writing-partition-column-in-parquet-file-duckdb).\n\n#### Partitioning parquet files by number of rows or file size\n\nDuckDB does not directly support writing out based on a number of rows or file size. However, DuckDB includes an implicit column in every table called rowid that maintains the insertion order of the row. This column can be used to partition writes to parquet based on \u201cpartition_id\u201d, i.e.\n\nCopy code\n\n```\nCOPY (\nSELECT\n*, -- Select all original columns\n(rowid/100000)::int as partition_id\nFROM raw_orders -- Assuming raw_orders has order_timestamp\n)\nTO 'output_orders_generated_partitions' (\nFORMAT PARQUET,\nPARTITION_BY partition_id, -- Use the newly created columns\nOVERWRITE_OR_IGNORE TRUE\n);\n```\n\n\nThis will split the files based on the partition_id, without first sorting the data. In order to use this to get files of a certain size, change the calculation for your partition_id. When using this approach, it should be noted that fixed files sizes are not supported and that rowid is not updated when data is deleted, so if deletes are common the data, alternatives such as ROW_NUMBER() can be used, although this can require the entire dataset to be sorted before writing so should be used sparingly with large datasets.\n\n## Partitioned Writes with DuckLake\n\nDuckLake offers a different paradigm for managing partitioned data within a lakehouse context. Instead of relying solely on the physical Hive directory structure, DuckLake uses a dedicated metadata catalog (stored in a SQL database) to manage information about data files and their partitions.\n\nWhen you define partition keys for a DuckLake table using `ALTER TABLE ... SET PARTITIONED BY`\n\n, DuckLake ensures that new data written to that table is physically split into separate data files based on those keys, similar to Hive partitioning. However, the key difference is that DuckLake stores the mapping between partition values and the corresponding data files in its catalog in addition to the file paths.\n\nThis partitioning strategy provides several advantages: improved query planning where query engines can leverage the metadata in the catalog for more efficient partition pruning without needing to list directories or read file footers on the storage layer, partition evolution where you can change the partitioning scheme of a DuckLake table over time and this only affects new data written while previously written data retains its original partitioning, and flexibility where partition keys do not necessarily need to be embedded in the file paths, offering more flexibility in file organization.\n\nHere's how you interact with partitioning in DuckLake:\n\nCopy code\n\n```\n-- Install the ducklake extension\nINSTALL ducklake;\n-- Attach to a DuckLake database\nATTACH 'ducklake:my_ducklake.ducklake' AS my_ducklake;\nUSE my_ducklake;\n-- Copy the previous orders table into the ducklake\nCREATE TABLE my_ducklake.orders AS SELECT * FROM my_database.orders;\n-- Set partitioning keys for the table\nALTER TABLE orders SET PARTITIONED BY (year, month);\n-- Subsequent INSERT or COPY operations into 'my_ducklake.orders'\n-- will automatically partition data by year and month based on the table definition.\n-- Remove partitioning keys\nALTER TABLE orders RESET PARTITIONED BY;\n```\n\n\nThis approach shifts the focus from the physical directory structure as the primary source of partitioning information to the centralized metadata catalog, enabling more dynamic and robust data management.\n\n## Encrypted Data with DuckLake\n\nSecuring data is important, especially when storing it in a data lake on potentially untrusted storage locations. DuckLake provides a built-in mechanism for encrypting data files written to the data store.\n\nWhen the encrypted mode is enabled for a DuckLake catalog, all Parquet files created by DuckLake operations are automatically encrypted using Parquet encryption. The encryption keys are not static\u2014DuckLake automatically generates unique encryption keys for each file when it's written. These file-specific encryption keys are then securely stored within the DuckLake metadata catalog itself, in the encryption_key field of the ducklake_data_file table.\n\nWhen data needs to be read from encrypted files, DuckLake retrieves the corresponding encryption keys from the catalog and automatically uses them to decrypt the files during the read process. This process is transparent to the user, allowing interaction with encrypted DuckLake databases in the same manner as unencrypted ones.\n\nEnabling encryption is done during the initialization of the DuckLake catalog using the ATTACH statement with the ENCRYPTED flag:\n\nCopy code\n\n```\n-- Attach to or create an encrypted DuckLake database\nATTACH 'ducklake:encrypted_mylake.ducklake'\n(DATA_PATH 'untrusted_location/', ENCRYPTED);\n-- Now, any data written to tables within 'encrypted_mylake'\n-- will be automatically encrypted.\n```\n\n\nManaging the encryption keys within the trusted metadata catalog database adds an extra layer of security compared to storing keys alongside the data files or relying on external key management systems for this specific function. This feature is a useful advantage of using DuckLake for building a secure data lakehouse.\n\n## Performance Considerations & Best Practices\n\nWhile partitioning and formats like DuckLake can make your data much more navigable for query engines, there are some key considerations to keep in mind:\n\n### Avoid Too Many Small Partitions\n\nCreating a huge number of partitions (e.g., partitioning by a high-cardinality ID field or by the second) can be counterproductive. Each partition typically means at least one file, and managing thousands or millions of tiny files can slow down file listing operations (especially on cloud storage) and the write process itself. For `COPY TO PARTITION_BY`\n\n, this also directly translates to a large number of directories.\nA good rule of thumb is to aim for partitions that are reasonably sized, perhaps at least 100MB each, though the ideal size depends on your data and access patterns. DuckLake mitigates the \"small files problem\" to some extent by storing metadata in a database and supporting features like data inlining for small changes, but the principle of avoiding excessive partitions still holds.\n\n### Limit Open Files During Write (for `COPY TO`\n\n)\n\nWhen writing to many partitions simultaneously using `COPY TO`\n\n, DuckDB needs to keep multiple files open. You can control the maximum number of files kept open before flushing using a setting:\n\nCopy code\n\n```\nSET partitioned_write_max_open_files = 50; -- Default is 100\n```\n\n\nLowering this value might help if you run into memory issues or \"too many open files\" errors during large partitioned writes, potentially at the cost of some write speed. DuckLake's write process is managed differently via its catalog and internal mechanisms, so this setting is primarily relevant for direct `COPY TO PARTITION_BY`\n\noperations.\n\n### File Format Matters\n\nParquet generally offers superior read performance due to its columnar nature and built-in statistics (like min/max values per column chunk, called row groups). These statistics can help DuckDB (and DuckLake) [skip reading parts of files](https://duckdb.org/2021/06/25/querying-parquet.html) even within a partition. CSV doesn't have these advantages, so if query speed is important to you, Parquet is usually the way to go. DuckLake specifically mandates Parquet for its data files.\n\n### Consider Data Skew\n\nIf your partitioning key is skewed (e.g., 90% of your data falls into one year partition), the benefits of partitioning might be limited for queries hitting that large partition. Choose your partitioning columns wisely, considering your typical query patterns. This applies to both `COPY TO PARTITION_BY`\n\nand DuckLake partitioning.\n\n## Wrapping Up\n\nThere you have it\u2014a practical look at how DuckDB's `COPY TO ... PARTITION_BY`\n\ncan help you export data into a structured, Hive-partitioned layout, and how DuckLake builds upon the concept of partitioning with advanced features like catalog-managed metadata and data encryption.\n\nWhile no data organization strategy fits all use cases, partitioning gives you flexibility to match your storage layout to your query patterns. Though, remember the trade-offs: partitioning adds some overhead during writes and works best when your query patterns align with your partition keys. But when used appropriately, both `COPY TO PARTITION_BY`\n\nand DuckLake's partitioning are useful techniques to have in your data engineering arsenal.\n\nSo next time you're about to export that massive dataset, consider whether a bit of partitioning (whether via `COPY TO`\n\nor within a DuckLake structure) might make your future self thank you. As we like to say in the data world, \"A minute of partitioning saves an hour of querying!\"\n\nStart using MotherDuck now!",
  [2m  â”‚[0m     "numLines": 399,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 399
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     "content": "---\ntitle: csv-files-persist-duckdb-solution\ncontent_type: blog\nsource_url: https://motherduck.com/blog/csv-files-persist-duckdb-solution\nindexed_at: '2025-11-25T19:57:13.232430'\ncontent_hash: 28f2997ebdb99804\nhas_code_examples: true\nhas_step_by_step: true\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Why CSV Files Won\u2019t Die and How DuckDB Conquers Them\n\n2025/02/04 - 9 min read\n\nBY\n\n[Mehdi Ouazza](https://motherduck.com/authors/mehdi-ouazza/)\n\nI've been working in the data field for a decade, across various companies, and one constant challenge that\u2019s almost unavoidable is dealing with CSV files.\n\nYes, there are far more efficient formats, such as [Parquet](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/), which avoid schema nightmares thanks to their typing, but CSV files persist for many reasons:\n\n- They\u2019re easy to edit and read, requiring no dependencies\u2014just open the file.\n- They\u2019re universal: many services still exchange data in CSV format.\n- Want to download data from social media or your CRM? CSV.\n- Need transaction history from your bank? CSV.\n\nHowever, this simplicity comes with its own set of challenges, especially if you want to process CSVs without breaking pipelines or pulling your hair out.\n\nFortunately, DuckDB has an exceptional CSV parser. The team behind it invested heavily in building their own, and in this post, I\u2019ll show you a real-world example where I had to parse multiple CSV files. I\u2019ll also share some SQL tricks and demonstrate how smoothly everything worked using DuckDB and MotherDuck, resulting in a ready-to-query database.\n\nThe cherry on top? The final output is a database containing all Stack Overflow survey responses from the past seven years. Stick around if you\u2019re curious about extracting insights or querying the data yourself!\n\n## The biggest challenges when reading CSVs\n\nIn my opinion, there are four significant challenges when working with CSV files:\n\n1. **Schema Management**\n2. **Row-Level Errors**\n3. **Encoding Issues**\n\nThese challenges become even more complex when handling multiple CSVs that need to be read or joined to each other.\n\nLet\u2019s see how we address these issues with Stack Overflow survey data.\n\n## About the Dataset\n\nEach year, Stack Overflow publishes the results of their developer survey, including raw data in\u2014you guessed it\u2014CSV format. These files are available on their website: [https://survey.stackoverflow.co/](https://survey.stackoverflow.co/).\n\nHere\u2019s an example of how the dataset is organized:\n\n```css\nCopy code\n\n\u251c\u2500\u2500 raw\n\u2502   \u251c\u2500\u2500 2011 Stack Overflow Survey Results.csv\n\u2502   \u251c\u2500\u2500 2012 Stack Overflow Survey Results.csv\n\u2502   \u251c\u2500\u2500 2013 Stack Overflow Survey Responses.csv\n\u2502   \u251c\u2500\u2500 2014 Stack Overflow Survey Responses.csv\n\u2502   \u251c\u2500\u2500 2015 Stack Overflow Developer Survey Responses.csv\n\u2502   \u251c\u2500\u2500 2016 Stack Overflow Survey Results\n\u2502   \u2502   \u251c\u2500\u2500 2016 Stack Overflow Survey Responses.csv\n\u2502   \u2502   \u2514\u2500\u2500 READ_ME_-_The_Public_2016_Stack_Overflow_Developer_Survey_Results.txt\n\u2502   \u251c\u2500\u2500 stack-overflow-developer-survey-2017\n\u2502   \u2502   \u251c\u2500\u2500 DeveloperSurvey2017QuestionaireCleaned.pdf\n\u2502   \u2502   \u251c\u2500\u2500 README_2017.txt\n\u2502   \u2502   \u251c\u2500\u2500 survey_results_public.csv\n\u2502   \u2502   \u2514\u2500\u2500 survey_results_schema.csv\n\u2502   \u251c\u2500\u2500 stack-overflow-developer-survey-2018\n\u2502   \u2502   \u251c\u2500\u2500 Developer_Survey_Instrument_2018.pdf\n\u2502   \u2502   \u251c\u2500\u2500 README_2018.txt\n\u2502   \u2502   \u251c\u2500\u2500 survey_results_public.csv\n\u2502   \u2502   \u2514\u2500\u2500 survey_results_schema.csv\n\u2502   \u251c\u2500\u2500 stack-overflow-developer-survey-2019\n\u2502   \u2502   \u251c\u2500\u2500 README_2019.txt\n\u2502   \u2502   \u251c\u2500\u2500 so_survey_2019.pdf\n\u2502   \u2502   \u251c\u2500\u2500 survey_results_public.csv\n\u2502   \u2502   \u2514\u2500\u2500 survey_results_schema.csv\n[..]\n```\n\nKey observations:\n\n1. **Schema Changes Over the Years**\n\n\nSome questions and their formats evolve annually, making it difficult to standardize across years.\n2. **Pre-2016 Format**\n\n\nEach column represents a question, with names like:\n\n`What Country or Region do you live in?, How old are you?, How many years of IT/Programming experience do you have?, ...`\n\nAdditional challenges include:\n\n\u2022 Column names with unusual characters.\n\n\u2022 Querying such column names can be tedious.\n\nFrom 2017 onward, Stack Overflow improved the exports by separating:\n\n\u2022 A file containing the answers (columns with clean names for each question).\n\n\u2022 A schema file (.csv) that maps question codes to full question text.\n\nTo keep things manageable, I focused on datasets from 2017 onward.\n\n## Manual cleaning over automation\n\nWe\u2019ve all wasted hours trying to automate tasks that could have been done manually in minutes. This is a common trap for data engineers. Sometimes, quick manual cleanup is the most efficient approach.\n\nHere\u2019s what I did:\n\n\u2022 Placed all CSVs in a single folder.\n\n\u2022 Renamed files by adding the corresponding year as a prefix (e.g., `<year>_<file_name>`).\n\n\u2022 Ensured column names in schema files were consistent (e.g., renamed name to qname where needed).\n\nThese steps took less than five minutes and saved me headaches later. Not everything needs to be automated!\n\n## Loading the CSVs\n\nNow for the exciting part: loading the data. DuckDB supports glob patterns for loading multiple files. For complex structures like [Hive partitions](https://duckdb.org/docs/data/partitioning/hive_partitioning.html), it works seamlessly too.\n\nHere\u2019s the core query for loading survey results:\n\n```sql\nCopy code\n\nCREATE OR REPLACE TABLE stackoverflow_survey.survey_results AS\n    SELECT\n        * EXCLUDE (filename),\n        substring(parse_filename(filename), 1, 4) as year,\n    FROM read_csv_auto(\n        'data_2017_2024/*survey_results*.csv',\n        union_by_name=true,\n        filename=true)\n```\n\n**Breakdown:**\n\n1. We `CREATE` a table based on a `SELECT` statement.\n2. We select all columns but `EXCLUDE` the filename. This is a path of the containing file; we get this one by enabling `filename=true`.\n3. We parse the `filename` to get only the year. As we have a convention on the file name to prefix by `<year>`, we take the first four chars and create a `year` column\n4. We use the glob pattern to only load `*survey_results*` as a single table (we'll do another query for the `survey_schemas`)\n\nAlright, let's run this one... \ud83d\ude4f\n\n```sql\nCopy code\n\nduckdb.duckdb.ConversionException: Conversion Error: CSV Error on Line: 35365\nOriginal Line: 35499,I am a developer by profession,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA\nError when converting column \"Hobbyist\". Could not convert string \"NA\" to 'BOOLEAN'\n\nColumn Hobbyist is being converted as type BOOLEAN\nThis type was auto-detected from the CSV file.\nPossible solutions:\n* Override the type for this column manually by setting the type explicitly, e.g. types={'Hobbyist': 'VARCHAR'}\n* Set the sample size to a larger value to enable the auto-detection to scan more values, e.g. sample_size=-1\n* Use a COPY statement to automatically derive types from an existing table.\n\n  file = ./2017_2024_schema/2020_survey_results_public.csv\n  delimiter = , (Auto-Detected)\n  quote = \" (Auto-Detected)\n  escape = \" (Auto-Detected)\n  new_line = \\n (Auto-Detected)\n  header = true (Auto-Detected)\n  skip_rows = 0 (Auto-Detected)\n  comment = \\0 (Auto-Detected)\n  date_format =  (Auto-Detected)\n  timestamp_format =  (Auto-Detected)\n  null_padding = 0\n  sample_size = 20480\n  ignore_errors = false\n  all_varchar = 0\n```\n\nBad news, it didn't successfully parse the CSVs. But the GREAT news is that we have an excellent log error!\n\nWe know :\n\n- On which line we have an issue\n- A proper error message `Could not convert string \"NA\" to 'BOOLEAN'`\n- Possibles solutions\n\nThis saves so much time! Sometimes, just one row can mess up the whole process, and if the error message isn\u2019t clear, you\u2019re stuck guessing what went wrong. You might even end up throwing out your CSV or trying random fixes over and over.\n\nFor us, increasing the sample\\_size fixed the problem right away. \ud83d\udc4d\n\n## Wrapping up and automate the rest\n\nWith the initial query successful, the next steps were to:\n\n1. Repeat the process for schema files.\n2. Add row count checks to ensure no data was lost during merging of the CSVs\n\nHere's a generic function to wrap the query we saw and run them depending on the pattern name of the files (either for `results` or `schemas`).\n\n```python\nCopy code\n\nCSV_DIR = './data_2017_2024'\n\n# Global configuration\nFILE_CONFIGS = [\\\n    {'pattern': 'schema', 'table': 'survey_schemas'},\\\n    {'pattern': 'public', 'table': 'survey_results'}\\\n]\n\ndef process_survey_files(csv_dir: str) -> None:\n    \"\"\"\n    Process Stack Overflow survey CSV files and load them into DuckDB tables\n    \"\"\"\n    con = duckdb.connect('stackoverflow_survey.db')\n\n    for config in FILE_CONFIGS:\n        logging.info(f\"Processing {config['pattern']} files...\")\n        con.execute(f\"\"\"\n            CREATE OR REPLACE TABLE stackoverflow_survey.{config['table']} AS\n            SELECT\n                * EXCLUDE (filename),\n                substring(parse_filename(filename), 1, 4) as year,\n            FROM read_csv_auto(\n                '{csv_dir}/*{config['pattern']}*.csv',\n                union_by_name=true,\n                filename=true,\n                sample_size=-1\n            )\n        \"\"\")\n\n        # Log row count\n        count = con.execute(f\"SELECT COUNT(*) FROM stackoverflow_survey.{config['table']}\").fetchone()[0]\n        logging.info(f\"Loaded {count} rows into {config['table']}\")\n\n        # Log unique years\n        years = con.execute(f\"SELECT DISTINCT year FROM stackoverflow_survey.{config['table']} ORDER BY year\").fetchall()\n        logging.info(f\"{config['table']} years: {[year[0] for year in years]}\")\n\n    con.close()\n```\n\nFinally, we added another function to check row count and make sure we didn't lose any rows during the process :\n\n```python\nCopy code\n\ndef verify_row_counts(csv_dir: str) -> None:\n    \"\"\"\n    Verify that the sum of individual file counts matches the merged table counts\n    \"\"\"\n    con = duckdb.connect('stackoverflow_survey.db')\n\n    for config in FILE_CONFIGS:\n        pattern = config['pattern']\n        table = config['table']\n\n        logging.info(f\"\\nVerifying {pattern} files counts...\")\n        individual_counts = 0\n\n        for filename in os.listdir(csv_dir):\n            if pattern in filename and filename.endswith('.csv'):\n                file_path = os.path.join(csv_dir, filename)\n                count = con.execute(f\"SELECT COUNT(*) FROM read_csv_auto('{file_path}')\").fetchone()[0]\n                logging.info(f\"{filename}: {count} rows\")\n                individual_counts += count\n\n        merged_count = con.execute(f\"SELECT COUNT(*) FROM stackoverflow_survey.{table}\").fetchone()[0]\n        logging.info(f\"Individual {pattern} files total: {individual_counts}\")\n        logging.info(f\"Merged {table} total: {merged_count}\")\n\n        assert individual_counts  merged_count, f\"{pattern} row count mismatch: {individual_counts} != {merged_count}\"\n\n    con.close()\n    logging.info(\"\u2705 All row counts verified successfully!\")\n```\n\n## Sharing the dataset\n\nNow that I have a DuckDB database containing both tables (results and schemas), the only thing left is to share it! Let's see how that works with MotherDuck.\n\nI\u2019m using the DuckDB CLI, but this could also be part of a Python script. It\u2019s just four simple commands:\n\n```sql\nCopy code\n\nduckdb\nD ATTACH 'stackoverflow_survey.db'\nD ATTACH 'md:'\nD CREATE DATABASE cloud_stackoverflow_survey FROM stackoverflow_survey;\nD CREATE SHARE FROM cloud_stackoverflow_survey;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                            share_url                            \u2502\n\u2502                             varchar                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 md:_share/sample_data/23b0d623-1361-421d-ae77-125701d471e6      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n1. We attach the local DuckDB database with `ATTACH` command.\n2. We connect to MotherDuck using `ATTACH 'md';`. Note that I have my [`motherduck_token`](https://motherduck.com/docs/key-tasks/authenticating-and-connecting-to-motherduck/authenticating-to-motherduck/#authentication-using-an-access-token) stored in an `ENV`.\n3. We upload the database to MotherDuck using the `CREATE DATABASE x FROM x`\n4. We create a public share so that anyone can start querying!\n\nTo make it even easier for MotherDuck users, I put this one in the existing demo database [`sample_data`](https://motherduck.com/docs/getting-started/sample-data-queries/datasets/), which is attached by default for any users.\n\n## Querying the dataset\n\nThis dataset offers plenty of opportunities to uncover insights, but I\u2019ll wrap up this blog with a simple query that wasn\u2019t included in the original StackOverflow study.\n\nI wanted to explore the average happiness score of people based on their work location (remote, in-person, or hybrid).\n\n```sql\nCopy code\n\nSELECT RemoteWork,\n       AVG(CAST(JobSat AS DOUBLE)) AS AvgJobSatisfaction,\n       COUNT(*) AS RespondentCount\nFROM sample_data.stackoverflow_survey.survey_results\nWHERE JobSat NOT IN ('NA')\n  AND RemoteWork NOT IN ('NA')\n  AND YEAR='2024'\nGROUP BY ALL;\n```\n\nand the results :\n\n```sql\nCopy code\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              RemoteWork              \u2502 AvgJobSatisfaction \u2502 RespondentCount \u2502\n\u2502               varchar                \u2502       double       \u2502      int64      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 In-person                            \u2502  6.628152818991098 \u2502            5392 \u2502\n\u2502 Remote                               \u2502  7.072592992884806 \u2502           11103 \u2502\n\u2502 Hybrid (some remote, some in-person) \u2502  6.944303596894311 \u2502           12622 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nTwo interesting takeaways: remote and ybrid workers make up the majority of survey responses, and on average, they seem to be happier too!\n\nCheck out [our documentation](https://motherduck.com/docs/getting-started/sample-data-queries/stackoverflow-survey/) if you want to explore this dataset further.\n\nIn the meantime, get ready to tackle future CSV challenges with ease\u2014DuckDB and MotherDuck (start for [free!](https://motherduck.com/get-started/)) have got you covered!\n\n* * *\n\n### Why DuckDB\u2019s CSV Parser is Special\n\n- [https://duckdb.org/2023/10/27/csv-sniffer.html](https://duckdb.org/2023/10/27/csv-sniffer.html)\n- [https://duckdb.org/2024/12/05/csv-files-dethroning-parquet-or-not.html](https://duckdb.org/2024/12/05/csv-files-dethroning-parquet-or-not.html)\n- [Why CSVs Still Matter: The Indispensable File Format](https://youtu.be/I07qV2hij4E?si=DjCapBT3eg5UWLdn)\n\n### TABLE OF CONTENTS\n\n[The biggest challenges when reading CSVs](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#the-biggest-challenges-when-reading-csvs)\n\n[About the Dataset](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#about-the-dataset)\n\n[Manual cleaning over automation](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#manual-cleaning-over-automation)\n\n[Loading the CSVs](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#loading-the-csvs)\n\n[Wrapping up and automate the rest](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#wrapping-up-and-automate-the-rest)\n\n[Sharing the dataset](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#sharing-the-dataset)\n\n[Querying the dataset](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#querying-the-dataset)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![DuckDB Ecosystem: January 2025](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fnewsletter_a65cff5430.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025/)\n\n[2025/01/10 - Simon Sp\u00e4ti](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025/)\n\n### [DuckDB Ecosystem: January 2025](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025)\n\nDuckDB Monthly #25: PyIceberg, 0$ data distribution and more!\n\n[![Local dev and cloud prod for faster dbt development](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FLocal_Dev_Cloud_Prod_083b07b92e.png&w=3840&q=75)](https://motherduck.com/blog/dual-execution-dbt/)\n\n[2025/01/16 - Jacob Matson](https://motherduck.com/blog/dual-execution-dbt/)\n\n### [Local dev and cloud prod for faster dbt development](https://motherduck.com/blog/dual-execution-dbt)\n\nSpark the Joy of beautiful local development workflows with MotherDuck & dbt\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 418,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 418
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! Now I have comprehensive information. Let me write the answer to the specified file:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_1.md

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Since this is a new file, let me check if it exists first:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ ls -la /tmp/answer_without_kg_1.md 2>&1 || echo "File does not exist"
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m -rw-------  1 julien  wheel  5678 Dec  4 21:45 /tmp/answer_without_kg_1.md
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m The file exists, so let me read it first then write:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /tmp/answer_without_kg_1.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/tmp/answer_without_kg_1.md",
  [2m  â”‚[0m     "content": "# Answer\n\nBased on the MotherDuck documentation, **Parquet is the most efficient file format for loading data into MotherDuck**. Here's why:\n\n## Parquet: The Optimal Format\n\nParquet is a columnar storage file format that offers significant advantages for loading data into MotherDuck and DuckDB:\n\n### Superior Compression\nParquet achieves exceptional compression ratios through specialized encoding techniques including dictionary encoding, run-length encoding (RLE), and delta encoding. This reduces storage footprint and accelerates data transfer, which translates to faster loading times and lower costs.\n\n### Columnar Storage Architecture\nUnlike row-based formats like CSV, Parquet stores data by columns rather than rows. This columnar layout enables:\n- **Faster query performance** through column pruning and predicate pushdown\n- **Reduced I/O operations** by reading only necessary columns\n- **Better compression** since values within a column are of the same data type\n\n### Native DuckDB Integration\nDuckDB (which powers MotherDuck) has optimized support for Parquet files:\n- Direct file access without requiring data importation\n- Ability to query Parquet files directly using SQL\n- Seamless reading and writing with simple commands like `COPY`\n\n### Performance Benefits\nThe columnar format allows DuckDB to leverage modern CPU architectures through:\n- Vectorized query execution processing data in batches\n- SIMD (Single Instruction, Multiple Data) parallel processing\n- Efficient use of CPU caches\n\n## Alternative Formats Comparison\n\n### CSV Files\nWhile CSV is simple and portable, it has significant limitations:\n- No compression or schema management\n- Inefficient for large-scale datasets\n- Row-based format doesn't optimize for analytics\n- Still supported by DuckDB but less efficient than Parquet\n\n### Apache ORC\nORC is another columnar format optimized for Hadoop ecosystems with ACID transaction support. However, Parquet has broader adoption and better integration with DuckDB.\n\n### Apache Avro\nAvro is a row-based format focused on data serialization and schema evolution. While useful for data exchange and streaming scenarios, it's less efficient than Parquet for analytical workloads.\n\n### Table Formats: Delta Lake and Apache Iceberg\nThese formats build on Parquet's strengths, using Parquet as their underlying storage while adding advanced features like ACID transactions and time travel. For MotherDuck specifically:\n- **DuckLake** (MotherDuck's native format) stores metadata in a relational database for faster transactions\n- **MotherDuck native storage** offers 2-10x faster query performance compared to open formats\n\n## Loading Best Practices\n\n### Batch Loading\nLoad data in large batches (thousands or millions of rows at a time) rather than row-by-row for optimal performance. Columnar databases like MotherDuck are optimized for bulk operations.\n\n### Direct File Reading\nDuckDB can query Parquet files directly without a formal `CREATE TABLE` step:\n```sql\nSELECT * FROM 'data.parquet'\n```\n\n### Data Organization\nFor maximum efficiency:\n- **Sort or cluster data** by commonly filtered columns (like timestamps)\n- **Use partitioning** to break large tables into manageable pieces\n- **Leverage zone maps** for automatic data skipping during queries\n\n### Format Conversion\nDuckDB makes it simple to convert between formats. For example, converting CSV to Parquet:\n```bash\nduckdb -c \"COPY (SELECT * FROM 'data.csv') TO 'data.parquet';\"\n```\n\n## Storage Trade-offs\n\nWhen choosing between formats in MotherDuck:\n\n**DuckLake (Open Format)**:\n- Broader ecosystem compatibility\n- Support for multiple compute engines\n- Better for petabyte-scale workloads\n\n**MotherDuck Native Storage**:\n- 2-10x faster query performance\n- Optimized read/write throughput\n- Better caching and regional performance\n\nA recommended architecture is to keep raw data in DuckLake/Parquet for openness, then move processed (silver/gold layer) data to MotherDuck native storage for maximum performance.\n\n## Conclusion\n\nParquet is unequivocally the most efficient file format for loading data into MotherDuck due to its columnar architecture, superior compression, and native integration with DuckDB. While other formats are supported, Parquet provides the best balance of storage efficiency, loading speed, and query performance for analytical workloads.\n\n## Sources\n\nList each source file you actually read and used, showing the EXACT LOCAL FILE PATH:\n- .kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md - Comprehensive guide explaining Parquet format benefits, compression techniques, comparison with alternatives (CSV, ORC, Avro, Delta Lake, Iceberg), and DuckDB integration\n- .kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md - Technical details on columnar vs row-oriented storage, compression methods (dictionary encoding, RLE, delta encoding, FSST), vectorized execution, and best practices for implementation\n- .kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md - Practical tutorial on converting between CSV and Parquet formats using DuckDB, demonstrating the COPY command and direct file querying\n- .kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md - Information on data ingestion methods, direct file reading capabilities, and the efficiency of querying Parquet files without formal table creation\n- .kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md - Best practices for loading data, performance optimization strategies, storage trade-offs between DuckLake and MotherDuck native storage, and batch loading recommendations\n",
  [2m  â”‚[0m     "numLines": 101,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 101
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now I can write my updated answer:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_1.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "update",
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_1.md",
  [2m  â”‚[0m   "content": "<6285 chars>",
  [2m  â”‚[0m   "structuredPatch": [
  [2m  â”‚[0m     {
  [2m  â”‚[0m       "oldStart": 1,
  [2m  â”‚[0m       "oldLines": 100,
  [2m  â”‚[0m       "newStart": 1,
  [2m  â”‚[0m       "newLines": 102,
  [2m  â”‚[0m       "lines": [
  [2m  â”‚[0m         " # Answer",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-Based on the MotherDuck documentation, **Parquet is the most efficient file format for loading data into MotherDuck**. Here's why:",
  [2m  â”‚[0m         "+When loading data into MotherDuck, **Parquet is the most efficient file format**, followed by other columnar formats. Here's what makes different formats efficient or inefficient:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Parquet: The Optimal Format",
  [2m  â”‚[0m         "+## Most Efficient: Parquet",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-Parquet is a columnar storage file format that offers significant advantages for loading data into MotherDuck and DuckDB:",
  [2m  â”‚[0m         "+Parquet is the optimal choice for loading data into MotherDuck due to several key advantages:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### Superior Compression",
  [2m  â”‚[0m         "-Parquet achieves exceptional compression ratios through specialized encoding techniques including dictionary encoding, run-length encoding (RLE), and delta encoding. This reduces storage footprint and accelerates data transfer, which translates to faster loading times and lower costs.",
  [2m  â”‚[0m         "+1. **Columnar Storage**: Parquet organizes data by columns rather than rows, which enables faster retrieval when analytical queries focus on specific columns. All values for a particular column are stored contiguously on disk, enabling better compression ratios.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### Columnar Storage Architecture",
  [2m  â”‚[0m         "-Unlike row-based formats like CSV, Parquet stores data by columns rather than rows. This columnar layout enables:",
  [2m  â”‚[0m         "-- **Faster query performance** through column pruning and predicate pushdown",
  [2m  â”‚[0m         "-- **Reduced I/O operations** by reading only necessary columns",
  [2m  â”‚[0m         "-- **Better compression** since values within a column are of the same data type",
  [2m  â”‚[0m         "+2. **Superior Compression**: Parquet uses specialized encoding techniques such as dictionary encoding, run-length encoding, and delta encoding to optimize storage. This reduces the data footprint, translating to cost savings and improved access speeds by minimizing I/O operations.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### Native DuckDB Integration",
  [2m  â”‚[0m         "-DuckDB (which powers MotherDuck) has optimized support for Parquet files:",
  [2m  â”‚[0m         "-- Direct file access without requiring data importation",
  [2m  â”‚[0m         "-- Ability to query Parquet files directly using SQL",
  [2m  â”‚[0m         "-- Seamless reading and writing with simple commands like `COPY`",
  [2m  â”‚[0m         "+3. **Query Optimization Features**:",
  [2m  â”‚[0m         "+   - **Predicate Pushdown**: Filtering operations are pushed closer to the storage layer, reducing the amount of data that needs processing",
  [2m  â”‚[0m         "+   - **Column Pruning**: Only the columns needed for a query are read from disk",
  [2m  â”‚[0m         "+   - **Built-in Statistics**: Parquet files contain min/max values per column chunk (row groups) that help DuckDB skip reading parts of files that don't match query predicates",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### Performance Benefits",
  [2m  â”‚[0m         "-The columnar format allows DuckDB to leverage modern CPU architectures through:",
  [2m  â”‚[0m         "-- Vectorized query execution processing data in batches",
  [2m  â”‚[0m         "-- SIMD (Single Instruction, Multiple Data) parallel processing",
  [2m  â”‚[0m         "-- Efficient use of CPU caches",
  [2m  â”‚[0m         "+4. **Schema Evolution Support**: Parquet handles evolving data structures with ease, allowing seamless schema modifications like adding or altering columns without disrupting existing workflows.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Alternative Formats Comparison",
  [2m  â”‚[0m         "+5. **DuckDB/MotherDuck Optimization**: DuckDB's architecture is purpose-built to leverage Parquet's column-oriented design. The platform can directly query Parquet files without importing them first, and supports efficient SQL-driven exports to Parquet with advanced compression options.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "+## Other Format Considerations",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         " ### CSV Files",
  [2m  â”‚[0m         "-While CSV is simple and portable, it has significant limitations:",
  [2m  â”‚[0m         "-- No compression or schema management",
  [2m  â”‚[0m         "-- Inefficient for large-scale datasets",
  [2m  â”‚[0m         "-- Row-based format doesn't optimize for analytics",
  [2m  â”‚[0m         "-- Still supported by DuckDB but less efficient than Parquet",
  [2m  â”‚[0m         "+- **Pros**: Simple, universal, human-readable, and easy to edit with no dependencies",
  [2m  â”‚[0m         "+- **Cons**:",
  [2m  â”‚[0m         "+  - Lacks compression and schema management",
  [2m  â”‚[0m         "+  - Row-based format (not optimized for analytical queries)",
  [2m  â”‚[0m         "+  - No built-in statistics for query optimization",
  [2m  â”‚[0m         "+  - Requires more storage space",
  [2m  â”‚[0m         "+  - Can have encoding and parsing issues",
  [2m  â”‚[0m         "+- **Use Case**: Good for data exchange and initial data loading, but should be converted to Parquet for efficient querying",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         " ### Apache ORC",
  [2m  â”‚[0m         "-ORC is another columnar format optimized for Hadoop ecosystems with ACID transaction support. However, Parquet has broader adoption and better integration with DuckDB.",
  [2m  â”‚[0m         "+- **Pros**: Columnar storage with efficient compression, ACID transaction support",
  [2m  â”‚[0m         "+- **Cons**: Primarily optimized for Hive/Hadoop ecosystems",
  [2m  â”‚[0m         "+- **Efficiency**: Similar to Parquet but less universally supported",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         " ### Apache Avro",
  [2m  â”‚[0m         "-Avro is a row-based format focused on data serialization and schema evolution. While useful for data exchange and streaming scenarios, it's less efficient than Parquet for analytical workloads.",
  [2m  â”‚[0m         "+- **Pros**: Flexible row-based format with good schema evolution support",
  [2m  â”‚[0m         "+- **Cons**: Row-based design makes it less efficient for analytical queries compared to columnar formats",
  [2m  â”‚[0m         "+- **Use Case**: Better for streaming and cross-system data exchange than analytics",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### Table Formats: Delta Lake and Apache Iceberg",
  [2m  â”‚[0m         "-These formats build on Parquet's strengths, using Parquet as their underlying storage while adding advanced features like ACID transactions and time travel. For MotherDuck specifically:",
  [2m  â”‚[0m         "-- **DuckLake** (MotherDuck's native format) stores metadata in a relational database for faster transactions",
  [2m  â”‚[0m         "-- **MotherDuck native storage** offers 2-10x faster query performance compared to open formats",
  [2m  â”‚[0m         "+### Delta Lake and Apache Iceberg",
  [2m  â”‚[0m         "+- **Note**: These are table formats, not file formats. They typically use Parquet as their underlying file format while adding transaction management, versioning, and metadata capabilities.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Loading Best Practices",
  [2m  â”‚[0m         "+## Performance Best Practices",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### Batch Loading",
  [2m  â”‚[0m         "-Load data in large batches (thousands or millions of rows at a time) rather than row-by-row for optimal performance. Columnar databases like MotherDuck are optimized for bulk operations.",
  [2m  â”‚[0m         "+When loading data into MotherDuck:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### Direct File Reading",
  [2m  â”‚[0m         "-DuckDB can query Parquet files directly without a formal `CREATE TABLE` step:",
  [2m  â”‚[0m         "-```sql",
  [2m  â”‚[0m         "-SELECT * FROM 'data.parquet'",
  [2m  â”‚[0m         "-```",
  [2m  â”‚[0m         "+1. **Use Partitioned Writes**: For large datasets, partition Parquet files using `COPY TO ... PARTITION_BY` with appropriate partition keys (e.g., by date) to improve query performance through partition pruning.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### Data Organization",
  [2m  â”‚[0m         "-For maximum efficiency:",
  [2m  â”‚[0m         "-- **Sort or cluster data** by commonly filtered columns (like timestamps)",
  [2m  â”‚[0m         "-- **Use partitioning** to break large tables into manageable pieces",
  [2m  â”‚[0m         "-- **Leverage zone maps** for automatic data skipping during queries",
  [2m  â”‚[0m         "+2. **Avoid Too Many Small Files**: Aim for partition sizes of at least 100MB each. Thousands of tiny files slow down file listing operations, especially on cloud storage.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### Format Conversion",
  [2m  â”‚[0m         "-DuckDB makes it simple to convert between formats. For example, converting CSV to Parquet:",
  [2m  â”‚[0m         "-```bash",
  [2m  â”‚[0m         "-duckdb -c \"COPY (SELECT * FROM 'data.csv') TO 'data.parquet';\"",
  [2m  â”‚[0m         "-```",
  [2m  â”‚[0m         "+3. **Consider Compression**: DuckDB supports compression options like GZIP and ZSTD when writing Parquet files, which can further reduce storage costs.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Storage Trade-offs",
  [2m  â”‚[0m         "+4. **Direct Parquet Queries**: DuckDB can query Parquet files directly without loading them into a table first, which is faster for exploratory analysis.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-When choosing between formats in MotherDuck:",
  [2m  â”‚[0m         "+5. **DuckLake for Advanced Features**: For petabyte-scale workloads requiring open format compatibility with Spark and other engines, consider DuckLake. However, MotherDuck's native storage offers 2-10x faster query performance for most workloads.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-**DuckLake (Open Format)**:",
  [2m  â”‚[0m         "-- Broader ecosystem compatibility",
  [2m  â”‚[0m         "-- Support for multiple compute engines",
  [2m  â”‚[0m         "-- Better for petabyte-scale workloads",
  [2m  â”‚[0m         "+## Storage Trade-offs",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "+**DuckLake (with Parquet)**:",
  [2m  â”‚[0m         "+- Open format with broader ecosystem compatibility",
  [2m  â”‚[0m         "+- Support for Spark and other compute engines",
  [2m  â”‚[0m         "+- Better suited for petabyte-scale workloads",
  [2m  â”‚[0m         "+- Supports encryption for secure data lakes",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         " **MotherDuck Native Storage**:",
  [2m  â”‚[0m         "-- 2-10x faster query performance",
  [2m  â”‚[0m         "-- Optimized read/write throughput",
  [2m  â”‚[0m         "+- 2-10x faster query performance than DuckLake",
  [2m  â”‚[0m         "+- Optimized for read/write throughput",
  [2m  â”‚[0m         " - Better caching and regional performance",
  [2m  â”‚[0m         "+- Best for most analytical workloads",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-A recommended architecture is to keep raw data in DuckLake/Parquet for openness, then move processed (silver/gold layer) data to MotherDuck native storage for maximum performance.",
  [2m  â”‚[0m         "+## Practical Loading Examples",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Conclusion",
  [2m  â”‚[0m         "+**Convert CSV to Parquet for efficient loading:**",
  [2m  â”‚[0m         "+```bash",
  [2m  â”‚[0m         "+duckdb -c \"COPY (SELECT * FROM 'data.csv') TO 'data.parquet';\"",
  [2m  â”‚[0m         "+```",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-Parquet is unequivocally the most efficient file format for loading data into MotherDuck due to its columnar architecture, superior compression, and native integration with DuckDB. While other formats are supported, Parquet provides the best balance of storage efficiency, loading speed, and query performance for analytical workloads.",
  [2m  â”‚[0m         "+**Query Parquet files directly:**",
  [2m  â”‚[0m         "+```sql",
  [2m  â”‚[0m         "+SELECT * FROM 'data.parquet';",
  [2m  â”‚[0m         "+```",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "+**Partitioned writes for large datasets:**",
  [2m  â”‚[0m         "+```sql",
  [2m  â”‚[0m         "+COPY orders TO 'output_orders_parquet' (",
  [2m  â”‚[0m         "+  FORMAT PARQUET,",
  [2m  â”‚[0m         "+  PARTITION_BY (year, month)",
  [2m  â”‚[0m         "+);",
  [2m  â”‚[0m         "+```",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         " ## Sources",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:",
  [2m  â”‚[0m         "-- .kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md - Comprehensive guide explaining Parquet format benefits, compression techniques, comparison with alternatives (CSV, ORC, Avro, Delta Lake, Iceberg), and DuckDB integration",
  [2m  â”‚[0m         "-- .kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md - Technical details on columnar vs row-oriented storage, compression methods (dictionary encoding, RLE, delta encoding, FSST), vectorized execution, and best practices for implementation",
  [2m  â”‚[0m         "-- .kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md - Practical tutorial on converting between CSV and Parquet formats using DuckDB, demonstrating the COPY command and direct file querying",
  [2m  â”‚[0m         "-- .kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md - Information on data ingestion methods, direct file reading capabilities, and the efficiency of querying Parquet files without formal table creation",
  [2m  â”‚[0m         "-- .kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md - Best practices for loading data, performance optimization strategies, storage trade-offs between DuckLake and MotherDuck native storage, and batch loading recommendations",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md - Comprehensive guide on Parquet format benefits including columnar storage, compression techniques, predicate pushdown, column pruning, schema evolution, and DuckDB integration",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md - Tutorial on converting between CSV and Parquet formats using DuckDB's COPY command with practical examples",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md - Discussion on CSV file format characteristics and limitations compared to columnar formats",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md - Details on efficient data loading into DuckLake, performance optimization with MotherDuck, and storage trade-offs between DuckLake and MotherDuck native storage (2-10x performance difference)",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md - In-depth guide on partitioned writes with COPY TO PARTITION_BY, Hive partitioning, compression options, filename patterns, and performance best practices including avoiding too many small files",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md - Real-world example of loading CSV files with DuckDB, comparison with Parquet efficiency, and detailed explanation of why Parquet is superior for analytical workloads"
  [2m  â”‚[0m       ]
  [2m  â”‚[0m     }
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "originalFile": "# Answer\n\nBased on the MotherDuck documentation, **Parquet is the most efficient file format for loading data into MotherDuck**. Here's why:\n\n## Parquet: The Optimal Format\n\nParquet is a columnar storage file format that offers significant advantages for loading data into MotherDuck and DuckDB:\n\n### Superior Compression\nParquet achieves exceptional compression ratios through specialized encoding techniques including dictionary encoding, run-length encoding (RLE), and delta encoding. This reduces storage footprint and accelerates data transfer, which translates to faster loading times and lower costs.\n\n### Columnar Storage Architecture\nUnlike row-based formats like CSV, Parquet stores data by columns rather than rows. This columnar layout enables:\n- **Faster query performance** through column pruning and predicate pushdown\n- **Reduced I/O operations** by reading only necessary columns\n- **Better compression** since values within a column are of the same data type\n\n### Native DuckDB Integration\nDuckDB (which powers MotherDuck) has optimized support for Parquet files:\n- Direct file access without requiring data importation\n- Ability to query Parquet files directly using SQL\n- Seamless reading and writing with simple commands like `COPY`\n\n### Performance Benefits\nThe columnar format allows DuckDB to leverage modern CPU architectures through:\n- Vectorized query execution processing data in batches\n- SIMD (Single Instruction, Multiple Data) parallel processing\n- Efficient use of CPU caches\n\n## Alternative Formats Comparison\n\n### CSV Files\nWhile CSV is simple and portable, it has significant limitations:\n- No compression or schema management\n- Inefficient for large-scale datasets\n- Row-based format doesn't optimize for analytics\n- Still supported by DuckDB but less efficient than Parquet\n\n### Apache ORC\nORC is another columnar format optimized for Hadoop ecosystems with ACID transaction support. However, Parquet has broader adoption and better integration with DuckDB.\n\n### Apache Avro\nAvro is a row-based format focused on data serialization and schema evolution. While useful for data exchange and streaming scenarios, it's less efficient than Parquet for analytical workloads.\n\n### Table Formats: Delta Lake and Apache Iceberg\nThese formats build on Parquet's strengths, using Parquet as their underlying storage while adding advanced features like ACID transactions and time travel. For MotherDuck specifically:\n- **DuckLake** (MotherDuck's native format) stores metadata in a relational database for faster transactions\n- **MotherDuck native storage** offers 2-10x faster query performance compared to open formats\n\n## Loading Best Practices\n\n### Batch Loading\nLoad data in large batches (thousands or millions of rows at a time) rather than row-by-row for optimal performance. Columnar databases like MotherDuck are optimized for bulk operations.\n\n### Direct File Reading\nDuckDB can query Parquet files directly without a formal `CREATE TABLE` step:\n```sql\nSELECT * FROM 'data.parquet'\n```\n\n### Data Organization\nFor maximum efficiency:\n- **Sort or cluster data** by commonly filtered columns (like timestamps)\n- **Use partitioning** to break large tables into manageable pieces\n- **Leverage zone maps** for automatic data skipping during queries\n\n### Format Conversion\nDuckDB makes it simple to convert between formats. For example, converting CSV to Parquet:\n```bash\nduckdb -c \"COPY (SELECT * FROM 'data.csv') TO 'data.parquet';\"\n```\n\n## Storage Trade-offs\n\nWhen choosing between formats in MotherDuck:\n\n**DuckLake (Open Format)**:\n- Broader ecosystem compatibility\n- Support for multiple compute engines\n- Better for petabyte-scale workloads\n\n**MotherDuck Native Storage**:\n- 2-10x faster query performance\n- Optimized read/write throughput\n- Better caching and regional performance\n\nA recommended architecture is to keep raw data in DuckLake/Parquet for openness, then move processed (silver/gold layer) data to MotherDuck native storage for maximum performance.\n\n## Conclusion\n\nParquet is unequivocally the most efficient file format for loading data into MotherDuck due to its columnar architecture, superior compression, and native integration with DuckDB. While other formats are supported, Parquet provides the best balance of storage efficiency, loading speed, and query performance for analytical workloads.\n\n## Sources\n\nList each source file you actually read and used, showing the EXACT LOCAL FILE PATH:\n- .kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md - Comprehensive guide explaining Parquet format benefits, compression techniques, comparison with alternatives (CSV, ORC, Avro, Delta Lake, Iceberg), and DuckDB integration\n- .kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md - Technical details on columnar vs row-oriented storage, compression methods (dictionary encoding, RLE, delta encoding, FSST), vectorized execution, and best practices for implementation\n- .kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md - Practical tutorial on converting between CSV and Parquet formats using DuckDB, demonstrating the COPY command and direct file querying\n- .kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md - Information on data ingestion methods, direct file reading capabilities, and the efficiency of querying Parquet files without formal table creation\n- .kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md - Best practices for loading data, performance optimization strategies, storage trade-offs between DuckLake and MotherDuck native storage, and batch loading recommendations\n"
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! I've successfully completed the task. Here's a summary of what I did:
  [94mâ”‚[0m 
  [94mâ”‚[0m ## Task Completed âœ“
  [94mâ”‚[0m 
  [94mâ”‚[0m I searched through the `.kurt/sources/` directory and found relevant information about file formats for loading data into MotherDuck. 
  [94mâ”‚[0m 
  [94mâ”‚[0m **Key findings:**
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. **Parquet is the most efficient format** for loading data into MotherDuck due to:
  [94mâ”‚[0m    - Columnar storage architecture
  [94mâ”‚[0m    - Superior compression (dictionary, run-length, delta encoding)
  [94mâ”‚[0m    - Query optimization features (predicate pushdown, column pruning)
  [94mâ”‚[0m    - Built-in statistics for skipping irrelevant data
  [94mâ”‚[0m    - Native DuckDB/MotherDuck integration
  [94mâ”‚[0m 
  [94mâ”‚[0m 2. **Other formats analyzed:**
  [94mâ”‚[0m    - CSV: Simple but inefficient for analytics
  [94mâ”‚[0m    - Apache ORC: Good but less supported than Parquet
  [94mâ”‚[0m    - Apache Avro: Better for streaming than analytics
  [94mâ”‚[0m    - Delta Lake/Iceberg: Table formats that use Parquet underneath
  [94mâ”‚[0m 
  [94mâ”‚[0m 3. **Performance best practices:**
  [94mâ”‚[0m    - Use partitioned writes for large datasets
  [94mâ”‚[0m    - Avoid too many small files (aim for 100MB+ partitions)
  [94mâ”‚[0m    - Consider compression options
  [94mâ”‚[0m    - Use direct Parquet queries
  [94mâ”‚[0m 
  [94mâ”‚[0m **Sources used:** I read 6 comprehensive markdown files from the MotherDuck documentation covering Parquet benefits, CSV handling, partitioned writes, and efficient data loading strategies.
  [94mâ”‚[0m 
  [94mâ”‚[0m The complete answer with detailed information and all sources has been written to `/tmp/answer_without_kg_1.md`.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 7,012 | Cost: $0.4305
     Cumulative: 7,012 tokens | $0.4305
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: heuristic: Explicit completion signal detected

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 18 | Duration: 117.3s
â•‘    Tokens: 7,012 | Cost: $0.4305
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  Running LLM judge evaluation...
   ğŸ§  LLM Judge score: 1.00

======================================================================
â“ Question 2/3
...How does MotherDuck integrate with DuckDB?
======================================================================


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 2
â”‚ Answer this question by searching through the .kurt/sources/ directory:

...How does MotherDuck integrate with DuckDB?

Required Steps:
1. Search for relevant markdown files in .kurt/sources/ using glob and grep
2. Read the most relevant files (aim for 3-5 most relevant sources)
3. As you read each file, keep track of:
   - The file path
   - The file title/name
   - Key information extracted from that file
4. Synthesize a comprehensive answer based on the information found
5. Write your answer to: /tmp/answer_without_kg_2.md

IMPORTANT - Your answer MUST have this structure:
```markdown
# Answer

[Your comprehensive answer here, drawing from the sources you read]

## Sources

List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
[etc.]

DO NOT use URLs or document titles as links. Show the actual file path you read with the Read tool.
```

The Sources section is MANDATORY - you must list every file you read to answer the question.

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I'll help you answer this question by searching through the `.kurt/sources/` directory for information about how MotherDuck integrates with DuckDB.
  [94mâ”‚[0m 
  [94mâ”‚[0m Let me start by exploring what files are available and finding relevant content.
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Glob â†’ **/*.md

  [96mğŸ”§ TOOL:[0m Grep â†’ DuckDB.*integrat|integrat.*DuckDB

  [96mğŸ”§ TOOL:[0m Grep â†’ MotherDuck.*DuckDB|DuckDB.*MotherDuck
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/scaling-duckdb-panel-ondemand.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/take-flight-with-the-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/data-discoverability-secoda-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-power-of-wasm-for-analytics-duckdb-in-the-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-surprising-birth-of-duckdb-ft-co-creator-hannes-muhleisen.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/about-us.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/acceptable-use-policy.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/adithya-krishnan.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/aditya-aomani.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/alex-monahan.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/amrutha-gujjar.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/andrew-mcewen.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/andrew-witten.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/anna-geller.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/antony-courtney.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/boaz-leskes.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/brett-griffin.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/christophe-oudar.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/daniel-palma.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/diptanu-gon-choudhury.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/doug-raymond.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/emily-lucek.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/faraz-hameed.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/garrett-obrien.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/graham-wetzler.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/hamilton-ulmer.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/jacob-matson.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/jelte-fennema-nio.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/jessica-libman.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/jordan-tigani.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/joseph-hwang.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/leslie-thomas.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/luciano-galvao-filho.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/manveer-chawla.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/marc-d.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/marcos-ortiz.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/mehdi-ouazza.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/michael-hunger.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/mike-spaulding.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/mike-xu.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/nick-ursa.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/nouras-haddad.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/oli-dinov.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/peter-boncz.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/pranav-aurora.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/ryan-boyd.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/sahil-gupta.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/saqib-rasul.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/sean-m-tracey.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/sheila-sitaram.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/simon-spati.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/stephanie-wang.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/sung-won-chung.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/thomas-tfmv-mcgeehan.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/till-dohmen.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/tino-tereshko.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/vasilije-markovic.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/yannick-welsch.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/yves-le-maout.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/authors/zhenni-wu.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/MotherDuck-Visualize-Embeddings-Marimo.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/analyze-data-in-azure-with-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/analyze-json-data-using-sql.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/analyze-sqlite-databases-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/analyze-x-data-nodejs-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-duckdb-snippet-sets-with-motherduck-sharing-databases.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-mega-giga-instance-sizes-huge-scale.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-series-seed-and-a.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/announcing-small-data-sf-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/big-data-is-dead.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/bigquery-to-duckdb-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/building-data-applications-with-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/building-motherduck-partner-ecosystem.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/data-app-generator.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/data-engineer-highlights-PyConDE-2023.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/data-engineering-toolkit-essential-tools.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/data-engineering-toolkit-infrastructure-devops.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/data-engineers-answer-10-top-reddit-questions.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-dec-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-nov-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-oct-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/differential-storage-building-block-for-data-warehouse.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/dosomething-motherduck-data-warehouse-ROI.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-110-hidden-gems.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-cognee-sql-analytics-graph-rag.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2023.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_zuug0_jh/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eight.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "durationMs": 552,
  [2m  â”‚[0m   "numFiles": 100,
  [2m  â”‚[0m   "truncated": true
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/take-flight-with-the-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/going-beyond-the-dataframe.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ibis-one-library-to-query-any-backend.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-vs-pandas-vs-polars-for-python-devs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ducklake-big-data-small-coalesce-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-python-end-to-end-data-engineering-project-13.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me search for more specific information about integration and architecture:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ hybrid|cloud|local|sync

  [96mğŸ”§ TOOL:[0m Grep â†’ attach|database.*attach|remote.*database
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/data-discoverability-secoda-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-power-of-wasm-for-analytics-duckdb-in-the-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-future-of-bi-exploring-the-impact-of-bi-as-code-tools-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-in-100-seconds-by-a-duck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/moving-forward-from-ad-hoc-reports-with-duckdb-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/panel-fixing-the-data-engineering-lifecycle-coalesce-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-ga-produck-tour.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/getting-started-with-motherduck-webinar.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-bootstrap-a-data-warehouse-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/is-bi-too-big-for-small-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-vs-pandas-vs-polars-for-python-devs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 30,
  [2m  â”‚[0m   "appliedLimit": 30
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/privacy-policy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dominik-moritz.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-future-of-bi-bi-as-code-duckdb-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vibe-coding-sql-cursor.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/spark-ducklake-getting-started.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-keyboard-shortcuts-for-joyful-querying.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/read-scaling-preview.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/redshift-files-hunt-for-big-data.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me read some of the most relevant files to understand how MotherDuck integrates with DuckDB:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/product/duckdb-users.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     "content": "---\ntitle: announcing-motherduck-duckdb-in-the-cloud\ncontent_type: blog\nsource_url: https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud\nindexed_at: '2025-11-25T19:57:33.138925'\ncontent_hash: a5ee88402c607603\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Announcing MotherDuck: Hybrid Execution Scales DuckDB from your Laptop into the Cloud\n\n2023/06/22 - 4 min read\n\nBY\nMotherDuck team\n\nDuckDB has become widely known as \u201cSQLite for Analytics\u201d \u2013 a powerful SQL analytics engine with broad adoption in development workflows, ad-hoc analytics on the laptop and embedded applications. MotherDuck wants to make it even easier to use, so we\u2019ve worked alongside the creators of DuckDB to build a cloud-based serverless analytics platform. Today is a large milestone in that journey \u2013 MotherDuck is now available by invitation.\n\nMotherDuck in 100 seconds (by a duck \ud83e\udd86) - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[MotherDuck in 100 seconds (by a duck \ud83e\udd86)](https://www.youtube.com/watch?list=PLIYcNkSjh-0zP7fwKzhnbx5ur1Mf80Q_p&v=BINA_ytZXMY)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\n1/1\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nFull screen is unavailable. [Learn More](https://support.google.com/youtube/answer/6276924)\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?list=PLIYcNkSjh-0zP7fwKzhnbx5ur1Mf80Q_p&v=BINA_ytZXMY&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n[Previous (SHIFT+p)](https://www.youtube.com/watch?list=PLIYcNkSjh-0zP7fwKzhnbx5ur1Mf80Q_p&v=BINA_ytZXMY \"Previous (SHIFT+p)\")\n\n0:00 / 1:51\n\n\u2022Live\n\n\u2022\n\n## Hybrid execution: cloud and laptop working together\n\nData scientists, analysts, and engineers love DuckDB because it works great no matter where their data lives. Since many data professionals have powerful laptops sitting 85% idle, they often want to bring the data to their local machine to make it even more efficient to crunch, especially when performing ad hoc analysis and development. MotherDuck lets you analyze this local data locally, while still JOINing with data processed in the cloud, giving you efficient use of all your compute resources.\n\nIn the example below, the table `yellow_cab_nyc` lives in MotherDuck in the cloud, and I have a CSV on my laptop table with currency conversions. We want to see the average cost of NYC taxi trips by passenger count in different currencies by JOINing these two tables. Yes, we\u2019re seamlessly joining data on my laptop with data in the cloud!\n\n![Post asset](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fexample_hybrid_7bb8f08e2d.png&w=3840&q=75)\n\nYou can even do hybrid query execution with data stored in s3, with MotherDuck securely storing and managing your AWS credentials.\n\n![example_s3.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fexample_s3_d97e746203.png&w=3840&q=75)\n\nNote, these examples are part of our [sample datasets and queries](https://motherduck.com/docs/category/sample-datasets--queries/), feel free to run them yourself!\n\nYou might wonder how this works under the covers. By connecting your DuckDB instance to MotherDuck, you establish a radically different type of distributed system - one, in which one node is MotherDuck in the cloud, and another node is wherever your DuckDB lives, be it your laptop or a lambda, Python or CLI, JDBC or MotherDuck\u2019s own web app. Both nodes execute queries in concert in the most optimal way, automatically routing parts of queries to the right location.\n\n## MotherDuck includes a web notebook and Git-style Collaboration\n\nWant to run some quick SQL queries without downloading and installing DuckDB? The MotherDuck web application provides a notebook-like UI. This enables you to analyze local CSVs and parquet files, upload them and manage them alongside your other data stored in MotherDuck.\n\n![app_motherduck_beta.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fapp_motherduck_beta_v2_69fbe04dcf.png%3Fupdated_at%3D2023-06-21T23%3A24%3A18.647Z&w=3840&q=75)\n\nAs a DuckDB-in-the-cloud company, naturally MotherDuck embeds DuckDB in its web application using WASM. Results of your SQL queries are cached in this DuckDB instance, enabling you to instantly sort, pivot, and filter query results!\n\nWant to share your DuckDB data with colleagues? Using SQL, you can create a shareable snapshot of your data, which your colleagues can easily attach in MotherDuck.\n![example_share.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fexample_share_cfe1d14570.png&w=3840&q=75)\n\nThis SQL command will return a shareable URL which can then be used by your colleague to access the shared database.\n![example_attach.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fexample_attach_ff22a57c32.png&w=3840&q=75)\n\n## Anywhere you can Duck, you can MotherDuck\n\nDuckDB has been starred by over 10k developers on GitHub, and it might be due to the simplicity of getting up and running with a downloadable, open source analytics engine. We want to continue (and improve!) that amazing experience as we bring DuckDB to the cloud.\n\nOne way to do this is by ensuring MotherDuck works well with many of the most popular technologies in the modern data stack, including ingestion, orchestration and BI+Visualization tools.\n\n![partner-logos.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fpartner_logos_242d207bb6.png&w=3840&q=75)\n\nWe strive to make MotherDuck as easy to adopt as DuckDB. To that end, any DuckDB instance in the world running in Python or CLI can connect to MotherDuck with a single line of code. Suddenly, by running this command your DuckDB magically becomes supercharged by MotherDuck. Such ease of onboarding could only have been possible via close collaboration with the creators of DuckDB!\n\n![d_to_md.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fd_to_md_1dfe63b09b.png&w=3840&q=75)\n\n## Continuing to Make Analytics Ducking Awesome\n\nOne of the primary reasons we were driven to build a serverless analytics platform on top of DuckDB was their fast-paced innovation. Many features in DuckDB have gone from thoughts in academic papers to committed code in a few weeks.\n\nWe\u2019re launching MotherDuck now and doing weekly releases because we admire and want to emulate this speed of execution. Thanks in advance for all the feedback you can provide to make MotherDuck a better product!\n\n## Get Started\n\n[Request an invite](https://motherduck.com/) now to get started using MotherDuck, and join the flock on [slack.motherduck.com](https://slack.motherduck.com/).\n\nAnd, if you\u2019re in San Francisco next week, don\u2019t forget to [register for the MotherDuck Party](https://motherduck-party.eventbrite.com/), watch DuckDB co-creator Hannes [keynote the Data + AI conference](https://www.databricks.com/dataaisummit/session/data-ai-summit-keynote-thursday), and join MotherDuck co-founder Ryan Boyd in his [technical session](https://www.databricks.com/dataaisummit/session/if-duck-quacks-forest-and-everyone-hears-should-you-care).\n\n### TABLE OF CONTENTS\n\n[Hybrid execution: cloud and laptop working together](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#hybrid-execution-cloud-and-laptop-working-together)\n\n[MotherDuck includes a web notebook and Git-style Collaboration](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#motherduck-includes-a-web-notebook-and-git-style-collaboration)\n\n[Anywhere you can Duck, you can MotherDuck](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#anywhere-you-can-duck-you-can-motherduck)\n\n[Continuing to Make Analytics Ducking Awesome](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#continuing-to-make-analytics-ducking-awesome)\n\n[Get Started](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#get-started)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![This Month in the DuckDB Ecosystem: June 2023](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fthumbnail_duckdb_newsletter_1_34b0dc65cf.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-seven/)\n\n[2023/06/16 - Marcos Ortiz](https://motherduck.com/blog/duckdb-ecosystem-newsletter-seven/)\n\n### [This Month in the DuckDB Ecosystem: June 2023](https://motherduck.com/blog/duckdb-ecosystem-newsletter-seven)\n\nThis Month in the DuckDB Ecosystem: June 2023\n\n[![DuckDB Tutorial For Beginners](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgetting_started_duckdb_thumbnail_70b197b1ab.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n[2024/10/31 - Mehdi Ouazza, Ryan Boyd](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n### [DuckDB Tutorial For Beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners)\n\nGet up to speed quickly with DuckDB, including installation, VSCode workflow integration and your first SQL analytics project.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 188,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 188
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     "content": "---\ntitle: bringing-duckdb-to-the-cloud-dual-execution-explained\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained\nindexed_at: '2025-11-25T20:45:20.295187'\ncontent_hash: ab4030806c409492\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nBringing DuckDB to the Cloud: Dual Execution Explained - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Bringing DuckDB to the Cloud: Dual Execution Explained](https://www.youtube.com/watch?v=2eVNm4A8sLw)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=2eVNm4A8sLw&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 1:08:11\n\n\u2022Live\n\n\u2022\n\nYouTubeQuack & CodeMotherDuck Features\n\n# Bringing DuckDB to the Cloud: Dual Execution Explained\n\n2024/06/28\n\nBringing DuckDB's analytical power to the cloud requires more than just running it on a server. While DuckDB excels at processing data on a local machine, the dynamics of data analysis change when dealing with cloud-scale data, collaboration, and shared resources. This shift introduces challenges around [security](https://motherduck.com/learn-more/secure-startup-data-warehouse/), concurrent access, and performance, which is where a purpose-built cloud architecture becomes essential.\n\nMotherDuck, a modern cloud data warehouse, is built on DuckDB to solve these challenges. It extends DuckDB\u2019s capabilities without forking the open source project, creating a hybrid system that intelligently balances local and cloud computing. In a conversation with MotherDuck founding engineer Stephanie, she explained the architecture and the innovative query model, known as **dual execution**, that makes this possible.\n\n* * *\n\n## The MotherDuck Architecture: Beyond Hosted DuckDB\n\nA common misconception is that MotherDuck is simply DuckDB hosted in the cloud. The reality is a more sophisticated system designed to overcome the limitations of using a locally-optimized engine in a distributed environment. MotherDuck\u2019s architecture is built on three key components and a core philosophy of not forking DuckDB.\n\nInstead of maintaining a separate version, MotherDuck leverages DuckDB\u2019s powerful **extension system**. This allows MotherDuck to add new capabilities at multiple layers, including the SQL parser, optimizer, and storage interface, while staying current with the latest open source DuckDB releases. This tight integration means users benefit from the rapid innovation of the DuckDB community almost immediately.\n\nThe architecture can be broken down into three main layers:\n\n1. **The Client Extension**: This is how MotherDuck integrates with the DuckDB ecosystem. Whether a user is working in the CLI, a Python script, or a JDBC connection, a simple `ATTACH` command connects their local DuckDB instance to the MotherDuck cloud. This extension is even used to run DuckDB in the browser via WASM for the MotherDuck UI, enabling client-side processing to reduce latency and cloud compute costs.\n2. **The Compute Layer**: In the cloud, queries are processed by containerized DuckDB instances, fondly called \"ducklings.\" These compute resources are scaled based on user needs, providing the necessary CPU and memory to handle complex analytical queries on large datasets.\n3. **The Storage Layer**: DuckDB's native file format is optimized for a single writer on a local file system. This model is not well-suited for the cloud, where multiple users need to read and write to the same database concurrently. To solve this, MotherDuck implemented a differential storage system that maps a logical database file to append-only snapshot layers in cloud object storage. This design is cloud-friendly, enabling efficient in-place updates and forming the foundation for features like database sharing and time travel.\n\n* * *\n\n## Enabling Collaboration with Database Sharing and Secure Credentials\n\nThis unique architecture transforms DuckDB from a \"single-player\" tool into a collaborative \"multiplayer\" platform. One of the most significant advantages is **database sharing**. Instead of emailing SQL scripts or passing around large database files, team members within the same organization can grant query access to their databases. This streamlines collaboration, ensuring everyone works from a consistent and up-to-date version of the data.\n\nSecurity is another critical aspect of any cloud data platform. MotherDuck provides a **centralized Secret Manager**, co-designed with the DuckDB team. Users can create persistent, encrypted secrets for accessing external data sources like AWS S3, Google Cloud Storage, or Delta Lake. Once a secret is created in MotherDuck, it can be reused across different clients and sessions without needing to expose or reconfigure credentials on each local machine.\n\n* * *\n\n## Understanding Dual Execution\n\nThe most innovative feature of MotherDuck's architecture is its **dual execution model**. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\n\nThe process was demonstrated with a query joining two tables. When both tables reside in a MotherDuck database, the query is executed entirely in the cloud. An `EXPLAIN` plan for this query reveals that all operators, from the table scans to the final join, are marked as **remote**. The only local operation is downloading the final result set to the client.\n\nThe real power of dual execution becomes apparent when a query involves both local and cloud data. In the demonstration, one of the remote tables was copied to a local Parquet file. The query was then modified to join this local file with the table that remained in the cloud.\n\nThe `EXPLAIN` plan for this new query showed a mix of local and remote operations. The scan of the Parquet file and its associated processing happened **locally**, leveraging the user's machine. The scan of the cloud table happened **remotely** on MotherDuck's compute. The system then efficiently transferred only the necessary intermediate data to complete the join. This hybrid approach avoids needlessly uploading the local file to the cloud or downloading the entire remote table, resulting in faster and more efficient queries.\n\n* * *\n\n## Taking Control with Manual Execution Overrides\n\nWhile MotherDuck's optimizer is designed to make the most efficient choice automatically, some situations require manual control. For this, users can use the `md_run` parameter within scan functions like `read_parquet()` or `read_csv()`.\n\nBy setting `md_run = 'local'`, a user can force the scan of a remote S3 file to be executed on their local client. This involves downloading the data, which might be desirable if the data needs to be processed near the user's location or if they prefer using local credentials. Conversely, setting `md_run = 'remote'` forces the operation to run in the MotherDuck cloud.\n\nThis control is especially useful for performance tuning. A direct comparison showed that querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds). By pushing the computation to the data's location, MotherDuck minimizes network I/O and delivers results more quickly.\n\n* * *\n\n## A Symbiotic Future for Local and Cloud Analytics\n\nMotherDuck's architecture is a thoughtful extension of DuckDB's core principles, adapting its local-first power for the demands of the cloud. The dual execution model provides a flexible and efficient bridge between a user's laptop and cloud data, optimizing workloads in a way that pure-cloud or pure-local systems cannot.\n\nThis relationship is symbiotic. As the largest production user of DuckDB, MotherDuck continuously pushes the engine to its limits, uncovering opportunities for improvement and contributing enhancements back to the open source project. This collaboration ensures that both DuckDB and MotherDuck will continue to evolve, offering a powerful and seamless analytical experience that spans from local exploration to cloud-scale production.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Ficeberg_video_thumbnail_adfdabe3d6.jpg&w=3840&q=75)\\\\\n\\\\\n46:26](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n[2025-06-12](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n### [Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse)\n\nBuilding a data stack means choosing between easy SaaS and complex open-source. Apache Iceberg is a middle ground, but its catalog is a hurdle. New tools now simplify using Iceberg with DuckDB to create a flexible, local data lakehouse.\n\nMotherDuck Features\n\n[![\"Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FG_Se_B_Sox_AW_Fg_HD_f4fddaa9ab.jpg&w=3840&q=75)](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n[2025-04-23](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n### [Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly)\n\nHamilton Ulmer shares insights from MotherDuck's Instant SQL Mode, exploring how real-time query result previews eliminate the traditional write-run-debug cycle through client-side parsing and DuckDB-WASM caching.\n\nSQL\n\nTalk\n\nMotherDuck Features\n\n[![\"DuckDB: Run dbt build with sub-second execution times\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_03_26_at_3_40_16_PM_c6e3a8096f.png&w=3840&q=75)\\\\\n\\\\\n24:06](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n[2025-03-13](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n### [DuckDB: Run dbt build with sub-second execution times](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times)\n\nWhether you're new to DuckDB or looking to optimize your workflows, this session will provide practical insights to help you leverage its full potential.\n\nYouTube\n\nData Pipelines\n\ndbt\n\nMotherDuck Features\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 184,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 184
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     "content": "---\ntitle: duckdb-motherduck-for-beginners-your-ultimate-guide\ncontent_type: event\nsource_url: https://motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide\nindexed_at: '2025-11-25T20:44:57.276294'\ncontent_hash: 9ac8939ea24395a7\nhas_code_examples: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nDuckDB & MotherDuck for Beginners: Your Ultimate Guide - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[DuckDB & MotherDuck for Beginners: Your Ultimate Guide](https://www.youtube.com/watch?v=WYV8hvJOAQE)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Why am I seeing this?](https://support.google.com/youtube/answer/9004474?hl=en)\n\n[Watch on](https://www.youtube.com/watch?v=WYV8hvJOAQE&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 36:27\n\n\u2022Live\n\n\u2022\n\nYouTube\n\n# DuckDB & MotherDuck for Beginners: Your Ultimate Guide\n\n2025/02/21\n\n## Why DuckDB is Revolutionizing Data Analytics\n\nDuckDB has experienced explosive growth in popularity, with download statistics showing remarkable adoption rates, particularly in the Python ecosystem. This open-source analytical database is designed to handle everything from quick data exploration to complex data pipelines, and it's already trusted by multiple companies in production environments.\n\n## Understanding the Small and Medium Data Revolution\n\nThe tech industry has long operated under the assumption that analytics requires big data infrastructure. However, this paradigm is being challenged by three key insights:\n\n1. **Most workloads aren't big data**: According to data from AWS Redshift and other cloud analytical databases, approximately 83% of users work with query sizes up to 1TB, and 94% work with data below 10TB.\n\n2. **Modern hardware is incredibly powerful**: Today's single machines can handle up to 24TB of memory on AWS, making distributed systems unnecessary for most use cases.\n\n3. **Distributed systems are expensive**: They require significant IO network traffic for coordination, and the development experience is considerably more complex.\n\n\nDuckDB capitalizes on these realities by enabling users to work with large datasets on a single machine, whether locally on a laptop or in the cloud via a scale-up strategy.\n\n## How DuckDB Works: The In-Process Advantage\n\nDuckDB is an open-source, in-process analytical database written in C++, designed as a single self-contained binary with all dependencies included. This architecture sets it apart from traditional databases.\n\n### Traditional Database Architecture\n\nDatabases typically fall into two categories:\n\n- **OLTP (Online Transaction Processing)**: Databases like PostgreSQL and MySQL, optimized for handling transactions with small datasets. Query times typically range from 1-10 milliseconds.\n\n- **OLAP (Online Analytical Processing)**: Databases like BigQuery and Snowflake, built for analytical queries processing large datasets. Query times can range from 100 milliseconds to several minutes.\n\n\nMost traditional databases use a client-server architecture where the database runs as a separate process, and applications connect to it through SQL queries.\n\n### The In-Process Revolution\n\nIn-process databases run directly within the application process itself. While SQLite pioneered this approach for OLTP workloads, DuckDB introduces something new: an in-process OLAP database optimized for analytical workloads.\n\nThis design enables DuckDB to:\n\n- Run on virtually any platform (laptops to cloud workflows)\n- Integrate seamlessly with any programming language\n- Execute in web browsers via WebAssembly\n- Eliminate network overhead for local operations\n\n## Getting Started with DuckDB\n\n### Installation and Basic Usage\n\nDuckDB can be installed through various methods:\n\n- Direct binary download for CLI usage\n- Package managers (Homebrew for macOS)\n- Language-specific packages (Python, R, Java, etc.)\n\nThe CLI provides a powerful interface for data exploration:\n\n```sql\nCopy code\n\n-- Simple query reading from S3\nFROM 's3://bucket/path/to/file.parquet' LIMIT 5;\n```\n\n### Key Features in Action\n\n**Friendly SQL Dialect**: DuckDB extends standard SQL with productivity enhancements, such as the FROM-first syntax shown above.\n\n**Automatic File Format Detection**: DuckDB automatically detects and handles various file formats including Parquet, CSV, JSON, Iceberg, and Delta Lake.\n\n**Extension System**: DuckDB's functionality is modular through extensions. Core extensions (like HTTPFS for S3 access) are auto-loaded when needed, while community extensions can be installed manually.\n\n## Data Persistence and the DuckDB File Format\n\nBy default, DuckDB operates in-memory, but it offers powerful persistence options:\n\n### Creating and Managing Databases\n\n```sql\nCopy code\n\n-- Attach or create a database\nATTACH 'mydatabase.ddb';\n\n-- Create a table from a query\nCREATE TABLE mytable AS SELECT * FROM source_data;\n```\n\n### The DuckDB File Format\n\nDuckDB's native file format (.ddb or .db) is:\n\n- Self-contained (all tables and metadata in one file)\n- ACID-compliant\n- Highly compressed\n- Optimized for analytical workloads\n\n### Exporting Data\n\nDuckDB supports seamless data export:\n\n```sql\nCopy code\n\n-- Export to CSV\nCOPY (SELECT * FROM mytable) TO 'output.csv';\n```\n\n## Managing Secrets and Authentication\n\nDuckDB includes a comprehensive secret management system for secure cloud access:\n\n```sql\nCopy code\n\n-- Create temporary secret using AWS credential chain\nCREATE SECRET (\n    TYPE S3,\n    PROVIDER credential_chain\n);\n```\n\nThis approach supports:\n\n- AWS SSO authentication\n- Temporary and persistent secrets\n- Multiple cloud providers\n- Secure credential storage\n\n## Scaling to the Cloud with MotherDuck\n\nMotherDuck supercharges DuckDB by transforming it from a single-player to a multiplayer analytics experience. The integration is remarkably simple:\n\n### Connecting to MotherDuck\n\n```sql\nCopy code\n\n-- Connect to MotherDuck with one command\nATTACH 'md:';\n```\n\nAuthentication requires only a MotherDuck token, which can be set as an environment variable.\n\n### Key MotherDuck Features\n\n**Dual Execution**: MotherDuck enables intelligent query execution, automatically determining whether to run computations locally or in the cloud based on data location and query requirements.\n\n**Database Sharing**: Create and share cloud databases with simple commands:\n\n```sql\nCopy code\n\n-- Create a share\nCREATE SHARE myshare FROM mydatabase;\n```\n\n**Performance Benefits**: Leveraging cloud infrastructure provides:\n\n- High-bandwidth connections to cloud storage\n- Elimination of local network bottlenecks\n- Seamless collaboration features\n\n### Cloud Storage Integration\n\nMotherDuck dramatically improves performance when querying cloud storage. In benchmarks, queries that take 11 seconds locally can complete in just 2 seconds when leveraging MotherDuck's cloud infrastructure and network proximity to storage.\n\n## The Complete Analytics Toolkit\n\nDuckDB and MotherDuck together provide:\n\n- **Simplicity**: One-command installation and cloud connection\n- **Flexibility**: Run anywhere from browsers to cloud environments\n- **Performance**: Lightning-fast analytical queries on small to medium datasets\n- **Integration**: Native support for numerous file formats and cloud providers\n- **Collaboration**: Easy data sharing and team workflows\n\nWhether you're analyzing data on your laptop, scaling computations in the cloud, or building production data pipelines, DuckDB offers a modern approach to analytics that challenges traditional big data assumptions while delivering exceptional performance and developer experience.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Data-based: Going Beyond the Dataframe\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FData_based_f32745b461.png&w=3840&q=75)](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n[2025-11-20](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n### [Data-based: Going Beyond the Dataframe](https://motherduck.com/videos/going-beyond-the-dataframe)\n\nLearn how to turbocharge your Python data work using DuckDB and MotherDuck with Pandas. We walk through performance comparisons, exploratory data analysis on bigger datasets, and an end-to-end ML feature engineering pipeline.\n\nWebinar\n\nPython\n\nAI, ML and LLMs\n\n[![\"Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FHex_Webinar_778e3959e4.png&w=3840&q=75)](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n[2025-11-14](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n### [Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck)\n\nAI isn't here to replace data work, it's here to make it better. Watch this webinar to see how Hex and MotherDuck build AI workflows that prioritize context, iteration, and real-world impact.\n\nWebinar\n\nAI, ML and LLMs\n\n[![\"Lies, Damn Lies, and Benchmarks\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FLies_Damn_Lies_and_Benchmarks_Thumbnail_404db1bf46.png&w=3840&q=75)](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n[2025-10-31](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n### [Lies, Damn Lies, and Benchmarks](https://motherduck.com/videos/lies-damn-lies-and-benchmarks)\n\nWhy do database benchmarks so often mislead? MotherDuck CEO Jordan Tigani discusses the pitfalls of performance benchmarking, lessons from BigQuery, and why your own workload is the only benchmark that truly matters.\n\nStream\n\nInterview\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 301,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 301
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md",
  [2m  â”‚[0m     "content": "---\ntitle: cidr-paper-hybrid-query-processing-motherduck\ncontent_type: event\nsource_url: https://motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck\nindexed_at: '2025-11-25T19:56:16.076582'\ncontent_hash: 23a9f33edc9cbbbb\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Just Released: Hybrid Query Processing Paper at CIDR 2024\n\n2024/01/16 - 2 min read\n\nBY\n\n[Peter Boncz](https://motherduck.com/authors/peter-boncz/)\n\nThe Conference on Innovative Data systems Research (CIDR) is underway in California and we\u2019re proud to be presenting a peer-reviewed [paper on the MotherDuck hybrid query processing architecture](https://www.cidrdb.org/cidr2024/papers/p46-atwal.pdf).\n\n[Hybrid query processing](https://motherduck.com/learn-more/hybrid-analytics-guide/) allows you to execute database queries either on your local machine, in the cloud, or using a combination of both. It adds useful capabilities to DuckDB, for instance the sharing of DuckDB databases between different team members via the cloud. It also allows you to create web applications with DuckDB running inside your browser, that can jointly execute queries with MotherDuck in the cloud.\n\nThe research and implementation of this architecture has been a collaboration between MotherDuck, DuckDB Labs and myself as a visiting database researcher on sabbatical from CWI, the Dutch national computer science research institute from which DuckDB was born.\n\nBecause designing and implementing a cutting-edge database system like MotherDuck is non-trivial, there are in fact quite a bit of research elements in what we do, even when software engineering. For example, we need to understand how to optimally plan hybrid queries when there are asymmetrical network connections (like in consumer internet) or cost differences in storage, compute and energy. This is why the collaboration between academia and industry is so important in databases; it has already provided a lot of inspiration for my research group at CWI while providing benefits to MotherDuck\u2019s users.\n\nI look forward to gaining other inspiration for my research group and MotherDuck from my fellow researchers at CIDR. Although CIDR is a relatively small conference, it attracts a distinguished audience of researchers and practitioners working in data systems attending it. The conference was originally created by two Turing Award winners: Jim Gray and Michael Stonebraker, both founding figures of the database field.\n\nOur CIDR paper is [now available for download](https://www.cidrdb.org/cidr2024/papers/p46-atwal.pdf) and provides an in-depth view of MotherDuck and our hybrid query architecture. I truly hope that you also will find it interesting. If so, please spread the word and pass it along to people who you think also will find this interesting!\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![Why Python Developers Need DuckDB (And Not Just Another DataFrame Library)](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Fwhy_pythondev_1_22167e31bf.png&w=3840&q=75)](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/)\n\n[2025/10/08 - Mehdi Ouazza](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/)\n\n### [Why Python Developers Need DuckDB (And Not Just Another DataFrame Library)](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries)\n\nUnderstand why a database is much more than just a dataframe library\n\n[![DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_4_1_b6209aca06.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n[2025/10/09 - Alex Monahan, Garrett O'Brien](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n### [DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/blog/announcing-duckdb-141-motherduck)\n\nMotherDuck now supports DuckDB 1.4.1 and DuckLake 0.3, with new SQL syntax, faster sorting, Iceberg interoperability, and more. Read on for the highlights from these major releases.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 74,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 74
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     "content": "---\ntitle: duckdb-users\ncontent_type: product_page\nsource_url: https://motherduck.com/product/duckdb-users/\nindexed_at: '2025-11-25T20:15:52.565114'\ncontent_hash: dfb4ed9cd3383b9d\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n# local + Cloud analytics,  backed by the same database\n\nA duckdb-powered data warehouse for production grade analytics\n\n[Get Started](https://motherduck.com/get-started/)\n\nLEARN MORE\n\nWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuck\n\n## Transform Solo DuckDB Into a Collaborative Powerhouse\n\nDuckDB's in-process nature means your data stays trapped on your machine. MotherDuck eliminates this limitation with a simple \"md:\" prefix.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fshare-data-not-files.db8baefd.png&w=3840&q=75)\n\n### Share Data, Not Files\n\nNo more exporting files or managing S3 buckets to share your work. Create database shares with specific team members while maintaining the familiar DuckDB experience.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmultiple-users-one-database.0b9d7dc7.png&w=3840&q=75)\n\n### Multiple Users, One Database\n\nEnable simultaneous access for Data Engineers, Analytics teams, and Scientists to the same datasets. Each user gets their own dedicated compute instance \u201cduckling\u201d for concurrent queries without conflicts.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-bring-the-flock-together.be57c6e9.png&w=2048&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-bring-the-flock-together.be57c6e9.png&w=2048&q=75)\n\n## Local Speed with Cloud-Scale Production Power\n\nEnjoy the best of both worlds with a seamless dev/prod workflow for your data. Develop locally for immediate feedback, then deploy identical workloads to the cloud with zero code changes.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Frapid-iteration-zero-cost.ff94f539.png&w=3840&q=75)\n\n### Rapid Iteration at Zero Cost\n\nExperiment locally with lightning-fast feedback cycles without spending a dime. When your solution is ready, your exact same queries work in the cloud by simply adding \"md:\" to your connection.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftest-safely-deploy-confidently.ed329fbf.png&w=3840&q=75)\n\n### Test Safely, Deploy Confidently\n\nPrototype complex data transformations without risking expensive cloud resources. This local dev, cloud prod model ensures what works locally will perform at scale in production.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fship-data-solutions-faster.bdfdd521.png&w=3840&q=75)\n\n### Ship Data Solutions Faster\n\nEliminate the friction between development and deployment environments. Your team can innovate with quick local tests and push proven solutions to cloud production seamlessly.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-scalable-node-architecture.91c200dd.png&w=1920&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-scalable-node-architecture.91c200dd.png&w=1920&q=75)\n\n## Familiar Interface with Intelligent Superpowers\n\nMotherDuck maintains the DuckDB interface you love while adding thoughtful, AI-backed intelligent assistance. Experience enhanced productivity with features designed for SQL experts and business users.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fvisual-insights-at-a-glance.60ec6358.png&w=3840&q=75)\n\n### Visual Insights at a Glance\n\nBeautiful visualizations help you understand complex data patterns immediately. MotherDuck's design affordances make exploration intuitive while preserving the power of DuckDB's analytical capabilities.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fautomatic-error-correction.fe59afe0.png&w=3840&q=75)\n\n### Automatic Error Correction\n\n\"Fix It\" repairs common syntax errors with a single click, eliminating frustration from missing commas or quotes. This intelligent assistance ensures everyone on your team can work productively with SQL, regardless of expertise level.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fnatural-language-to-sql.72331bc3.png&w=3840&q=75)\n\n### Natural Language to SQL\n\nNever written SQL before? Non-technical team members can contribute with \"PROMPT QUERY\" turning plain English into correct SQL. This feature democratizes data access by removing the barrier of SQL syntax knowledge.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-ergonomic-and-effortless.b3f51a46.png&w=3840&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-ergonomic-and-effortless.b3f51a46.png&w=3840&q=75)\n\n## Features\n\n[![Cloud database storage feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-cloud.3e9c0a0a.png&w=640&q=75)\\\\\n**Cloud database storage** \\\\\nManage your DuckDB database catalog in the cloud for easy scaling and collaboration.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/cloud-database-storage/)\n\n[![SQL analytics engine feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-analytics.f04308e0.png&w=640&q=75)\\\\\n**SQL analytics engine** \\\\\nEfficiently run the same SQL queries on data stored locally, in MotherDuck or in your data lake.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/sql-analytics-engine/)\n\n[![Simplified database sharing feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-db.11e966ef.png&w=640&q=75)\\\\\n**Simplified database sharing** \\\\\nCreate a shareable snapshot of your data, which your colleagues can easily attach and query in MotherDuck.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/simplified-database-sharing/)\n\n[![Dual query execution feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-query.28419698.png&w=640&q=75)\\\\\n**Dual query execution** \\\\\nUse idle compute on your laptop in concert with the cloud to increase speed and lower cost.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/dual-query-execution/)\n\n[![Notebook-like UI feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-ui.c08b01e4.png&w=1080&q=75)\\\\\n**Notebook-like UI** \\\\\nUse the web to browse the data catalog, write SQL, filter and sort results and share data.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/notebook-like-ui/)\n\n[![Strong DuckDB ecosystem feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-ecosystem.0ab95f7c.png&w=640&q=75)\\\\\n**Strong DuckDB ecosystem** \\\\\nUse with 25+ tools in the modern data stack for import, orchestration and business intelligence.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/strong-duckdb-ecosystem/)\n\n## Architecture\n\nManaged DuckDB-in-the-cloud\n\n## Ecosystem\n\nModern Duck Stack\n\n### CLOUD DATA WAREHOUSE\n\n### Sources\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpostgres-sql.d737f4f5.png&w=750&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Famazon-s3.72386dfc.png&w=640&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsalesforce.35d52b31.png&w=384&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fstripe.2160e881.png&w=384&q=75)\n\n[**Business Intelligence**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftableau.0ec523e2.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpower-bi.f3563826.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fomni.cb7aa381.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmetabase.5f8fe44e.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Business+Intelligence) [**Ingestion**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffivetran.26f3817d.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fairbyte.8371d2f0.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Festuary.babad369.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlthub.e9892b97.png&w=384&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Ingestion) [**Data Science & AI**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhex.275dad7d.png&w=256&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fjupyter.2a6af3de.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fcolab.30ab10af.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmarimo.b699c73d.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Data+Science+%26+AI) [**Reverse ETL**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fcensus.352f1d69.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Reverse+ETL) [**Transformation**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdbt.fd2184d1.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftobiko.d0e3d1e5.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsql-mesh.6fceb012.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Transformation) [**Dev Tools**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdatagrip.f48eba23.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpuppy-graph.848400c6.png&w=750&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdagster.e1970a7c.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Dev+Tools)\n\n[**Orchestration**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fairflow.7f70081a.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdagster.e1970a7c.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fprefect.fd6371b3.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fkestra.aa10acfc.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Orchestration)\n\n[**Data Quality**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmonte-carlo.2143f962.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsecoda.9b7e86fb.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsoda.59e5aa02.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fgreat-expectations.c544f1ef.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Data+Quality)\n\n## Case Studies\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nOur data pipelines used to take eight hours. Now they're taking eight minutes, and I see a world where they take eight seconds. This is why we made the big bet on DuckDB and MotherDuck. It's only possible with DuckDB and MotherDuck.\n\n[READ MORE](https://motherduck.com/case-studies/finqore/)\n\n![Jim O\u2019Neill's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fjim_oneil_photo_930c637f49.jpg&w=3840&q=75)\n\nJim O\u2019Neill\n\nCo-founder and CTO at FinQore\n\nOur data pipelines used to take eight hours. Now they're taking eight minutes, and I see a world where they take eight seconds. This is why we made the big bet on DuckDB and MotherDuck. It's only possible with DuckDB and MotherDuck.\n\n[READ MORE](https://motherduck.com/case-studies/finqore/)\n\nWith MotherDuck, it only took an hour to get an app up and running, which was pretty cool. It uses 18 million rows of data, but once it\u2019s loaded, we have all this interactivity right on the website. And since it's all coming from MotherDuck, we can create a shareable snapshot of our data that others can attach to and query.\n\n[READ MORE](https://motherduck.com/case-studies/dominik-moritz/)\n\n![Dominik Moritz's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdominik_berlin_f8a31170af.jpg&w=3840&q=75)\n\nDominik Moritz\n\nProfessor at Carnegie Mellon University\n\nWith MotherDuck, it only took an hour to get an app up and running, which was pretty cool. It uses 18 million rows of data, but once it\u2019s loaded, we have all this interactivity right on the website. And since it's all coming from MotherDuck, we can create a shareable snapshot of our data that others can attach to and query.\n\n[READ MORE](https://motherduck.com/case-studies/dominik-moritz/)\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nOur data pipelines used to take eight hours. Now they're taking eight minutes, and I see a world where they take eight seconds. This is why we made the big bet on DuckDB and MotherDuck. It's only possible with DuckDB and MotherDuck.\n\n[READ MORE](https://motherduck.com/case-studies/finqore/)\n\n![Jim O\u2019Neill's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fjim_oneil_photo_930c637f49.jpg&w=3840&q=75)\n\nJim O\u2019Neill\n\nCo-founder and CTO at FinQore\n\nOur data pipelines used to take eight hours. Now they're taking eight minutes, and I see a world where they take eight seconds. This is why we made the big bet on DuckDB and MotherDuck. It's only possible with DuckDB and MotherDuck.\n\n[READ MORE](https://motherduck.com/case-studies/finqore/)\n\nWith MotherDuck, it only took an hour to get an app up and running, which was pretty cool. It uses 18 million rows of data, but once it\u2019s loaded, we have all this interactivity right on the website. And since it's all coming from MotherDuck, we can create a shareable snapshot of our data that others can attach to and query.\n\n[READ MORE](https://motherduck.com/case-studies/dominik-moritz/)\n\n![Dominik Moritz's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdominik_berlin_f8a31170af.jpg&w=3840&q=75)\n\nDominik Moritz\n\nProfessor at Carnegie Mellon University\n\nWith MotherDuck, it only took an hour to get an app up and running, which was pretty cool. It uses 18 million rows of data, but once it\u2019s loaded, we have all this interactivity right on the website. And since it's all coming from MotherDuck, we can create a shareable snapshot of our data that others can attach to and query.\n\n[READ MORE](https://motherduck.com/case-studies/dominik-moritz/)\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n## Get in Touch\n\nWe\u2019d love to learn more about what you\u2019re building. A Product Expert will fly right back to you with resources to help you take flight.\n\nFirst Name\n\nLast Name\n\nE-mail\n\nCompany\n\nLeave us a message.\n\nThis site is protected by reCAPTCHA and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply.\n\nSubmit\n\n![Duck looking up](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduck-looking-up.1c763865.png&w=3840&q=75)\n\nAuthorization Response\n\nreCAPTCHA\n\nRecaptcha requires verification.\n\n[Privacy](https://www.google.com/intl/en/policies/privacy/) \\- [Terms](https://www.google.com/intl/en/policies/terms/)\n\nprotected by **reCAPTCHA**\n\n[Privacy](https://www.google.com/intl/en/policies/privacy/) \\- [Terms](https://www.google.com/intl/en/policies/terms/)",
  [2m  â”‚[0m     "numLines": 403,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 403
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me read one more file to get additional details about the connection mechanism:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ ATTACH.*md:|connection.*token|extension

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     "content": "---\ntitle: announcing-motherduck-general-availability-data-warehousing-with-duckdb\ncontent_type: blog\nsource_url: https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb\nindexed_at: '2025-11-25T19:56:19.426175'\ncontent_hash: baf2faf8b3f47cb9\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Announcing MotherDuck General Availability: Data Warehousing with DuckDB at Scale\n\n2024/06/11 - 7 min read\n\nBY\nMotherDuck team\n\nOver the last year, thousands of users have tested, validated and helped improve MotherDuck as a serverless data warehouse and backend for interactive apps. We\u2019ve now solidified the product, pricing, partnerships, support teams and internal business processes needed to reach an important milestone: General Availability (GA).\n\nMotherDuck and DuckDB are making analytics ducking awesome for the 99% of users who do not need a complex data infrastructure and for whom [big data is truly dead](https://motherduck.com/blog/big-data-is-dead/). MotherDuck now has many customers in production experiencing the simplicity and efficiency of DuckDB with the collaboration and scale of a serverless cloud data warehouse.\n\n![MotherDuck goes GA summary](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmotherduck_goes_ga_embed_smaller_woline_56d7780de8.jpg&w=3840&q=75)\n\n## Production-ready DuckDB\n\nJust last week, DuckDB Labs announced that DuckDB has reached 1.0.0 and is now committed to backwards compatibility. In our [post congratulating the DuckDB team](https://motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release), we outlined why database nerds love DuckDB: performance, innovation velocity, versatility, ease of use, rich and user-friendly SQL, and extreme portability. Thanks to DuckDB, analytics can run virtually anywhere, liberated from the shackles of complex and expensive distributed systems. As an embedded database, it\u2019s the perfect \u2018Lego\u2019 building block that can snap into any process just by linking in a library. These same characteristics led us to build a cloud data warehouse on top of DuckDB and in collaboration with the creators.\n\n## Simple, Multiplayer at Scale\n\nMotherDuck makes it simple to start uploading and querying your data, whether it sits on your local machine, in blob storage or even on the web. The data can be in many different formats, including parquet, csv, json, Iceberg and Delta Lake. Your local DuckDB can work seamlessly with MotherDuck using [Dual Execution](https://motherduck.com/product/#:~:text=Hybrid,%20Dual%20Query%20Execution), with parts of your queries running locally and other parts scaling to the cloud.\n\nThe cloud creates unique opportunities for sharing data. MotherDuck allows you to upload your data and share a named snapshot with your colleagues in two lines of SQL. Although snapshots can be very useful to have a consistent view of your data across the team for tasks like building machine learning models, snapshots can also be automatically updated. Now with MotherDuck GA, shares can be [restricted to your organization and easily discoverable](https://motherduck.com/docs/key-tasks/sharing-data/sharing-within-org).\n\nA cloud data warehouse needs to scale for all your users and applications. MotherDuck eliminates fighting over common resources by assigning separate, isolated compute instances to each user and simplifying administration and costs for organizations. These compute instances individually scale up to handle workloads of many terabytes for some of our customers. They also scale down to zero when they\u2019re not being used, so you don\u2019t pay when you\u2019re not actively running queries.\n\n## Unmatched Efficiency of Pricing and Execution\n\nCustomers have frequently referenced [high costs for status-quo cloud data warehouses](https://motherduck.com/learn-more/modern-data-warehouse-playbook/) as a big concern. Because of the efficiency of DuckDB\u2019s query engine and MotherDuck\u2019s scale-up architecture, we\u2019re able to offer [pricing](https://motherduck.com/product/pricing/) that is often an order of magnitude lower than other alternatives\n\nNot only is the pricing competitive, but it\u2019s also fine-grained and efficient. By billing at second-level granularity, you only pay for the cloud CPU time you actually use. And, when we\u2019re able to take advantage of your local compute through Dual Execution, you don\u2019t pay at all.\n\n> \u201cWith MotherDuck working to solve amazing problems through data, our behaviors have changed because we know we don't have to pay enormous costs every time we run a query, so we've got almost limitless performance,\u201d said Ravi Chandra, Chief Technology Officer at Dexibit.\n\n## Backed by a World-Class Team\n\nThe team building MotherDuck hails from some of the top companies in data: Google BigQuery, Snowflake, Databricks, SingleStore and more. We\u2019re united by shared values and a shared mission to make analytics ducking awesome.\n\nOur friends at Looker were known to have the best customer success organization in the data industry: the Department of Customer Love, founded by [Margaret Rosas](https://www.linkedin.com/in/mrosas/). Margaret has joined us at MotherDuck to lead our [customer success team](https://motherduck.com/customer-support/), the Hatchery, where our customers are nurtured and taught to fly.\n\nAs we go GA, we also wanted to consolidate engineering under a single leader who can help us scale the team. \u200b\u200bWe\u2019ve asked [Frances Perry](https://www.linkedin.com/in/frances-perry/) to lead our engineering organization. Frances came to us from Google where she was an engineering director on Google Compute Engine, built Google\u2019s internal data processing infrastructure and also released that infrastructure to the world as Cloud Dataflow.\n\n## Now SOC 2 Certified\n\nWe know that [trust and security](https://motherduck.com/trust-and-security/) are critical as you choose a data warehouse to power your business. We leverage a defense in-depth strategy, maintain operational security processes, and build customer trust through certified auditor attestations.\n\nMotherDuck successfully underwent an audit for SOC 2 Type I, which evaluates our systems relevant to security, availability, and confidentiality. With this attestation completed, we have a Type II planned for later in 2024.\n\nTo continue strengthening internal processes and controls, [Myoung Kang](https://www.linkedin.com/in/myoungkang/) has joined the company full-time as Head of Operations. Myoung is a renowned startup veteran who has worked for many companies, including Notion, Convex, and Preset where she was interim CFO.\n\n## Expanded Modern Duck Stack\n\nMotherDuck partners with more than [50 leading companies and technologies](https://motherduck.com/ecosystem/) to make the [Modern Duck Stack](https://motherduck.com/product/#ecosystem). Alongside MotherDuck GA, we\u2019re excited to announce that some of the most requested BI, data integration and data observability tools have been added to the flock.\n\n- **Tableau**: 60,000 companies globally rely on Tableau (part of Salesforce) for data visualization. Tableau Desktop and Server now support MotherDuck, with Tableau Cloud support coming later this year. The connector can be easily found on the [Tableau Exchange](https://exchange.tableau.com/products/1021)\n- **PowerBI**: 5 million organizations worldwide use Microsoft Power BI for data visualization, including 97% of Fortune 500. The [MotherDuck connector](https://motherduck.com/docs/integrations/bi-tools/powerbi) for Power BI is officially launched, and MotherDuck has been accepted to the Microsoft for Startups Founders Hub program\n- **Fivetran**: Fivetran is the leader in data integration for the modern data stack, powering 5,000 customers. The [MotherDuck destination](https://fivetran.com/docs/destinations/motherduck) connector was developed in close collaboration with the Fivetran engineering team, and is now an official Fivetran destination.\n- **Monte Carlo**: Monte Carlo, the leader in data observability, has built a [MotherDuck integration](https://docs.getmontecarlo.com/docs/motherduck). It allows our customers to monitor their databases and look for anomalies through custom SQL rules, which can be created in either the UI wizard and/or programmatically via monitors as code.\n\n## New Startup Program with $10k in Credits\n\nMotherDuck has partnered with leading VC firms to offer $10k in credits to eligible startups in need of a data warehouse or backend for their data apps. Early stage startups with up to 300 employees and less than $100M in funding can submit a [short application](https://motherduck.com/startups/).\n\n## Take Flight with MotherDuck - Now GA\n\nIf you don\u2019t already have a MotherDuck account, visit [app.motherduck.com](https://app.motherduck.com/) to get started. We have a [fully-featured 30-day free trial of the Standard Plan](https://motherduck.com/product/pricing/) and a forever Free Plan available for ongoing usage.\n\n> \u201cOur data pipelines used to take eight hours. Now they're taking eight minutes, and I see a world where they take eight seconds. This is why we made the big bet on DuckDB and MotherDuck. It's only possible with DuckDB and MotherDuck,\u201d said Jim O'Neill, Co-founder and CTO at FinQore.\n\nIf you\u2019re not quite ready to get started, you can [learn more](https://motherduck.com/product/) about the product, [browse our docs](https://motherduck.com/docs/), and read about how [FinQore](https://motherduck.com/case-studies/saasworks/), [Dexibit](https://motherduck.com/case-studies/dexibit/) and [Mosaic](https://motherduck.com/case-studies/dominik-moritz/) use MotherDuck.\n\nWe also have an upcoming [live demo and discussion](https://motherduck.com/getting-started-with-motherduck/) on **Tuesday, June 18th at 10am Pacific**.\n\nLastly, if you\u2019re in San Francisco, [join us to celebrate](https://www.eventbrite.com/e/motherducking-party-after-dataai-summit-san-francisco-tickets-901904038257) tonight at our MotherDuck\u2019ing Party happening alongside the Data + AI Summit.\n\n### TABLE OF CONTENTS\n\n[Production-ready DuckDB](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#production-ready-duckdb)\n\n[Simple, Multiplayer at Scale](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#simple-multiplayer-at-scale)\n\n[Unmatched Efficiency of Pricing and Execution](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#unmatched-efficiency-of-pricing-and-execution)\n\n[Backed by a World-Class Team](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#backed-by-a-world-class-team)\n\n[Now SOC 2 Certified](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#now-soc-2-certified)\n\n[Expanded Modern Duck Stack](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#expanded-modern-duck-stack)\n\n[New Startup Program with $10k in Credits](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#new-startup-program-with-10k-in-credits)\n\n[Take Flight with MotherDuck - Now GA](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#take-flight-with-motherduck-now-ga)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![MotherDuck is Landing in Europe! Announcing our EU Region](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Feu_launch_blog_b165ff2751.png&w=3840&q=75)](https://motherduck.com/blog/motherduck-in-europe/)\n\n[2025/09/24 - Garrett O'Brien, Sheila Sitaram](https://motherduck.com/blog/motherduck-in-europe/)\n\n### [MotherDuck is Landing in Europe! Announcing our EU Region](https://motherduck.com/blog/motherduck-in-europe)\n\nServerless analytics built on DuckDB, running entirely in the EU.\n\n[![DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_4_1_b6209aca06.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n[2025/10/09 - Alex Monahan, Garrett O'Brien](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n### [DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/blog/announcing-duckdb-141-motherduck)\n\nMotherDuck now supports DuckDB 1.4.1 and DuckLake 0.3, with new SQL syntax, faster sorting, Iceberg interoperability, and more. Read on for the highlights from these major releases.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 146,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 146
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/spatial-data-management-with-duckdb-ft-mattforrest.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 10,
  [2m  â”‚[0m   "appliedLimit": 10
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! Now I have enough information to write a comprehensive answer. Let me write it to the specified file:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_2.md

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Credit balance is too low
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 8,122 | Cost: $0.2556
     Cumulative: 8,122 tokens | $0.2556
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âš ï¸  Agent asked question but no UserAgent available
     Detection: llm_fallback_failed: litellm.RateLimitError: RateLimitError: OpenAIException - You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors., defaulting to continue

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 13 | Duration: 104.1s
â•‘    Tokens: 8,122 | Cost: $0.2556
â•‘    Stop reason: Agent asked question but no UserAgent available
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  Running LLM judge evaluation...
   ğŸ§  LLM Judge score: 0.00

======================================================================
â“ Question 3/3
...What SQL features from DuckDB are not yet supported in MotherDuck?
======================================================================


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 3
â”‚ Answer this question by searching through the .kurt/sources/ directory:

...What SQL features from DuckDB are not yet supported in MotherDuck?

Required Steps:
1. Search for relevant markdown files in .kurt/sources/ using glob and grep
2. Read the most relevant files (aim for 3-5 most relevant sources)
3. As you read each file, keep track of:
   - The file path
   - The file title/name
   - Key information extracted from that file
4. Synthesize a comprehensive answer based on the information found
5. Write your answer to: /tmp/answer_without_kg_3.md

IMPORTANT - Your answer MUST have this structure:
```markdown
# Answer

[Your comprehensive answer here, drawing from the sources you read]

## Sources

List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
[etc.]

DO NOT use URLs or document titles as links. Show the actual file path you read with the Read tool.
```

The Sources section is MANDATORY - you must list every file you read to answer the question.

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Credit balance is too low
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 0 | Cost: $0.0000
     Cumulative: 0 tokens | $0.0000
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âš ï¸  Agent asked question but no UserAgent available
     Detection: llm_fallback_failed: litellm.RateLimitError: RateLimitError: OpenAIException - You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors., defaulting to continue

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 0 | Duration: 26.9s
â•‘    Tokens: 0 | Cost: $0.0000
â•‘    Stop reason: Agent asked question but no UserAgent available
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  Running LLM judge evaluation...
   ğŸ§  LLM Judge score: 0.00
