# Scenario: answer_motherduck_without_kg

**Status**: âœ… PASSED
**Timestamp**: 20251228_210127

**Duration**: 103.18 seconds
**Tokens Used**: 0

---

## Conversation


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ SCENARIO: answer_motherduck_without_kg
   Answer MotherDuck questions by searching sources directly (no KG)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


ğŸ” Verifying .claude installation...
   âœ“ .claude folder exists at /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.claude
   âœ“ Skills: 0 found
   âœ“ Commands: 0 found

ğŸ” Filtering to questions: [7]

ğŸ§ª Running 1 of 10 question(s) defined in questions_motherduck.yaml


======================================================================
â“ Question 7/10
...If I have a CSV on my laptop and a table in S3, what's the most efficient way to join them using MotherDuck?
======================================================================


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 7
â”‚ Answer this question by searching through the .kurt/sources/ directory:

...If I have a CSV on my laptop and a table in S3, what's the most efficient way to join them using MotherDuck?

Required Steps:
1. Search for relevant markdown files in .kurt/sources/ using glob and grep
2. Read the most relevant files (aim for 3-5 most relevant sources)
3. As you read each file, keep track of:
   - The file path
   - The file title/name
   - Key information extracted from that file
4. Synthesize a comprehensive answer based on the information found
5. Write your answer to: /tmp/answer_without_kg_7.md

IMPORTANT - Your answer MUST have this structure:
```markdown
# Answer

[Your comprehensive answer here, drawing from the sources you read]

## Sources

List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
[etc.]

DO NOT use URLs or document titles as links. Show the actual file path you read with the Read tool.
```

The Sources section is MANDATORY - you must list every file you read to answer the question.

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [96mğŸ”§ TOOL:[0m Glob â†’ **/*.md

  [96mğŸ”§ TOOL:[0m Grep â†’ CSV.*S3|S3.*CSV|join.*local|local.*join

  [96mğŸ”§ TOOL:[0m Grep â†’ CSV|local file

  [96mğŸ”§ TOOL:[0m Grep â†’ S3.*table|table.*S3|attach.*S3
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/quacking-around-the-web.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/local-dev-cloud-prod-dagster-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-struct-nested-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/index.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-summary-chapter2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/community-and-open-source.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/six-reasons-duckdb-slaps.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg_duckdb-postgresql-extension-for-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-duckdb-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-kestra-etl-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/local-duckdb-ui-visual-data-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/getting-started-ducklake-table-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/git-for-data-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/from-data-lake-to-lakehouse-duckdb-portable-catalog.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/fabi-ai-llm-prompt-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-excel-extension.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-one.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-oct-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-data-in-azure-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/ryan-boyd.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/mehdi-ouazza.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 53
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/about-us.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/acceptable-use-policy.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/adithya-krishnan.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/aditya-aomani.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/alex-monahan.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/amrutha-gujjar.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/andrew-mcewen.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/andrew-witten.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/anna-geller.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/antony-courtney.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/boaz-leskes.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/brett-griffin.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/christophe-oudar.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/daniel-palma.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/diptanu-gon-choudhury.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/doug-raymond.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/emily-lucek.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/faraz-hameed.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/garrett-obrien.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/graham-wetzler.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/hamilton-ulmer.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/jacob-matson.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/jelte-fennema-nio.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/jessica-libman.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/jordan-tigani.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/joseph-hwang.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/leslie-thomas.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/luciano-galvao-filho.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/manveer-chawla.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/marc-d.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/marcos-ortiz.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/mehdi-ouazza.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/michael-hunger.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/mike-spaulding.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/mike-xu.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/nick-ursa.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/nouras-haddad.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/oli-dinov.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/peter-boncz.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/pranav-aurora.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/ryan-boyd.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/sahil-gupta.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/saqib-rasul.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/sean-m-tracey.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/sheila-sitaram.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/simon-spati.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/stephanie-wang.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/sung-won-chung.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/thomas-tfmv-mcgeehan.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/till-dohmen.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/tino-tereshko.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/vasilije-markovic.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/yannick-welsch.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/yves-le-maout.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/authors/zhenni-wu.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/MotherDuck-Visualize-Embeddings-Marimo.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/analyze-data-in-azure-with-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/analyze-json-data-using-sql.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/analyze-sqlite-databases-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/analyze-x-data-nodejs-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/announcing-duckdb-snippet-sets-with-motherduck-sharing-databases.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/announcing-mega-giga-instance-sizes-huge-scale.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/announcing-series-seed-and-a.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/announcing-small-data-sf-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/big-data-is-dead.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/bigquery-to-duckdb-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/building-data-applications-with-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/building-motherduck-partner-ecosystem.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/data-app-generator.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/data-engineer-highlights-PyConDE-2023.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/data-engineering-toolkit-essential-tools.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/data-engineering-toolkit-infrastructure-devops.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/data-engineers-answer-10-top-reddit-questions.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-dec-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-nov-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-oct-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/differential-storage-building-block-for-data-warehouse.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/dosomething-motherduck-data-warehouse-ROI.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/duckdb-110-hidden-gems.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/duckdb-cognee-sql-analytics-graph-rag.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2023.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eight.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "durationMs": 1038,
  [2m  â”‚[0m   "numFiles": 100,
  [2m  â”‚[0m   "truncated": true
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/faster-data-pipelines-development-with-mcp-and-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/going-beyond-the-dataframe.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ibis-one-library-to-query-any-backend.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-vs-pandas-vs-polars-for-python-devs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ducklake-big-data-small-coalesce-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-dbt-accelerating-the-developer-experience-with-local-power.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-python-end-to-end-data-engineering-project-13.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-new-paradigm-for-data-visualization-with-just-sql-markdown.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ai-powered-bi-can-llms-really-generate-your-dashboards-ft-michael-driscoll.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/sql-duckdb-book-form.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/quacking-around-the-web.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/react-components-testing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/product-analytics-motherduck-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/web-assembly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-use-cases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/pandas-dataframes-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-struct-nested-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-lake-vs-data-warehouse-vs-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/index.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/auto inference.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data pipeline.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/dataset.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Pandas DataFrames.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/DuckDB CLI.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/DuckDB.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/ETL.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/CLI.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/CSV.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-brief.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-summary-chapter2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-news.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/gardyn.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/kultura-capital.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/reflex-careers-gig-economy-retail.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/community-and-open-source.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streaming-oracle-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/tableau-cloud-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/six-reasons-duckdb-slaps.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/solving-advent-code-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/spark-ducklake-getting-started.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/semantic-layer-duckdb-tutorial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-faker-duckdb-exploration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-duckdb-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/preswald-health-data-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pushing-geo-boundaries-with-motherduck-geobase.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/perf-is-not-enough.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-kestra-etl-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-open-for-all-with-series-b.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/just-enough-sql-for-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/local-duckdb-ui-visual-data-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/log-processing-savings-bacalhau-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-motherduck-for-business-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/json-log-analysis-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/google-sheets-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/galileo-world-geospatial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/geospatial-for-beginner-duckdb-spatial-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/getting-started-ducklake-table-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/getting-started-gis-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/git-for-data-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/from-data-lake-to-lakehouse-duckdb-portable-catalog.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-wasm-in-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/effortless-etl-unstructured-data-unstructuredio-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/estuary-streaming-cdc-replication.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/fabi-ai-llm-prompt-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-python-e2e-data-engineering-project-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-text2sql-llm.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-the-great-federator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-three.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-two.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-excel-extension.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-puppygraph-graph-model-on-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-one.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eleven.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-essential-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineers-answer-10-top-reddit-questions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-app-generator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-snippet-sets-with-motherduck-sharing-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-x-data-nodejs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-data-in-azure-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/sheila-sitaram.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/ryan-boyd.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/luciano-galvao-filho.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/mehdi-ouazza.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 155
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/postgres-integration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/quacking-around-the-web.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/ducklake-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-lake-vs-data-warehouse-vs-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/tableau.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/hevo.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/reflex-careers-gig-economy-retail.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vibe-coding-sql-cursor.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/six-reasons-duckdb-slaps.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-duckdb-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-mooncake-columnstore.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/postgres-duckdb-options.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pushing-geo-boundaries-with-motherduck-geobase.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-kestra-etl-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/log-processing-savings-bacalhau-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/json-log-analysis-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/how-to-extract-analytics-from-bluesky.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/getting-started-ducklake-table-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/flight-sql-vs-rest-vs-jdbc.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/from-data-lake-to-lakehouse-duckdb-portable-catalog.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/ducklake-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-python-e2e-data-engineering-project-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-the-great-federator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-excel-extension.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-dec-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-nov-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/differential-storage-building-block-for-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/simon-spati.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/pranav-aurora.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/jacob-matson.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/mehdi-ouazza.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 60
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search for more specific terms related to local files, S3 access, and joining data:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ hybrid.*execution|dual.*execution|local.*cloud

  [96mğŸ”§ TOOL:[0m Grep â†’ attach|ATTACH.*database|httpfs|s3_access

  [96mğŸ”§ TOOL:[0m Grep â†’ read_csv|read_parquet.*s3
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/pandas-dataframes-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/auto inference.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data pipeline.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/dataset.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/S3 bucket.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/ETL.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/CSV.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-summary-chapter2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/solving-advent-code-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/semantic-layer-duckdb-tutorial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-duckdb-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-kestra-etl-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-union-orchestration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/just-enough-sql-for-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/log-processing-savings-bacalhau-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/json-log-analysis-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/google-sheets-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-text2sql-llm.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-puppygraph-graph-model-on-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-x-data-nodejs-duckdb.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 40
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-ga-produck-tour.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/faster-data-pipelines-development-with-mcp-and-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/build-bigger-with-small-ai-running-small-models-locally.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-duck-in-the-hand-is-worth-two-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/sql-duckdb-book-form.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/quacking-around-the-web.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/react-components-testing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/local-dev-cloud-prod-dagster-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/product-analytics-motherduck-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/web-assembly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-use-cases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/pandas-dataframes-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-struct-nested-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/index.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/acid-transactions-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Jupyter notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/MotherDuck extension.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Dagster.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Cloud Storage.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/contact-us/product-expert.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-summary-chapter2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/kultura-capital.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/community-and-open-source.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/atm-com-analytics-costs-sql-expressibility.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dominik-moritz.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-future-of-bi-bi-as-code-duckdb-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vibe-coding-sql-cursor.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/simplifying-iot-analytics-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/six-reasons-duckdb-slaps.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-recap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/scaling-duckdb-with-ducklings.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg to motherduck at motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg_duckdb-postgresql-extension-for-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-mooncake-columnstore.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-vercel-marketplace-native-integration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-window-functions-in-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-kestra-etl-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-open-for-all-with-series-b.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-union-orchestration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/local-duckdb-ui-visual-data-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/log-processing-savings-bacalhau-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-column-explorer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/json-log-analysis-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/galileo-world-geospatial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/from-data-lake-to-lakehouse-duckdb-portable-catalog.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-wasm-in-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-versus-pandas-versus-polars.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-on-edge-raspberry-pi.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-ten.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eight.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-cognee-sql-analytics-graph-rag.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-essential-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-infrastructure-devops.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-oct-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/building-data-applications-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/building-motherduck-partner-ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-app-generator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-small-data-sf-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/big-data-is-dead.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/bigquery-to-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-series-seed-and-a.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-json-data-using-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/jacob-matson.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/jordan-tigani.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/mehdi-ouazza.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/doug-raymond.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 122
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/privacy-policy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/hack-night.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-summary-chapter2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dominik-moritz.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-future-of-bi-bi-as-code-duckdb-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vibe-coding-sql-cursor.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-keyboard-shortcuts-for-joyful-querying.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/spark-ducklake-getting-started.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/read-scaling-preview.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/redshift-files-hunt-for-big-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg to motherduck at motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/postgres-duckdb-options.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-kestra-etl-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-reflections-sigmod-pods-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-union-orchestration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/local-duckdb-ui-visual-data-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/how-to-extract-analytics-from-bluesky.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/geospatial-for-beginner-duckdb-spatial-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/getting-started-ducklake-table-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/from-data-lake-to-lakehouse-duckdb-portable-catalog.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/effortless-etl-unstructured-data-unstructuredio-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/estuary-streaming-cdc-replication.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-python-e2e-data-engineering-project-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-the-great-federator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-versus-pandas-versus-polars.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-three.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-excel-extension.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-on-edge-raspberry-pi.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-one.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-110-hidden-gems.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-essential-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-dec-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-nov-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/differential-storage-building-block-for-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/bigquery-to-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-snippet-sets-with-motherduck-sharing-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-sqlite-databases-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-data-in-azure-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/MotherDuck-Visualize-Embeddings-Marimo.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 81
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me read the most relevant files about hybrid execution, dual execution, and the no-ETL query capability:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/dual-execution-dbt.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     "content": "---\ntitle: hybrid-analytics-guide\ncontent_type: tutorial\nsource_url: https://motherduck.com/learn-more/hybrid-analytics-guide\nindexed_at: '2025-11-25T09:57:23.352209'\ncontent_hash: 91344eaa7ee54322\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO LEARN](https://motherduck.com/learn-more/)\n\n# Hybrid Analytics: Query Local & Cloud Data Instantly\n\n12 min readBY\n\n[Aditya Somani](https://motherduck.com/authors/aditya-aomani/)\n\n![Hybrid Analytics: Query Local & Cloud Data Instantly](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FZero_latency_e8da68f055.png&w=3840&q=75)\n\nAre your analytics queries timing out in Postgres? Does your laptop grind to a halt when you try to analyze a large CSV file? If you feel caught between the memory limits of local tools and the spiraling costs of cloud data warehouses, you're not alone. The traditional divide between local development and cloud-scale analytics creates friction, slows down projects, and drains budgets.\n\nBut there is a modern, more efficient way. Imagine running complex SQL queries on a 20 GB Parquet file directly on your laptop, without a flicker of memory strain. Picture joining a local spreadsheet with a massive table in Amazon S3 using a single, elegant command. This is the reality of the **hybrid analytics workflow**, a powerful approach that unifies your local machine and the cloud into a single, unified data environment.\n\nThis guide is your definitive resource for understanding and implementing this new workflow. We'll break down the core concepts, provide practical examples, and show you how to solve the most common data bottlenecks for good. You will learn why traditional databases fail for analytics, how to analyze datasets larger than your RAM, and how the power of \"Dual Execution\" lets you instantly join local files with cloud data, all without the cost and complexity of managing a server cluster.\n\n## Why Are My Analytics Queries on Postgres & MySQL Timing Out?\n\nIf your analytical queries on a database like PostgreSQL or MySQL are consistently slow or timing out, you've hit a fundamental architectural limit. These databases are masterpieces of engineering for **[Online Transaction Processing (OLTP)](https://estuary.dev/blog/postgres-to-motherduck/)**. These are the small, fast operations that power applications, like creating a user or updating an order. However, this same design becomes a major bottleneck for **[Online Analytical Processing (OLAP)](https://en.wikipedia.org/wiki/Online_analytical_processing)**, which involves complex queries that scan and aggregate vast amounts of data.\n\nThe root of the problem is their **[row-oriented](https://en.wikipedia.org/wiki/Data_orientation)** storage model. A row-store keeps all data for a single record together on disk, which is efficient for retrieving an entire user profile. But for an analytical query that only needs to sum the `sale_amount` column across millions of rows, a row-store is forced to read every single column for every single row. This wastes an enormous amount of I/O and CPU cycles on data it doesn't need, which is the primary reason your dashboards are slow and your queries fail.\n\nThe next logical step is to move analytical workloads to a system built on a **[columnar architecture](https://motherduck.com/learn-more/columnar-storage-guide/)**. A columnar database organizes data by column, storing all values for `sale_amount` together. When your query asks for the sum of `sale_amount`, the database reads _only_ that column, greatly reducing I/O and speeding up performance by orders of magnitude. Modern analytical engines like DuckDB couple this with **[vectorized query execution](https://15721.courses.cs.cmu.edu/spring2024/notes/06-vectorization.pdf)**, a technique that processes data in large batches or \"vectors\" instead of row-by-row. This method fully utilizes modern CPU capabilities to perform a single instruction on multiple data points simultaneously, leading to large gains in computational efficiency.\n\n| Architecture | Row-Oriented (OLTP) | Column-Oriented (OLAP) |\n| --- | --- | --- |\n| **Primary Use Case** | Application backends, frequent small updates. | Business intelligence, data analytics, large scans. |\n| **Data Storage** | All data for a single record is stored together. | All data for a single column is stored together. |\n| **Query Example** | `SELECT * FROM users WHERE user_id = 123;` (Fast) | `SELECT SUM(sale_amount) FROM sales;` (Slow) |\n| **Best For** | Fast writes and single-record lookups. | Fast aggregations and filtering on specific columns. |\n\nBy offloading analytics to a purpose-built columnar engine, you let your transactional database continue to excel at what it does best while your analytical queries run in a [high-speed, optimized environment](https://motherduck.com/blog/postgres-duckdb-options/).\n\n## How Can I Analyze a 20 GB Parquet File on My Laptop Without It Crashing?\n\nThe second major bottleneck for data professionals is local memory. Trying to load a 20 GB file into a traditional in-memory library like Pandas on a 16 GB laptop will almost certainly result in a `MemoryError`. This happens because these tools must load the entire dataset into your computer's RAM before they can begin processing.\n\nThe modern solution is **out-of-core processing**, a strategy where data is processed in manageable chunks directly from disk without ever being fully loaded into RAM. This is the key feature of engines like DuckDB, which uses a [streaming execution model to handle datasets far larger than your available memory](https://duckdb.org/2024/07/09/memory-management.html). If a complex operation requires more memory than is available (by default, 80% of your system's RAM), DuckDB can \"spill\" intermediate results to a temporary file on disk, ensuring your query completes without crashing.\n\nThis capability turns your laptop into a surprisingly powerful analytics workstation. With an out-of-core engine, analyzing that 20 GB file becomes straightforward. You can simply use SQL to query it directly from a Python script or Jupyter notebook.\n\n```python\nCopy code\n\nimport duckdb\n\n# This query runs out-of-core, never loading the full 20GB file into RAM.\n# DuckDB's streaming engine processes the file in chunks.\nresult = duckdb.sql(\"\"\"\n  SELECT\n      product_category,\n      AVG(sale_price) as avg_price\n  FROM read_parquet('large_sales_dataset.parquet')\n  WHERE region = 'North America'\n  GROUP BY product_category\n  ORDER BY avg_price DESC;\n\"\"\").arrow()\n\nprint(result)\n```\n\nThis simple, powerful approach allows you to perform heavy-duty data analysis on your local machine, iterating quickly without waiting for a cloud cluster to spin up or worrying about memory crashes.\n\n## How Can I Join Local CSVs with Cloud Data in a Single Query?\n\nThis is where the hybrid workflow really shows its strength. Joining a local CSV of fresh sales data with a historical customer table in an S3 bucket has traditionally been a major challenge. The problem is \"data gravity,\" since it's [slow and expensive to move large amounts of data](https://motherduck.com/learn-more/no-etl-query-raw-files/). A naive query would have to either upload your entire local file to the cloud or download the entire cloud table to your laptop, both of which are very inefficient.\n\nMotherDuck solves this with an innovative query planner called **Dual Execution**. It treats your laptop and the MotherDuck cloud as two nodes in a single distributed system. When you run a hybrid query, the planner intelligently breaks it down and pushes computation to where the data lives, minimizing data transfer.\n\n> \"The most innovative feature of MotherDuck's architecture is its dual execution model. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\" - [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained/)\n\nImagine you want to join a local CSV of new product pricing with a massive sales table in MotherDuck, but only for a specific product. Instead of moving entire tables, the Dual Execution planner does the following:\n\n1. **Local Scan:** It scans the small pricing CSV on your laptop.\n2. **Filter Locally:** It applies the filter for the specific product on your machine.\n3. **Transfer Minimal Data:** It sends only the single, filtered pricing row (a few bytes of data) to the cloud.\n4. **Join in the Cloud:** It performs the final join against the massive sales table in the MotherDuck cloud.\n\nThis process can reduce network traffic by orders of magnitude compared to traditional methods, turning a difficult query into an interactive one. You can see this in action with a simple `EXPLAIN` statement, which [shows which parts of the query run locally `(L)` and which run remotely `(R)`](https://motherduck.com/docs/key-tasks/running-hybrid-queries/).\n\n## Why Are Data Engineers So Excited About Dual-Execution Engines?\n\nData engineers are excited because dual-execution engines solve one of their biggest challenges: the \"it worked on my machine\" problem. In traditional setups, the tools and data used for local development are often completely different from the production cloud environment, leading to bugs and deployment failures.\n\nMotherDuck eliminates this by using the **exact same DuckDB engine** both locally and in the cloud. A query that works on your laptop is guaranteed to work in the cloud, creating a direct path from development to production. This greatly improves developer experience (DX) and accelerates iteration cycles.\n\nThis architecture provides three key benefits:\n\n1. **Faster Development:** Engineers can build and test pipelines with the zero-latency feedback of local development before scaling to the cloud.\n2. **Lower Costs:** By using the free, powerful compute on users' laptops and minimizing data transfer, this model significantly reduces cloud bills.\n3. **Better Collaboration:** It transforms the traditionally \"single-player\" DuckDB into a [\"multiplayer\" platform](https://motherduck.com/videos/339/bringing-duckdb-to-the-cloud-dual-execution-explained/) where teams can share databases and work from a single source of truth.\n\nThis powerful combination of local speed and cloud scale is backed by real-world success.\n\n> **[Case Study: Finqore's 60x Pipeline Acceleration](https://motherduck.com/case-studies/)**\n> Finqore, a financial technology company, was struggling with an 8-hour data pipeline built on Postgres. By migrating to a hybrid workflow with MotherDuck, they were able to use DuckDB's performance and MotherDuck's serverless scale to transform that pipeline into an **8-minute workflow**, a 60x improvement that unlocked real-time data exploration for their team.\n\n## How Can I Run SQL Analytics Without Managing Clusters?\n\nThe operational overhead of provisioning, scaling, and maintaining server clusters is a major drain on data teams. **[Serverless SQL analytics](https://motherduck.com/docs/concepts/architecture-and-capabilities/)** removes this burden entirely. Instead of managing infrastructure, you simply write and run your queries, and the platform handles the rest.\n\nThis model is not only simpler but also much more cost-effective for the \"spiky\" workloads typical of analytics. You pay only for the compute you actually use, avoiding the massive costs of idle clusters.\n\n| Feature / Scenario | Traditional Cloud Warehouse (e.g., Snowflake/BigQuery) | Hybrid Approach (Local + Serverless) |\n| --- | --- | --- |\n| **Development & Prototyping** | Billed for active compute time, even for small test queries. | **Free.** Uses local machine resources with no cloud costs. |\n| **Pricing Model** | Complex credits or per-TB scanned, which is hard to predict. | Simple, usage-based billing; pay only for queries you run. |\n| **Idle Compute** | Billed for provisioned clusters, even when idle, which can create a costly [idle tax on short-running queries](https://motherduck.com/learn-more/reduce-snowflake-costs-duckdb). | **No cost.** Serverless architecture has no idle compute. |\n| **Hardware Requirement** | Entirely reliant on expensive, provisioned cloud hardware. | Leverages powerful, existing local hardware (laptops, servers). |\n\nBy adopting a serverless, hybrid approach, you can reduce your cloud data warehouse costs while empowering your team with a faster, more flexible workflow.\n\n## What Lightweight SQL Solutions Can Be Integrated with Jupyter Notebooks?\n\nFor data scientists who work primarily in Jupyter, several excellent tools bring the power of SQL directly into the notebook environment. The most powerful option for serious analytics is the **DuckDB engine** itself. It can query Pandas DataFrames, Arrow tables, and large local files directly with full SQL support. Its performance on analytical queries is excellent in the embedded space.\n\nFor comparison, while Python's standard library includes **SQLite**, it's a row-based OLTP engine and is significantly slower than DuckDB for analytical queries. An independent benchmark found DuckDB to be **[12-35 times faster](https://motherduck.com/learn-more/duckdb-vs-sqlite-databases/)** for aggregations on a 100-million-row dataset. For any analytical task in a notebook, DuckDB is the clear choice for performance and scalability.\n\nAnother popular tool is **[JupySQL](https://jupysql.ploomber.io/)**, which provides convenient SQL \"magics\" (`%sql`, `%%sql`) that turn a notebook cell into a SQL editor. It connects to various backends, including DuckDB, making it great for quick, ad-hoc exploration.\n\n## How Can I Keep Queries on My Laptop in Sync With the Cloud?\n\nThe key to keeping local and cloud queries synchronized is **single-engine semantics**. Because MotherDuck uses the same DuckDB engine on your laptop and in the cloud, a query validated locally is guaranteed to behave identically in production. This eliminates the most common source of dev-prod drift.\n\nA modern dev-to-prod workflow combines this with tools like dbt (data build tool). An analyst can develop and test a dbt model on their laptop against local data for rapid iteration. Once validated, they can promote the model to run in the cloud against the full production dataset with a [single configuration change](https://motherduck.com/blog/dual-execution-dbt/), ensuring complete consistency. This workflow combines the speed of local development with the scale and reliability of a production cloud environment.\n\n## When Should I Not Use This Hybrid Approach?\n\nWhile the hybrid, in-process model is very useful for analytics, it's not a solution for every problem. It's crucial to understand its limitations to know when a different architecture is needed.\n\n1. **High-Concurrency Applications:** An in-process engine like DuckDB is not designed to be a client-server database serving hundreds of simultaneous application users. For powering a public-facing web application, a traditional OLTP database like PostgreSQL or a distributed SQL database is the right choice.\n2. **Real-Time, High-Volume Ingestion:** If your use case involves ingesting thousands of events per second in real-time, a specialized streaming platform or real-time OLAP database would be more suitable.\n3. **Truly Massive, Multi-Terabyte Workloads:** While DuckDB can handle surprisingly large datasets on a single node, if your active working set is in the tens or hundreds of terabytes, you've reached the scale where a distributed cloud data warehouse becomes necessary to parallelize work across a large cluster.\n\nThe beauty of the MotherDuck ecosystem is that it provides a smooth way to scale up. You can start with a local-first, hybrid workflow and, as your concurrency or data scale needs grow, easily push more of the workload to the MotherDuck cloud without changing your core tools or SQL logic.\n\n### TABLE OF CONTENTS\n\n[Why Are My Analytics Queries on Postgres & MySQL Timing Out?](https://motherduck.com/learn-more/hybrid-analytics-guide/#why-are-my-analytics-queries-on-postgres-mysql-timing-out)\n\n[How Can I Analyze a 20 GB Parquet File on My Laptop Without It Crashing?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-analyze-a-20-gb-parquet-file-on-my-laptop-without-it-crashing)\n\n[How Can I Join Local CSVs with Cloud Data in a Single Query?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-join-local-csvs-with-cloud-data-in-a-single-query)\n\n[Why Are Data Engineers So Excited About Dual-Execution Engines?](https://motherduck.com/learn-more/hybrid-analytics-guide/#why-are-data-engineers-so-excited-about-dual-execution-engines)\n\n[How Can I Run SQL Analytics Without Managing Clusters?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-run-sql-analytics-without-managing-clusters)\n\n[What Lightweight SQL Solutions Can Be Integrated with Jupyter Notebooks?](https://motherduck.com/learn-more/hybrid-analytics-guide/#what-lightweight-sql-solutions-can-be-integrated-with-jupyter-notebooks)\n\n[How Can I Keep Queries on My Laptop in Sync With the Cloud?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-keep-queries-on-my-laptop-in-sync-with-the-cloud)\n\n[When Should I Not Use This Hybrid Approach?](https://motherduck.com/learn-more/hybrid-analytics-guide/#when-should-i-not-use-this-hybrid-approach)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\n## FAQS\n\n### My analytics queries on Postgres / MySQL are timing out. What\u2019s the next step up?\n\nThe next step is to move your analytical workloads from a row-oriented (OLTP) database like Postgres to a purpose-built columnar (OLAP) engine like DuckDB. Columnar engines are designed for analytics and are orders of magnitude faster for large scans and aggregations because they only read the specific columns needed for a query.\n\n### How can I analyze a 20 GB Parquet file on my laptop without it crashing?\n\nUse a query engine that supports \"out-of-core\" processing, like DuckDB. It processes data in manageable chunks directly from your disk instead of loading the entire file into RAM. This allows you to analyze datasets that are significantly larger than your computer's available memory.\n\n### Why is it so hard to join local CSVs with cloud data?\n\nThe main challenge is \"data gravity\"\u2014it's slow and expensive to move large datasets across a network. Traditional tools force you to either upload your entire local file or download the entire cloud table. A modern hybrid platform with a dual-execution engine solves this by intelligently minimizing data transfer.\n\n### Can I mix local datasets and S3 data in a single query?\n\nYes. With a hybrid analytics platform like MotherDuck, you can write a single SQL query that joins local files (e.g., CSVs) with data in cloud storage (e.g., Parquet files in S3). The system treats them as if they exist in one unified environment, abstracting away their physical location.\n\n### How can I run SQL analytics without managing clusters?\n\nAdopt a serverless SQL analytics platform. These services handle all infrastructure provisioning, scaling, and maintenance for you. You simply run your queries and pay only for the compute you use, which eliminates the high cost and operational overhead of managing idle clusters.\n\n### Why are data engineers excited about dual-execution engines?\n\nDual-execution engines solve the \"it worked on my machine\" problem by using the exact same query engine on the developer's laptop and in the cloud. This guarantees consistency, speeds up development cycles, lowers costs by leveraging local compute, and enables better team collaboration on a single source of truth.\n\n### What lightweight SQL solutions can be integrated with Jupyter notebooks?\n\nDuckDB is the leading lightweight SQL solution for Jupyter. It can be installed via pip and can directly query Pandas DataFrames, Arrow tables, and large local files with full SQL support. It is significantly faster for analytical queries than other embedded options like SQLite.\n\n### How can I keep queries on my laptop in sync with the cloud?\n\nUse a platform that offers single-engine semantics, like MotherDuck. Because it runs the same DuckDB engine locally and in the cloud, a query validated on your laptop is guaranteed to behave identically in production. This eliminates dev-prod drift and ensures consistency.\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 212,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 212
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     "content": "---\ntitle: dual-execution-dbt\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/dual-execution-dbt\nindexed_at: '2025-11-25T19:57:41.215232'\ncontent_hash: 2b6c5ae395798479\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Local dev and cloud prod for faster dbt development\n\n2025/01/16 - 8 min read\n\nBY\n\n[Jacob Matson](https://motherduck.com/authors/jacob-matson/)\n\n## Introducktion\n\nI hate waiting for slow pipelines to run, so I am delighted to share some strategies to iterate on your data problems at maximum speed - MotherDuck even gave a talk on this concept at [dbt Coalesce in 2024](https://www.youtube.com/watch?v=oqwIHvSfOVQ). By harnessing the capabilities of DuckDB locally, backed by MotherDuck in the cloud, we can unlock an incredibly fast and efficient development cycle. We'll explore how to configure your dbt profile for dual execution and share some tips on how much data to bring local. By implementing these techniques, you can significantly accelerate your data pipeline development and iterate even faster to solve business problems.\n\nLooking to following along in the code?\n\nCheck out the [example repo](https://github.com/motherduckdb/motherduck-examples/tree/main/dbt-dual-execution)!\n\n![Instant feedback loop](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FInstant_feedback_loop_b99078a679.png&w=3840&q=75)\n\n## Setting up your Profile\n\nIn order to take advantage of these capabilities, we need to configure our dbt profile to execute in the correct place, as well as define the behavior that we want in our sources. In the example dbt profile below, `prod` runs entirely in the cloud, while `local` runs mostly on local but is also linked to MotherDuck for reading data into your local database.\n\n```yml\nCopy code\n\ndual_execution:\n  outputs:\n    local:\n      type: duckdb\n      path: local.db\n      attach:\n        - path: \"md:\"\t# attaches all MotherDuck databases\n    prod:\n      type: duckdb\n      path: \"md:jdw\"\n  target: local\n```\n\n## Sources & Models\n\nWith your sources, you need to define which ones to replicate entirely, which ones are ok as views, and which ones to sample. Keep in mind for sampling, you need to think about your data model and make sure that related samples are hydrated (i.e. if you only bring in 100 customers, you need to make sure you also bring in their orders too).\n\nIn my example project using TPC-DS as the source data, I am sampling 1% of the data when running locally on the large tables. In general, I am aiming to keep the datasets less than a million rows per table, although there is no hard limit. For the remaining tables, I am replicating the entire data set locally since they are so small.\n\nThe way that we conditionally sample our models is by using the [\u2018target\u2019 variable](https://docs.getdbt.com/reference/dbt-jinja-functions/target). You can add this parameter by checking your `target` and running it conditionally on your model.\n\nAn example sql snippet is below (using jinja).\n\n```sql\nCopy code\n\nfrom {{ source(\"tpc-ds\", \"catalog_sales\") }}\n{% if target.name == 'local' %} using sample 1 % {% endif %}\n```\n\nAs an example of a simple \u201ccreate local table from cloud\u201d, consider the following query plan. The \u201cL\u201d indicates Local and the \u201cR\u201d indicates Remote (i.e. MotherDuck).\n\n```bash\nCopy code\n\n\ud83e\udd86 explain create table\n        \"local\".\"main\".\"call_center\"\n      as (\n        from \"jdw_dev\".\"jdw_tpcds\".\"call_center\"\n      );\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502\u2502       Physical Plan       \u2502\u2502\n\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 BATCH_CREATE_TABLE_AS (L) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    DOWNLOAD_SOURCE (L)    \u2502\n\u2502    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n\u2502        bridge_id: 1       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  BATCH_DOWNLOAD_SINK (R)  \u2502\n\u2502    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n\u2502        bridge_id: 1       \u2502\n\u2502       parallel: true      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       SEQ_SCAN  (R)       \u2502\n\u2502    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n\u2502        call_center        \u2502\n\u2502                           \u2502\n\u2502        Projections:       \u2502\n\u2502     cc_call_center_sk     \u2502\n\u2502     cc_call_center_id     \u2502\n\u2502     cc_rec_start_date     \u2502\n\u2502      cc_rec_end_date      \u2502\n\u2502     cc_closed_date_sk     \u2502\n\u2502      cc_open_date_sk      \u2502\n\u2502          cc_name          \u2502\n\u2502          cc_class         \u2502\n\u2502        cc_employees       \u2502\n\u2502          cc_sq_ft         \u2502\n\u2502          cc_hours         \u2502\n\u2502         cc_manager        \u2502\n\u2502         cc_mkt_id         \u2502\n\u2502        cc_mkt_class       \u2502\n\u2502        cc_mkt_desc        \u2502\n\u2502     cc_market_manager     \u2502\n\u2502        cc_division        \u2502\n\u2502      cc_division_name     \u2502\n\u2502         cc_company        \u2502\n\u2502      cc_company_name      \u2502\n\u2502      cc_street_number     \u2502\n\u2502       cc_street_name      \u2502\n\u2502       cc_street_type      \u2502\n\u2502      cc_suite_number      \u2502\n\u2502          cc_city          \u2502\n\u2502         cc_county         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nThis can also be extended to your `sources.yml` if necessary for testing local datasets (i.e. json or parquet on experimental pipelines that have not yet made it to your data lake). Configuring these is similar:\n\n```ini\nCopy code\n\n{%if- target.name == 'local' -%}\n   meta:\n      external_location:\n        data/tpcds/{name}.parquet\n{%- endif -%}\n```\n\n## Running your pipeline\n\nOnce you have this configuration in place, you can simply run your pipeline as normal, although for ease of use, you may want to add tags to the models that you are working on so you can avoid going back to the cloud data set too often. This can be set simply in the `dbt_project.yml` like this:\n\n```yml\nCopy code\n\nmodels:\n  dual_execution:\n    tpcds:\n      raw:\n        +tags: ['raw']\n        +materialized: table\n      queries:\n        +materialized: view\n        +tags: ['queries']\n```\n\nFrom there, it is as simple as running `dbt build -s tag:raw` to load your raw data and then for subsequent query iteration, run `dbt build -s tag:queries` in the CLI. The subsequent runs can be visualized like this:\n\n![data flow cloud to local](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdata_flow_cloud_to_local_792a293673.png&w=3840&q=75)\n\n## Shipping dev to the cloud\n\nCertain tables may need to be available in your cloud data warehouse for testing even in the local workflow. This may be something like a BI tool, that is connected to your cloud instance and is difficult to run locally. This can be accomplished by setting the database attribute in your model, so that after the model is run, it is available in the cloud as well.\n\n```yml\nCopy code\n\n{{ config(\n    database=\"jdw_dev\",\n    schema=\"local_to_prod\"\n    materialized=\"table\"\n) }}\n```\n\nIt should be noted that this is a static configuration that is best used for testing. If you don\u2019t want to manually flip models between dev / prod destinations, you can define the database as an attribute of a specific model in your `dbt_project.yml` file.\n\n## Wrapping up\n\nAs you can see from this example, using MotherDuck\u2019s dual execution allows us to leverage the unique value proposition of DuckDB to run an accelerated development cycle on your local machine. With some basic optimization, we can get ~5x faster dbt runs by making the data smaller and using local compute. This is a very powerful combination for rapidly iterating on your pipeline and then pushing a high quality change back into your production environment.\n\nWant to learn more? Join our webinar about Local Dev & Cloud Prod on [February 13th, 2025](https://lu.ma/0die8ual?utm_source=blog).\n\n### TABLE OF CONTENTS\n\n[Introducktion](https://motherduck.com/blog/dual-execution-dbt/#introducktion)\n\n[Setting up your Profile](https://motherduck.com/blog/dual-execution-dbt/#setting-up-your-profile)\n\n[Sources & Models](https://motherduck.com/blog/dual-execution-dbt/#sources-models)\n\n[Running your pipeline](https://motherduck.com/blog/dual-execution-dbt/#running-your-pipeline)\n\n[Shipping dev to the cloud](https://motherduck.com/blog/dual-execution-dbt/#shipping-dev-to-the-cloud)\n\n[Wrapping up](https://motherduck.com/blog/dual-execution-dbt/#wrapping-up)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![What\u2019s New: Streamlined User Management, Metadata, and UI Enhancements](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FMother_Duck_Feature_Roundup_2_47f5d902c0.png&w=3840&q=75)](https://motherduck.com/blog/data-warehouse-feature-roundup-dec-2024/)\n\n[2024/12/21 - Sheila Sitaram](https://motherduck.com/blog/data-warehouse-feature-roundup-dec-2024/)\n\n### [What\u2019s New: Streamlined User Management, Metadata, and UI Enhancements](https://motherduck.com/blog/data-warehouse-feature-roundup-dec-2024)\n\nDecember\u2019s feature roundup is focused on improving the user experience on multiple fronts. Introducing the User Management REST API, the Table Summary, and a read-only MD\\_INFORMATION\\_SCHEMA for metadata.\n\n[![DuckDB Ecosystem: January 2025](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fnewsletter_a65cff5430.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025/)\n\n[2025/01/10 - Simon Sp\u00e4ti](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025/)\n\n### [DuckDB Ecosystem: January 2025](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025)\n\nDuckDB Monthly #25: PyIceberg, 0$ data distribution and more!\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 245,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 245
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     "content": "---\ntitle: 'No-ETL: Query Raw CSV & JSON Files Directly with SQL'\ncontent_type: guide\ndescription: Skip complex ETL. Learn the No-ETL method for startups to query multiple\n  raw CSV, JSON, & Parquet files directly with SQL. Get insights in minutes, not months.\npublished_date: '2025-10-17T00:00:00'\nsource_url: https://motherduck.com/learn-more/no-etl-query-raw-files\nindexed_at: '2025-11-25T20:37:09.260086'\ncontent_hash: 317312307e45ea1c\nhas_step_by_step: true\nhas_narrative: true\n---\n\n# The No-ETL Playbook: How to Query Raw CSV & JSON Files Directly with SQL\n\nFor startups and small companies, your most valuable data, like customer sign-ups, sales transactions, and product usage logs, is likely scattered across hundreds of local files like CSVs, Excel spreadsheets, and JSON logs. The conventional wisdom says you need to build a complex and expensive **Extract, Transform, Load (ETL)** pipeline before you can even begin to analyze it. But for a lean team that needs to move fast, this approach is a trap. It\u2019s slow, rigid, and drains precious engineering resources before you\u2019ve asked your first question.\n\nWhat if you could skip the pipeline and go straight to the insights?\n\nA modern, **\"No-ETL\"** approach allows you to do just that. Instead of spending months building a complex data infrastructure, you can use simple SQL to query your raw data files directly where they live, on your laptop or in cloud storage. This guide will show you how this lean, serverless method turns fragmented files into a powerful, queryable database in minutes, not months. You will learn why traditional ETL is a bottleneck, how to get immediate answers from your raw files, and how to consolidate thousands of fragmented files into a single, high-performance database, all while unifying data across your local machine and the cloud.\n\n## Why Are Traditional Data Pipelines a Trap for Startups?\n\nFor decades, the path from data to decision was a one-way street paved with ETL. This process involves extracting data from various sources, transforming it into a rigid, predefined schema, and loading it into a central data warehouse. While this model can work for large enterprises with stable processes, it creates a \"pipeline city\" that demands constant maintenance and becomes a major bottleneck for agile startups that need to pivot quickly.\n\nThe \"No-ETL\" philosophy flips this script entirely. Instead of moving all your data to a central processing location, you bring the processing power directly to your data. By pointing SQL at the files you already have, you can eliminate 80-90% of the traditional pipeline steps. This approach, powered by MotherDuck's serverless engine, allows teams to [consolidate data from dozens of sources in days, not months](https://motherduck.com/learn-more/modern-data-warehouse-playbook/).\n\nThis shift dramatically accelerates your time-to-insight and slashes operational costs. Processing a 100MB CSV file on your laptop is instantaneous and costs nothing, whereas using a traditional cloud warehouse incurs charges for compute time and network data transfer. Companies that adopt this lean model report [ 70-90% cost reductions](https://motherduck.com/learn-more/modern-data-warehouse-playbook/) compared to traditional cloud data warehouses.\n\n### How Did Five Startups Slash Data Prep Time from Hours to Minutes?\n\nThe benefits of abandoning premature ETL aren't just theoretical. Startups across various industries have replaced slow, brittle pipelines and overloaded databases with MotherDuck\u2019s query-in-place model, leading to dramatic improvements in speed, cost, and agility. These changes often lead to huge improvements, not just small percentage gains.\n\n| Company & Industry | Previous State & Pain Point | Transformation with MotherDuck | Quantified Outcome |\n|---|---|---|---|\nFinqore (FinTech) | 8-hour data pipelines using Postgres for complex financial data and a heavy reliance on manual Excel processes. | Replaced Postgres to process and unify data directly, enabling a real-time metrics explorer and AI agents. |\n8 hours to 8 minutes |\n\n**Gardyn**(IoT / AgTech)[Pipeline time cut from](https://motherduck.com/case-studies/gardyn/)at 10x lower cost than other data warehouses.**over 24 hours to under 1 hour****UDisc**(Sports Tech)[dbt job time reduced from](https://motherduck.com/case-studies/udisc-motherduck-sports-management/), and typical queries dropped from minutes to**6 hours to 30 minutes****5 seconds**.**Dexibit**(Analytics)[Analytical query times reduced from](https://motherduck.com/case-studies/dexibit/), eliminating traditional data warehouse costs.**minutes to a few seconds****Layers**(SaaS)[dashboards loading in](https://motherduck.com/case-studies/layers-multi-tenant-data-warehouse/).**110 ms**## How Can You Get Instant Answers from Excel & CSV Files?\n\nFor many teams, valuable data is trapped in local spreadsheets. The traditional path to analyzing this data, which includes manual imports, database setup, and schema definition, is slow and frustrating. MotherDuck eliminates this friction by allowing you to query CSV and Excel files directly with SQL, just as if they were database tables.\n\nThis is powered by DuckDB's [ read_csv_auto](https://duckdb.org/docs/stable/data/csv/auto_detection.html) and\n\n[functions, which automatically infer column names, data types, and file dialects (like delimiters) by sampling the file. This](https://duckdb.org/docs/stable/guides/file_formats/excel_import.html)\n\n`read_xlsx`\n\n**schema-on-read**capability means a non-technical user can go from a local file to a powerful SQL query in seconds, without writing any\n\n`CREATE TABLE`\n\nstatements or managing a database server.For a local CSV file, a query is as simple as this:\n\nCopy code\n\n```\nSELECT\nproduct_category,\nSUM(sale_amount) AS total_sales\nFROM 'transactions.csv'\nGROUP BY ALL\nORDER BY total_sales DESC;\n```\n\n\nMotherDuck [automatically detects the columns and their types](https://duckdb.org/docs/stable/data/csv/overview.html), allowing you to filter, aggregate, and sort on the fly. This direct-query capability extends to files stored in cloud object storage and [even Google Sheets](https://motherduck.com/docs/key-tasks/data-warehousing/Replication/spreadsheets/), providing a unified way to access spreadsheet data wherever it lives.\n\n### What About Messy Spreadsheets? How to Handle Common Pitfalls in SQL\n\n\"Wild\" CSV and Excel files often have inconsistencies. Instead of spending time on manual cleanup, you can handle these common issues directly in your SQL query using optional parameters.\n\n| Pitfall | Problem Description | SQL Solution with MotherDuck |\n|---|---|---|\nIncorrect Delimiter | A CSV file uses a pipe (`|` ) or semicolon (`;` ) instead of a comma, causing columns to be misread. | Use the\n`delim` parameter |\n\n`read_csv('data.csv', delim = '|')`\n\n.**Inconsistent Date Formats**`MM/DD/YYYY`\n\nor `DD-Mon-YY`\n\n.[to provide the exact format string:](https://duckdb.org/docs/stable/data/csv/overview.html)`dateformat`\n\nparameter`read_csv('data.csv', dateformat = '%m/%d/%Y')`\n\n.**Header and Footer Rows**[to select only the data cells:](https://duckdb.org/docs/stable/guides/file_formats/excel_import.html)`range`\n\nparameter`range = 'A5:Z100'`\n\n.**Mixed Data Types**[, then use](https://duckdb.org/docs/stable/guides/file_formats/excel_import.html)`all_varchar = true`\n\n`TRY_CAST()`\n\nin your `SELECT`\n\nstatement to safely convert types.## How Do I Turn Thousands of Fragmented Files into a Single Database?\n\nA common challenge for growing companies is data fragmentation, where analytics data is spread across thousands of individual files in cloud storage like Amazon S3. MotherDuck sidesteps complex ingestion jobs by [treating an entire folder of files as a single, queryable database table](https://duckdb.org/docs/stable/data/multiple_files/overview.html).\n\nBy using SQL with glob patterns, you can instantly query a whole collection of Parquet, CSV, or JSON files directly in S3. For example, the following query will scan all Parquet files for the year 2025, no matter how many subdirectories they are in, and treat them as one large table:\n\nCopy code\n\n```\nSELECT\nevent_type,\nCOUNT(*) AS event_count\nFROM read_parquet('s3://my-bucket/logs/2025/**/*.parquet')\nGROUP BY event_type;\n```\n\n\nThis is a high-performance feature, not just a convenience. MotherDuck\u2019s query engine [pushes down filters and projections to the file level](https://duckdb.org/docs/stable/data/parquet/overview.html), minimizing the amount of data read from cloud storage and reducing costs. For even greater efficiency, organizing files using [ Hive-style partitioning](https://duckdb.org/docs/stable/guides/performance/how_to_tune_workloads.html) (e.g.,\n\n`/year=2025/month=10/`\n\n) allows the engine to skip entire folders that don\u2019t match a query\u2019s `WHERE`\n\nclause, dramatically reducing scan time and cost.## How Can I Join Local Files with Data in the Cloud?\n\nStartups rarely have their data in one neat location. You might have recent sales data in a CSV on your laptop, historical logs in an S3 bucket, and user profiles in a managed database. MotherDuck\u2019s [ Dual Execution feature](https://motherduck.com/docs/concepts/architecture-and-capabilities/) unifies these fragmented datasets into a single analytical layer without requiring you to move the data first.\n\nWhen your local DuckDB client is connected to MotherDuck, they form a distributed system where the query optimizer [intelligently routes parts of your query to where the data lives](https://motherduck.com/videos/339/bringing-duckdb-to-the-cloud-dual-execution-explained/). A query on a local file runs entirely on your machine's resources. A query on a large S3 file runs in the MotherDuck cloud to use its scale. Most powerfully, a join between the two is automatically optimized to minimize data transfer.\n\nAn analyst can prototype a query by joining a local spreadsheet with a massive cloud table. MotherDuck is smart enough to automatically push filters down to the local file, send only the small, filtered result to the cloud, and perform the final join there. This [avoids the slow and costly process of uploading entire local files](https://motherduck.com/learn-more/fix-slow-bi-dashboards/) just to perform a join. The entire process is smooth, and you simply write standard SQL.\n\n## How Can You Explore Data Before Building a Formal Pipeline?\n\nThe schema-on-read approach is powerful because it lets you [explore and understand your data before committing](https://motherduck.com/duckdb-book-summary-chapter5/) to a rigid transformation pipeline. This \"explore-then-model\" workflow de-risks data projects by allowing you to assess data quality, discover hidden patterns, and validate business assumptions upfront. With MotherDuck, you can use a suite of simple SQL commands to profile your raw CSV, JSON, and Parquet files directly.\n\n### How Can You See the Structure of Your JSON and Parquet Files Automatically?\n\nMotherDuck provides powerful functions to look inside the structure of your semi-structured and columnar files without manually parsing them.\n\nFor **Parquet files**, you can [query the file\u2019s internal metadata directly](https://duckdb.org/docs/stable/data/parquet/metadata.html) to see column names, types, and nullability. This is very helpful for understanding the data you've received from a partner or another system.\n\nCopy code\n\n```\nSELECT * FROM parquet_schema('your_file.parquet');\n```\n\n\nFor **JSON files**, the [ read_json_auto function automatically infers a schema](https://duckdb.org/docs/stable/data/json/loading_json.html), representing nested objects as\n\n`STRUCT`\n\ns and arrays as `LIST`\n\ns. You can see this inferred schema by creating a temporary table and describing it:Copy code\n\n```\nCREATE TABLE temp_json AS SELECT * FROM read_json_auto('api_response.json');\nDESCRIBE temp_json;\n```\n\n\n### How Can You Use SQL to Explore JSON Data Without Knowing Its Structure?\n\nJSON's nested, schema-less nature makes it notoriously difficult to analyze with traditional SQL. MotherDuck lets you [explore and query deeply nested JSON files immediately](https://motherduck.com/blog/analyze-json-data-using-sql/), even with zero prior knowledge of their structure.\n\nYou can instantly navigate the nested structure using simple **dot notation** (e.g., `SELECT user.name.first FROM 'users.json'`\n\n) and flatten complex arrays into rows using the ** UNNEST** function. This turns what was once a painful data preparation task into a simple, interactive exploration process.\n\nCopy code\n\n```\n-- Explore nested JSON and flatten an array of items into separate rows\nSELECT\norder_id,\ncustomer.id AS customer_id,\nitem.product_id::INTEGER,\nitem.quantity::INTEGER\nFROM read_json_auto('orders.json'), UNNEST(line_items) AS t(item);\n```\n\n\n## Is the \"No-ETL\" Approach a Permanent Solution?\n\nFor a lean team, building a full-blown ETL pipeline too early is a strategic error. It locks you into a rigid structure before you fully understand your data's value or how your business questions will evolve. The \"No-ETL\" approach is a better starting point for most startups.\n\nHowever, \"No-ETL\" does not mean \"Never-ETL.\" As your company matures and your data processes become more standardized, certain triggers justify introducing a more formalized, lightweight EL(T) process, where raw data is loaded into cloud storage and then transformed within the warehouse. You should consider this change when you need things like **repeatability for audits**, such as for financial reporting or compliance. It also becomes valuable when you need to improve **performance on complex joins** for frequently-run dashboards, or when business operations depend on [ strict data freshness SLAs](https://motherduck.com/learn-more/modern-data-warehouse-use-cases/) of minutes, not hours. Finally, as your team grows, a formalized model is essential for\n\n[, allowing you to systematically clean data or mask PII before exposing it to a wider audience.](https://motherduck.com/blog/motherduck-kestra-etl-pipelines/)\n\n**data governance at scale**This **progressive modeling pattern**, which involves starting with raw files, creating semantic views, and only materializing tables when necessary, allows your data architecture to evolve with your business, not against it.\n\n## Your Path Forward: From Files to Insights\n\nThe message for startups is clear: stop building pipelines and start asking questions. The modern, No-ETL approach used by MotherDuck removes the friction between your data and your decisions. By letting your entire team query raw files directly with the SQL they already know, you unlock a level of speed and agility that traditional data stacks simply cannot match. Start by exploring your local files, scale to the cloud, and let your data architecture grow with your business needs. The power to be data-driven is no longer locked behind complex engineering projects. It's right there in your files, waiting for a query.\n\nStart using MotherDuck now!\n\n## FAQS\n\n### Do I really need an ETL pipeline if I\u2019m just getting started?\n\nFor most startups, no. A \"No-ETL\" approach lets you query raw CSV, JSON, and Excel files directly with SQL. This is faster and cheaper, allowing you to get insights immediately without building complex infrastructure.\n\n### How can I analyze data in Excel and CSV files without hassle?\n\nYou can use SQL to query local Excel and CSV files as if they were database tables. Modern tools like DuckDB automatically detect columns and data types, so you can go from a spreadsheet to analysis in seconds without any setup.\n\n### How do I query thousands of files in a folder as a single database?\n\nUse SQL with glob patterns (e.g., `FROM 's3://bucket/logs/**/*.parquet'`\n\n). This treats an entire folder of files in cloud storage as one large, queryable table, eliminating the need for complex data ingestion jobs.\n\n### Can I explore my data before building an ETL pipeline?\n\nYes, this is a key benefit of the No-ETL approach. By querying raw files directly, you can profile data quality, discover patterns, and validate assumptions with SQL before committing to a rigid transformation model.\n\n### How can I see the structure of my JSON and Parquet files automatically?\n\nUse simple SQL commands. For Parquet, `parquet_schema('file.parquet')`\n\nreveals the schema. For JSON, `read_json_auto()`\n\ninfers the structure, which you can view with a `DESCRIBE`\n\ncommand on the query result.\n\n### Why is schema design so difficult for startups?\n\nStartups evolve rapidly, causing data sources and business needs to change constantly. A rigid, upfront schema (schema-on-write) becomes a bottleneck. A flexible, schema-on-read approach is better as it applies structure at query time.",
  [2m  â”‚[0m     "numLines": 229,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 229
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md",
  [2m  â”‚[0m     "content": "---\ntitle: cidr-paper-hybrid-query-processing-motherduck\ncontent_type: event\nsource_url: https://motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck\nindexed_at: '2025-11-25T19:56:16.076582'\ncontent_hash: 23a9f33edc9cbbbb\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Just Released: Hybrid Query Processing Paper at CIDR 2024\n\n2024/01/16 - 2 min read\n\nBY\n\n[Peter Boncz](https://motherduck.com/authors/peter-boncz/)\n\nThe Conference on Innovative Data systems Research (CIDR) is underway in California and we\u2019re proud to be presenting a peer-reviewed [paper on the MotherDuck hybrid query processing architecture](https://www.cidrdb.org/cidr2024/papers/p46-atwal.pdf).\n\n[Hybrid query processing](https://motherduck.com/learn-more/hybrid-analytics-guide/) allows you to execute database queries either on your local machine, in the cloud, or using a combination of both. It adds useful capabilities to DuckDB, for instance the sharing of DuckDB databases between different team members via the cloud. It also allows you to create web applications with DuckDB running inside your browser, that can jointly execute queries with MotherDuck in the cloud.\n\nThe research and implementation of this architecture has been a collaboration between MotherDuck, DuckDB Labs and myself as a visiting database researcher on sabbatical from CWI, the Dutch national computer science research institute from which DuckDB was born.\n\nBecause designing and implementing a cutting-edge database system like MotherDuck is non-trivial, there are in fact quite a bit of research elements in what we do, even when software engineering. For example, we need to understand how to optimally plan hybrid queries when there are asymmetrical network connections (like in consumer internet) or cost differences in storage, compute and energy. This is why the collaboration between academia and industry is so important in databases; it has already provided a lot of inspiration for my research group at CWI while providing benefits to MotherDuck\u2019s users.\n\nI look forward to gaining other inspiration for my research group and MotherDuck from my fellow researchers at CIDR. Although CIDR is a relatively small conference, it attracts a distinguished audience of researchers and practitioners working in data systems attending it. The conference was originally created by two Turing Award winners: Jim Gray and Michael Stonebraker, both founding figures of the database field.\n\nOur CIDR paper is [now available for download](https://www.cidrdb.org/cidr2024/papers/p46-atwal.pdf) and provides an in-depth view of MotherDuck and our hybrid query architecture. I truly hope that you also will find it interesting. If so, please spread the word and pass it along to people who you think also will find this interesting!\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![Why Python Developers Need DuckDB (And Not Just Another DataFrame Library)](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Fwhy_pythondev_1_22167e31bf.png&w=3840&q=75)](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/)\n\n[2025/10/08 - Mehdi Ouazza](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/)\n\n### [Why Python Developers Need DuckDB (And Not Just Another DataFrame Library)](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries)\n\nUnderstand why a database is much more than just a dataframe library\n\n[![DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_4_1_b6209aca06.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n[2025/10/09 - Alex Monahan, Garrett O'Brien](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n### [DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/blog/announcing-duckdb-141-motherduck)\n\nMotherDuck now supports DuckDB 1.4.1 and DuckLake 0.3, with new SQL syntax, faster sorting, Iceberg interoperability, and more. Read on for the highlights from these major releases.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 74,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 74
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me read a file specifically about querying S3 data:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/duckdb-the-great-federator.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md",
  [2m  â”‚[0m     "content": "---\ntitle: querying-data-from-s3-with-3-lines-in-your-terminal\ncontent_type: event\nsource_url: https://motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal\nindexed_at: '2025-11-25T20:44:10.612560'\ncontent_hash: 86949cc4a5628597\nhas_code_examples: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nQuerying Data From S3 With 3 Lines In Your Terminal - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Querying Data From S3 With 3 Lines In Your Terminal](https://www.youtube.com/watch?v=2Ug5HpWY4qs)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=2Ug5HpWY4qs&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 0:56\n\n\u2022Live\n\n\u2022\n\nYouTubeShort\n\n# Querying Data From S3 With 3 Lines In Your Terminal\n\n2024/01/10\n\nThis short from Mehdi Ouazza shows you how to use DuckDB to query data from S3 data lakes with 3 lines in your terminal.\n\n## Related Videos\n\n[![\"One data tool with all its dependencies: DuckDB and extensions\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fvideo_we_RMT_Aj_Udic_9e9f5c6d41.jpg&w=3840&q=75)\\\\\n\\\\\n0:00:55](https://motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions/)\n\n[2024-06-21](https://motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions/)\n\n### [One data tool with all its dependencies: DuckDB and extensions](https://motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions)\n\nLearn about DuckDB extensions, including the ability to query data in your AWS S3-powered data lake.\n\nYouTube\n\nShort\n\n[![\"No SQL standard for 7 years ?!\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fvideo_Joro_Ss_Keq_Sk_9a6deced1a.jpg&w=3840&q=75)\\\\\n\\\\\n0:00:47](https://motherduck.com/videos/no-sql-standard-for-7-years/)\n\n[2024-01-22](https://motherduck.com/videos/no-sql-standard-for-7-years/)\n\n### [No SQL standard for 7 years ?!](https://motherduck.com/videos/no-sql-standard-for-7-years)\n\nThis short of Mehdi with Alex Monahan discusses the lack of evolution of the SQL standard. \u201c(SQL) stands for Structured Query Language, not Standard Query Language\u201d\n\nYouTube\n\nShort\n\nSQL\n\n[![\"Simple way to convert CSV and Parquet files\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fvideo_Y_G_Xdbet9_Gk_6fc9d7bdd0.jpg&w=3840&q=75)\\\\\n\\\\\n0:01:00](https://motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files/)\n\n[2024-01-17](https://motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files/)\n\n### [Simple way to convert CSV and Parquet files](https://motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files)\n\nWant convert CSV to Parquet or vice versa? In this YouTube short, Mehdi shows you how to convert using DuckDB as a Swiss Army Knife to convert data formats.\n\nYouTube\n\nShort\n\nTutorial\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 130,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 130
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval___w2kamk/.kurt/sources/motherduck.com/blog/duckdb-the-great-federator.md",
  [2m  â”‚[0m     "content": "---\ntitle: duckdb-the-great-federator\ncontent_type: blog\nsource_url: https://motherduck.com/blog/duckdb-the-great-federator\nindexed_at: '2025-11-25T19:58:18.622062'\ncontent_hash: 03b2907ba9fd3b07\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# DuckDB, the great federator?\n\n2024/01/04 - 13 min read\n\nBY\n\n[Christophe Oudar](https://motherduck.com/authors/christophe-oudar/)\n\nMoving data sounds straightforward, but it\u2019s increasingly becoming a significant challenge. With the surge in data creation and the diversity of data types, integrating different systems is turning into a major hurdle. In this blog, we\u2019ll explore how we\u2019ve reached this complex juncture and examine the solutions available today, with a special focus on federated queries. This approach promises to minimize data movement and streamline our data infrastructure. We\u2019ll delve into a practical example, demonstrating how emerging technologies like DuckDB can be instrumental in this context.\n\n## A growing ecosystem of standards\n\nSoftware engineering has been producing regularly new data storage format, databases or data system. There\u2019s now so many kinds of data sources that a good chunk of data engineers is about plugging sources to sinks.\n\n![Alt text for the image](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fstandards_f761c1393c.png&w=3840&q=75)_XKCD comics about standards_\n\nFor instance, most data platforms will have to handle:\n\n- **Structured Data**: This type of data is highly organized and formatted. Examples include data stored in SQL databases, spreadsheets, or CSV files.\n- **Semi-structured Data**: It doesn\u2019t have a strict structure like structured data but contains some level of organization. Examples include JSON, XML, log files, and NoSQL databases.\n- **Unstructured Data**: This data doesn\u2019t have a predefined structure and doesn\u2019t fit neatly into tables. Examples include text data, images, videos, audio files, social media posts, and documents.\n\nThe primary focus of data engineers revolves around connecting those diverse data sources to generate valuable datasets that fuel algorithms, services, or dashboards.\n\n## Standard approaches\n\n### A plumber job challenge\n\nI\u2019ve developed multiple customized jobs aimed at transferring data seamlessly across various platforms, such as:\n\n- Moving data between MySQL and BigQuery\n- Integrating Kafka with BigQuery\n- Synchronizing data between S3 and BigQuery\n- And more\u2026\n\nEnhancing these jobs with new features and ensuring they offer broad support requires significant effort. However, the real challenge lies in the ongoing maintenance, which demands extensive time investment due to several factors:\n\n- Numerous dependencies, potentially conflicting\n- Evolving APIs of languages and frameworks\n- Managing the deployment and runtime environments of those jobs\n\nThese needs are not unique to just a few companies; they have become increasingly demanding within the continuously maturing data engineering ecosystem. Consequently, some engineers have taken the initiative to develop frameworks and SaaS solutions to address these challenges, saving us invaluable hours of labor.\n\nThese data integration systems excel in extracting data and effectively mapping data types to their respective destinations.\n\n### Data integration systems\n\nIn recent years, there has been a notable emergence of tools and platforms designed to facilitate the interoperability of data sources. Among these, certain managed data integration platforms, such as Fivetran, have stood out by offering an extensive array of integrations spanning databases, file formats, and APIs. They streamline the process of ingesting data from CRM platforms like Salesforce, eliminating the requirement for programming expertise.\n\nHowever, these platforms have their limitations, employing a generic approach that may not cater to every user\u2019s specific requirements. This becomes evident when there\u2019s a need to access certain options, APIs, or authentication patterns that aren\u2019t supported by these platforms. Whether it\u2019s due to the necessity for customization, concerns about privacy, or cost considerations, open-source software (OSS) alternatives like Meltano, Airbyte, or dlt have emerged as viable solutions.\n\n### Replication freshness\n\nLet\u2019s consider a scenario where we\u2019re enhancing an analytics UI for a large-scale ecommerce corporation, displaying product lists alongside metrics like page views and the quantity of products added to carts. When our operations team introduces a new item for sale and accesses the UI, they naturally expect to see the latest addition. However, if our data replication occurs only once every hour, there\u2019s a likelihood that the newly added item might not immediately appear. In such cases, implementing a \u201clast updated at\u201d warning becomes necessary to communicate that some items might not be visible. One potential workaround involves creating two separate views to ensure the visibility and update of newly created items.\n\nTraditional data integration systems are typically not optimized for real-time replication. To address the latency in replication, there are real-time solutions available, such as change data capture platforms like Debezium. These platforms enable streaming data from databases to systems like Kafka, which then manages the task of materializing the current state of data in your data lake. This approach works seamlessly when integrated with platforms like Iceberg, which supports time travel features. However, setting up these solutions can be quite labor-intensive, especially if opting against managed solutions like Tabular.\n\nAlternatively, managed solutions like Datastream exist, offering data replication onto platforms such as BigQuery. Yet, these solutions come with their own limitations, such as restricted support for schema changes.\n\n### Full database replication\n\nIf dealing with a vast database, you might want to extract only a portion of the rows, as replicating the entire dataset demands considerable time, computational resources, and storage that could be conserved.\n\nConsider a scenario where you\u2019re managing a multi-tenant database and need to synchronize only select segments of it. However, depending on how you\u2019ve implemented the segmentation (whether at the database, table, or row level), achieving the desired filtering might be challenging due to constraints within the data integration platform. Furthermore, these tools lack a universal method to apply filters, and customizing filters for different connectors becomes necessary.\n\n## Enter Federated Queries\n\nFederated queries present a robust resolution to the integration challenge. Fundamentally, they facilitate effortless retrieval and manipulation of data from diverse sources, enabling applications to gather insights from databases, file systems, APIs, and beyond. This unified perspective eliminates the necessity for intricate ETL procedures or data migrations. Achieving such queries often involves the utilization of addons or extensions known as\u00a0**Foreign Data Wrappers**.\n\n## Foreign data wrappers ecosystem\n\nForeign data wrappers have a longstanding history in the tech landscape, with examples such as mysql\\_fdw (Postgres\u2019 MySQL foreign data wrapper) dating back to 2011. Various databases like Postgres and query engines such as Trino have adopted connectors for external tables, yet the level of integration across platforms can significantly differ. Depending on the target, the capabilities for pushdown operations can vary widely. For instance, employing a foreign data wrapper around an RDBMS like MySQL often brings features such as:\n\n- Column pushdown\n- Predicate pushdown\n- Join pushdown\n- Aggregate pushdown\n- Limit offset pushdown\n\nPostgres\u2019 MySQL FDW already encompasses all these pushdown techniques. However, when dealing with file-based access like JSON, the engine handling the data source must manage the actual data operations. In such cases, the engine takes on the majority of the workload, emphasizing efficiency, especially when constructing latency-sensitive applications.\n\n## What about DuckDB?\n\nDuckDB stands out in its capacity: its drivers open up an in-process OLAP query engine, equipped with an advanced SQL language, compatible with a wide array of applications. Moreover, DuckDB provides the capability to craft potent extensions, empowering developers to link various data sources using high-performance languages such as C++ or Rust. Though creating those connectors require some effort, the end users can enjoy a natural developer experience on the SQL end.\n\nMany of these extensions, fostered by DuckDB Labs and its community, function as foreign data wrappers tailored for DuckDB. Examples include those designed for Postgres, MySQL, or Athena. While some are in their early stages and may not yet fully support pushdowns, the development of advanced features is actively underway.\n\nWhat distinguishes DuckDB from larger platforms like Trino or Clickhouse? DuckDB excels with small and medium-sized datasets due to its single-machine architecture and in-process methodology, drastically reducing response times. Adding to this advantage is its effortless setup process: simply integrate the DuckDB driver into your application and seamlessly connect databases using SQL, treating them as if they were native.\n\n### A quick example\n\nLet\u2019s demonstrate the previously quoted example in action. Suppose the product data resides in a MySQL database, while the analytics data is stored as a DuckDB file on S3. Firstly, let\u2019s load the extensions and connect to the databases. The procedure would resemble the following SQL commands:\n\n```ini\nCopy code\n\nINSTALL mysql_scanner;\nINSTALL httpfs;\n\nLOAD mysql_scanner;\nLOAD httpfs;\n\nCALL load_aws_credentials();\n\nATTACH 'host=127.0.0.1 user=root port=3306 database=product_db' AS product_db (TYPE MYSQL_SCANNER);\n\nATTACH 's3://<bucket>/product_stats.db' (READ_ONLY);\n```\n\nAs you can observe, once the connections are established and initialized with the database attachments, we can retrieve the actual data seamlessly, as if the data were co-located:\n\n```sql\nCopy code\n\nSELECT product.id, product.name, product_stats.views_count, product_stats.in_basket_count\nFROM product_db.product\nJOIN product_stats.product_stats ON product.id = product_stats.product_id\nWHERE product.name LIKE \"%duck%\"\nLIMIT 100 OFFSET 0\n```\n\nWith this approach in place, the developer\u2019s journey becomes significantly smoother when tasked with implementing a product that necessitates filtering, pagination, and sorting functionalities.\n\n### An experiment\n\nIn a recent endeavor, I brought an idea to life by constructing a proof of concept on two MySQL servers, mirroring the previous approach. The steps were as follows:\n\n- I initiated a connection pool from a Scala application to DuckDB, laying the groundwork for the database attachments.\n- I crafted a query to unify two tables, each residing in a separate database.\n- I executed the query, parsed the resulting data, and returned the content.\n\nThe response time clocked in at approximately five seconds. While this isn\u2019t overly lengthy, it\u2019s worth noting that bypassing DuckDB and opting for requests and in-memory joins could potentially trim this down to a brisk 200 milliseconds, given that each query takes about 70 milliseconds on a standalone SQL client.\n\nYou might be curious about the factors contributing to this duration. Here are a few insights:\n\n- To push down predicates, the extension fetches the table schema information prior to constructing the actual MySQL query. Although this information is cached post the initial request, failing to run a pre-cache request for table schemas could tack on an extra 2\u20133 seconds to your response time.\n- All requests are encapsulated in a transaction, which could introduce unnecessary overhead.\n- Depending on the nature of the request, the absence of a connection pool might lead to sequential database queries, thereby slowing down the process.\n- Lastly, I observed that executing the full request, once the schema was cached, took around 2.5 seconds (as measured by the time command in bash), while the profiling details reported a response time of approximately 1.5 seconds on DuckDB.\n\nThere\u2019s ample scope for enhancement, but it\u2019s crucial to remember that we\u2019re still navigating the nascent stages of the DuckDB extensions ecosystem.\n\n## Going further\n\nAs I\u2019ve been architecting solutions across diverse data scopes, the concept of abstracting query federation has been a recurring idea. In a large organization that values team autonomy, it\u2019s not uncommon to encounter numerous databases when building a cross-functional feature. There are several patterns to simplify this complexity, with semantic layers often being the most effective for maintaining consistent definitions. However, there are scenarios where semantic layers may not be the ideal choice. For instance, your database or some of its features may not be supported, or the time and cost associated with semantic layers may not be feasible.\n\nIn such cases, employing views, particularly DuckDB views, can be a powerful alternative. Here\u2019s why:\n\n- Views allow you to encapsulate actual data source accesses, leveraging the robust SQL features of DuckDB.\n- View definitions can be stored within the DuckDB database format, making it convenient to share across applications that need access to these definitions.\n- The flexibility of views allows you to interchange the actual data sources behind the definitions. This is because the references are tied to the database alias used during attachment. This means you can maintain the same definitions whether you\u2019re referencing an online database like Postgres or its table dumps in Parquet. This can be particularly useful when building unit tests on your view logic, as you can simply use a different offline source to keep your test stack and fixtures straightforward.\n- The versatility of views extends to creating views from other views. This can be beneficial when you want to layer abstractions and allow teams and projects to have their own isolated DuckDB view definitions. You can then consolidate these by attaching each of these DuckDB view definitions once again, or even merge them by copying them into your own.\n\n## Limitations\n\nThe potential of DuckDB as a federated query layer is immense, but the extensions, such as\u00a0**duckdb\\_mysql**, need to enhance their support for advanced pushdowns to truly excel. For example, the current filter pushdown is rather rudimentary and only works with a single value, not a list. I\u2019ve been\u00a0[exploring ways to bolster support for more pushdown features](https://github.com/duckdb/duckdb_mysql/pull/10). Additionally, as previously discussed, eager fetching of schemas could be beneficial to mitigate the cold start effect. In pursuit of this, I\u2019ve been\u00a0[probing the addition of a specific function](https://github.com/duckdb/duckdb_mysql/pull/15)\u00a0to facilitate this. There\u2019s undoubtedly more ground to cover, so if you\u2019re intrigued and want to contribute to these developments, your input would be most welcome!\n\n## The silver bullet?\n\nDuckDB boasts numerous impressive use cases, and data source federation stands out among them. However, is it the ultimate solution for all scenarios? Let\u2019s delve into situations where it fits perfectly and where it might not be the most suitable choice.\n\nWhen to consider using DuckDB for data source federation:\n\n- Building APIs that rely on multiple data sources while aiming for a responsive latency (< 1s).\n- Conducting exploration or troubleshooting that necessitates quick correlation across various data sources.\n- Creating small to medium-sized data rollups that merge fact and dimensional data from diverse sources, eliminating the need for replication concerns.\n\nWhen it might not be the best choice:\n\n- Handling joins with exceptionally large data volumes (e.g., > 1 TB of data, a scenario where DuckDB might not have been thoroughly stress-tested on a very large VM).\n- Requiring advanced pushdowns/features on foreign data wrappers that are still in an immature stage (e.g., Iceberg integration).\n- Needing access to a data source for which no ongoing development is underway and lacking the capacity or expertise to create it.\n- Operating on a specific setup that DuckDB (or its extensions) does not support or isn\u2019t optimized for. For instance, some extensions are not built to run on some linux ARM versions.\n- Demanding extremely low latency (i.e., < 100ms).\n- Expecting a high volume of simultaneous client requests performing similar queries concurrently.\n\n## Conclusion\n\nFederated queries offer an excellent solution for managing diverse data sources, and I strongly believe that DuckDB will become increasingly accessible and significant in the coming months. However, it\u2019s crucial to clearly define your use cases, as this approach may occasionally prove counterproductive. Nonetheless, when it aligns with your needs, DuckDB offers a multitude of advantages: enhanced performance, advanced SQL functionalities, and convenient methods for testing logic using mock data. Whether opting for DuckDB or another platform, witnessing data infrastructure tools expand their support by incorporating more data sources or refining pushdown logics is a gratifying development. Hence, it\u2019s worth considering for your upcoming data engineering projects due to its practicality.\n\nFor those intrigued by DuckDB, exploring [MotherDuck](https://motherduck.com/) as a SaaS platform to test and manage the runtime could be beneficial! Although the team [plans to introduce additional extensions in the future](https://motherduck.com/docs/architecture-and-capabilities/#considerations-and-limitations), you can already gain insight into DuckDB\u2019s capabilities by utilizing sources like Parquet or CSV.\n\n### TABLE OF CONTENTS\n\n[A growing ecosystem of standards](https://motherduck.com/blog/duckdb-the-great-federator/#a-growing-ecosystem-of-standards)\n\n[Standard approaches](https://motherduck.com/blog/duckdb-the-great-federator/#standard-approaches)\n\n[Enter Federated Queries](https://motherduck.com/blog/duckdb-the-great-federator/#enter-federated-queries)\n\n[Foreign data wrappers ecosystem](https://motherduck.com/blog/duckdb-the-great-federator/#foreign-data-wrappers-ecosystem)\n\n[What about DuckDB?](https://motherduck.com/blog/duckdb-the-great-federator/#what-about-duckdb)\n\n[Going further](https://motherduck.com/blog/duckdb-the-great-federator/#going-further)\n\n[Limitations](https://motherduck.com/blog/duckdb-the-great-federator/#limitations)\n\n[The silver bullet?](https://motherduck.com/blog/duckdb-the-great-federator/#the-silver-bullet)\n\n[Conclusion](https://motherduck.com/blog/duckdb-the-great-federator/#conclusion)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![Introducing FixIt: an unreasonably effective AI error fixer for SQL](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Ffixit_social_47f52f8fb0.png&w=3840&q=75)](https://motherduck.com/blog/introducing-fixit-ai-sql-error-fixer/)\n\n[2024/01/03 - Till D\u00f6hmen, Hamilton Ulmer](https://motherduck.com/blog/introducing-fixit-ai-sql-error-fixer/)\n\n### [Introducing FixIt: an unreasonably effective AI error fixer for SQL](https://motherduck.com/blog/introducing-fixit-ai-sql-error-fixer)\n\nFixIt will correct mistakes in your SQL queries based on the schema and DuckDB syntax. Based on a large language model (LLM).\n\n[![DuckDB Tutorial For Beginners](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgetting_started_duckdb_thumbnail_70b197b1ab.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n[2024/10/31 - Mehdi Ouazza, Ryan Boyd](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n### [DuckDB Tutorial For Beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners)\n\nGet up to speed quickly with DuckDB, including installation, VSCode workflow integration and your first SQL analytics project.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 256,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 256
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now I have enough information to write a comprehensive answer. Let me create the response file:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_7.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "create",
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_7.md",
  [2m  â”‚[0m   "content": "<4042 chars>",
  [2m  â”‚[0m   "structuredPatch": [],
  [2m  â”‚[0m   "originalFile": null
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I've written the answer to `/tmp/answer_without_kg_7.md`. 
  [94mâ”‚[0m 
  [94mâ”‚[0m **Summary**: To join a local CSV with an S3 table in MotherDuck, you use the **Dual Execution** feature. This automatically optimizes the query by:
  [94mâ”‚[0m 1. Scanning and filtering your local CSV on your laptop
  [94mâ”‚[0m 2. Sending only the minimal necessary data to the cloud
  [94mâ”‚[0m 3. Performing the join with the S3 table in the MotherDuck cloud
  [94mâ”‚[0m 
  [94mâ”‚[0m You simply write a single SQL query joining the local file path with the cloud table, and MotherDuck's query planner handles all the optimization automatically - no manual ETL or data movement required.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 3,762 | Cost: $0.5742
     Cumulative: 3,762 tokens | $0.5742
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent is providing information and summarizing the process rather than asking for further input or clarification.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 14 | Duration: 85.0s
â•‘    Tokens: 3,762 | Cost: $0.5742
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  Running LLM judge evaluation...
   ğŸ§  LLM Judge score: 0.81
