# How to Migrate Data from a Local DuckDB Database to MotherDuck

There are several methods to migrate data from a local DuckDB database to MotherDuck, ranging from simple one-command uploads to more granular table-by-table approaches.

## Method 1: Upload Entire Local Database (Simplest)

The easiest way to migrate your local DuckDB database to MotherDuck is using a single command:

```sql
CREATE DATABASE remote_database_name FROM CURRENT_DATABASE();

-- or more generally
CREATE DATABASE remote_database_name FROM '<local database name>';
```

**Important considerations:**
- The local and remote database names **must be different**, otherwise you'll get an error: `Catalog Error: error while importing share: Schema with name <local-database-name> does not exist!`
- For large databases, this can take considerable time (e.g., ~1 hour for an 11GB database, transferring 15GB of data)
- This method uploads all tables and data from your local database to MotherDuck

## Method 2: Connect and Attach Local Database

First, connect to MotherDuck by attaching it to your local DuckDB session:

```sql
-- From DuckDB CLI
.open md:

-- Or attach MotherDuck
ATTACH 'md:';
```

When you first connect, you'll be prompted to authenticate via SSO. The system will provide a motherduck_token that you should save as an environment variable:

```bash
export motherduck_token='eyJhbGciOiJI..._Jfo'
```

## Method 3: Create Database and Copy Tables from Files

If you have your data exported to files (like Parquet or CSV), you can create tables directly in MotherDuck from those files:

```sql
-- Create the database in MotherDuck
CREATE DATABASE so;

-- Create tables from Parquet files on S3
CREATE TABLE users AS
FROM 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/users.parquet';

CREATE TABLE posts AS
FROM 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/posts.parquet';

-- Or from local files
CREATE TABLE population AS
FROM 'local_file.parquet';

-- Repeat for other tables
```

This approach works well when:
- You have data already exported to cloud storage (S3, GCS, etc.)
- You want to selectively migrate specific tables
- You want to transform data during migration

## Method 4: Copy Data Table-by-Table Using ATTACH

You can also use the standard SQL `CREATE TABLE AS SELECT` (CTAS) pattern to copy tables:

```sql
-- Attach your local DuckDB database
ATTACH 'my_local_data.duckdb' AS local_db;

-- Connect to MotherDuck
ATTACH 'md:' AS motherduck;

-- Create database in MotherDuck (if needed)
CREATE DATABASE my_remote_db;
USE my_remote_db;

-- Copy tables from local to MotherDuck
CREATE TABLE population AS SELECT * FROM local_db.population;
CREATE TABLE sales AS SELECT * FROM local_db.sales;
```

## Understanding DuckDB's Dual Execution Model

MotherDuck uses a "dual execution" model that intelligently decides whether to run queries locally or in the cloud:

- When both tables reside in MotherDuck, queries run entirely in the cloud
- When joining local files with cloud tables, DuckDB processes data where it makes most sense
- The system minimizes data movement by transferring only necessary intermediate data
- You can manually control execution location using the `md_run` parameter:

```sql
-- Force local execution
SELECT * FROM read_parquet('s3://bucket/file.parquet', md_run='local');

-- Force remote execution in MotherDuck
SELECT * FROM read_parquet('s3://bucket/file.parquet', md_run='remote');
```

**Performance example:** Querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to local execution that had to download the data first (36 seconds).

## Data Persistence Options

DuckDB offers two modes:
1. **In-memory** (default): Data disappears when session ends - use `duckdb.connect(database=':memory:')`
2. **Persistent**: Data saved to a database file

To create a persistent local database before uploading:

```bash
# Start DuckDB with a database file
duckdb my_local_data.duckdb

# Or from Python
import duckdb
con = duckdb.connect(database='my_local_data.duckdb')
```

Or attach an existing database if DuckDB is already running:

```sql
ATTACH DATABASE '/path/to/your/database.db' AS mydb;
```

## Python Example

Here's a complete Python example for migrating data using the relational API:

```python
import duckdb

# Connect to persistent local database
con = duckdb.connect(database='my_local_data.duckdb')

# Load data into local database
population_relation = con.read_csv("https://bit.ly/3KoiZR0")
population_relation.to_table("population")  # Persist as table

# Connect to MotherDuck
con.sql("ATTACH 'md:my_cloud_db'")
con.sql("USE my_cloud_db")

# Copy table to MotherDuck
con.sql("CREATE TABLE population AS SELECT * FROM my_local_data.population")
```

## Best Practices

1. **For small to medium databases**: Use Method 1 (single command upload) for simplicity
2. **For large databases**: Consider exporting to Parquet files on cloud storage first, then use Method 3 to leverage MotherDuck's high-bandwidth cloud connections
3. **For selective migration**: Use Method 4 to copy only specific tables
4. **Set environment variable**: Always export your `motherduck_token` to avoid repeated authentication
5. **Test connection first**: Verify you can connect to MotherDuck before starting large migrations
6. **Monitor progress**: Large uploads can take time; plan accordingly
7. **Leverage cloud proximity**: MotherDuck's cloud infrastructure provides high-bandwidth connections to cloud storage, making it faster to load from S3/GCS than from local files

## Working with Extensions

If you need to read from cloud storage, make sure to install and load the necessary extensions:

```sql
-- Install httpfs extension for S3 access
INSTALL httpfs;
LOAD httpfs;

-- Configure for public S3 bucket
SET s3_region='us-east-1';

-- For private buckets, use CREATE SECRET
CREATE SECRET (
    TYPE S3,
    PROVIDER credential_chain
);
```

## Sources

- [DuckDB & MotherDuck for Beginners: Your Ultimate Guide](https://motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide) - Comprehensive guide covering DuckDB basics and MotherDuck integration, including how to connect with `ATTACH 'md:'`, data persistence with `.to_table()`, and the dual execution model
- [Exploring StackOverflow with DuckDB on MotherDuck (Part 2)](https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2) - Tutorial demonstrating the `CREATE DATABASE FROM CURRENT_DATABASE()` method and table creation from Parquet files on S3, with real-world timing examples
- [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained) - Deep dive into MotherDuck's architecture and dual execution model for hybrid local/cloud queries, with performance comparisons and manual execution control
- [DuckDB Tutorial For Beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners) - Covers DuckDB fundamentals including database persistence with `ATTACH DATABASE`, working with various file formats, and extensions for cloud storage access
- [DuckDB Python Quickstart (Part 1)](https://motherduck.com/learn-more/duckdb-python-quickstart-part1) - Details on connecting to persistent databases, the `to_table()` method for converting relations to tables, reading files with `read_csv()` and `read_parquet()`, and MotherDuck integration via Python
