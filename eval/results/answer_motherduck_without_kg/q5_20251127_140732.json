{
  "question": "How do I set up MotherDuck to work with dbt?",
  "answer": "# How to Set Up MotherDuck to Work with dbt\n\nMotherDuck integrates seamlessly with dbt (data build tool) through the `dbt-duckdb` adapter, allowing you to perform data transformations directly within MotherDuck's cloud data warehouse. Here's a comprehensive guide to getting started.\n\n## Prerequisites\n\nBefore you begin, you'll need:\n- A MotherDuck account (sign up at [motherduck.com](https://app.motherduck.com/?auth_flow=signup))\n- Python 3.x installed\n- Basic familiarity with dbt and SQL\n\n## Installation\n\n### 1. Install the dbt-duckdb Adapter\n\nInstall the dbt-duckdb package with MotherDuck support:\n\n```bash\npip install dbt-duckdb[md]\n# or with Poetry\npoetry add dbt-duckdb[md]\n```\n\nThe `[md]` extra installs the necessary dependencies for MotherDuck integration.\n\n### 2. Get Your MotherDuck Service Token\n\n1. Sign in to your MotherDuck account\n2. Click on **Settings** in the upper right corner\n3. Copy your **Service Token** to your clipboard\n4. Export it as an environment variable:\n\n```bash\nexport motherduck_token=<your_motherduck_token>\n```\n\n## Configuration\n\n### 3. Initialize Your dbt Project\n\nIf you don't already have a dbt project:\n\n```bash\ndbt init your_project_name\n```\n\n### 4. Configure profiles.yml\n\nCreate or update your `profiles.yml` file with MotherDuck connection details. The key difference between local DuckDB and MotherDuck is the `path` parameter:\n\n**Basic Configuration:**\n\n```yaml\nyour_project_name:\n  outputs:\n    dev:\n      type: duckdb\n      path: /tmp/dbt.duckdb  # Local development\n      threads: 16\n\n    prod:\n      type: duckdb\n      path: md:your_database_name  # MotherDuck cloud\n      threads: 16\n\n  target: dev\n```\n\n**Advanced Configuration with AWS S3:**\n\nIf you're working with data in S3, you'll need additional configuration:\n\n```yaml\nyour_project_name:\n  target: dev\n  outputs:\n    dev:\n      type: duckdb\n      schema: dev_schema\n      path: 'md:your_database_name'\n      threads: 16\n      extensions:\n        - httpfs\n      settings:\n        s3_region: \"{{ env_var('S3_REGION', 'us-west-1') }}\"\n        s3_access_key_id: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n        s3_secret_access_key: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n\n    prod:\n      type: duckdb\n      schema: prod_schema\n      path: 'md:your_database_name'\n      threads: 16\n      extensions:\n        - httpfs\n      settings:\n        s3_region: us-west-1\n        s3_access_key_id: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n        s3_secret_access_key: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n```\n\n### 5. Set Up Environment Variables\n\nFor AWS S3 integration, export your credentials:\n\n```bash\nexport motherduck_token=<your_motherduck_token>\nexport S3_REGION=<your_region>\nexport S3_ACCESS_KEY_ID=<your_access_key_id>\nexport S3_SECRET_ACCESS_KEY=<your_secret_access_key>\n```\n\n## Verify Connection\n\n### 6. Test Your Connection\n\nRun `dbt debug` to verify that dbt can connect to MotherDuck:\n\n```bash\ndbt debug\n```\n\nYou should see confirmation that the connection is successful.\n\n### 7. Run Your First Model\n\nCreate a simple dbt model or run an existing one:\n\n```bash\ndbt run\n# or for a specific target\ndbt run --target prod\n```\n\n## Key Features and Benefits\n\n### Local Development, Cloud Production\n\nSince both dbt and DuckDB can run locally, you can:\n- Develop and test using the same technologies you run in production\n- Run models locally for fast iteration\n- Seamlessly transition to MotherDuck for production workloads\n\n### Dynamic Execution\n\nMotherDuck intelligently runs queries either in the cloud or locally based on what's most efficient, providing \"dynamic execution out of the box.\"\n\n### Working with S3 Data\n\nMotherDuck can directly query data from S3 without copying it:\n\n```sql\n-- Reference S3 data directly in your dbt models\nSELECT * FROM 's3://your-bucket/path/to/data.csv'\n```\n\nOr use MotherDuck's public datasets:\n\n```sql\nSELECT * FROM 's3://us-prd-motherduck-open-datasets/jaffle_shop/csv/raw_customers.csv'\n```\n\n### Incremental Models\n\nConfigure incremental materializations in your `dbt_project.yml`:\n\n```yaml\nmodels:\n  your_project:\n    your_model:\n      +materialized: \"{{ 'incremental' if target.name == 'prod' else 'table' }}\"\n      +unique_key: load_id\n```\n\n## Best Practices\n\n1. **Use Environment Variables**: Keep credentials secure by using environment variables rather than hardcoding them\n2. **Separate Dev and Prod**: Use different targets for local development and production deployments\n3. **Enable httpfs Extension**: This DuckDB extension is essential for working with S3 and remote files\n4. **Fast Iteration**: Take advantage of local DuckDB for rapid development before deploying to MotherDuck\n5. **Unit Testing**: Since DuckDB runs in-process, you can write genuine unit tests without cloud dependencies\n\n## Troubleshooting\n\n- **Authentication Issues**: Ensure your `motherduck_token` environment variable is set correctly\n- **S3 Access**: Verify your AWS credentials are properly configured if working with S3 data\n- **Version Compatibility**: Make sure your DuckDB version matches what MotherDuck supports (check MotherDuck documentation for the current supported version)\n\n## Additional Resources\n\nFor more advanced patterns and examples:\n- Check out the [jaffle_shop_duckdb](https://github.com/sungchun12/jaffle_shop_duckdb) demo repository\n- See the [stocks demo](https://github.com/matsonj/stocks) for performance-optimized pipelines\n- Review the [pypi-duck-flow](https://github.com/mehd-io/pypi-duck-flow) end-to-end project",
  "answer_file": "/tmp/answer_without_kg_5.md",
  "token_usage": {
    "total_tokens": 0.0,
    "duration_seconds": 107.31257510185242
  },
  "cached_response": true,
  "llm_judge": {
    "overall_score": 1.0,
    "component_scores": {
      "accuracy": 1.0,
      "completeness": 1.0,
      "relevance": 1.0,
      "clarity": 1.0
    },
    "feedback": "The generated answer is highly accurate, complete, relevant, and clear. It not only addresses the question directly but also expands on the canonical answer by providing additional context, examples, and best practices, making it a valuable resource for users looking to set up MotherDuck with dbt."
  },
  "command": "question:q5",
  "returncode": 0
}