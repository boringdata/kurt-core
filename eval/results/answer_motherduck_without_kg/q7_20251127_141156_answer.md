# How to Join a Local CSV with an S3 Table Using MotherDuck

If you have a CSV file on your laptop and a table stored in S3, the most efficient way to join them using MotherDuck is to leverage **Dual Execution** â€” MotherDuck's innovative hybrid query execution model that intelligently minimizes data movement.

## The Most Efficient Approach

Simply write a standard SQL join query that references both your local CSV and your S3 data:

```sql
SELECT
    local.customer_id,
    local.recent_purchase,
    cloud.historical_data
FROM read_csv('local_sales.csv') AS local
JOIN read_parquet('s3://my-bucket/customer_history.parquet') AS cloud
    ON local.customer_id = cloud.customer_id
WHERE local.purchase_date >= '2025-01-01';
```

## How Dual Execution Makes This Efficient

MotherDuck's **Dual Execution** model is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on your laptop or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.

When you run a hybrid query joining local and cloud data, the planner automatically:

1. **Local Scan & Filter**: Scans your local CSV on your laptop and applies any filters there
2. **Minimal Transfer**: Sends only the filtered results (potentially just a few kilobytes) to the cloud
3. **Cloud Processing**: Scans the S3 table in the MotherDuck cloud where the data already lives
4. **Optimized Join**: Performs the final join in the cloud

### Example Optimization in Action

Imagine you want to join a local CSV of new product pricing with a massive sales table in MotherDuck, filtered for a specific product. Instead of moving entire tables, the Dual Execution planner:

1. **Local Scan**: Scans the small pricing CSV on your laptop
2. **Filter Locally**: Applies the filter for the specific product on your machine
3. **Transfer Minimal Data**: Sends only the single, filtered pricing row (a few bytes of data) to the cloud
4. **Join in the Cloud**: Performs the final join against the massive sales table in the MotherDuck cloud

This process can reduce network traffic by **orders of magnitude** compared to traditional methods, turning a difficult query into an interactive one.

## Key Benefits

- **No ETL Required**: No need to upload your entire CSV to S3 or download the entire S3 table locally
- **Minimizes Data Movement**: Only the minimum necessary data crosses the network boundary
- **Single Query**: Write standard SQL as if all data exists in one unified environment
- **Cost Efficient**: Leverages your laptop's free compute resources and minimizes cloud data transfer costs
- **Faster Development**: Zero-latency feedback for local development before scaling to cloud
- **Better Collaboration**: Transforms DuckDB from "single-player" to "multiplayer" platform

## Setup: Connecting to MotherDuck

To enable hybrid execution, connect your local DuckDB instance to MotherDuck:

```sql
ATTACH 'md:my_database';
```

After this simple `ATTACH` command, any query can seamlessly mix local files and cloud data. MotherDuck uses the exact same DuckDB engine both locally and in the cloud, ensuring that a query validated locally is guaranteed to behave identically in production.

## Verifying the Execution Plan

You can use `EXPLAIN` to see which parts of your query run locally versus remotely:

```sql
EXPLAIN
SELECT *
FROM 'local_data.csv' AS local
JOIN read_parquet('s3://bucket/table.parquet') AS cloud
    ON local.id = cloud.id;
```

The explain plan will show:
- Operations marked `(L)` run locally on your laptop
- Operations marked `(R)` run remotely in MotherDuck cloud

This transparency helps you understand and optimize the execution strategy.

## Manual Control (Optional)

While the automatic optimization usually makes the best choice, you can manually control execution location using the `md_run` parameter:

```sql
-- Force S3 scan to run locally (downloads data first)
FROM read_parquet('s3://bucket/file.parquet', md_run = 'local')

-- Force S3 scan to run remotely in MotherDuck cloud
FROM read_parquet('s3://bucket/file.parquet', md_run = 'remote')
```

**Performance comparison**: In testing, querying a 2GB Parquet file on S3 remotely in MotherDuck (25 seconds) was significantly faster than downloading and processing locally (36 seconds). By pushing computation to where the data lives, MotherDuck minimizes network I/O and delivers results more quickly.

## Advanced CSV Handling

MotherDuck's CSV reader automatically detects structure (headers, delimiters, data types), but you can override for messy files:

```sql
-- Let MotherDuck auto-detect (recommended)
FROM 'local_sales.csv'

-- Or specify details for problematic CSVs
FROM read_csv('local_sales.csv',
    delim = '|',                    -- Custom delimiter
    header = true,                  -- Force header presence
    dateformat = '%m/%d/%Y',        -- Custom date format
    ignore_errors = true            -- Skip problematic rows
)
```

For more advanced CSV techniques, see the "Taming Wild CSVs" guide in the sources below.

## Querying Multiple Files in S3

You can also query entire folders of files in S3 as a single table using glob patterns:

```sql
SELECT
    event_type,
    COUNT(*) AS event_count
FROM read_parquet('s3://my-bucket/logs/2025/**/*.parquet')
GROUP BY event_type;
```

This treats all matching Parquet files as one unified dataset, with intelligent filter pushdown to minimize data scanned.

## Why Data Engineers Are Excited

Data engineers love dual-execution engines because they solve the "it worked on my machine" problem. Since MotherDuck uses the exact same DuckDB engine locally and in the cloud:

- **Faster Development**: Build and test pipelines with zero-latency local feedback before scaling to cloud
- **Lower Costs**: Use free laptop compute and minimize data transfer
- **Consistency Guaranteed**: A query that works on your laptop is guaranteed to work in the cloud
- **Smooth Scale-Up**: Start local-first and seamlessly push more workload to cloud as needs grow

## Real-World Performance

Companies have achieved dramatic improvements with this approach:
- **Finqore**: Reduced 8-hour data pipelines to 8 minutes (60x improvement)
- **Gardyn**: Cut pipeline time from 24+ hours to under 1 hour
- **UDisc**: Reduced dbt jobs from 6 hours to 30 minutes, with queries dropping from minutes to 5 seconds

## Sources

- [Hybrid Analytics: Query Local & Cloud Data Instantly](https://motherduck.com/learn-more/hybrid-analytics-guide/) - Comprehensive guide on joining local CSVs with cloud data, including detailed explanation of Dual Execution
- [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained/) - Technical deep dive into MotherDuck's dual execution architecture and query optimization from MotherDuck founding engineer
- [The No-ETL Playbook: How to Query Raw CSV & JSON Files Directly with SQL](https://motherduck.com/learn-more/no-etl-query-raw-files/) - Guide on querying raw files in S3 and joining local files with cloud data
- [Taming Wild CSVs: Advanced DuckDB Techniques for Data Engineers](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/) - Advanced CSV handling techniques including error handling and schema detection
- [S3 bucket - DuckDB Data Engineering Glossary](https://motherduck.com/glossary/S3 bucket/) - How to query data stored in S3 buckets with DuckDB syntax
- [Querying Data From S3 With 3 Lines In Your Terminal](https://motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal/) - Quick demonstration of S3 querying
