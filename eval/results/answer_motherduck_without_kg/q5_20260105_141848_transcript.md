# Scenario: answer_motherduck_without_kg

**Status**: âœ… PASSED
**Timestamp**: 20260105_141848

**Duration**: 413.79 seconds
**Tokens Used**: 0

---

## Conversation


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ SCENARIO: answer_motherduck_without_kg
   Answer MotherDuck questions by searching sources directly (no KG)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


ğŸ” Verifying .claude installation...
   âœ“ .claude folder exists at /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.claude
   âœ“ Skills: 0 found
   âœ“ Commands: 0 found

ğŸ§ª Running 10 of 10 question(s) defined in questions_motherduck.yaml


======================================================================
â“ Question 1/10
...... What file formats are most efficient for loading data into MotherDuck?
======================================================================


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 1
â”‚ Answer this question by searching through the .kurt/sources/ directory:

...... What file formats are most efficient for loading data into MotherDuck?

Required Steps:
1. Search for relevant markdown files in .kurt/sources/ using glob and grep
2. Read the most relevant files (aim for 3-5 most relevant sources)
3. As you read each file, keep track of:
   - The file path
   - The file title/name
   - Key information extracted from that file
4. Synthesize a comprehensive answer based on the information found
5. Write your answer to: /tmp/answer_without_kg_1.md

IMPORTANT - Your answer MUST have this structure:
```markdown
# Answer

[Your comprehensive answer here, drawing from the sources you read]

## Sources

List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
[etc.]

DO NOT use URLs or document titles as links. Show the actual file path you read with the Read tool.
```

The Sources section is MANDATORY - you must list every file you read to answer the question.

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [96mğŸ”§ TOOL:[0m Grep â†’ file format.*load|loading.*format|efficient.*format|format.*efficient|parquet|csv|json.*load|load.*data

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/**/*load*.md

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/**/*format*.md

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/**/*import*.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "durationMs": 481,
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/glossary/data load tool (dlt).md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "durationMs": 622,
  [2m  â”‚[0m   "numFiles": 2,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/spatial-data-management-with-duckdb-ft-mattforrest.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/leveraging-duckdb-and-delta-lake-together.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/faster-data-pipelines-development-with-mcp-and-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/going-beyond-the-dataframe.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ibis-one-library-to-query-any-backend.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-vs-pandas-vs-polars-for-python-devs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ducklake-big-data-small-coalesce-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-dbt-accelerating-the-developer-experience-with-local-power.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-python-end-to-end-data-engineering-project-13.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/4-lightning-talks-on-practical-ai-workflows-from-notion-1password-motherduck-evidence.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-duck-in-the-hand-is-worth-two-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-new-paradigm-for-data-visualization-with-just-sql-markdown.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/sql-duckdb-book-form.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/quacking-around-the-web.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-OLAP.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/product-analytics-motherduck-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/web-assembly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-use-cases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/pandas-dataframes-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-struct-nested-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/ducklake-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-lake-vs-data-warehouse-vs-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-warehouse-tco.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/hack-night.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/index.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/acid-transactions-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/big-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/relational database.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/storage layer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/storage.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/time-series.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/auto inference.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data build tool (dbt).md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data engineering.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data load tool (dlt).md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data pipeline.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/database.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/dataset.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/SQL analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Pandas DataFrames.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/S3 bucket.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/DuckDB CLI.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/DuckDB.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/ELT.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/ETL.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/EXPLAIN clause.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Amazon S3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Apache Arrow.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/CLI.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Cloud Storage.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/CSV.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/sqlmesh.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/paradime.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/sling-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/fivetran.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/hevo.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/mage.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/meltano.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/dlt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/airbyte.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/artie.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/cloudquery.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/contact-us/product-expert.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-brief.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-summary-chapter2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-news.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/gardyn.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/layers-multi-tenant-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/reflex-careers-gig-economy-retail.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/community-and-open-source.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/atm-com-analytics-costs-sql-expressibility.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dexibit.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dominik-moritz.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-simple-joys-of-scaling-up.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/unstructured-analysis-tensorlake-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vector-technologies-ai-data-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-everybody-hates-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-keyboard-shortcuts-for-joyful-querying.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streaming-data-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streaming-oracle-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streamlining-ai-agents-duckdb-rag-solutions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/summer-data-engineering-roadmap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/tableau-cloud-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/simplifying-iot-analytics-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/six-reasons-duckdb-slaps.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-recap-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-recap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/solving-advent-code-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/spark-ducklake-getting-started.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/secoda-motherduck-integration-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/semantic-layer-duckdb-tutorial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-faker-duckdb-exploration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/quacktosql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/read-scaling-preview.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/redshift-files-hunt-for-big-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/scaling-duckdb-with-ducklings.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg to motherduck at motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg_duckdb-postgresql-extension-for-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-duckdb-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-mooncake-columnstore.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/postgres-duckdb-options.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/preswald-health-data-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pushing-geo-boundaries-with-motherduck-geobase.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/perf-is-not-enough.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-in-europe.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-kestra-etl-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-open-for-all-with-series-b.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-reflections-sigmod-pods-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-union-orchestration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/just-enough-sql-for-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/local-duckdb-ui-visual-data-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/log-processing-savings-bacalhau-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-column-explorer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-fixit-ai-sql-error-fixer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-motherduck-for-business-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/json-log-analysis-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/google-sheets-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/how-to-extract-analytics-from-bluesky.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/galileo-world-geospatial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/geospatial-for-beginner-duckdb-spatial-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/getting-started-ducklake-table-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/getting-started-gis-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/git-for-data-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-ducks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/from-data-lake-to-lakehouse-duckdb-portable-catalog.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-wasm-in-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/ducklake-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/effortless-etl-unstructured-data-unstructuredio-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/estuary-streaming-cdc-replication.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/fabi-ai-llm-prompt-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-python-e2e-data-engineering-project-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-text2sql-llm.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-the-great-federator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-versus-pandas-versus-polars.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-three.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-two.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-excel-extension.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-puppygraph-graph-model-on-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-one.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-seven.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-ten.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-nine.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eleven.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-four.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-110-hidden-gems.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-cognee-sql-analytics-graph-rag.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineer-highlights-PyConDE-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-essential-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-infrastructure-devops.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineers-answer-10-top-reddit-questions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-nov-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/differential-storage-building-block-for-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/building-data-applications-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/building-motherduck-partner-ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-app-generator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-small-data-sf-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/big-data-is-dead.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/bigquery-to-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-snippet-sets-with-motherduck-sharing-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-mega-giga-instance-sizes-huge-scale.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-series-seed-and-a.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-sqlite-databases-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-x-data-nodejs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-data-in-azure-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-json-data-using-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/MotherDuck-Visualize-Embeddings-Marimo.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/sheila-sitaram.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/ryan-boyd.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/jacob-matson.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/luciano-galvao-filho.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/mehdi-ouazza.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/adithya-krishnan.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/andrew-witten.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 287
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/getting-started-ducklake-table-format.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "durationMs": 911,
  [2m  â”‚[0m   "numFiles": 5,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search for more specific content about data loading and file formats:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ parquet|csv|json|iceberg|delta|efficient.*load|performance.*load

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     "content": "---\ntitle: why-choose-parquet-table-file-format\ncontent_type: tutorial\nsource_url: https://motherduck.com/learn-more/why-choose-parquet-table-file-format\nindexed_at: '2025-11-25T09:56:57.618325'\ncontent_hash: 2fb6399ae13aa7e1\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO LEARN](https://motherduck.com/learn-more/)\n\n# Parquet File Format: What It Is, Benefits, and Alternatives\n\n11 min readBY\n\n[Aditya Somani](https://motherduck.com/authors/aditya-aomani/)\n\n![Parquet File Format: What It Is, Benefits, and Alternatives](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdataframe_3_3e66a00165.png&w=3840&q=75)\n\nIn data engineering, efficient storage and processing of massive datasets is often very important. As data volumes continue to grow, organizations are turning to innovative file formats and table structures to optimize their data warehousing and analytics workflows. Among these, the Apache Parquet file format has emerged as a popular choice for its columnar storage layout and impressive compression capabilities.\n\nParquet, designed for use with large-scale data processing frameworks like Apache Spark and Hadoop, has gained significant traction in both the \"big data\" and \"small data\" communities. Its ability to enable efficient data storage and retrieval, coupled with its support for complex data types and schema evolution, makes it a compelling option for a wide range of data-intensive applications.\n\nIn this article, we will dive deep into the world of Parquet, exploring its key features, benefits, and use cases. We will also compare Parquet to other popular file formats and table structures, such as Apache Iceberg and Delta Lake, to help you make informed decisions when designing your data architecture.\n\n## Key Takeaways\n\n- **What is Parquet**: An open-source columnar storage file format for efficient analytics.\n- **Core Benefits**: Superior compression, faster query performance (via column pruning and predicate pushdown), and schema evolution support.\n- **Common Alternatives**: Compared to row-based (CSV, Avro), columnar (ORC), and table formats (Iceberg, Delta Lake).\n- **DuckDB & MotherDuck**: Parquet integrates seamlessly with DuckDB for high-performance SQL queries directly on Parquet files.\n\n## What is Parquet?\n\nParquet is a [columnar storage file format](https://motherduck.com/learn-more/columnar-storage-guide/). When data engineers ask 'what is a Parquet file?', the simple answer is that it's a file that stores data in columns, not rows. This Parquet data format is designed for efficient data processing, particularly in the context of big data applications. Developed as part of the Apache Hadoop ecosystem, Parquet has gained widespread adoption due to its ability to optimize storage and query performance.\n\n### Columnar Storage Format\n\nOne of the key characteristics of Parquet is its columnar storage layout. Unlike traditional row-based formats, Parquet organizes data by columns rather than rows. This means that all values for a particular column are stored contiguously on disk, enabling faster retrieval and better compression ratios.\n\nBy storing data in a columnar fashion, Parquet excels at analytical queries that involve reading a subset of columns from a large dataset. This is particularly beneficial for data warehousing and business intelligence scenarios, where queries often focus on specific columns rather than entire rows.\n\nThe columnar storage format also allows for more efficient compression techniques. Since values within a column tend to be of the same data type and exhibit similar patterns, Parquet can apply appropriate compression algorithms to achieve higher compression ratios compared to row-based formats. This reduction in storage footprint translates to lower storage costs and faster query execution, as less data needs to be read from disk.\n\n![Post Image](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fstorage_comparison_1_5c87b9f5c1.svg&w=3840&q=75)\n\n### Open Source and Widely Supported\n\nParquet is an open-source project governed by the Apache Software Foundation, ensuring its transparency, community-driven development, and long-term sustainability. The open-source nature of Parquet has fostered a vibrant ecosystem, with contributions from industry leaders and a growing user base.\n\nOne of the key advantages of Parquet's open-source status is its wide support across various big data processing frameworks. Major platforms like Apache Spark, Apache Hadoop, and Presto have native support for reading and writing Parquet files, making it easy to integrate Parquet into existing data pipelines.\n\nThis broad compatibility ensures interoperability between different tools and systems, allowing organizations to leverage Parquet across their data ecosystem. It also mitigates the risk of vendor lock-in, as Parquet can be used with a variety of open-source and commercial solutions.\n\n## Why Use Parquet?\n\nParquet stands out in data processing environments for its ability to efficiently manage large datasets while ensuring high performance. Its unique approach to data storage enhances retrieval speeds and optimizes storage efficiency. This is achieved through intelligent data organization and compression strategies that minimize unnecessary data handling.\n\n### Efficient Compression and Encoding\n\nParquet's architecture benefits from exceptional data compression capabilities, essential for managing extensive datasets. Instead of treating data uniformly, Parquet leverages specialized encoding techniques such as dictionary, run-length, and delta encoding to optimize storage. These methods reduce the data footprint, translating into cost savings and improved access speeds. By minimizing I/O operations, Parquet enhances query performance, making it ideal for data-intensive tasks.\n\n![Post Image](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fencoding_compression_e1bd66680f.svg&w=3840&q=75)\n\n### Schema Evolution and Nested Data Types\n\nParquet is designed to handle evolving data structures with ease, supporting seamless schema modifications. This flexibility allows for the addition or alteration of columns without disrupting existing workflows, ensuring continuous data integrity. Parquet's proficiency with complex data structures\u2014like nested fields\u2014facilitates versatile data modeling. Its adaptability is vital for accommodating dynamic business requirements and integrating diverse datasets.\n\n### Predicate Pushdown and Column Pruning\n\nParquet effectively supports techniques like predicate pushdown and column pruning, which are crucial for optimizing data queries. By bringing filtering and aggregation operations closer to the storage layer, Parquet reduces the amount of data that needs processing. This approach not only speeds up queries but also lowers computational demands, enabling swift responses. Consequently, Parquet allows data professionals to execute complex queries on extensive datasets efficiently, providing timely and actionable insights.\n\n## Parquet Alternatives and Comparisons\n\nNavigating the landscape of data storage solutions reveals a variety of formats, each offering unique strengths tailored to specific needs. While Parquet remains a prominent choice, exploring its alternatives can provide valuable insights into selecting the best fit for particular data environments.\n\n### CSV Files\n\nCSV files are a straightforward, text-based format that organizes data in a tabular form with rows and columns separated by delimiters. This simplicity makes CSV highly portable and easy to use for data exchange across different systems. However, it lacks advanced functionalities like compression and schema management, which limits its efficiency in handling large-scale datasets.\n\n### Apache ORC\n\nApache ORC stands out with its columnar storage capabilities, optimized for high-performance data processing tasks. It excels in compressing and managing large datasets efficiently, offering features like ACID transactions that ensure data integrity during updates and queries. ORC's tight integration with Hive-specific functionalities makes it an appealing choice for Hadoop ecosystems, enabling seamless operations within Hive data warehouses.\n\n### Apache Avro\n\nApache Avro offers a flexible, row-based format that emphasizes efficient data serialization and schema evolution. Avro provides robust support for evolving schemas, allowing changes without requiring data rewrites\u2014ideal for applications needing frequent schema updates and cross-system data exchange. Its compact binary serialization format enhances data transmission efficiency across distributed systems.\n\n### Delta Lake and Apache Iceberg\n\nDelta Lake and Apache Iceberg build on the strengths of Parquet, introducing advanced table management features. Delta Lake, with its seamless Spark integration, offers capabilities like ACID transactions and data versioning, supporting both batch and streaming data processing. This makes it suitable for environments requiring consistent data updates and real-time analytics.\n\nApache Iceberg is crafted to optimize large-scale data lake operations, providing comprehensive support for multiple file formats, including Parquet. It facilitates complex data management with features like schema evolution and time travel, ensuring data consistency and adaptability across diverse processing engines and storage solutions.\n\n## Choosing the Right Format\n\nSelecting the optimal data format necessitates a keen understanding of your data access patterns. For workloads requiring extensive data scans with a focus on specific data attributes, leveraging formats designed for efficient data retrieval can significantly enhance performance. In contrast, for scenarios where frequent updates or point-specific data access is essential, a format that facilitates rapid row-level operations may be more advantageous.\n\n### Ecosystem Compatibility\n\nThe integration of a data format with existing systems is paramount for operational efficiency. Formats that align well with current data processing tools and frameworks simplify the implementation process and reduce potential disruptions. Evaluating the collective expertise of your team with a given format can inform the decision-making process, ensuring a smooth transition and effective utilization of the chosen technology.\n\n### Data Volume and Scalability\n\nAnticipating the trajectory of data growth and volume is critical in format selection. For environments managing substantial datasets, selecting a format that balances storage efficiency with retrieval speed is crucial. Formats equipped with advanced scalability features, such as those designed for extensive data management, offer robust solutions to handle burgeoning data lakes. These options ensure data integrity while maintaining high performance as data scales.\n\n## How you read and write parquet files in DuckDB\n\nDuckDB provides extensive support for Parquet files, seamlessly integrating them into workflows that demand high-performance data analysis. Users can take advantage of DuckDB's capabilities while preserving the structural and performance benefits of Parquet.\n\n### Reading Parquet Files\n\nWith DuckDB, accessing Parquet files becomes a streamlined process, eliminating the need for data importation. This approach offers:\n\n- **Direct File Access**: DuckDB enables SQL queries directly on Parquet files, allowing immediate data exploration without additional data loading steps.\n- **Optimized Column Retrieval**: By leveraging Parquet's columnar nature, DuckDB efficiently processes column-specific queries, ensuring rapid data retrieval and minimizing unnecessary data scans.\n\nThese features facilitate efficient and effective data analysis, maximizing the performance advantages inherent in Parquet's design.\n\n### Writing Parquet Files\n\nExporting data to Parquet format in DuckDB is straightforward, ensuring that the advantages of Parquet's compression and structuring are retained:\n\n- **SQL-Driven Export**: Users can export results to Parquet with simple SQL commands, preserving the data's integrity and organization.\n- **Advanced Compression Options**: DuckDB supports the use of Parquet's compression methods during export, optimizing file size for storage efficiency.\n\nThese functionalities enable seamless data management from analysis to storage, ensuring that DuckDB users can fully leverage the benefits of both technologies in their data workflows.\n\n## How the support for Parquet files in DuckDB differs from the support for other open file formats like Iceberg and Delta Lake\n\nDuckDB excels in its support for Parquet files, offering direct integration that maximizes the potential of Parquet\u2019s columnar storage benefits. This integration underscores DuckDB\u2019s focus on performance and efficiency. In comparison, its handling of other formats like Iceberg and Delta Lake involves additional considerations due to their distinct capabilities in data management and transactional features.\n\n### Parquet Integration in DuckDB\n\nDuckDB\u2019s architecture is purpose-built to leverage the column-oriented design of Parquet. This allows for seamless data processing and minimizes overhead, enhancing query performance. The optimized handling of Parquet files means that DuckDB can efficiently execute analytical tasks by taking full advantage of Parquet\u2019s compression and encoded structures, making it an ideal choice for applications requiring swift data interrogation.\n\n### Handling Iceberg and Delta Lake\n\nWhen it comes to supporting Iceberg and Delta Lake, DuckDB must navigate the complexities introduced by these formats\u2019 advanced features. Both formats provide robust table management functionalities that extend beyond simple storage solutions and may not be fully supported.\n\n### Performance Considerations\n\nThe performance dynamics between DuckDB and these formats are influenced by the specific use cases they address. Parquet\u2019s design aligns well with DuckDB\u2019s strengths, facilitating efficient data retrieval processes. In contrast, utilizing Iceberg or Delta Lake may necessitate additional performance considerations, particularly when dealing with their advanced metadata and transaction management capabilities, which may impact processing efficiency based on workload demands.\n\nDuckDB\u2019s approach to these formats showcases its flexibility and adaptability, offering robust support for Parquet while accommodating the advanced features of Iceberg and Delta Lake for more complex data management needs.\n\nThe ongoing transformation in data storage and management highlights Parquet's role as a cornerstone of [modern data strategies](https://motherduck.com/learn-more/modern-data-warehouse-playbook/). Its architecture aligns with the demands of large-scale data operations, offering efficient solutions that meet the needs of complex analytics. This adaptability ensures Parquet remains integral across diverse computing environments, valued for its performance and reliability.\n\nEmerging table formats, including Delta Lake and Apache Iceberg, introduce enhancements that build on Parquet's capabilities. These formats offer advanced features like transactional support and time travel, catering to sophisticated data management requirements. Their ability to efficiently handle massive datasets makes them indispensable for organizations seeking comprehensive data governance solutions.\n\nDuckDB's integration of Parquet reflects its commitment to high-performance data processing, optimizing Parquet's strengths for analytical workflows. The platform navigates the complexities of newer formats like Iceberg and Delta Lake, ensuring robust support while maintaining efficient data operations. This synergy exemplifies how DuckDB leverages Parquet's strengths alongside the advanced capabilities of modern table formats to deliver effective data solutions.\n\nAs the data landscape continues to evolve, understanding the intricacies of Parquet and its alternatives is crucial for making informed decisions about your data architecture. By leveraging the strengths of Parquet and integrating it with powerful tools like DuckDB, you can unlock the full potential of your data and drive meaningful insights. If you're ready to experience the benefits of Parquet and explore a collaborative data warehousing solution, [join us at MotherDuck and get started today](https://app.motherduck.com/?auth_flow=signup).\n\n### TABLE OF CONTENTS\n\n[Key Takeaways](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#key-takeaways)\n\n[What is Parquet?](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#what-is-parquet)\n\n[Why Use Parquet?](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#why-use-parquet)\n\n[Parquet Alternatives and Comparisons](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#parquet-alternatives-and-comparisons)\n\n[Choosing the Right Format](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#choosing-the-right-format)\n\n[How you read and write parquet files in DuckDB](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#how-you-read-and-write-parquet-files-in-duckdb)\n\n[How the support for Parquet files in DuckDB differs from the support for other open file formats like Iceberg and Delta Lake](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#how-the-support-for-parquet-files-in-duckdb-differs-from-the-support-for-other-open-file-formats-like-iceberg-and-delta-lake)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\n## FAQS\n\n### What is the Parquet file format?\n\nApache Parquet is an open-source columnar storage file format designed for efficient data processing. Unlike row-based formats like CSV, it stores data by columns, enabling faster query performance and superior compression for large-scale analytical workloads.\n\n### What are the main benefits of using Parquet files?\n\nParquet's main benefits include efficient data compression and encoding, schema evolution support, and performance optimization. It uses techniques like predicate pushdown and column pruning to speed up analytical queries by only reading the data that is needed.\n\n### What are the alternatives to Parquet?\n\nCommon alternatives to Parquet include row-based formats like CSV and Avro, and other columnar formats like Apache ORC. Newer table formats like Delta Lake and Apache Iceberg often use Parquet as their underlying file storage format while adding transactional guarantees.\n\n## Additional Resources\n\n[Docs\\\\\n\\\\\nWhat is a Data Warehouse?](https://motherduck.com/learn-more/what-is-a-data-warehouse/) [Docs\\\\\n\\\\\nWhat is OLAP?](https://motherduck.com/learn-more/what-is-OLAP/) [Docs\\\\\n\\\\\nLoading Data into MotherDuck with Parquet](https://motherduck.com/docs/key-tasks/loading-data-into-motherduck/loading-data-md-python/) [Docs\\\\\n\\\\\nUse DuckDB to Convert CSV to Parquet](https://duckdbsnippets.com/snippets/6/quickly-convert-a-csv-to-parquet-bash-function) [Video\\\\\n\\\\\nQuerying Parquet Files on S3 with DuckDB](https://www.youtube.com/watch?v=fZj6kTwXN1U)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 212,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 212
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     "content": "---\ntitle: csv-files-persist-duckdb-solution\ncontent_type: blog\nsource_url: https://motherduck.com/blog/csv-files-persist-duckdb-solution\nindexed_at: '2025-11-25T19:57:13.232430'\ncontent_hash: 28f2997ebdb99804\nhas_code_examples: true\nhas_step_by_step: true\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Why CSV Files Won\u2019t Die and How DuckDB Conquers Them\n\n2025/02/04 - 9 min read\n\nBY\n\n[Mehdi Ouazza](https://motherduck.com/authors/mehdi-ouazza/)\n\nI've been working in the data field for a decade, across various companies, and one constant challenge that\u2019s almost unavoidable is dealing with CSV files.\n\nYes, there are far more efficient formats, such as [Parquet](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/), which avoid schema nightmares thanks to their typing, but CSV files persist for many reasons:\n\n- They\u2019re easy to edit and read, requiring no dependencies\u2014just open the file.\n- They\u2019re universal: many services still exchange data in CSV format.\n- Want to download data from social media or your CRM? CSV.\n- Need transaction history from your bank? CSV.\n\nHowever, this simplicity comes with its own set of challenges, especially if you want to process CSVs without breaking pipelines or pulling your hair out.\n\nFortunately, DuckDB has an exceptional CSV parser. The team behind it invested heavily in building their own, and in this post, I\u2019ll show you a real-world example where I had to parse multiple CSV files. I\u2019ll also share some SQL tricks and demonstrate how smoothly everything worked using DuckDB and MotherDuck, resulting in a ready-to-query database.\n\nThe cherry on top? The final output is a database containing all Stack Overflow survey responses from the past seven years. Stick around if you\u2019re curious about extracting insights or querying the data yourself!\n\n## The biggest challenges when reading CSVs\n\nIn my opinion, there are four significant challenges when working with CSV files:\n\n1. **Schema Management**\n2. **Row-Level Errors**\n3. **Encoding Issues**\n\nThese challenges become even more complex when handling multiple CSVs that need to be read or joined to each other.\n\nLet\u2019s see how we address these issues with Stack Overflow survey data.\n\n## About the Dataset\n\nEach year, Stack Overflow publishes the results of their developer survey, including raw data in\u2014you guessed it\u2014CSV format. These files are available on their website: [https://survey.stackoverflow.co/](https://survey.stackoverflow.co/).\n\nHere\u2019s an example of how the dataset is organized:\n\n```css\nCopy code\n\n\u251c\u2500\u2500 raw\n\u2502   \u251c\u2500\u2500 2011 Stack Overflow Survey Results.csv\n\u2502   \u251c\u2500\u2500 2012 Stack Overflow Survey Results.csv\n\u2502   \u251c\u2500\u2500 2013 Stack Overflow Survey Responses.csv\n\u2502   \u251c\u2500\u2500 2014 Stack Overflow Survey Responses.csv\n\u2502   \u251c\u2500\u2500 2015 Stack Overflow Developer Survey Responses.csv\n\u2502   \u251c\u2500\u2500 2016 Stack Overflow Survey Results\n\u2502   \u2502   \u251c\u2500\u2500 2016 Stack Overflow Survey Responses.csv\n\u2502   \u2502   \u2514\u2500\u2500 READ_ME_-_The_Public_2016_Stack_Overflow_Developer_Survey_Results.txt\n\u2502   \u251c\u2500\u2500 stack-overflow-developer-survey-2017\n\u2502   \u2502   \u251c\u2500\u2500 DeveloperSurvey2017QuestionaireCleaned.pdf\n\u2502   \u2502   \u251c\u2500\u2500 README_2017.txt\n\u2502   \u2502   \u251c\u2500\u2500 survey_results_public.csv\n\u2502   \u2502   \u2514\u2500\u2500 survey_results_schema.csv\n\u2502   \u251c\u2500\u2500 stack-overflow-developer-survey-2018\n\u2502   \u2502   \u251c\u2500\u2500 Developer_Survey_Instrument_2018.pdf\n\u2502   \u2502   \u251c\u2500\u2500 README_2018.txt\n\u2502   \u2502   \u251c\u2500\u2500 survey_results_public.csv\n\u2502   \u2502   \u2514\u2500\u2500 survey_results_schema.csv\n\u2502   \u251c\u2500\u2500 stack-overflow-developer-survey-2019\n\u2502   \u2502   \u251c\u2500\u2500 README_2019.txt\n\u2502   \u2502   \u251c\u2500\u2500 so_survey_2019.pdf\n\u2502   \u2502   \u251c\u2500\u2500 survey_results_public.csv\n\u2502   \u2502   \u2514\u2500\u2500 survey_results_schema.csv\n[..]\n```\n\nKey observations:\n\n1. **Schema Changes Over the Years**\n\n\nSome questions and their formats evolve annually, making it difficult to standardize across years.\n2. **Pre-2016 Format**\n\n\nEach column represents a question, with names like:\n\n`What Country or Region do you live in?, How old are you?, How many years of IT/Programming experience do you have?, ...`\n\nAdditional challenges include:\n\n\u2022 Column names with unusual characters.\n\n\u2022 Querying such column names can be tedious.\n\nFrom 2017 onward, Stack Overflow improved the exports by separating:\n\n\u2022 A file containing the answers (columns with clean names for each question).\n\n\u2022 A schema file (.csv) that maps question codes to full question text.\n\nTo keep things manageable, I focused on datasets from 2017 onward.\n\n## Manual cleaning over automation\n\nWe\u2019ve all wasted hours trying to automate tasks that could have been done manually in minutes. This is a common trap for data engineers. Sometimes, quick manual cleanup is the most efficient approach.\n\nHere\u2019s what I did:\n\n\u2022 Placed all CSVs in a single folder.\n\n\u2022 Renamed files by adding the corresponding year as a prefix (e.g., `<year>_<file_name>`).\n\n\u2022 Ensured column names in schema files were consistent (e.g., renamed name to qname where needed).\n\nThese steps took less than five minutes and saved me headaches later. Not everything needs to be automated!\n\n## Loading the CSVs\n\nNow for the exciting part: loading the data. DuckDB supports glob patterns for loading multiple files. For complex structures like [Hive partitions](https://duckdb.org/docs/data/partitioning/hive_partitioning.html), it works seamlessly too.\n\nHere\u2019s the core query for loading survey results:\n\n```sql\nCopy code\n\nCREATE OR REPLACE TABLE stackoverflow_survey.survey_results AS\n    SELECT\n        * EXCLUDE (filename),\n        substring(parse_filename(filename), 1, 4) as year,\n    FROM read_csv_auto(\n        'data_2017_2024/*survey_results*.csv',\n        union_by_name=true,\n        filename=true)\n```\n\n**Breakdown:**\n\n1. We `CREATE` a table based on a `SELECT` statement.\n2. We select all columns but `EXCLUDE` the filename. This is a path of the containing file; we get this one by enabling `filename=true`.\n3. We parse the `filename` to get only the year. As we have a convention on the file name to prefix by `<year>`, we take the first four chars and create a `year` column\n4. We use the glob pattern to only load `*survey_results*` as a single table (we'll do another query for the `survey_schemas`)\n\nAlright, let's run this one... \ud83d\ude4f\n\n```sql\nCopy code\n\nduckdb.duckdb.ConversionException: Conversion Error: CSV Error on Line: 35365\nOriginal Line: 35499,I am a developer by profession,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA\nError when converting column \"Hobbyist\". Could not convert string \"NA\" to 'BOOLEAN'\n\nColumn Hobbyist is being converted as type BOOLEAN\nThis type was auto-detected from the CSV file.\nPossible solutions:\n* Override the type for this column manually by setting the type explicitly, e.g. types={'Hobbyist': 'VARCHAR'}\n* Set the sample size to a larger value to enable the auto-detection to scan more values, e.g. sample_size=-1\n* Use a COPY statement to automatically derive types from an existing table.\n\n  file = ./2017_2024_schema/2020_survey_results_public.csv\n  delimiter = , (Auto-Detected)\n  quote = \" (Auto-Detected)\n  escape = \" (Auto-Detected)\n  new_line = \\n (Auto-Detected)\n  header = true (Auto-Detected)\n  skip_rows = 0 (Auto-Detected)\n  comment = \\0 (Auto-Detected)\n  date_format =  (Auto-Detected)\n  timestamp_format =  (Auto-Detected)\n  null_padding = 0\n  sample_size = 20480\n  ignore_errors = false\n  all_varchar = 0\n```\n\nBad news, it didn't successfully parse the CSVs. But the GREAT news is that we have an excellent log error!\n\nWe know :\n\n- On which line we have an issue\n- A proper error message `Could not convert string \"NA\" to 'BOOLEAN'`\n- Possibles solutions\n\nThis saves so much time! Sometimes, just one row can mess up the whole process, and if the error message isn\u2019t clear, you\u2019re stuck guessing what went wrong. You might even end up throwing out your CSV or trying random fixes over and over.\n\nFor us, increasing the sample\\_size fixed the problem right away. \ud83d\udc4d\n\n## Wrapping up and automate the rest\n\nWith the initial query successful, the next steps were to:\n\n1. Repeat the process for schema files.\n2. Add row count checks to ensure no data was lost during merging of the CSVs\n\nHere's a generic function to wrap the query we saw and run them depending on the pattern name of the files (either for `results` or `schemas`).\n\n```python\nCopy code\n\nCSV_DIR = './data_2017_2024'\n\n# Global configuration\nFILE_CONFIGS = [\\\n    {'pattern': 'schema', 'table': 'survey_schemas'},\\\n    {'pattern': 'public', 'table': 'survey_results'}\\\n]\n\ndef process_survey_files(csv_dir: str) -> None:\n    \"\"\"\n    Process Stack Overflow survey CSV files and load them into DuckDB tables\n    \"\"\"\n    con = duckdb.connect('stackoverflow_survey.db')\n\n    for config in FILE_CONFIGS:\n        logging.info(f\"Processing {config['pattern']} files...\")\n        con.execute(f\"\"\"\n            CREATE OR REPLACE TABLE stackoverflow_survey.{config['table']} AS\n            SELECT\n                * EXCLUDE (filename),\n                substring(parse_filename(filename), 1, 4) as year,\n            FROM read_csv_auto(\n                '{csv_dir}/*{config['pattern']}*.csv',\n                union_by_name=true,\n                filename=true,\n                sample_size=-1\n            )\n        \"\"\")\n\n        # Log row count\n        count = con.execute(f\"SELECT COUNT(*) FROM stackoverflow_survey.{config['table']}\").fetchone()[0]\n        logging.info(f\"Loaded {count} rows into {config['table']}\")\n\n        # Log unique years\n        years = con.execute(f\"SELECT DISTINCT year FROM stackoverflow_survey.{config['table']} ORDER BY year\").fetchall()\n        logging.info(f\"{config['table']} years: {[year[0] for year in years]}\")\n\n    con.close()\n```\n\nFinally, we added another function to check row count and make sure we didn't lose any rows during the process :\n\n```python\nCopy code\n\ndef verify_row_counts(csv_dir: str) -> None:\n    \"\"\"\n    Verify that the sum of individual file counts matches the merged table counts\n    \"\"\"\n    con = duckdb.connect('stackoverflow_survey.db')\n\n    for config in FILE_CONFIGS:\n        pattern = config['pattern']\n        table = config['table']\n\n        logging.info(f\"\\nVerifying {pattern} files counts...\")\n        individual_counts = 0\n\n        for filename in os.listdir(csv_dir):\n            if pattern in filename and filename.endswith('.csv'):\n                file_path = os.path.join(csv_dir, filename)\n                count = con.execute(f\"SELECT COUNT(*) FROM read_csv_auto('{file_path}')\").fetchone()[0]\n                logging.info(f\"{filename}: {count} rows\")\n                individual_counts += count\n\n        merged_count = con.execute(f\"SELECT COUNT(*) FROM stackoverflow_survey.{table}\").fetchone()[0]\n        logging.info(f\"Individual {pattern} files total: {individual_counts}\")\n        logging.info(f\"Merged {table} total: {merged_count}\")\n\n        assert individual_counts  merged_count, f\"{pattern} row count mismatch: {individual_counts} != {merged_count}\"\n\n    con.close()\n    logging.info(\"\u2705 All row counts verified successfully!\")\n```\n\n## Sharing the dataset\n\nNow that I have a DuckDB database containing both tables (results and schemas), the only thing left is to share it! Let's see how that works with MotherDuck.\n\nI\u2019m using the DuckDB CLI, but this could also be part of a Python script. It\u2019s just four simple commands:\n\n```sql\nCopy code\n\nduckdb\nD ATTACH 'stackoverflow_survey.db'\nD ATTACH 'md:'\nD CREATE DATABASE cloud_stackoverflow_survey FROM stackoverflow_survey;\nD CREATE SHARE FROM cloud_stackoverflow_survey;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                            share_url                            \u2502\n\u2502                             varchar                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 md:_share/sample_data/23b0d623-1361-421d-ae77-125701d471e6      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n1. We attach the local DuckDB database with `ATTACH` command.\n2. We connect to MotherDuck using `ATTACH 'md';`. Note that I have my [`motherduck_token`](https://motherduck.com/docs/key-tasks/authenticating-and-connecting-to-motherduck/authenticating-to-motherduck/#authentication-using-an-access-token) stored in an `ENV`.\n3. We upload the database to MotherDuck using the `CREATE DATABASE x FROM x`\n4. We create a public share so that anyone can start querying!\n\nTo make it even easier for MotherDuck users, I put this one in the existing demo database [`sample_data`](https://motherduck.com/docs/getting-started/sample-data-queries/datasets/), which is attached by default for any users.\n\n## Querying the dataset\n\nThis dataset offers plenty of opportunities to uncover insights, but I\u2019ll wrap up this blog with a simple query that wasn\u2019t included in the original StackOverflow study.\n\nI wanted to explore the average happiness score of people based on their work location (remote, in-person, or hybrid).\n\n```sql\nCopy code\n\nSELECT RemoteWork,\n       AVG(CAST(JobSat AS DOUBLE)) AS AvgJobSatisfaction,\n       COUNT(*) AS RespondentCount\nFROM sample_data.stackoverflow_survey.survey_results\nWHERE JobSat NOT IN ('NA')\n  AND RemoteWork NOT IN ('NA')\n  AND YEAR='2024'\nGROUP BY ALL;\n```\n\nand the results :\n\n```sql\nCopy code\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              RemoteWork              \u2502 AvgJobSatisfaction \u2502 RespondentCount \u2502\n\u2502               varchar                \u2502       double       \u2502      int64      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 In-person                            \u2502  6.628152818991098 \u2502            5392 \u2502\n\u2502 Remote                               \u2502  7.072592992884806 \u2502           11103 \u2502\n\u2502 Hybrid (some remote, some in-person) \u2502  6.944303596894311 \u2502           12622 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nTwo interesting takeaways: remote and ybrid workers make up the majority of survey responses, and on average, they seem to be happier too!\n\nCheck out [our documentation](https://motherduck.com/docs/getting-started/sample-data-queries/stackoverflow-survey/) if you want to explore this dataset further.\n\nIn the meantime, get ready to tackle future CSV challenges with ease\u2014DuckDB and MotherDuck (start for [free!](https://motherduck.com/get-started/)) have got you covered!\n\n* * *\n\n### Why DuckDB\u2019s CSV Parser is Special\n\n- [https://duckdb.org/2023/10/27/csv-sniffer.html](https://duckdb.org/2023/10/27/csv-sniffer.html)\n- [https://duckdb.org/2024/12/05/csv-files-dethroning-parquet-or-not.html](https://duckdb.org/2024/12/05/csv-files-dethroning-parquet-or-not.html)\n- [Why CSVs Still Matter: The Indispensable File Format](https://youtu.be/I07qV2hij4E?si=DjCapBT3eg5UWLdn)\n\n### TABLE OF CONTENTS\n\n[The biggest challenges when reading CSVs](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#the-biggest-challenges-when-reading-csvs)\n\n[About the Dataset](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#about-the-dataset)\n\n[Manual cleaning over automation](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#manual-cleaning-over-automation)\n\n[Loading the CSVs](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#loading-the-csvs)\n\n[Wrapping up and automate the rest](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#wrapping-up-and-automate-the-rest)\n\n[Sharing the dataset](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#sharing-the-dataset)\n\n[Querying the dataset](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#querying-the-dataset)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![DuckDB Ecosystem: January 2025](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fnewsletter_a65cff5430.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025/)\n\n[2025/01/10 - Simon Sp\u00e4ti](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025/)\n\n### [DuckDB Ecosystem: January 2025](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025)\n\nDuckDB Monthly #25: PyIceberg, 0$ data distribution and more!\n\n[![Local dev and cloud prod for faster dbt development](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FLocal_Dev_Cloud_Prod_083b07b92e.png&w=3840&q=75)](https://motherduck.com/blog/dual-execution-dbt/)\n\n[2025/01/16 - Jacob Matson](https://motherduck.com/blog/dual-execution-dbt/)\n\n### [Local dev and cloud prod for faster dbt development](https://motherduck.com/blog/dual-execution-dbt)\n\nSpark the Joy of beautiful local development workflows with MotherDuck & dbt\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 418,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 418
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-OLAP.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/product-analytics-motherduck-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/web-assembly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-use-cases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/pandas-dataframes-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-struct-nested-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/ducklake-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-application.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-lake-vs-data-warehouse-vs-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-warehouse-tco.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 29
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me read a few more relevant files about data loading efficiency:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     "content": "---\ntitle: 'No-ETL: Query Raw CSV & JSON Files Directly with SQL'\ncontent_type: guide\ndescription: Skip complex ETL. Learn the No-ETL method for startups to query multiple\n  raw CSV, JSON, & Parquet files directly with SQL. Get insights in minutes, not months.\npublished_date: '2025-10-17T00:00:00'\nsource_url: https://motherduck.com/learn-more/no-etl-query-raw-files\nindexed_at: '2025-11-25T20:37:09.260086'\ncontent_hash: 317312307e45ea1c\nhas_step_by_step: true\nhas_narrative: true\n---\n\n# The No-ETL Playbook: How to Query Raw CSV & JSON Files Directly with SQL\n\nFor startups and small companies, your most valuable data, like customer sign-ups, sales transactions, and product usage logs, is likely scattered across hundreds of local files like CSVs, Excel spreadsheets, and JSON logs. The conventional wisdom says you need to build a complex and expensive **Extract, Transform, Load (ETL)** pipeline before you can even begin to analyze it. But for a lean team that needs to move fast, this approach is a trap. It\u2019s slow, rigid, and drains precious engineering resources before you\u2019ve asked your first question.\n\nWhat if you could skip the pipeline and go straight to the insights?\n\nA modern, **\"No-ETL\"** approach allows you to do just that. Instead of spending months building a complex data infrastructure, you can use simple SQL to query your raw data files directly where they live, on your laptop or in cloud storage. This guide will show you how this lean, serverless method turns fragmented files into a powerful, queryable database in minutes, not months. You will learn why traditional ETL is a bottleneck, how to get immediate answers from your raw files, and how to consolidate thousands of fragmented files into a single, high-performance database, all while unifying data across your local machine and the cloud.\n\n## Why Are Traditional Data Pipelines a Trap for Startups?\n\nFor decades, the path from data to decision was a one-way street paved with ETL. This process involves extracting data from various sources, transforming it into a rigid, predefined schema, and loading it into a central data warehouse. While this model can work for large enterprises with stable processes, it creates a \"pipeline city\" that demands constant maintenance and becomes a major bottleneck for agile startups that need to pivot quickly.\n\nThe \"No-ETL\" philosophy flips this script entirely. Instead of moving all your data to a central processing location, you bring the processing power directly to your data. By pointing SQL at the files you already have, you can eliminate 80-90% of the traditional pipeline steps. This approach, powered by MotherDuck's serverless engine, allows teams to [consolidate data from dozens of sources in days, not months](https://motherduck.com/learn-more/modern-data-warehouse-playbook/).\n\nThis shift dramatically accelerates your time-to-insight and slashes operational costs. Processing a 100MB CSV file on your laptop is instantaneous and costs nothing, whereas using a traditional cloud warehouse incurs charges for compute time and network data transfer. Companies that adopt this lean model report [ 70-90% cost reductions](https://motherduck.com/learn-more/modern-data-warehouse-playbook/) compared to traditional cloud data warehouses.\n\n### How Did Five Startups Slash Data Prep Time from Hours to Minutes?\n\nThe benefits of abandoning premature ETL aren't just theoretical. Startups across various industries have replaced slow, brittle pipelines and overloaded databases with MotherDuck\u2019s query-in-place model, leading to dramatic improvements in speed, cost, and agility. These changes often lead to huge improvements, not just small percentage gains.\n\n| Company & Industry | Previous State & Pain Point | Transformation with MotherDuck | Quantified Outcome |\n|---|---|---|---|\nFinqore (FinTech) | 8-hour data pipelines using Postgres for complex financial data and a heavy reliance on manual Excel processes. | Replaced Postgres to process and unify data directly, enabling a real-time metrics explorer and AI agents. |\n8 hours to 8 minutes |\n\n**Gardyn**(IoT / AgTech)[Pipeline time cut from](https://motherduck.com/case-studies/gardyn/)at 10x lower cost than other data warehouses.**over 24 hours to under 1 hour****UDisc**(Sports Tech)[dbt job time reduced from](https://motherduck.com/case-studies/udisc-motherduck-sports-management/), and typical queries dropped from minutes to**6 hours to 30 minutes****5 seconds**.**Dexibit**(Analytics)[Analytical query times reduced from](https://motherduck.com/case-studies/dexibit/), eliminating traditional data warehouse costs.**minutes to a few seconds****Layers**(SaaS)[dashboards loading in](https://motherduck.com/case-studies/layers-multi-tenant-data-warehouse/).**110 ms**## How Can You Get Instant Answers from Excel & CSV Files?\n\nFor many teams, valuable data is trapped in local spreadsheets. The traditional path to analyzing this data, which includes manual imports, database setup, and schema definition, is slow and frustrating. MotherDuck eliminates this friction by allowing you to query CSV and Excel files directly with SQL, just as if they were database tables.\n\nThis is powered by DuckDB's [ read_csv_auto](https://duckdb.org/docs/stable/data/csv/auto_detection.html) and\n\n[functions, which automatically infer column names, data types, and file dialects (like delimiters) by sampling the file. This](https://duckdb.org/docs/stable/guides/file_formats/excel_import.html)\n\n`read_xlsx`\n\n**schema-on-read**capability means a non-technical user can go from a local file to a powerful SQL query in seconds, without writing any\n\n`CREATE TABLE`\n\nstatements or managing a database server.For a local CSV file, a query is as simple as this:\n\nCopy code\n\n```\nSELECT\nproduct_category,\nSUM(sale_amount) AS total_sales\nFROM 'transactions.csv'\nGROUP BY ALL\nORDER BY total_sales DESC;\n```\n\n\nMotherDuck [automatically detects the columns and their types](https://duckdb.org/docs/stable/data/csv/overview.html), allowing you to filter, aggregate, and sort on the fly. This direct-query capability extends to files stored in cloud object storage and [even Google Sheets](https://motherduck.com/docs/key-tasks/data-warehousing/Replication/spreadsheets/), providing a unified way to access spreadsheet data wherever it lives.\n\n### What About Messy Spreadsheets? How to Handle Common Pitfalls in SQL\n\n\"Wild\" CSV and Excel files often have inconsistencies. Instead of spending time on manual cleanup, you can handle these common issues directly in your SQL query using optional parameters.\n\n| Pitfall | Problem Description | SQL Solution with MotherDuck |\n|---|---|---|\nIncorrect Delimiter | A CSV file uses a pipe (`|` ) or semicolon (`;` ) instead of a comma, causing columns to be misread. | Use the\n`delim` parameter |\n\n`read_csv('data.csv', delim = '|')`\n\n.**Inconsistent Date Formats**`MM/DD/YYYY`\n\nor `DD-Mon-YY`\n\n.[to provide the exact format string:](https://duckdb.org/docs/stable/data/csv/overview.html)`dateformat`\n\nparameter`read_csv('data.csv', dateformat = '%m/%d/%Y')`\n\n.**Header and Footer Rows**[to select only the data cells:](https://duckdb.org/docs/stable/guides/file_formats/excel_import.html)`range`\n\nparameter`range = 'A5:Z100'`\n\n.**Mixed Data Types**[, then use](https://duckdb.org/docs/stable/guides/file_formats/excel_import.html)`all_varchar = true`\n\n`TRY_CAST()`\n\nin your `SELECT`\n\nstatement to safely convert types.## How Do I Turn Thousands of Fragmented Files into a Single Database?\n\nA common challenge for growing companies is data fragmentation, where analytics data is spread across thousands of individual files in cloud storage like Amazon S3. MotherDuck sidesteps complex ingestion jobs by [treating an entire folder of files as a single, queryable database table](https://duckdb.org/docs/stable/data/multiple_files/overview.html).\n\nBy using SQL with glob patterns, you can instantly query a whole collection of Parquet, CSV, or JSON files directly in S3. For example, the following query will scan all Parquet files for the year 2025, no matter how many subdirectories they are in, and treat them as one large table:\n\nCopy code\n\n```\nSELECT\nevent_type,\nCOUNT(*) AS event_count\nFROM read_parquet('s3://my-bucket/logs/2025/**/*.parquet')\nGROUP BY event_type;\n```\n\n\nThis is a high-performance feature, not just a convenience. MotherDuck\u2019s query engine [pushes down filters and projections to the file level](https://duckdb.org/docs/stable/data/parquet/overview.html), minimizing the amount of data read from cloud storage and reducing costs. For even greater efficiency, organizing files using [ Hive-style partitioning](https://duckdb.org/docs/stable/guides/performance/how_to_tune_workloads.html) (e.g.,\n\n`/year=2025/month=10/`\n\n) allows the engine to skip entire folders that don\u2019t match a query\u2019s `WHERE`\n\nclause, dramatically reducing scan time and cost.## How Can I Join Local Files with Data in the Cloud?\n\nStartups rarely have their data in one neat location. You might have recent sales data in a CSV on your laptop, historical logs in an S3 bucket, and user profiles in a managed database. MotherDuck\u2019s [ Dual Execution feature](https://motherduck.com/docs/concepts/architecture-and-capabilities/) unifies these fragmented datasets into a single analytical layer without requiring you to move the data first.\n\nWhen your local DuckDB client is connected to MotherDuck, they form a distributed system where the query optimizer [intelligently routes parts of your query to where the data lives](https://motherduck.com/videos/339/bringing-duckdb-to-the-cloud-dual-execution-explained/). A query on a local file runs entirely on your machine's resources. A query on a large S3 file runs in the MotherDuck cloud to use its scale. Most powerfully, a join between the two is automatically optimized to minimize data transfer.\n\nAn analyst can prototype a query by joining a local spreadsheet with a massive cloud table. MotherDuck is smart enough to automatically push filters down to the local file, send only the small, filtered result to the cloud, and perform the final join there. This [avoids the slow and costly process of uploading entire local files](https://motherduck.com/learn-more/fix-slow-bi-dashboards/) just to perform a join. The entire process is smooth, and you simply write standard SQL.\n\n## How Can You Explore Data Before Building a Formal Pipeline?\n\nThe schema-on-read approach is powerful because it lets you [explore and understand your data before committing](https://motherduck.com/duckdb-book-summary-chapter5/) to a rigid transformation pipeline. This \"explore-then-model\" workflow de-risks data projects by allowing you to assess data quality, discover hidden patterns, and validate business assumptions upfront. With MotherDuck, you can use a suite of simple SQL commands to profile your raw CSV, JSON, and Parquet files directly.\n\n### How Can You See the Structure of Your JSON and Parquet Files Automatically?\n\nMotherDuck provides powerful functions to look inside the structure of your semi-structured and columnar files without manually parsing them.\n\nFor **Parquet files**, you can [query the file\u2019s internal metadata directly](https://duckdb.org/docs/stable/data/parquet/metadata.html) to see column names, types, and nullability. This is very helpful for understanding the data you've received from a partner or another system.\n\nCopy code\n\n```\nSELECT * FROM parquet_schema('your_file.parquet');\n```\n\n\nFor **JSON files**, the [ read_json_auto function automatically infers a schema](https://duckdb.org/docs/stable/data/json/loading_json.html), representing nested objects as\n\n`STRUCT`\n\ns and arrays as `LIST`\n\ns. You can see this inferred schema by creating a temporary table and describing it:Copy code\n\n```\nCREATE TABLE temp_json AS SELECT * FROM read_json_auto('api_response.json');\nDESCRIBE temp_json;\n```\n\n\n### How Can You Use SQL to Explore JSON Data Without Knowing Its Structure?\n\nJSON's nested, schema-less nature makes it notoriously difficult to analyze with traditional SQL. MotherDuck lets you [explore and query deeply nested JSON files immediately](https://motherduck.com/blog/analyze-json-data-using-sql/), even with zero prior knowledge of their structure.\n\nYou can instantly navigate the nested structure using simple **dot notation** (e.g., `SELECT user.name.first FROM 'users.json'`\n\n) and flatten complex arrays into rows using the ** UNNEST** function. This turns what was once a painful data preparation task into a simple, interactive exploration process.\n\nCopy code\n\n```\n-- Explore nested JSON and flatten an array of items into separate rows\nSELECT\norder_id,\ncustomer.id AS customer_id,\nitem.product_id::INTEGER,\nitem.quantity::INTEGER\nFROM read_json_auto('orders.json'), UNNEST(line_items) AS t(item);\n```\n\n\n## Is the \"No-ETL\" Approach a Permanent Solution?\n\nFor a lean team, building a full-blown ETL pipeline too early is a strategic error. It locks you into a rigid structure before you fully understand your data's value or how your business questions will evolve. The \"No-ETL\" approach is a better starting point for most startups.\n\nHowever, \"No-ETL\" does not mean \"Never-ETL.\" As your company matures and your data processes become more standardized, certain triggers justify introducing a more formalized, lightweight EL(T) process, where raw data is loaded into cloud storage and then transformed within the warehouse. You should consider this change when you need things like **repeatability for audits**, such as for financial reporting or compliance. It also becomes valuable when you need to improve **performance on complex joins** for frequently-run dashboards, or when business operations depend on [ strict data freshness SLAs](https://motherduck.com/learn-more/modern-data-warehouse-use-cases/) of minutes, not hours. Finally, as your team grows, a formalized model is essential for\n\n[, allowing you to systematically clean data or mask PII before exposing it to a wider audience.](https://motherduck.com/blog/motherduck-kestra-etl-pipelines/)\n\n**data governance at scale**This **progressive modeling pattern**, which involves starting with raw files, creating semantic views, and only materializing tables when necessary, allows your data architecture to evolve with your business, not against it.\n\n## Your Path Forward: From Files to Insights\n\nThe message for startups is clear: stop building pipelines and start asking questions. The modern, No-ETL approach used by MotherDuck removes the friction between your data and your decisions. By letting your entire team query raw files directly with the SQL they already know, you unlock a level of speed and agility that traditional data stacks simply cannot match. Start by exploring your local files, scale to the cloud, and let your data architecture grow with your business needs. The power to be data-driven is no longer locked behind complex engineering projects. It's right there in your files, waiting for a query.\n\nStart using MotherDuck now!\n\n## FAQS\n\n### Do I really need an ETL pipeline if I\u2019m just getting started?\n\nFor most startups, no. A \"No-ETL\" approach lets you query raw CSV, JSON, and Excel files directly with SQL. This is faster and cheaper, allowing you to get insights immediately without building complex infrastructure.\n\n### How can I analyze data in Excel and CSV files without hassle?\n\nYou can use SQL to query local Excel and CSV files as if they were database tables. Modern tools like DuckDB automatically detect columns and data types, so you can go from a spreadsheet to analysis in seconds without any setup.\n\n### How do I query thousands of files in a folder as a single database?\n\nUse SQL with glob patterns (e.g., `FROM 's3://bucket/logs/**/*.parquet'`\n\n). This treats an entire folder of files in cloud storage as one large, queryable table, eliminating the need for complex data ingestion jobs.\n\n### Can I explore my data before building an ETL pipeline?\n\nYes, this is a key benefit of the No-ETL approach. By querying raw files directly, you can profile data quality, discover patterns, and validate assumptions with SQL before committing to a rigid transformation model.\n\n### How can I see the structure of my JSON and Parquet files automatically?\n\nUse simple SQL commands. For Parquet, `parquet_schema('file.parquet')`\n\nreveals the schema. For JSON, `read_json_auto()`\n\ninfers the structure, which you can view with a `DESCRIBE`\n\ncommand on the query result.\n\n### Why is schema design so difficult for startups?\n\nStartups evolve rapidly, causing data sources and business needs to change constantly. A rigid, upfront schema (schema-on-write) becomes a bottleneck. A flexible, schema-on-read approach is better as it applies structure at query time.",
  [2m  â”‚[0m     "numLines": 229,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 229
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md",
  [2m  â”‚[0m     "content": "---\ntitle: taming-wild-csvs-with-duckdb-data-engineering\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering\nindexed_at: '2025-11-25T19:57:41.542448'\ncontent_hash: 6594cfafde3c18de\nhas_code_examples: true\nhas_step_by_step: true\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Taming Wild CSVs: Advanced DuckDB Techniques for Data Engineers\n\n2025/05/17 - 12 min read\n\nBY\n\n[Ryan Boyd](https://motherduck.com/authors/ryan-boyd/)\n\nIt's 2:30 AM. The office is empty. Your coffee's gone cold, and you're staring blankly at your screen as it mockingly displays: `Error: Could not convert string 'N/A' to INTEGER` on line 56,789. All you wanted was to import a \"simple\" CSV export from that upstream system. Sound familiar?\n\nWe've all been in CSV purgatory. That moment when what should be a five-minute task turns into a multi-hour ordeal because somewhere, buried deep in that innocent-looking file, lurks an inconsistent delimiter, a rogue quote, or my personal favorite\u2014columns that mysteriously appear and disappear like fish fry diving underwater to avoid being eaten by our duck friends.\n\nI've spent countless hours wrestling with problematic CSVs, but after discovering some of DuckDB's lesser-known features, those late-night CSV battles have become far less common. While DuckDB's automatic CSV reader is already impressively smart, knowing a few advanced techniques can save you from writing custom preprocessing scripts when things get messy.\n\nIn this guide, I'll share the DuckDB techniques that have repeatedly saved me from CSV hell:\n\n- How to diagnose what DuckDB actually thinks your [**CSV looks like**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#peeking-under-the-hood-sniffcsv)\n- Deep dive into the [**CSV sniffer**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#how-the-sniffer-works) and how it works under the hood\n- Ways to [**handle problematic rows**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#wrangling-the-errors) without aborting your entire import\n- Strategies for [**dealing with inconsistent schemas**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#handling-shifting-schemas-unionbyname) across files\n- [**Fine-tuning options**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#fine-tuning-detection-and-overrides) when auto-detection needs a helping hand\n- Real-world robustness: how DuckDB performs on a [**benchmark of messy CSVs**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#duckdbs-csv-parser-champion-of-the-pollock-benchmark)\n\nLet's dive in and see if we can make your next CSV import session a little less...quackers.\n\n## **Peeking Under the Hood: sniff\\_csv**\n\nBefore attempting to load the data using DuckDB\u2019s auto-detection capabilities, it's incredibly useful to understand what DuckDB _thinks_ it's dealing with. Is it guessing the delimiter correctly? Did it detect the header? What types is it inferring? The [sniff\\_csv()](https://duckdb.org/docs/stable/data/csv/auto_detection.html) function is your reconnaissance tool here.\n\nInstead of blindly running `read_csv` and potentially hitting errors, run `sniff_csv` first. It analyzes a sample of the file and reports back the detected dialect, types, header presence, and more.\n\n**Let\u2019s imagine a file [events.csv](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/events.csv):**\n\n```txt\nCopy code\n\nEventDate|Organizer|City|Venue\n2022-03-01|OpenTech|San Francisco, CA|Moscone Center, Hall A\n2022-03-02|OpenTech|San Francisco, CA|Moscone Center, Hall B\n2022-03-03|OpenTech|San Francisco, CA|Moscone Center, Hall C\n```\n\n**Let's see what DuckDB makes of this:**\n\n```sql\nCopy code\n\nFROM sniff_csv('events.csv');\n```\n\nINFO: Using full http URLs\nFor brevity, we'll just include the filename in each of the example commands below. You can download the example files using the links (eg, how events.csv is linked above). However, you can also specify the full http URL instead because DuckDB supports the http protocol for retrieving CSV files.\n\n```sql\nCopy code\n\nFROM sniff_csv('http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/events.csv');\n```\n\n**You can also control how much of the file it samples:**\n\n```sql\nCopy code\n\nFROM sniff_csv('events.csv', sample_size=10000); -- Sample 10k rows\n```\n\n**Or sample the whole file (careful with huge files!):**\n\n```sql\nCopy code\n\nFROM sniff_csv('events.csv', sample_size=-1);\n```\n\n**The output provides a wealth of information in a single row table:**\n\n- `Delimiter`, `Quote`, `Escape`, `NewLineDelimiter`: The detected structural characters.\n- `SkipRows`: Number of rows it thinks should be skipped at the start.\n- `HasHeader`: Boolean flag if a header is detected.\n- `Columns`: A list of structs showing detected column names and types (e.g., `{'name': 'VARCHAR', 'age': 'BIGINT'}`).\n- `DateFormat`, `TimestampFormat`: Any special date/time formats it detected.\n- `Prompt`: This is extremely useful! It gives you a `read_csv` command _with_ all the detected options explicitly set. You can copy, paste, and modify this as needed.\n\nRunning `sniff_csv` first can save you significant guesswork when an import fails. If the detected `Delimiter` is wrong, or it thinks `HasHeader` is true when it isn't, you know exactly which options to override in your `read_csv` call.\n\n### **How the Sniffer Works**\n\nDuckDB's CSV sniffer actually operates through multiple phases to determine the best way to read your file:\n\n1. **Dialect Detection**: At the time of writing, sniffer tests 24 different combinations of dialect configurations (delimiters, quotes, escapes, newlines) to see which one creates the most consistent number of columns across rows.\n\n2. **Type Detection**: After determining the dialect, the sniffer analyzes the first chunk of data (2048 rows by default) to detect column types, trying to cast values from most to least specific types (SQLNULL \u2192 BOOLEAN \u2192 BIGINT \u2192 DOUBLE \u2192 TIME \u2192 DATE \u2192 TIMESTAMP \u2192 VARCHAR).\n\n3. **Header Detection**: The sniffer checks if the first valid line can be cast to the detected column types. If it can't, that line is considered a header.\n\n4. **Type Replacement**: If you specified column types, they override the sniffer's detected types.\n\n5. **Type Refinement**: The sniffer validates the detected types on more data using vectorized casting.\n\n\n**Here's a code example showing how to inspect what the sniffer sees in a more complex scenario:**\n\n```sql\nCopy code\n\n-- Examine what the sniffer detects with a larger sample size\nSELECT\n    Delimiter, Quote, Escape, SkipRows, HasHeader, DateFormat, TimestampFormat\nFROM sniff_csv('events.csv',  sample_size=50000);\n\n-- To see the detected column types\nSELECT unnest(Columns)\nFROM sniff_csv('events.csv');\n```\n\nWhen I was working with a dataset containing 20+ columns of mixed types, the `unnest(Columns)` trick was particularly helpful to see exactly which columns were being detected as which types, saving a ton of back-and-forth troubleshooting.\n\n## **Wrangling the Errors: ignore\\_errors, store\\_rejects, and More**\n\nSo `sniff_csv` looks good, but your file _still_ has issues\u2014maybe just a few problematic rows scattered throughout millions of good ones. By default, DuckDB will halt the import on the first error. But often, you just want the valid data and to deal with the bad rows separately.\n\n### **Option 1: Just Skip 'Em (`ignore_errors`)**\n\nThe simplest approach is to tell DuckDB to skip rows that cause parsing or casting errors using `ignore_errors = true`.\n\nLet's imagine a file [`faulty_folks.csv`](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/faulty_folks.csv):\n\n```txt\nCopy code\n\nName,Age\nAlice,30\nBob,forty-two\nCharlie,35\n```\n\nTrying to read this normally with explicit types will fail on Bob's age:\n\n```sql\nCopy code\n\n-- This will error out!\nSELECT * FROM read_csv('faulty_folks.csv', header=true, columns={'Name':'VARCHAR', 'Age':'INTEGER'});\n```\n\nBut if we just want Alice and Charlie:\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('faulty_folks.csv',\n    header = true,\n    -- Specify expected types\n    columns = {'Name': 'VARCHAR', 'Age': 'INTEGER'},\n    ignore_errors = true  -- The key part!\n    );\n```\n\n**Explanation:**\n\n- We define the `columns` we expect, including the `INTEGER` type for `Age`.\n- `ignore_errors = true` tells the reader: if you hit a row where 'Age' can't become an `INTEGER` (like \"forty-two\"), just drop that row and keep going.\n\n**Output:**\n\n| Name | Age |\n| --- | --- |\n| Alice | 30 |\n| Charlie | 35 |\n\nBob gets left behind, but the import succeeds with the valid rows. This approach skips rows with various issues: casting errors, wrong number of columns, unescaped quotes, etc.\n\n### **Option 2: Skip and Store (store\\_rejects)**\n\nIgnoring errors is okay, but generally, you need to know _what_ went wrong and _which_ rows were rejected. Maybe you need to fix the source data or report the issues. This is where `store_rejects = true` becomes invaluable.\n\nWhen you use [`store_rejects`](https://duckdb.org/docs/stable/data/csv/overview.html), DuckDB still skips the bad rows (like `ignore_errors`), but it also logs detailed information about each rejected row and the error encountered into two temporary tables: `reject_scans` and `reject_errors`.\n\n```sql\nCopy code\n\n-- Read the file, storing rejected rows\nSELECT * FROM read_csv(\n    'faulty_folks.csv',\n    header = true,\n    columns = {'Name': 'VARCHAR', 'Age': 'INTEGER'},\n    store_rejects = true -- Store info about errors\n    -- Optional: Customize table names and limit\n    -- rejects_scan = 'my_scan_info',\n    -- rejects_table = 'my_rejected_rows',\n    -- rejects_limit = 100 -- Store max 100 errors per file\n);\n\n-- Now, let's see what was rejected\nFROM reject_errors;\n-- And details about the scan itself\nFROM reject_scans;\n```\n\n**Explanation:**\n\n1. The `read_csv` call runs, skips Bob's row, and returns Alice and Charlie just like before.\n2. The key difference: `store_rejects = true` populates the temporary tables.\n3. `FROM reject_errors;` shows details about the failed rows:\n\n   - `scan_id`, `file_id`: Link back to the specific scan/file.\n   - `line`: The original line number in the CSV.\n   - `column_idx`, `column_name`: Which column had the issue (if applicable).\n   - `error_type`: The category of error (e.g., `CAST`, `TOO_MANY_COLUMNS`).\n   - `csv_line`: The actual content of the rejected line.\n   - `error_message`: The specific error message DuckDB generated.\n4. `FROM reject_scans;` gives metadata about the `read_csv` operation itself (delimiter, quote rule, schema used, file path, etc.).\n\nI've found this incredibly useful for debugging dirty data. You get the clean data loaded _and_ a detailed report on the rejects, all within DuckDB. No more grep-ing through massive files trying to find that one problematic line!\n\n### **Option 3: Relaxing the Rules (strict\\_mode=false and null\\_padding=true)**\n\nSometimes, you just want to _get the data in_, even if it\u2019s a little messy. That\u2019s where DuckDB's more forgiving CSV parsing options can help you out. **`strict_mode = false`** option tells DuckDB to loosen up its parsing expectations. It will _try_ to read rows even if they contain typical formatting problems like:\n\n- Unescaped quote characters in fields (e.g., `\"15\" Laptop\"`).\n- Rows with _more_ columns than defined (DuckDB just drops the extras).\n- Mixed newline formats (like mixing `\\n` and `\\r\\n`).\n\nWARNING: fast, but not precise\nWhen you set `strict_mode=false`, you\u2019re trusting DuckDB to make its best guess. That works great when you want results fast\u2014but double-check the output if data precision matters!\n\nAnother commonly used option is **`null_padding = true,`** which handles rows that come up _short_, meaning they have fewer columns than expected. Instead of throwing an error, DuckDB just fills in the blanks with `NULL`.\n\nLet\u2019s look at an example. Here's a messy CSV file named [`inventory.csv`](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/inventory.csv):\n\n```txt\nCopy code\n\nItemID,Description,Price\n101,\"15\" Laptop\",999.99\n102,\"Wireless Mouse\"\n103,\"Mechanical Keyboard\",129.99,ExtraField\n```\n\nThis file includes:\n\n- An unescaped quote in the first row\u2019s description\n- A missing price in the second row\n- An extra column in the third row\n\nTry reading it normally:\n\n```sql\nCopy code\n\nFROM read_csv('inventory.csv');\n```\n\nDuckDB will skip all lines except the last.\n\n![DuckDB CLI image](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimage1_7ef2565c8e.png&w=3840&q=75)\n\nBut with relaxed settings:\n\n```sql\nCopy code\n\n-- Parsing a messy CSV while gracefully handling missing and extra fields\nFROM read_csv('inventory.csv',\n    strict_mode = false,   -- Forgive formatting quirks\n    null_padding = true   -- Fill missing columns with NULLs\n);\n```\n\n**Resulting Table:**\n\n| ItemID | Description | Price | column3 |\n| --- | --- | --- | --- |\n| 101 | 15 Laptop | 999.99 | NULL |\n| 102 | Wireless Mouse | NULL | NULL |\n| 103 | Mechanical Keyboard | 129.99 | ExtraField |\n\n## **Handling Shifting Schemas: union\\_by\\_name**\n\nHere's another common headache: you have multiple CSV files to load (e.g., monthly reports), but the columns aren't _quite_ the same across files. Maybe a column was added in March, or the order changed in June. Trying to read them together with a simple `read_csv('monthly_report_*.csv')` might fail or produce misaligned data.\n\nDuckDB's `union_by_name = true` option handles this elegantly. When reading multiple files (using globs or lists), it aligns columns based on their _header names_ rather than their position. If a file is missing a column found in other files, it fills it with `NULL`.\n\nImagine [`report_jan.csv`](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/report_jan.csv):\n\n```txt\nCopy code\n\nUserID,MetricA,MetricB\n1,10,100\n2,15,110\n```\n\nAnd [`report_feb.csv`](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/report_feb.csv):\n\n```txt\nCopy code\n\nUserID,MetricB,MetricC,MetricA\n3,120,xyz,20\n4,125,abc,25\n```\n\nNotice the different order and the new `MetricC` in February.\n\n```sql\nCopy code\n\nSELECT *\nFROM read_csv(\n    ['report_jan.csv', 'report_feb.csv'], -- List of files (or glob)\n    union_by_name = true -- The magic!\n);\n```\n\n**Explanation:**\n\n- DuckDB reads the headers from all files involved.\n- It creates a combined schema containing _all_ unique column names (`UserID`, `MetricA`, `MetricB`, `MetricC`).\n- For each file, it matches the data to the combined schema based on the header names found _in that specific file_.\n- Where a column doesn't exist in a file (like `MetricC` in `report_jan.csv`), it inserts `NULL`.\n\n**Output:**\n\n| UserID | MetricA | MetricB | MetricC |\n| --- | --- | --- | --- |\n| 1 | 10 | 100 | NULL |\n| 2 | 15 | 110 | NULL |\n| 3 | 20 | 120 | xyz |\n| 4 | 25 | 125 | Abc |\n\n## **Fine-Tuning Detection and Overrides**\n\nWhile auto-detection is great, sometimes you know better, or the sample DuckDB takes isn't quite representative. Here are some ways to fine-tune the process:\n\n**Bigger Sample:** If type detection seems off (e.g., a column that's mostly integers but has a few floats later gets detected as `BIGINT`), try increasing the sample size:\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('file.csv', sample_size = 50000);\n\n-- Or scan the whole file (can be slow for huge files)\nSELECT * FROM read_csv('file.csv', sample_size = -1);\n```\n\n**Manual Types:** Override specific column types if detection gets it wrong or if you want a different type:\n\n```sql\nCopy code\n\n-- Override by name\nSELECT * FROM read_csv('file.csv',\n  types = {'UserID': 'VARCHAR', 'TransactionAmount': 'DOUBLE'});\n\n-- Or by position if no headers\nSELECT * FROM read_csv('file.csv', header = false,\n  types = ['VARCHAR', 'DOUBLE', 'DATE']);\n```\n\n**Force Header/No Header:** If header detection fails (common if all columns look like strings):\n\n```sql\nCopy code\n\n-- Force header presence\nSELECT * FROM read_csv('file.csv', header = true);\n\n-- Or no header with custom names\nSELECT * FROM read_csv('file.csv',\n header = false,\n names = ['colA', 'colB', 'colC']);\n```\n\n**Date/Timestamp Formats:** If dates aren't ISO 8601 (`YYYY-MM-DD`) or times aren't standard:\n\n```sql\nCopy code\n\n SELECT * FROM read_csv('file.csv',\n  dateformat = '%m/%d/%Y',\n  timestampformat = '%Y-%m-%dT%H:%M:%S.%f');\n```\n\n**Everything is a String:** If you want to load _everything_ as `VARCHAR` and deal with types later:\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('file.csv', all_varchar = true);\n```\n\n**Which Columns Can Be NULL?:** By default, an empty field is treated as `NULL`. If empty strings should be valid values:\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('file.csv',\n  force_not_null = ['column_name1', 'column_name2']);\n```\n\n**Clean Up Names:** Got headers with spaces or weird characters?\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('file.csv', normalize_names = true);\n```\n\nThis will [automatically clean them up](https://duckdb.org/docs/stable/data/csv/overview.html#parameters) (replacing non-alphanumeric with `_`, etc.) during import.\n\n## DuckDB's CSV Parser: Champion of the Pollock Benchmark\n\nFor those really interested in CSV robustness, there's an intriguing benchmark called [Pollock](https://hpi.de/naumann/projects/data-preparation/pollock.html) that evaluates how well different systems handle non-standard CSV files. The creators studied over 245,000 public CSV datasets to identify common violations of the RFC-4180 standard, then created test files with these issues.\n\nIn recent testing, DuckDB [ranked #1](https://github.com/HPI-Information-Systems/Pollock) in the benchmark when configured to handle problematic files, correctly reading 99.61% of the data across all test files. Even in auto-detect mode with minimal configuration, DuckDB still managed to read about 90.75% of the data correctly.\n\nThis is practical validation that the approaches we've covered in this article can handle the vast majority of real-world CSV issues you'll encounter.\n\n| System under test | Pollock score<br>(simple) | Pollock score<br>(weighted) |\n| --- | --- | --- |\n| DuckDB 1.2 | 9.961 | 9.599 |\n| SQLite 3.39.0 | 9.955 | 9.375 |\n| UniVocity 2.9.1 | 9.939 | 7.936 |\n| LibreOffice Calc 7.3.6 | 9.925 | 7.833 |\n| SpreadDesktop | 9.929 | 9.597 |\n| SpreadWeb | 9.721 | 9.431 |\n| Python native csv 3.10.5 | 9.721 | 9.436 |\n| Pandas 1.4.3 | 9.895 | 9.431 |\n| MySQL 8.0.31 | 9.587 | 7.484 |\n| Mariadb 10.9.3 | 9.585 | 7.483 |\n\n## **Taking Flight Beyond the Basics**\n\nWe've covered quite a bit in our journey through DuckDB's CSV capabilities\u2014from diagnosing issues with `sniff_csv` to handling errors with `ignore_errors` and `store_rejects`, merging inconsistent schemas with `union_by_name`, and fine-tuning the whole process with various overrides.\n\nWhat I've come to appreciate about DuckDB is that its CSV reader isn't just a basic loader\u2014it's a sophisticated tool designed to handle real-world data messiness directly within SQL. Most data tools can handle the perfect CSV file, but it's how they deal with the imperfect ones that really matters in day-to-day work.\n\nBy understanding these slightly more advanced options, you can often avoid external preprocessing steps, keeping your data loading logic right within your SQL workflow. The result is cleaner pipelines that are less likely to waddle when faced with unexpected CSV quirks.\n\nThe next time a tricky CSV lands on your desk, remember these techniques. They might just save you some time and frustration, letting you get back to the more interesting parts of data analysis sooner. Happy querying!\n\n### TABLE OF CONTENTS\n\n[Peeking Under the Hood: sniff\\_csv](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#peeking-under-the-hood-sniffcsv)\n\n[Wrangling the Errors: ignore\\_errors, store\\_rejects, and More](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#wrangling-the-errors-ignoreerrors-storerejects-and-more)\n\n[Handling Shifting Schemas: union\\_by\\_name](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#handling-shifting-schemas-unionbyname)\n\n[Fine-Tuning Detection and Overrides](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#fine-tuning-detection-and-overrides)\n\n[DuckDB's CSV Parser: Champion of the Pollock Benchmark](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#duckdbs-csv-parser-champion-of-the-pollock-benchmark)\n\n[Taking Flight Beyond the Basics](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#taking-flight-beyond-the-basics)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![MotherDuck lands on Tableau Cloud: Live, Fast Analytics Unleashed](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FTableau_Cloud_52bd53b821.png&w=3840&q=75)](https://motherduck.com/blog/tableau-cloud-motherduck/)\n\n[2025/05/06 - Jacob Matson](https://motherduck.com/blog/tableau-cloud-motherduck/)\n\n### [MotherDuck lands on Tableau Cloud: Live, Fast Analytics Unleashed](https://motherduck.com/blog/tableau-cloud-motherduck)\n\nUse MotherDuck to power your Tableau Cloud, Server, and Desktop dashboards.\n\n[![DuckDB Ecosystem: May 2025](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fduckdbeco_may_ca294a4d7f.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025/)\n\n[2025/05/08 - Simon Sp\u00e4ti](https://motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025/)\n\n### [DuckDB Ecosystem: May 2025](https://motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025)\n\nDuckDB Monthly #29: From Metabase to Doom, DuckDB powers everything.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 513,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 513
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md",
  [2m  â”‚[0m     "content": "---\ntitle: A Data Engineer's Guide to Columnar Storage\ncontent_type: guide\ndescription: A comprehensive guide to columnar databases for data engineers. Learn\n  about columnar vs. row-oriented architecture, vectorized execution, Parquet, and\n  implementation best practices.\npublished_date: '2025-10-17T00:00:00'\nsource_url: https://motherduck.com/learn-more/columnar-storage-guide\nindexed_at: '2025-11-25T10:52:22.420365'\ncontent_hash: 5dc1777d24bcb793\n---\n\nData keeps growing, and so does the challenge of analyzing it efficiently. While traditional row-based databases have served us well for transactional workloads, they often struggle when you need to crunch through massive datasets for analytics. This is where columnar storage comes in\u2014a different approach to organizing data that can dramatically improve query performance for analytical workloads.\n\nThis guide walks through everything you need to know about columnar storage: how it works, why it's faster for analytics, and how modern engines like DuckDB implement these concepts. Whether you're considering adopting a columnar database or just curious about the technology, you'll get a practical understanding of this foundational piece of the modern data stack.\n\n## What You'll Learn\n\nBy the end of this guide, you'll understand the fundamental differences between row-oriented and column-oriented data storage, the core principles that make columnar databases fast, and how modern engines implement techniques like vectorized execution and advanced compression. We'll also explore the role of standard columnar file formats like [Apache Parquet](https://parquet.apache.org/), examine how architectural patterns are evolving to handle collaboration and scale, and review common use cases and limitations of columnar systems.\n\n## Core Architectural Differences: Columnar vs. Row-Oriented Layouts\n\nThe fundamental difference between these two storage models comes down to data layout on disk. A row-oriented database stores all the values belonging to a single record together sequentially. In contrast, a column-oriented database groups all the values from a single column together.\n\nThis distinction might seem trivial, but it has profound implications for query performance, especially in analytical workloads. Consider a typical analytical query that calculates the average sale amount over time. In this scenario, you only need two columns from what might be a table with dozens of fields. A columnar system can read just those two columns, while a row-based system must read every single column for each row in the query range, even though most of that data is irrelevant.\n\n| Feature | Row-Oriented Storage | Column-Oriented Storage |\n|---|---|---|\nData Layout | Values for a single row are stored contiguously. | Values for a single column are stored contiguously. |\nPrimary Use Case | OLTP (Online Transaction Processing): frequent single-row reads, writes, and updates. |\n|\n\n**I/O Pattern****Compression****Query Performance**`SELECT *`\n\n).**Examples**## What Gives Columnar Databases Their Performance Edge?\n\nThe columnar layout enables several key advantages that make these systems particularly well-suited for analytical workloads.\n\n### Superior Compression\n\nWhen values of the same data type are stored together, they exhibit lower information entropy and can be compressed much more effectively than the mixed data types found in a row. It's common for columnar formats to achieve high compression ratios, which not only reduces storage costs but also speeds up queries by minimizing the amount of data that needs to be read from disk.\n\n### Reduced I/O for Analytical Queries\n\nThis is the primary performance benefit. By reading only the columns necessary for a query, columnar engines minimize disk I/O, which is often the main bottleneck in data processing. If your query needs 2 columns out of 100, a columnar system might only need to read 2% of the total data. Understanding this is the first step in solving what is effectively [a physics problem with a predictable hierarchy of performance bottlenecks](https://motherduck.com/learn-more/diagnose-fix-slow-queries/).\n\n### Efficient Aggregations\n\nWhen data is grouped by column, operations like `SUM()`\n\n, `AVG()`\n\n, or `COUNT()`\n\ncan be performed by tightly looping over a contiguous block of memory. This is far more efficient than fetching values scattered across different parts of rows.\n\n### Natural Fit for Modern CPUs\n\nThe columnar layout is highly compatible with modern CPU architecture. Data can be loaded into CPU registers in vectors\u2014batches of values\u2014and processed in parallel using [SIMD (Single Instruction, Multiple Data)](https://www.cs.columbia.edu/~kar/pubsk/simd.pdf) instructions. This leads to significant computational speedups by reducing instruction dispatch overhead and improving cache locality.\n\n## How Do High-Performance Engines Maximize Speed?\n\nModern columnar databases achieve their speed through a combination of sophisticated techniques. [DuckDB](https://motherduck.com/learn-more/what-is-duckdb/), an open-source embedded analytical database, serves as an excellent reference implementation of these principles.\n\n### Vectorized Query Execution\n\nInstead of processing data one row at a time (the \"Volcano\" or \"iterator\" model), [vectorized engines](https://hive.apache.org/docs/latest/vectorized-query-execution_34838326/) process data in batches (vectors) of thousands of values. This approach amortizes the overhead of function calls for query interpretation and fully leverages the parallel processing capabilities of modern CPUs by reducing cache misses. The entire query plan\u2014from scanning to filtering and aggregation\u2014operates on these vectors, keeping CPU caches full and pipelines efficient.\n\n### Smart Query Optimization\n\nHigh-performance engines intelligently minimize the amount of data they need to process. **Predicate pushdown** allows the engine to apply `WHERE`\n\nclause filters at the storage level, before data is even read into memory. This often works alongside **zone maps** (also called min/max indexing), where metadata for each data block is stored. If a query filters for `age > 40`\n\n, the engine can check the zone maps and skip reading any block where the maximum age is less than 40. DuckDB creates and uses these automatically to accelerate queries.\n\n### Out-of-Core Processing\n\nA key feature of robust columnar engines is their ability to handle datasets larger than available RAM. When a memory-intensive operation like a large sort or join exceeds the memory limit, the engine can offload intermediate data to temporary files on disk and process it in chunks. This allows engines like [DuckDB](https://duckdb.org/docs/stable/guides/performance/how_to_tune_workloads.html) to analyze datasets of 100GB or more on a laptop with only 16GB of RAM.\n\n## What Are the Key Columnar File Formats?\n\nIn the modern data stack, open columnar file formats have become essential for interoperability, allowing different tools and engines to work with the same data.\n\n### Apache Parquet: The Standard\n\nParquet has become the de facto standard columnar file format for data lakes. It's an open-source format highly optimized for analytical workloads, influenced by Google's Dremel paper. A Parquet file is structured into row groups, which contain column chunks. Each column chunk stores the data for a specific column within that row group and is further divided into pages. Crucially, Parquet files also store rich metadata and statistics\u2014min/max values, null counts\u2014for each column chunk, enabling the predicate pushdown and data skipping optimizations we discussed earlier.\n\n### Internal Storage Formats\n\nWhile engines like DuckDB have excellent readers for Parquet, they often use their own highly optimized internal columnar format for data stored within the database itself. Having control over their own format allows databases to implement specific compression schemes or indexing structures that are tightly integrated with their execution engine. DuckDB's internal format, for example, is designed to enable fast random access and efficient, ACID-compliant updates, which can be more challenging with file-based formats like Parquet that are typically immutable and require rewriting the entire file to change.\n\n| Feature | Apache Parquet | DuckDB Internal Format |\n|---|---|---|\nPrimary Design Goal | Write-once, read-many for data lake analytics. | High-performance, transactional (ACID) analytics with efficient updates. |\nStructure | File -> Row Groups -> Column Chunks -> Pages. | Row Groups -> Fixed-size blocks per column for random access. |\nMutability | Immutable; changes require rewriting the file. | Mutable; supports efficient INSERT, UPDATE, DELETE operations. |\nUse Case | Standard for data interchange and long-term storage in data lakes. | Optimized for active, high-performance analytical query processing within DuckDB. |\n\n## Which Compression Methods Provide Maximum Efficiency?\n\nColumnar databases use various specialized encoding techniques, often chosen automatically based on the data's characteristics, to maximize compression without sacrificing performance.\n\n| Compression Technique | How It Works | Best Suited For |\n|---|---|---|\nDictionary Encoding | Replaces frequently repeated values with smaller integer codes and stores a dictionary mapping. | Low to medium cardinality columns (e.g., country codes, product categories). |\nRun-Length Encoding (RLE) | Compresses sequences of identical values by storing the value once and a count of its repetitions. | Sorted columns or columns with long runs of identical values. |\nDelta Encoding | Stores the difference between consecutive values instead of the actual values. | Columns with slowly changing series data, like timestamps or sequential IDs. |\nFrame of Reference (FOR) | Subtracts a minimum value from a block of numbers and stores the smaller offsets. | Columns of integers with a limited range within blocks. |\nFSST (Fast Static Symbol Table) | Tokenizes strings and builds a static dictionary of common substrings to be represented by shorter codes. | High-cardinality string data like URLs or names where dictionary encoding is ineffective. |\n\n### Specialized String Compression\n\nFor string data that doesn't fit well with dictionary encoding\u2014like high-cardinality data such as URLs or names\u2014engines like DuckDB use specialized algorithms. FSST (Fast Static Symbol Table) tokenizes strings and builds a dictionary of common substrings, providing [good compression](https://www.vldb.org/pvldb/vol13/p2649-boncz.pdf) with very fast decompression speeds, often comparable to or better than LZ4.\n\n## How Have Data Platform Architectures Adapted to Columnar Storage?\n\nColumnar systems have evolved beyond monolithic data warehouses into flexible architectures that serve different needs. This evolution reflects a [broader industry shift](https://motherduck.com/blog/big-data-is-dead/), challenging the notion that all data problems are 'big data' problems.\n\n### The In-Process Model\n\nAn in-process database runs directly within an application, eliminating the need for a separate server and network communication. This model provides significant power and simplicity for local data processing.\n\nDuckDB exemplifies this approach as an embedded columnar database. It provides the power of a full analytical SQL engine in a lightweight library that can be integrated directly into Python, R, Java, or C++ applications. Its strengths include incredible speed for interactive analysis in notebooks, local ETL pipelines, and powering data-intensive applications. However, the purely embedded nature presents challenges for team-based work, as it lacks built-in mechanisms for shared state, concurrent access, and centralized governance.\n\n### The Hybrid Model\n\nThis emerging architecture combines the \"local-first\" performance of an embedded engine with the collaboration and scalability of a managed cloud backend. It aims to provide the best of both worlds.\n\n[MotherDuck](https://motherduck.com/product/) implements this hybrid model by extending DuckDB to the cloud. Users can continue working with the familiar DuckDB engine on their local machines but seamlessly query and manage shared data stored centrally in MotherDuck's serverless platform. The query optimizer in this model can decide whether to execute parts of a query locally or remotely, transferring data as needed. This architecture addresses the collaboration challenges of a purely embedded approach by providing a shared source of truth, access controls, and reliable storage, while still leveraging local compute for performance and responsiveness.\n\n## What Are the Most Suitable Applications for Columnar Systems?\n\nThe right columnar architecture depends on the specific problem you're trying to solve.\n\n### Embedded Columnar Databases\n\nDuckDB and similar embedded engines excel in several scenarios. They're perfect for interactive data analysis in notebooks, where data scientists need to quickly explore and analyze gigabytes of data on their laptop. They also work well for local ETL and data transformation pipelines, providing an efficient engine for cleaning, transforming, and enriching data before loading it into another system. Additionally, they can be embedded to power analytical features like dashboards and reports directly within applications.\n\n### Hybrid Columnar Platforms\n\nSolutions like MotherDuck address different needs. They're ideal for collaborative analytics among small-to-medium-sized teams when groups need to work on the same datasets without the complexity of a full-scale data warehouse. They enable centralized data management with decentralized, local-first computation, allowing organizations to govern key datasets centrally while empowering analysts to work with them locally. They're also well-suited for building internal data tools that require shared state and access control.\n\n### A Case Study in Hybrid Efficiency: Trunkrs\n\nA compelling example of the hybrid model's success is the logistics company [Trunkrs](https://motherduck.com/case-studies/trunkrs-same-day-delivery-motherduck-from-redshift/). Previously hampered by a slow and costly Redshift setup, their daily operational meetings struggled with sluggish query performance, preventing deep analysis. After migrating to MotherDuck, they experienced immediately faster, \"snappier\" responses. This allowed their teams to drill down into performance issues in real-time, solving problems more effectively and reducing repeated mistakes. The move not only lowered their data platform's complexity and cost but also made their data feel more like a responsive application, matching the efficiency of their logistics operations.\n\n### Cloud Data Warehouses\n\nTraditional cloud data warehouses like Snowflake and BigQuery remain the best choice for enterprise-scale analytics on massive, petabyte-scale datasets that require distributed, massively parallel processing architecture. They're also essential for serving concurrent BI and reporting needs for hundreds or thousands of users.\n\n| Tool | Architecture | Ideal Use Case | Key Differentiator |\n|---|---|---|---|\nSQLite | Row-Oriented, In-Process | General-purpose embedded database for applications. | Optimized for transactional integrity (OLTP) and low-latency writes. |\nDuckDB | Columnar, In-Process | High-performance interactive analytics and local ETL on a single machine. | \"\n|\n\n**MotherDuck****Snowflake / BigQuery**## What Are Some Effective Strategies for Columnar Database Implementation?\n\nTo maximize the benefits of a columnar system, it's crucial to move beyond a simple lift-and-shift approach and adopt practices that leverage its unique architecture.\n\n**Optimize Physical Data Layout**\n\nThe physical organization of data on disk is paramount.\n\n**Sorting and Clustering:**The single most effective optimization is sorting or clustering your data by a commonly filtered column, like a timestamp. This groups related data together, which dramatically improves the efficiency of compression (e.g., Run-Length Encoding) and enables highly effective data skipping via zone maps.**Partitioning:**Apply partitioning (e.g., by date range) to break large tables into smaller, more manageable pieces. Modern systems like Snowflake use micro-partitions, which are small, immutable chunks of data (e.g., 50-500MB) that contain per-column metadata. This allows the query optimizer to prune (ignore) the vast majority of partitions that are not relevant to a query, drastically reducing scan times.\n\n**Manage Data Ingestion and Updates Efficiently**\n\nColumnar stores are optimized for bulk operations, not single-row writes.\n\n**Batch Data Loads:**Always load data in large batches (thousands or millions of rows at a time). This allows the system to write directly into optimized, compressed columnar segments. Trickle-feeding data one row at a time leads to poor compression and fragmented storage, which harms query performance.**Use Delta Stores for Updates:**To handle updates without sacrificing read performance, many systems use a \"delta store\" or \"write-optimized store.\" New writes and updates go into this separate, row-oriented or memory-optimized store. Periodically, an efficient, often multi-core-aware, merge process combines the delta store into the main read-optimized column store.\n\n**Tune Query Execution and Schema Design**\n\n**Leverage Late Materialization:**Advanced columnar engines use a technique called[late materialization](https://15721.courses.cs.cmu.edu/spring2024/papers/04-execution1/shrinivas-icde2013.pdf). Instead of reconstructing full rows early in the query plan, the engine operates on column vectors for as long as possible, only materializing (stitching together) the final rows needed for the result set. This minimizes data movement and memory overhead.**Denormalize Strategically:**While normalization is essential for OLTP, analytical queries often perform better on wider, denormalized tables that eliminate the need for costly joins at query time. This is a trade-off that increases storage but can dramatically improve the performance of read-heavy workloads.**Use Materialized Views:**For complex, repetitive queries that power dashboards, pre-compute the results into a materialized view. This allows the database to serve results instantly by reading from the pre-calculated table rather than re-running the entire query.\n\n**Embrace Adaptive and Learned Optimizations**\n\n**Analyze Query Patterns:**Understanding which columns are used in filters, joins, and aggregations is key. This knowledge informs decisions about sort keys, partitioning, and denormalization.**Utilize Adaptive Techniques:**Modern systems are moving towards adaptive and AI-driven optimizations. This includes adaptive indexing, where indexes are refined on-the-fly based on query patterns, and learned encoding advisors that can predict the optimal compression scheme for each column, further reducing latency and storage.\n\n## What Are the Inherent Constraints and Compromises?\n\nDespite their analytical prowess, columnar databases aren't a universal solution and come with important trade-offs.\n\n### Transactional Workload Challenges\n\nColumnar systems are poorly suited for workloads with frequent, single-row inserts, updates, or deletes. Modifying a single logical row requires writing to multiple, separate column files, which is highly inefficient compared to row-based systems.\n\n### Write Amplification\n\nTo maintain performance, columnar systems often write data in large, immutable blocks. Updating data typically involves rewriting entire blocks, a phenomenon known as write amplification. This can also create challenges for handling high-concurrency writes.\n\n### SELECT * Query Performance\n\nQueries that retrieve all columns of a table can be slower on a columnar database than on a row-oriented one. This is because the engine has to perform the costly operation of reconstructing the row from various column files.\n\n### Small Dataset Overhead\n\nFor tables with only a few thousand rows, the overhead of columnar processing and metadata can sometimes make them slower than a simple row-oriented database like SQLite. Columnar storage really shines at scale.\n\nWARNING: Not a Silver Bullet While columnar databases offer significant advantages for analytical workloads, they're specialized tools. Don't expect them to replace your transactional databases or perform well on small datasets with heavy write workloads.## Is SQL the Standard for Columnar Databases?\n\nOne of the biggest advantages of modern columnar databases is that they almost universally use standard SQL as their query language. From an analyst's perspective, there's no difference between writing a query for a columnar database like DuckDB and a row-based database like PostgreSQL. They support familiar `SELECT`\n\n, `JOIN`\n\n, `GROUP BY`\n\n, and window functions.\n\nThis standardization means data teams can adopt powerful columnar technology without retraining analysts or abandoning existing SQL-based tools. While some systems may offer specialized functions as extensions, the core language remains the same.\n\n## The Path Forward\n\nColumnar storage has become a critical foundational technology for modern data analytics. As [noted](https://iaeme.com/MasterAdmin/Journal_uploads/IJITMIS/VOLUME_16_ISSUE_1/IJITMIS_16_01_038.pdf) in the *International Journal of Information Technology & Management Information System*:\n\n\"The adoption of columnar storage formats has revolutionized data processing capabilities in modern big data ecosystems, fundamentally transforming how organizations analyze and derive value from their data assets... This transformation extends beyond mere performance improvements, encompassing enhanced data compression, improved query optimization, and better resource utilization in distributed computing environments\" (Tatikonda, Pruthvi. International Journal of Information Technology & Management Information System, 2025).\n\n\nThe concept, first comprehensively introduced in a 1985 paper by GP Copeland and SN Khoshafian, has been refined over decades by systems like MonetDB and C-Store. By optimizing for how analytical queries actually access data, it provides substantial performance improvements over traditional row-based systems. The evolution of columnar architecture\u2014from massive cloud data warehouses to powerful embedded engines like DuckDB and innovative hybrid platforms like MotherDuck\u2014shows a clear trend toward making high-performance analytics more accessible, flexible, and scalable.\n\nUnderstanding the principles, advantages, and trade-offs of this architecture is essential for any data engineer looking to build efficient data platforms. Whether you're processing data locally, collaborating with a team, or scaling to enterprise volumes, there's likely a columnar solution that fits your needs. The key is matching the right architectural pattern to your specific use case and requirements.\n\n## Start Using MotherDuck to See Columnar Storage in Action\n\nMotherDuck offers a free trial for 21 days based on the open source DuckDB database engine, optimized with columnar storage for analytics. [Get Started Now](https://app.motherduck.com/)\n\n## Frequently Asked Questions\n\n### What is driving the widespread adoption of columnar databases for analytical tasks?\n\nThey are significantly faster and more cost-effective for analytical workloads. By only reading the columns needed for a query and using superior compression, they minimize I/O and reduce storage costs, allowing for interactive analysis on very large datasets.\n\n### In what ways does columnar storage boost performance and lower expenses?\n\nIt improves performance by dramatically reducing the amount of data read from disk for typical analytical queries. It reduces costs through high compression ratios that shrink the storage footprint and by lowering data scan costs in cloud environments.\n\n### When are columnar databases not the right choice?\n\nTheir primary limitation is poor performance on transactional (OLTP) workloads that involve frequent single-row inserts, updates, or deletes. They're specialized tools for analytics and aren't meant to replace general-purpose transactional databases.\n\n### What are some guidelines for implementing a columnar database effectively?\n\nThe most critical practices are sorting your data on ingestion based on common query filters, loading data in batches rather than row-by-row, and being explicit about selecting only the columns you need in your queries.\n\n### In which scenarios is an embedded database like DuckDB a strong option?\n\nDuckDB excels for interactive data analysis on a local machine (like in a Python notebook), for building efficient local ETL pipelines, and for embedding analytical capabilities directly into applications without needing an external server.\n\n### What kind of challenges does a hybrid architecture like MotherDuck overcome?\n\nA hybrid architecture addresses the collaboration and scaling challenges of purely embedded tools. It allows teams to work with centralized, managed datasets while still benefiting from the speed and simplicity of local processing, effectively bridging the gap between individual laptop analysis and shared cloud environments.\n\nStart using MotherDuck now!",
  [2m  â”‚[0m     "numLines": 255,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 255
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me search for any additional content about JSON files and loading performance:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/analyze-json-data-using-sql.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/analyze-json-data-using-sql.md",
  [2m  â”‚[0m     "content": "---\ntitle: analyze-json-data-using-sql\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/analyze-json-data-using-sql\nindexed_at: '2025-11-25T19:56:20.007742'\ncontent_hash: f6b3e80e1a28155a\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Analyze JSON Data Using SQL and DuckDB\n\n2024/01/10 - 12 min read\n\nBY\nDavid Neal\n\nYou have a deadline flying in fast and a boatload of data to dive through. You extract the data archive to discover all the files have a \u201c.json\u201d extension. Oh, no. JSON is for programmers, right? What do you do now?\n\nNo need to get your feathers in a fluff! DuckDB to the rescue!\n\nDuckDB is a featherlight yet powerful database that supports querying lots of data formats directly using SQL. It can query data locally on disk, in memory, in the cloud, or combine data from multiple sources in a single query!\n\nIn this post, we'll guide you through querying JSON data using DuckDB. You'll learn how to target the data you need with the precision of a duck snatching its favorite bread. Let\u2019s get quacking on how to query JSON data with DuckDB!\n\n## Prerequisites\n\n- Install DuckDB (instructions below).\n- Optional: View or download the [sample JSON data](https://github.com/reverentgeek/duckdb-json-tutorial) used in this tutorial.\n\n## What is JSON?\n\nJSON, which stands for JavaScript Object Notation, is a lightweight data format. It is designed to be fairly easy for humans to read and write, and easy for machines to parse and generate, making it a great way to share data. It was originally created for Web applications to share data between the browser and server and has become a standard for storing and sharing data in many other types of applications. Outside of the browser, JSON is typically stored in a text file with a `.json` extension.\n\nLet\u2019s waddle through some of the basics of JSON! JSON is built on two basic structures:\n\n- A collection of one or more name/value pairs surrounded by curly braces {}, each pair separated by commas.\n- A list of one or more values surrounded by brackets \\[\\], each value separated by commas.\n\nHere\u2019s an example:\n\n```json\nCopy code\n\n{\n  \"ducks\": [\\\n    {\\\n      \"name\": \"Quackmire\",\\\n      \"color\": \"green\",\\\n      \"actions\": [\\\n        \"swimming\",\\\n        \"waddling\",\\\n        \"quacking\"\\\n      ]\\\n    },\\\n    {\\\n      \"name\": \"Feather Locklear\",\\\n      \"color\": \"yellow\",\\\n      \"actions\": [\\\n        \"sunbathing\"\\\n      ]\\\n    },\\\n    {\\\n      \"name\": \"Duck Norris\",\\\n      \"color\": \"brown\",\\\n      \"actions\": [\\\n        \"karate chopping bread\"\\\n      ]\\\n    }\\\n  ],\n  \"totalDucks\": 3\n}\n```\n\n- All of the data is wrapped in curly braces {}, like a cozy nest.\n- Each duck is part of a \"ducks\" array (like a flock of ducks in a row), wrapped by square brackets \\[\\].\n- Each duck in the array is a set of \"name/value\" pairs. For example, \"name\": \"Duck Norris\" tells us one duck's name is Duck Norris.\n\nCurly braces {} are used to represent an object. You might also think of an object as a record, thing, or entity. The name/value pairs are sometimes called properties. The value associated with the name can represent text (a string), a number, true/false (a boolean), a collection of values (an array), or a nested object. An array is represented by square brackets \\[\\] and can be an ordered list of strings, numbers, booleans, or objects.\n\nThe JSON format can represent data structures ranging from simple to complex with nested objects and arrays! This makes it a great way to express and exchange data.\n\n## Install and execute DuckDB\n\nIf you don\u2019t already have DuckDB installed, flap on over to [duckdb.org](https://duckdb.org/#quickinstall) and follow the instructions for your operating system. In this tutorial, you\u2019ll be using DuckDB from the command line.\n\n- _Mac:_ Follow the Homebrew (`brew`) install instructions.\n- _Windows:_ Follow the `winget` install instructions.\n- _Linux:_ Download the appropriate archive for your OS and processor. Extract the `duckdb` executable binary from the archive to a folder where you easily execute it from your terminal.\n\n### Launch DuckDB from the command line\n\nAfter installing DuckDB, open (or reopen) your terminal or command prompt and enter the following to start an in-memory session of DuckDB.\n\n```sh\nCopy code\n\nduckdb\n```\n\n_Note: If you are running Linux, you\u2019ll want to change the current directory to where you extracted the binary and use `./duckdb` to execute the binary_\n\nIf all goes to plan, you should see a new `D` prompt ready for a command or SQL query, similar to the following.\n\n```sh\nCopy code\n\nEnter \".help\" for usage hints.\nConnected to a transient in-memory database.\nUse \".open FILENAME\" to reopen on a persistent database.\nD\n```\n\n### Run your first DuckDB SQL query\n\nFrom the `D` prompt on the command line, type in the following SQL query and press enter. Don\u2019t forget to include the semicolon at the end! SQL queries can span multiple lines, and the semicolon lets DuckDB know you are finished writing the query and it\u2019s ready to execute.\n\n```sql\nCopy code\n\nSELECT current_date - 7;\n```\n\nThe result returned should be the date from seven days ago. `current_date` is one of many SQL functions available, and can be useful for including in query results or filtering data.\n\n## Query JSON files with DuckDB\n\nThis ability to query raw files directly is the foundation of a modern [No-ETL approach](https://motherduck.com/learn-more/no-etl-query-raw-files/), which helps startups and lean teams avoid costly data engineering. In many cases, you can query data directly from a JSON file by specifying a path to the file.\n\n- Create a new text file named `ducks.json` and open it in a text editor.\n- Paste the following JSON data into the file and save it.\n\n```js\nCopy code\n\n[\\\n  {\\\n    \"id\": \"kA0KgL\",\\\n    \"color\": \"red\",\\\n    \"firstName\": \"Marty\",\\\n    \"lastName\": \"McFly\",\\\n    \"gender\": \"male\"\\\n  },\\\n  {\\\n    \"id\": \"dx3ngL\",\\\n    \"color\": \"teal\",\\\n    \"firstName\": \"Duckota\",\\\n    \"lastName\": \"Fanning\",\\\n    \"gender\": \"female\"\\\n  },\\\n  {\\\n    \"id\": \"FQ4dU1\",\\\n    \"color\": \"yellow\",\\\n    \"firstName\": \"Duck\",\\\n    \"lastName\": \"Norris\",\\\n    \"gender\": \"male\"\\\n  },\\\n  {\\\n    \"id\": \"JqS7ZZ\",\\\n    \"color\": \"red\",\\\n    \"firstName\": \"James\",\\\n    \"lastName\": \"Pond\",\\\n    \"gender\": \"male\"\\\n  },\\\n  {\\\n    \"id\": \"ZM5uJL\",\\\n    \"color\": \"black\",\\\n    \"firstName\": \"Darth\",\\\n    \"lastName\": \"Wader\",\\\n    \"gender\": \"male\"\\\n  }\\\n]\n```\n\nWith DuckDB running at the command line, paste the following query and press ENTER.\n\n```sql\nCopy code\n\nSELECT * FROM './ducks.json';\n```\n\nThe results should look similar to the following.\n\n```sh\nCopy code\n\nD SELECT * FROM './ducks.json';\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   id    \u2502  color  \u2502 firstName \u2502 lastName \u2502 gender  \u2502\n\u2502 varchar \u2502 varchar \u2502  varchar  \u2502 varchar  \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 kA0KgL  \u2502 red     \u2502 Marty     \u2502 McFly    \u2502 male    \u2502\n\u2502 dx3ngL  \u2502 teal    \u2502 Duckota   \u2502 Fanning  \u2502 female  \u2502\n\u2502 FQ4dU1  \u2502 yellow  \u2502 Duck      \u2502 Norris   \u2502 male    \u2502\n\u2502 JqS7ZZ  \u2502 red     \u2502 James     \u2502 Pond     \u2502 male    \u2502\n\u2502 ZM5uJL  \u2502 yellow  \u2502 Darth     \u2502 Wader    \u2502 male    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 5 rows                                   5 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Change DuckDB\u2019s output display\n\nIf you\u2019re not 100% satisfied with DuckDB\u2019s output to the console, there are lots of choices to customize the output. Type the following command to list the available output modes.\n\n```sh\nCopy code\n\n.help .mode\n```\n\nTry switching the output mode to column display and rerun the last query to see the difference.\n\n```sh\nCopy code\n\n.mode column\n```\n\n```sql\nCopy code\n\nD SELECT * FROM './ducks.json';\nid      color   firstName  lastName  gender\n------  ------  ---------  --------  ------\nkA0KgL  red     Marty      McFly     male\ndx3ngL  teal    Duckota    Fanning   female\nFQ4dU1  yellow  Duck       Norris    male\nJqS7ZZ  red     James      Pond      male\nZM5uJL  black   Darth      Wader     male\n```\n\nExperiment with other output modes until you find the one you like the most! If you want to switch back to the default DuckDB output mode, use the following command.\n\n```sh\nCopy code\n\n.mode duckbox\n```\n\n### Query multiple JSON files at once\n\nYou can query across multiple files at once using path wildcards. For example, to query all files that end with `.json`:\n\n```sh\nCopy code\n\nSELECT * FROM './*.json';\n```\n\nYou can query from specific files, too, such as:\n\n```sh\nCopy code\n\nSELECT * FROM './monthly-sales-2023*.json';\n```\n\n### Join JSON files together\n\nJust like joining tables together, if there is a common key in one or more different data files, you can join on that key.\n\nIn this example, we have one JSON file that contains a list of ducks in a sanctuary, including ID, name, and color. In another JSON file there is a log of all the the things the ducks were observed doing, surveyed every 10 minutes for a month. This second file has the date and time of the log, the action, and only the ID of the duck. To create a report that summarizes the ducks' activities, you would want to join them together.\n\n```sql\nCopy code\n\nSELECT ducks.firstName || ' ' || ducks.lastName AS duck_name,\n    samples.action,\n    COUNT(*) AS observations\nFROM    './samples.json' AS samples\nJOIN    './ducks.json' AS ducks ON ducks.id = samples.id\nGROUP BY ALL\nORDER BY 1, 3 DESC;\n```\n\n```sh\nCopy code\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   duck_name    \u2502         action          \u2502 observations \u2502\n\u2502    varchar     \u2502         varchar         \u2502    int64     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Captain Quack  \u2502 sleeping                \u2502          890 \u2502\n\u2502 Captain Quack  \u2502 quacking                \u2502          632 \u2502\n\u2502 Captain Quack  \u2502 eating                  \u2502          623 \u2502\n\u2502 Captain Quack  \u2502 annoying                \u2502          594 \u2502\n\u2502 Captain Quack  \u2502 swimming                \u2502          356 \u2502\n\u2502 Captain Quack  \u2502 waddling                \u2502          351 \u2502\n\u2502 Captain Quack  \u2502 sunbathing              \u2502          348 \u2502\n\u2502 Captain Quack  \u2502 twitching               \u2502          125 \u2502\n\u2502 Captain Quack  \u2502 flying                  \u2502          121 \u2502\n\u2502 Captain Quack  \u2502 dancing                 \u2502          117 \u2502\n\u2502 Captain Quack  \u2502 diving                  \u2502          106 \u2502\n\u2502 Captain Quack  \u2502 posting on social media \u2502           57 \u2502\n...\n```\n\n### Import JSON data into DuckDB for further analysis\n\nIf you have a lot of different JSON files, it might make sense to import the data into tables in your local DuckDB database. In the following example, you'll import the `ducks.json` file and `samples.json` together into one table.\n\n```sql\nCopy code\n\nCREATE OR REPLACE TABLE duck_samples AS\nSELECT CAST(samples.sampleTime AS date) AS sample_date,\n    ducks.firstName || ' ' || ducks.lastName AS duck_name,\n    samples.action,\n    COUNT(*) AS observations\nFROM    read_json('./samples.json', columns = { id: 'varchar', sampleTime: 'datetime', action: 'varchar' }) AS samples\nJOIN    './ducks.json' AS ducks ON ducks.id = samples.id\nGROUP BY ALL;\n```\n\nThis example uses the `read_json` function to customize the schema of the imported data, which can be useful for converting dates and times as the data is read and parsed from the JSON data.\n\nWith the `duck_samples` table populated, we can now use it to analyze the data in new ways, such as number of actions performed by all ducks on a given day.\n\n```sql\nCopy code\n\nSELECT ds.sample_date,\n    ds.action,\n    ds.observations,\n    round(( ds.observations / totals.total_obs ) * 100, 1) AS percent_total\nFROM ( SELECT sample_date, action, SUM(observations) AS observations FROM duck_samples GROUP BY ALL ) AS ds\n    JOIN ( SELECT sample_date, SUM(observations) AS total_obs FROM duck_samples GROUP BY ALL ) AS totals\n    ON ds.sample_date = totals.sample_date\nWHERE ds.sample_date = '2024-01-01'\nGROUP BY ALL\nORDER BY 3 DESC;\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sample_date \u2502         action          \u2502 observations \u2502 percent_total \u2502\n\u2502    date     \u2502         varchar         \u2502    int128    \u2502    double     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2024-01-01  \u2502 sleeping                \u2502         1551 \u2502          21.5 \u2502\n\u2502 2024-01-01  \u2502 quacking                \u2502          978 \u2502          13.6 \u2502\n\u2502 2024-01-01  \u2502 eating                  \u2502          977 \u2502          13.6 \u2502\n\u2502 2024-01-01  \u2502 annoying                \u2502          947 \u2502          13.2 \u2502\n\u2502 2024-01-01  \u2502 swimming                \u2502          612 \u2502           8.5 \u2502\n\u2502 2024-01-01  \u2502 waddling                \u2502          600 \u2502           8.3 \u2502\n\u2502 2024-01-01  \u2502 sunbathing              \u2502          598 \u2502           8.3 \u2502\n\u2502 2024-01-01  \u2502 flying                  \u2502          231 \u2502           3.2 \u2502\n\u2502 2024-01-01  \u2502 diving                  \u2502          220 \u2502           3.1 \u2502\n\u2502 2024-01-01  \u2502 twitching               \u2502          208 \u2502           2.9 \u2502\n\u2502 2024-01-01  \u2502 dancing                 \u2502          193 \u2502           2.7 \u2502\n\u2502 2024-01-01  \u2502 posting on social media \u2502           85 \u2502           1.2 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 12 rows                                                    4 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Query complex JSON data\n\nDepending on the structure of the JSON data you are working with it may be necessary to extract values from nested objects or arrays. Nested objects are referred to in DuckDB as a `struct` data type. In some cases, it's possible to access data directly in a struct using syntax that resembles schema or table namespaces. For example, imagine you have JSON file named `ducks-nested-name.json` with the following data.\n\n```json\nCopy code\n\n[\\\n  {\\\n    \"color\": \"red\",\\\n    \"name\": {\\\n      \"firstName\": \"Marty\",\\\n      \"lastName\": \"McFly\"\\\n    },\\\n    \"gender\": \"male\"\\\n  },\\\n  {\\\n    \"color\": \"teal\",\\\n    \"name\": {\\\n      \"firstName\": \"Duckota\",\\\n      \"lastName\": \"Fanning\"\\\n    },\\\n    \"gender\": \"female\"\\\n  },\\\n  {\\\n    \"color\": \"yellow\",\\\n    \"name\": {\\\n      \"firstName\": \"Duck\",\\\n      \"lastName\": \"Norris\"\\\n    },\\\n    \"gender\": \"male\"\\\n  }\\\n]\n```\n\nIf you query the file directly, the results would like the following.\n\n```sql\nCopy code\n\nD SELECT * FROM './ducks-nested-name.json';\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  color  \u2502                    name                     \u2502 gender  \u2502\n\u2502 varchar \u2502 struct(firstname varchar, lastname varchar) \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 red     \u2502 {'firstName': Marty, 'lastName': McFly}     \u2502 male    \u2502\n\u2502 teal    \u2502 {'firstName': Duckota, 'lastName': Fanning} \u2502 female  \u2502\n\u2502 yellow  \u2502 {'firstName': Duck, 'lastName': Norris}     \u2502 male    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nYou can access the nested values under `name` using the following syntax.\n\n```sql\nCopy code\n\nD SELECT color, name.firstName FROM './ducks-nested-name.json';\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  color  \u2502 firstName \u2502\n\u2502 varchar \u2502  varchar  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 red     \u2502 Marty     \u2502\n\u2502 teal    \u2502 Duckota   \u2502\n\u2502 yellow  \u2502 Duck      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nDuckDB provides the `unnest` function to help when dealing with nested data. Taking the first example with Quackmire, Feather Locklear, and Duck Norris, if you query this JSON data without using `unnest`, you'll see the following results.\n\n```sh\nCopy code\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                              ducks                              \u2502 totalDucks \u2502\n\u2502   struct(\"name\" varchar, color varchar, actions varchar[])[]    \u2502   int64    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [{'name': Quackmire, 'color': green, 'actions': [swimming, wa\u2026  \u2502          3 \u2502\\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\\n```\\\n\\\nTo make better use of the data in the `ducks` column, use the `unnest` function to destructure and flatten the data into their own columns.\\\n\\\n```sql\\\nCopy code\\\n\\\nD SELECT unnest(ducks, recursive:= true) AS ducks\\\nFROM './ducks-example.json';\\\n\\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\\n\u2502       name       \u2502  color  \u2502            actions             \u2502\\\n\u2502     varchar      \u2502 varchar \u2502           varchar[]            \u2502\\\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\\\n\u2502 Quackmire        \u2502 green   \u2502 [swimming, waddling, quacking] \u2502\\\n\u2502 Feather Locklear \u2502 yellow  \u2502 [sunbathing]                   \u2502\\\n\u2502 Duck Norris      \u2502 brown   \u2502 [karate chopping bread]        \u2502\\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\\n```\\\n\\\n## Query JSON data from an API\\\n\\\nDuckDB can also parse data directly from APIs that return JSON. The following example uses the [TVmaze API](https://www.tvmaze.com/api), a public API for TV shows.\\\n\\\n```sql\\\nCopy code\\\n\\\nD SELECT show.name, show.type, show.summary\\\nFROM read_json('https://api.tvmaze.com/search/shows?q=duck',\\\n       auto_detect=true);\\\n\\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\\n\u2502      show_name       \u2502  show_type   \u2502                            summary                             \u2502\\\n\u2502         json         \u2502     json     \u2502                              json                              \u2502\\\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\\\n\u2502 \"Duck Dynasty\"       \u2502 \"Reality\"    \u2502 \"<p>In <b>Duck Dynasty</b>, A&amp;E Network introduces the R\u2026  \u2502\\\n\u2502 \"Darkwing Duck\"      \u2502 \"Animation\"  \u2502 \"<p>In the city of St. Canard, the people are plagued by the\u2026  \u2502\\\n\u2502 \"Duck Dodgers\"       \u2502 \"Animation\"  \u2502 \"<p>Animated sci-fi series based on the alter ego of Looney \u2026  \u2502\\\n\u2502 \"Duck Patrol\"        \u2502 \"Scripted\"   \u2502 \"<p><b>Duck Patrol</b> deals with the activities of the offi\u2026  \u2502\\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\\n```\\\n\\\n## Learn more about DuckDB\\\n\\\nTo learn more about what you can do with DuckDB, check out the [DuckDB Snippets Library](https://duckdbsnippets.com/) or download a free copy of [DuckDB in Action](https://motherduck.com/duckdb-book-brief).\\\n\\\n### TABLE OF CONTENTS\\\n\\\n[Prerequisites](https://motherduck.com/blog/analyze-json-data-using-sql/#prerequisites)\\\n\\\n[What is JSON?](https://motherduck.com/blog/analyze-json-data-using-sql/#what-is-json)\\\n\\\n[Install and execute DuckDB](https://motherduck.com/blog/analyze-json-data-using-sql/#install-and-execute-duckdb)\\\n\\\n[Query JSON files with DuckDB](https://motherduck.com/blog/analyze-json-data-using-sql/#query-json-files-with-duckdb)\\\n\\\n[Query complex JSON data](https://motherduck.com/blog/analyze-json-data-using-sql/#query-complex-json-data)\\\n\\\n[Query JSON data from an API](https://motherduck.com/blog/analyze-json-data-using-sql/#query-json-data-from-an-api)\\\n\\\n[Learn more about DuckDB](https://motherduck.com/blog/analyze-json-data-using-sql/#learn-more-about-duckdb)\\\n\\\nStart using MotherDuck now!\\\n\\\n[Try 21 Days Free](https://motherduck.com/get-started/)\\\n\\\nGet Started\\\n\\\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\\\n\\\n### Subscribe to motherduck blog\\\n\\\nE-mail\\\n\\\nSubscribe to other MotherDuck Updates\\\n\\\nSubmit\\\n\\\n## PREVIOUS POSTS\\\n\\\n[![Why Python Developers Need DuckDB (And Not Just Another DataFrame Library)](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Fwhy_pythondev_1_22167e31bf.png&w=3840&q=75)](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/)\\\n\\\n[2025/10/08 - Mehdi Ouazza](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/)\\\n\\\n### [Why Python Developers Need DuckDB (And Not Just Another DataFrame Library)](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries)\\\n\\\nUnderstand why a database is much more than just a dataframe library\\\n\\\n[![DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_4_1_b6209aca06.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\\\n\\\n[2025/10/09 - Alex Monahan, Garrett O'Brien](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\\\n\\\n### [DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/blog/announcing-duckdb-141-motherduck)\\\n\\\nMotherDuck now supports DuckDB 1.4.1 and DuckLake 0.3, with new SQL syntax, faster sorting, Iceberg interoperability, and more. Read on for the highlights from these major releases.\\\n\\\n[View all](https://motherduck.com/blog/)\\\n\\\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 542,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 542
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now I have enough information to write a comprehensive answer. Let me write it to the specified file:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_1.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "create",
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_1.md",
  [2m  â”‚[0m   "content": "<5184 chars>",
  [2m  â”‚[0m   "structuredPatch": [],
  [2m  â”‚[0m   "originalFile": null
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I've completed the task. Here's a summary of what I found:
  [94mâ”‚[0m 
  [94mâ”‚[0m ## Summary
  [94mâ”‚[0m 
  [94mâ”‚[0m Based on searching through the `.kurt/sources/` directory, the **most efficient file formats for loading data into MotherDuck** are:
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. **Apache Parquet** (Most Efficient) - The recommended format due to:
  [94mâ”‚[0m    - Columnar storage that aligns with DuckDB's analytical engine
  [94mâ”‚[0m    - Superior compression (dictionary, run-length, delta encoding)
  [94mâ”‚[0m    - Reduced I/O through column pruning and predicate pushdown
  [94mâ”‚[0m    - Native integration with DuckDB
  [94mâ”‚[0m 
  [94mâ”‚[0m 2. **CSV Files** - Well-supported with DuckDB's exceptional parser:
  [94mâ”‚[0m    - Auto-detection of schema and dialects
  [94mâ”‚[0m    - #1 ranking on Pollock benchmark (99.61% accuracy)
  [94mâ”‚[0m    - Good error handling options for messy data
  [94mâ”‚[0m 
  [94mâ”‚[0m 3. **JSON Files** - Supported but less efficient:
  [94mâ”‚[0m    - Auto-detection with `read_json_auto`
  [94mâ”‚[0m    - Support for nested data via `unnest`
  [94mâ”‚[0m    - Best for semi-structured data and APIs
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer has been written to `/tmp/answer_without_kg_1.md` with full details and source citations from 6 relevant files in the Kurt sources directory.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 23,636 | Cost: $0.7129
     Cumulative: 23,636 tokens | $0.7129
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent clearly states that it has completed the task and provides a summary, indicating that it is not soliciting further user input.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 12 | Duration: 73.4s
â•‘    Tokens: 23,636 | Cost: $0.7129
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  Running LLM judge evaluation...
   ğŸ§  LLM Judge score: 1.00

======================================================================
â“ Question 2/10
...How does MotherDuck integrate with DuckDB?
======================================================================


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 2
â”‚ Answer this question by searching through the .kurt/sources/ directory:

...How does MotherDuck integrate with DuckDB?

Required Steps:
1. Search for relevant markdown files in .kurt/sources/ using glob and grep
2. Read the most relevant files (aim for 3-5 most relevant sources)
3. As you read each file, keep track of:
   - The file path
   - The file title/name
   - Key information extracted from that file
4. Synthesize a comprehensive answer based on the information found
5. Write your answer to: /tmp/answer_without_kg_2.md

IMPORTANT - Your answer MUST have this structure:
```markdown
# Answer

[Your comprehensive answer here, drawing from the sources you read]

## Sources

List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
[etc.]

DO NOT use URLs or document titles as links. Show the actual file path you read with the Read tool.
```

The Sources section is MANDATORY - you must list every file you read to answer the question.

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [96mğŸ”§ TOOL:[0m Grep â†’ DuckDB.*integrat|integrat.*DuckDB

  [96mğŸ”§ TOOL:[0m Grep â†’ MotherDuck.*DuckDB|DuckDB.*MotherDuck

  [96mğŸ”§ TOOL:[0m Grep â†’ hybrid execution|dual execution
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/data-discoverability-secoda-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/scaling-duckdb-panel-ondemand.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/take-flight-with-the-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-if-sql-queries-returned-results-instantly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/spatial-data-management-with-duckdb-ft-mattforrest.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/speed-querying-stackoverflow-data-with-duckdb-ft-michael-hunger.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/sql-ide-safari-harlequin-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/take-flight-with-dbt-and-duckdb-dropping-dev-warehouse-costs-to-zero.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-bad-the-good-and-the-ugly-navigating-the-new-waves-of-machine-learning-ft-adi-polak.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-future-of-bi-exploring-the-impact-of-bi-as-code-tools-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-power-of-wasm-for-analytics-duckdb-in-the-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-surprising-birth-of-duckdb-ft-co-creator-hannes-muhleisen.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-in-100-seconds-by-a-duck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/moving-forward-from-ad-hoc-reports-with-duckdb-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/no-sql-standard-for-7-years.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/panel-fixing-the-data-engineering-lifecycle-coalesce-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/is-bi-too-big-for-small-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/is-duckdb-the-secret-to-unlocking-your-gis-potential.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/leveraging-duckdb-and-delta-lake-together.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/lies-damn-lies-and-benchmarks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/machine-learning-and-ai-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-an-impact-with-a-research-project-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/monte-carlo-simulations-with-duckdb-ft-james-mcneill.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/more-than-a-vibe-ai-driven-sql-that-actually-works.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-ga-produck-tour.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/faster-data-pipelines-development-with-mcp-and-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/fixing-sql-spaghetti-effective-refactoring-techniques.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/friendly-sql-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/getting-started-with-motherduck-webinar.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/going-beyond-the-dataframe.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-bootstrap-a-data-warehouse-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ibis-one-library-to-query-any-backend.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-streamlit-crafting-dynamic-dashboards-and-data-apps.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-vs-pandas-vs-polars-for-python-devs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdbs-wild-ride-in-the-open-source-world-ft-co-creator-mark-raasveldt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ducklake-big-data-small-coalesce-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-and-recommenders-a-lightning-fast-synergy-ft-khalil-muhammad.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-dataviz-end-to-end-data-engineering-project-33.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-dbt-accelerating-the-developer-experience-with-local-power.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-dbt-end-to-end-data-engineering-project-23.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-experiments-peeking-into-the-future-of-analytics-ft-christophe-blefari.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-iceberg-the-future-of-lightweight-data-management.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-mosaic-interactive-insights-on-large-datasets.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-python-end-to-end-data-engineering-project-13.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/big-data-is-dead-long-live-hot-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/boosting-datasets-discoverability-for-ai-using-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/build-bigger-with-small-ai-running-small-models-locally.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/building-data-apps-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/can-duckdb-replace-your-data-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/cube-motherduck-data-app-monetization-success.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/cultivating-growth-how-gardyn-scaled-its-data-operations-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/data-visualisation-tools-with-duckdb-motherduck-hexpreset.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/4-lightning-talks-on-practical-ai-workflows-from-notion-1password-motherduck-evidence.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-duck-in-the-hand-is-worth-two-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-new-paradigm-for-data-visualization-with-just-sql-markdown.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ai-powered-bi-can-llms-really-generate-your-dashboards-ft-michael-driscoll.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/an-evolving-dag-for-the-llm-world-julia-schottenstein-of-langchain-at-small-data-sf-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/beyond-data-engineering-ft-joe-reis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/big-data-is-dead-analytics-is-alive.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/sql-duckdb-book-form.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/eu-region.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/postgres-integration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/quacking-around-the-web.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-OLAP.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/local-dev-cloud-prod-dagster-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/privacy-policy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/product-analytics-motherduck-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/sql-group-by-examples-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/web-assembly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-use-cases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/pandas-dataframes-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-struct-nested-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/ducklake-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-application-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-lake-vs-data-warehouse-vs-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-warehouse-tco.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/hack-night.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/index.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/acid-transactions-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/big-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/filter.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/foreign key.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/metadata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/primary key.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/relational API.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/relational database.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/relational object.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/result.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/storage layer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/storage.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/time-series.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/window functions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/auto inference.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/column.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/command line.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data app.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data build tool (dbt).md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data engineering.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data load tool (dlt).md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data model.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data pipeline.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data type.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data visualization.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/database.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/dataset.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/SQL analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/SQLAlchemy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Java programming language.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/JavaScript programming language.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Julia programming language.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Jupyter notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Linux.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/MotherDuck extension.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Online Analytical Processing (OLAP).md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Pandas DataFrames.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/PIVOT clause.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Plot.ly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/PyPi.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Python.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/S3 bucket.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/SELECT statement.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Common Table Expressions (CTEs).md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Dagster.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/DB-API 2.0.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/DB-API.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/DuckDB CLI.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/DuckDB.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/ELT.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/ENUM.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/ETL.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/EXPLAIN clause.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/GitHub.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Go programming language.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/INSERT statement.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/JSON.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/getting-started-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/7-Zip.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/AI.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/ALTER TABLE statement.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Amazon S3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Apache Arrow.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Apache Superset.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/API.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Business Intelligence (BI).md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/C/C++.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/CLI.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Cloud Storage.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/CSV.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/sqlmesh.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/streamlit.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/superset.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/tableau.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/zenlytic.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/events.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/metabase.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/ollama.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/omni.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/orchestra.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/power-bi.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/puppygraph.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/rill-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/sling-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/fabi.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/google-colab.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/hex.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/kestra.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/langchain.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/llamaindex.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/mage.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/marimo.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/meltano.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/cube.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/dagster.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/datalab.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/evidence.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/airflow.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/ascend.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/census.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/cloudquery.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-brief.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-summary-chapter2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-summary-chapter3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-summary-chapter4.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-news.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dosomething-non-profit-tco-cost-savings.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/finqore.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/gardyn.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/kultura-capital.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/layers-multi-tenant-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/trunkrs-same-day-delivery-motherduck-from-redshift.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/udisc-motherduck-sports-management.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/community-and-open-source.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/careers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dexibit.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dominik-moritz.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-future-of-bi-bi-as-code-duckdb-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-simple-joys-of-scaling-up.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/unstructured-analysis-tensorlake-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vector-technologies-ai-data-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vibe-coding-sql-cursor.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-everybody-hates-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-learn-sql-in-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-embeddings-for-semantic-meaning-in-text-and-rag.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-keyboard-shortcuts-for-joyful-querying.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-llm-prompt-function-gpt-models.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streaming-data-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streaming-oracle-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streamkap-mysql-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streamlining-ai-agents-duckdb-rag-solutions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/summer-data-engineering-roadmap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/tableau-cloud-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/simplifying-iot-analytics-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/six-reasons-duckdb-slaps.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-manifesto.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-recap-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-recap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-workshops-agenda.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/solving-advent-code-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/spark-ducklake-getting-started.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/secoda-motherduck-integration-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/semantic-layer-duckdb-tutorial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-faker-duckdb-exploration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/quacktosql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/read-scaling-preview.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/redshift-files-hunt-for-big-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/scaling-duckdb-with-ducklings.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg to motherduck at motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg_duckdb-postgresql-extension-for-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-duckdb-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-mooncake-columnstore.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/postgres-duckdb-options.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/preswald-health-data-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pushing-geo-boundaries-with-motherduck-geobase.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-vercel-marketplace-native-integration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-window-functions-in-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/new-collaboration-sharing-motherduck-data-warehouse-organization-auto-join.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/perf-is-not-enough.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-in-europe.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-kestra-etl-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-open-for-all-with-series-b.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-reflections-sigmod-pods-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-union-orchestration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-headquarters-seattle-opening.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/llm-data-pipelines-prompt-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/local-duckdb-ui-visual-data-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/log-processing-savings-bacalhau-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/in-the-news-podcasts-conferences.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-column-explorer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-fixit-ai-sql-error-fixer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-motherduck-for-business-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/json-log-analysis-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/google-sheets-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/guide-to-coalesce-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/hello-world.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/how-to-extract-analytics-from-bluesky.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/galileo-world-geospatial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/geospatial-for-beginner-duckdb-spatial-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/getting-started-ducklake-table-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/getting-started-gis-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/git-for-data-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/fake-duck-game.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-ducks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/fix-outdated-llm-documentation-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/flight-sql-vs-rest-vs-jdbc.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/from-data-lake-to-lakehouse-duckdb-portable-catalog.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-wasm-in-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/ducklake-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/effortless-etl-unstructured-data-unstructuredio-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/estuary-streaming-cdc-replication.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/fabi-ai-llm-prompt-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-python-e2e-data-engineering-project-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-text2sql-llm.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-the-great-federator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-versus-pandas-versus-polars.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-three.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-two.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-excel-extension.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-on-edge-raspberry-pi.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-puppygraph-graph-model-on-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-one.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-seven.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-six.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-ten.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-nine.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eight.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eleven.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-five.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-four.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/dosomething-motherduck-data-warehouse-ROI.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-110-hidden-gems.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-cognee-sql-analytics-graph-rag.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineer-highlights-PyConDE-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-essential-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-infrastructure-devops.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineers-answer-10-top-reddit-questions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-nov-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-oct-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/differential-storage-building-block-for-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/building-data-applications-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/building-motherduck-partner-ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-app-generator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-small-data-sf-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/big-data-is-dead.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/bigquery-to-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-snippet-sets-with-motherduck-sharing-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-mega-giga-instance-sizes-huge-scale.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-series-seed-and-a.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-sqlite-databases-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-x-data-nodejs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-data-in-azure-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-json-data-using-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/sheila-sitaram.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/simon-spati.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/sung-won-chung.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/till-dohmen.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/tino-tereshko.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/vasilije-markovic.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/zhenni-wu.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/pranav-aurora.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/ryan-boyd.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/jacob-matson.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/jelte-fennema-nio.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/jordan-tigani.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/joseph-hwang.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/luciano-galvao-filho.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/marcos-ortiz.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/mehdi-ouazza.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/michael-hunger.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/boaz-leskes.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/brett-griffin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/christophe-oudar.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/daniel-palma.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/emily-lucek.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/garrett-obrien.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/graham-wetzler.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/hamilton-ulmer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/about-us.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/adithya-krishnan.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/aditya-aomani.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/alex-monahan.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/andrew-witten.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/anna-geller.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 481
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/take-flight-with-the-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/going-beyond-the-dataframe.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ibis-one-library-to-query-any-backend.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-vs-pandas-vs-polars-for-python-devs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ducklake-big-data-small-coalesce-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-python-end-to-end-data-engineering-project-13.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/sql-duckdb-book-form.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/postgres-integration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/react-components-testing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/product-analytics-motherduck-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-struct-nested-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/MotherDuck extension.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Plot.ly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Python.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/DuckDB CLI.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/DuckDB.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Apache Arrow.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/C/C++.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/sqlmesh.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/streamlit.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/superset.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/tableau.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/zenlytic.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/metabase.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/omni.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/orchestra.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/power-bi.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/puppygraph.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/rill-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/sling-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/fabi.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/google-colab.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/hex.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/kestra.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/langchain.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/llamaindex.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/mage.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/marimo.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/meltano.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/cube.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/dagster.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/datalab.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/evidence.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/airflow.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/ascend.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/census.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/cloudquery.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-brief.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-summary-chapter2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dosomething-non-profit-tco-cost-savings.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/kultura-capital.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/trunkrs-same-day-delivery-motherduck-from-redshift.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/udisc-motherduck-sports-management.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/atm-com-analytics-costs-sql-expressibility.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-future-of-bi-bi-as-code-duckdb-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-simple-joys-of-scaling-up.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vector-technologies-ai-data-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-learn-sql-in-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-embeddings-for-semantic-meaning-in-text-and-rag.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-llm-prompt-function-gpt-models.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streaming-data-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streamlining-ai-agents-duckdb-rag-solutions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/tableau-cloud-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/six-reasons-duckdb-slaps.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-manifesto.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-recap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-workshops-agenda.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/secoda-motherduck-integration-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/semantic-layer-duckdb-tutorial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/redshift-files-hunt-for-big-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg_duckdb-postgresql-extension-for-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-duckdb-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-mooncake-columnstore.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/postgres-duckdb-options.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pushing-geo-boundaries-with-motherduck-geobase.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/new-collaboration-sharing-motherduck-data-warehouse-organization-auto-join.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/perf-is-not-enough.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-kestra-etl-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-open-for-all-with-series-b.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-union-orchestration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-headquarters-seattle-opening.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/local-duckdb-ui-visual-data-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/log-processing-savings-bacalhau-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-column-explorer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-fixit-ai-sql-error-fixer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/google-sheets-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/guide-to-coalesce-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/geospatial-for-beginner-duckdb-spatial-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/getting-started-gis-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/git-for-data-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/flight-sql-vs-rest-vs-jdbc.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-wasm-in-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/ducklake-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/effortless-etl-unstructured-data-unstructuredio-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/estuary-streaming-cdc-replication.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/fabi-ai-llm-prompt-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-text2sql-llm.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-the-great-federator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-two.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-excel-extension.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-on-edge-raspberry-pi.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-puppygraph-graph-model-on-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-seven.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-six.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-ten.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-nine.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eight.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eleven.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-five.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-four.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-110-hidden-gems.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-cognee-sql-analytics-graph-rag.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineer-highlights-PyConDE-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-essential-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-oct-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/differential-storage-building-block-for-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/building-data-applications-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/building-motherduck-partner-ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-app-generator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-small-data-sf-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-snippet-sets-with-motherduck-sharing-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-mega-giga-instance-sizes-huge-scale.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-x-data-nodejs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-data-in-azure-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/simon-spati.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/zhenni-wu.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/ryan-boyd.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/jacob-matson.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/adithya-krishnan.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/aditya-aomani.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 213
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/product-analytics-motherduck-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-use-cases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-struct-nested-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/index.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/contact-us/product-expert.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/community-and-open-source.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/atm-com-analytics-costs-sql-expressibility.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dominik-moritz.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/simplifying-iot-analytics-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/scaling-duckdb-with-ducklings.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg to motherduck at motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg_duckdb-postgresql-extension-for-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-mooncake-columnstore.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-window-functions-in-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-kestra-etl-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-union-orchestration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-column-explorer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/galileo-world-geospatial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-ten.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eight.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-oct-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/building-data-applications-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/building-motherduck-partner-ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/doug-raymond.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 57
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me read the most relevant files to understand how MotherDuck integrates with DuckDB:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/what-is-duckdb.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/glossary/MotherDuck extension.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     "content": "---\ntitle: bringing-duckdb-to-the-cloud-dual-execution-explained\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained\nindexed_at: '2025-11-25T20:45:20.295187'\ncontent_hash: ab4030806c409492\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nBringing DuckDB to the Cloud: Dual Execution Explained - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Bringing DuckDB to the Cloud: Dual Execution Explained](https://www.youtube.com/watch?v=2eVNm4A8sLw)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=2eVNm4A8sLw&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 1:08:11\n\n\u2022Live\n\n\u2022\n\nYouTubeQuack & CodeMotherDuck Features\n\n# Bringing DuckDB to the Cloud: Dual Execution Explained\n\n2024/06/28\n\nBringing DuckDB's analytical power to the cloud requires more than just running it on a server. While DuckDB excels at processing data on a local machine, the dynamics of data analysis change when dealing with cloud-scale data, collaboration, and shared resources. This shift introduces challenges around [security](https://motherduck.com/learn-more/secure-startup-data-warehouse/), concurrent access, and performance, which is where a purpose-built cloud architecture becomes essential.\n\nMotherDuck, a modern cloud data warehouse, is built on DuckDB to solve these challenges. It extends DuckDB\u2019s capabilities without forking the open source project, creating a hybrid system that intelligently balances local and cloud computing. In a conversation with MotherDuck founding engineer Stephanie, she explained the architecture and the innovative query model, known as **dual execution**, that makes this possible.\n\n* * *\n\n## The MotherDuck Architecture: Beyond Hosted DuckDB\n\nA common misconception is that MotherDuck is simply DuckDB hosted in the cloud. The reality is a more sophisticated system designed to overcome the limitations of using a locally-optimized engine in a distributed environment. MotherDuck\u2019s architecture is built on three key components and a core philosophy of not forking DuckDB.\n\nInstead of maintaining a separate version, MotherDuck leverages DuckDB\u2019s powerful **extension system**. This allows MotherDuck to add new capabilities at multiple layers, including the SQL parser, optimizer, and storage interface, while staying current with the latest open source DuckDB releases. This tight integration means users benefit from the rapid innovation of the DuckDB community almost immediately.\n\nThe architecture can be broken down into three main layers:\n\n1. **The Client Extension**: This is how MotherDuck integrates with the DuckDB ecosystem. Whether a user is working in the CLI, a Python script, or a JDBC connection, a simple `ATTACH` command connects their local DuckDB instance to the MotherDuck cloud. This extension is even used to run DuckDB in the browser via WASM for the MotherDuck UI, enabling client-side processing to reduce latency and cloud compute costs.\n2. **The Compute Layer**: In the cloud, queries are processed by containerized DuckDB instances, fondly called \"ducklings.\" These compute resources are scaled based on user needs, providing the necessary CPU and memory to handle complex analytical queries on large datasets.\n3. **The Storage Layer**: DuckDB's native file format is optimized for a single writer on a local file system. This model is not well-suited for the cloud, where multiple users need to read and write to the same database concurrently. To solve this, MotherDuck implemented a differential storage system that maps a logical database file to append-only snapshot layers in cloud object storage. This design is cloud-friendly, enabling efficient in-place updates and forming the foundation for features like database sharing and time travel.\n\n* * *\n\n## Enabling Collaboration with Database Sharing and Secure Credentials\n\nThis unique architecture transforms DuckDB from a \"single-player\" tool into a collaborative \"multiplayer\" platform. One of the most significant advantages is **database sharing**. Instead of emailing SQL scripts or passing around large database files, team members within the same organization can grant query access to their databases. This streamlines collaboration, ensuring everyone works from a consistent and up-to-date version of the data.\n\nSecurity is another critical aspect of any cloud data platform. MotherDuck provides a **centralized Secret Manager**, co-designed with the DuckDB team. Users can create persistent, encrypted secrets for accessing external data sources like AWS S3, Google Cloud Storage, or Delta Lake. Once a secret is created in MotherDuck, it can be reused across different clients and sessions without needing to expose or reconfigure credentials on each local machine.\n\n* * *\n\n## Understanding Dual Execution\n\nThe most innovative feature of MotherDuck's architecture is its **dual execution model**. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\n\nThe process was demonstrated with a query joining two tables. When both tables reside in a MotherDuck database, the query is executed entirely in the cloud. An `EXPLAIN` plan for this query reveals that all operators, from the table scans to the final join, are marked as **remote**. The only local operation is downloading the final result set to the client.\n\nThe real power of dual execution becomes apparent when a query involves both local and cloud data. In the demonstration, one of the remote tables was copied to a local Parquet file. The query was then modified to join this local file with the table that remained in the cloud.\n\nThe `EXPLAIN` plan for this new query showed a mix of local and remote operations. The scan of the Parquet file and its associated processing happened **locally**, leveraging the user's machine. The scan of the cloud table happened **remotely** on MotherDuck's compute. The system then efficiently transferred only the necessary intermediate data to complete the join. This hybrid approach avoids needlessly uploading the local file to the cloud or downloading the entire remote table, resulting in faster and more efficient queries.\n\n* * *\n\n## Taking Control with Manual Execution Overrides\n\nWhile MotherDuck's optimizer is designed to make the most efficient choice automatically, some situations require manual control. For this, users can use the `md_run` parameter within scan functions like `read_parquet()` or `read_csv()`.\n\nBy setting `md_run = 'local'`, a user can force the scan of a remote S3 file to be executed on their local client. This involves downloading the data, which might be desirable if the data needs to be processed near the user's location or if they prefer using local credentials. Conversely, setting `md_run = 'remote'` forces the operation to run in the MotherDuck cloud.\n\nThis control is especially useful for performance tuning. A direct comparison showed that querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds). By pushing the computation to the data's location, MotherDuck minimizes network I/O and delivers results more quickly.\n\n* * *\n\n## A Symbiotic Future for Local and Cloud Analytics\n\nMotherDuck's architecture is a thoughtful extension of DuckDB's core principles, adapting its local-first power for the demands of the cloud. The dual execution model provides a flexible and efficient bridge between a user's laptop and cloud data, optimizing workloads in a way that pure-cloud or pure-local systems cannot.\n\nThis relationship is symbiotic. As the largest production user of DuckDB, MotherDuck continuously pushes the engine to its limits, uncovering opportunities for improvement and contributing enhancements back to the open source project. This collaboration ensures that both DuckDB and MotherDuck will continue to evolve, offering a powerful and seamless analytical experience that spans from local exploration to cloud-scale production.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Ficeberg_video_thumbnail_adfdabe3d6.jpg&w=3840&q=75)\\\\\n\\\\\n46:26](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n[2025-06-12](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n### [Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse)\n\nBuilding a data stack means choosing between easy SaaS and complex open-source. Apache Iceberg is a middle ground, but its catalog is a hurdle. New tools now simplify using Iceberg with DuckDB to create a flexible, local data lakehouse.\n\nMotherDuck Features\n\n[![\"Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FG_Se_B_Sox_AW_Fg_HD_f4fddaa9ab.jpg&w=3840&q=75)](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n[2025-04-23](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n### [Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly)\n\nHamilton Ulmer shares insights from MotherDuck's Instant SQL Mode, exploring how real-time query result previews eliminate the traditional write-run-debug cycle through client-side parsing and DuckDB-WASM caching.\n\nSQL\n\nTalk\n\nMotherDuck Features\n\n[![\"DuckDB: Run dbt build with sub-second execution times\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_03_26_at_3_40_16_PM_c6e3a8096f.png&w=3840&q=75)\\\\\n\\\\\n24:06](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n[2025-03-13](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n### [DuckDB: Run dbt build with sub-second execution times](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times)\n\nWhether you're new to DuckDB or looking to optimize your workflows, this session will provide practical insights to help you leverage its full potential.\n\nYouTube\n\nData Pipelines\n\ndbt\n\nMotherDuck Features\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 184,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 184
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     "content": "---\ntitle: what-is-duckdb\ncontent_type: tutorial\nsource_url: https://motherduck.com/learn-more/what-is-duckdb\nindexed_at: '2025-11-25T09:56:48.634315'\ncontent_hash: 301f22bee0efb32c\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO LEARN](https://motherduck.com/learn-more/)\n\n# What is DuckDB?\n\n7 min read\n\n![What is DuckDB?](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Flearn_more_duck_db_df1112cb47.png&w=3840&q=75)\n\nDuckDB offers high-performance analytical database capabilities with simplicity, speed, and portability. Data scientists, application developers, data engineers, and analysts use DuckDB to process and analyze large datasets efficiently. As an embedded database system, DuckDB brings powerful analytical capabilities directly into your applications and workflows.\n\n## Understanding DuckDB: The Basics\n\nDuckDB functions as an embeddable SQL OLAP (Online Analytical Processing) database management system. Let's break down what this means:\n\n**Embeddable**: DuckDB runs directly within your application, eliminating the need for a separate server process. This design simplifies deployment and reduces overhead, making it ideal for applications that require built-in analytical capabilities.\n\n**SQL**: It supports SQL fully, making it familiar to those with database experience. DuckDB implements a wide range of SQL features, including complex joins, subqueries, window functions, and more. This comprehensive SQL support allows users to write sophisticated queries to analyze their data effectively.\n\n**OLAP**: DuckDB optimizes [analytical processing](https://motherduck.com/learn-more/what-is-OLAP/), excelling at complex queries on large datasets. Its architecture focuses on handling the types of queries common in data analysis and business intelligence, such as aggregations, complex joins, and scans of large portions of data.\n\n![Post Image](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fduckdb_ecosystem_svg_bcb500ebd1.svg%3Fv%3D3&w=3840&q=75)\n\n## When to Use DuckDB\n\nDuckDB excels in several scenarios:\n**Data Science and Analysis**: Query large datasets efficiently in Python or R. DuckDB integrates seamlessly with data science workflows, allowing you to perform SQL queries directly on [pandas DataFrames](https://motherduck.com/learn-more/pandas-dataframes-guide/) or R data frames without data transfer overhead.\n\n**Application Development**: Embed analytical capabilities in various application types. Whether you're building desktop software, mobile apps, or web applications, DuckDB provides a lightweight yet powerful solution for integrating data analysis features.\n\n**Data Engineering**: Handle different file formats and complex queries in data pipelines. DuckDB's ability to work with formats like CSV, Parquet, and JSON makes it a versatile tool for data transformation and preparation tasks.\n\n**Local Data Processing**: Perform heavy computations on local datasets without a database server. DuckDB shines when you need to analyze data that's too large for memory but not so large that it requires a distributed system, helping you avoid the ['big data tax' of over-provisioned cloud warehouses](https://motherduck.com/learn-more/modern-data-warehouse-playbook/).\n\n**Prototyping and Testing**: Set up database schemas and test query performance quickly. DuckDB's ease of use makes it an excellent tool for rapid prototyping and testing of data models before deploying to larger systems.\n\n## Key Features of DuckDB\n\nDuckDB stands out with several key features that make it a powerful tool for data analysis:\n\n**1\\. Simplicity**\n\n- Operates without external dependencies, simplifying installation and deployment\n- Integrates easily into various environments, from local development to production systems\n- Eliminates complex setup processes, allowing you to start analyzing data immediately\n- Reduces infrastructure needs, a key factor in [how DuckDB slashes cloud warehouse costs](https://motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck/)\n\n**2\\. Portability**\n\n- Runs on all major operating systems (Windows, macOS, Linux) and CPU architectures (x86, ARM)\n- Supports various programming languages, including Python, R, Java, C++, and more\n- Enables consistent performance across different platforms, from small edge devices to large servers\n\n**3\\. Rich Feature Set**\n\nDespite its simplicity, DuckDB offers a comprehensive set of features:\n\n- Complex SQL queries and window functions for sophisticated data analysis\n- ACID guarantees through a custom-built storage manager, ensuring data integrity\n- Support for common file formats (CSV, Parquet, JSON), allowing easy integration with existing data sources\n- Automatic indexing to optimize query performance\n- User-defined functions and aggregates, enabling customization for specific use cases\n\n**4\\. Speed**\n\nDuckDB uses a columnar-vectorized query execution engine, which provides several performance benefits:\n\n- Processes data in batches for better performance, reducing per-row overhead\n- Leverages modern CPU architectures, including SIMD instructions, for efficient data processing\n- Enables efficient I/O operations and compression, minimizing data movement and storage requirements\n- Implements advanced query optimization techniques, such as predicate pushdown and adaptive query execution\n\n**5\\. Extensibility**\n\nDuckDB's architecture allows for significant extensibility:\n\n- Add new data types to support domain-specific data\n- Create custom functions to implement specialized analytical operations\n- Integrate additional file formats to work with various data sources\n- Extend SQL syntax to support specific analytical needs\n\n## DuckDB vs Other Databases\n\nTo understand DuckDB's place in the database ecosystem, let's compare it to other database types:\n\n- **vs OLTP Databases (PostgreSQL, MySQL)**: DuckDB focuses on analytical queries, optimizing for reading and analyzing large volumes of data. In contrast, OLTP databases excel at transaction processing, handling many small, frequent updates efficiently.\n\n- **vs Other OLAP Databases**: DuckDB stands out with its embedded nature and simple deployment. While systems like Clickhouse or Apache Druid offer powerful distributed computing capabilities, DuckDB provides high performance in a single-node, embedded context, making it ideal for local data processing and embedded analytics.\n\n- **vs SQLite**: Both DuckDB and SQLite embed easily into applications, but they serve different purposes. DuckDB optimizes for analytical queries and large dataset processing, while SQLite targets transactional workloads and serves as a local data storage solution for applications.\n\n\n## DuckDB and MotherDuck\n\nWhen discussing DuckDB, it's important to distinguish between:\n\n**DuckDB**: The open-source database system we've been describing\n\n**MotherDuck**: A separate company that provides a cloud data warehouse built on DuckDB\n\nMotherDuck aims to offer additional features and scalability options while leveraging DuckDB's core strengths. Think of it as the difference between using Postgres locally and using a managed Postgres service in the cloud.\n\n## Getting Started with DuckDB\n\nInstall DuckDB easily via package managers or direct download. Here's a quick Python example to illustrate its simplicity:\n\nCreate a connection (this creates a new database if it doesn't exist)\n\n```ini\nCopy code\n\ncon = duckdb.connect('my_database.db')\n```\n\nCreate a table and insert data\n\n```lua\nCopy code\n\ncon.execute(\"CREATE TABLE users (id INTEGER, name VARCHAR)\")\ncon.execute(\"INSERT INTO users VALUES (1, 'Alice'), (2, 'Bob')\")\n```\n\nQuery the data\n\n```scss\nCopy code\n\nresult = con.execute(\"SELECT * FROM users\").fetchall()\nprint(result)\n```\n\nPerform a more complex query\n\n```ini\nCopy code\n\nresult = con.execute(\"\"\"\n    SELECT name, COUNT(*) as count\n    FROM users\n    GROUP BY name\n    HAVING COUNT(*) > 0\n\"\"\").fetchall()\nprint(result)\ncon.close()\n```\n\nThis example demonstrates how to create a database, insert data, and perform both simple and more complex queries using DuckDB.\n\n## Performance Tips\n\nTo maximize DuckDB's performance:\n\n- Choose appropriate data types for your columns to optimize storage and query speed\n- Use parallel query execution on multi-core systems to take advantage of DuckDB's ability to parallelize operations\n- Optimize queries with the EXPLAIN command to understand and improve query execution plans\n- Use the Parquet file format for large datasets to benefit from its columnar storage and compression capabilities\n- Understand the fundamental [physics of data warehouse performance](https://motherduck.com/learn-more/diagnose-fix-slow-queries/) to address bottlenecks in the right order, starting with I/O.\n\n## Conclusion\n\nDuckDB combines simplicity, speed, and powerful analytical capabilities in a unique package. Whether you analyze data, develop applications, or build data pipelines, DuckDB provides an efficient solution for processing large datasets locally. Its embedded nature and focus on analytical workloads make it a versatile tool in the modern data ecosystem.\nAs you explore DuckDB, you'll discover how it enhances your data workflows, from rapid prototyping to production-ready analytics. DuckDB's combination of SQL familiarity and high-performance analytics brings advanced data processing capabilities to a wide range of applications and use cases.\nHappy querying, and may DuckDB empower your data analysis journey!\n\n### TABLE OF CONTENTS\n\n[Understanding DuckDB: The Basics](https://motherduck.com/learn-more/what-is-duckdb/#understanding-duckdb-the-basics)\n\n[When to Use DuckDB](https://motherduck.com/learn-more/what-is-duckdb/#when-to-use-duckdb)\n\n[Key Features of DuckDB](https://motherduck.com/learn-more/what-is-duckdb/#key-features-of-duckdb)\n\n[DuckDB vs Other Databases](https://motherduck.com/learn-more/what-is-duckdb/#duckdb-vs-other-databases)\n\n[DuckDB and MotherDuck](https://motherduck.com/learn-more/what-is-duckdb/#duckdb-and-motherduck)\n\n[Getting Started with DuckDB](https://motherduck.com/learn-more/what-is-duckdb/#getting-started-with-duckdb)\n\n[Performance Tips](https://motherduck.com/learn-more/what-is-duckdb/#performance-tips)\n\n[Conclusion](https://motherduck.com/learn-more/what-is-duckdb/#conclusion)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\n## Additional Resources\n\n[Blog\\\\\n\\\\\nWhy use DuckDB for Analytics](https://motherduck.com/blog/six-reasons-duckdb-slaps/) [Docs\\\\\n\\\\\nUsing the DuckDB CLI](https://motherduck.com/docs/getting-started/connect-query-from-duckdb-cli/) [Video\\\\\n\\\\\nWhat's new in DuckDB & MotherDuck](https://www.youtube.com/watch?v=t_rLbKmld7g) [Docs\\\\\n\\\\\nDuckDB vs SQLite](https://motherduck.com/learn-more/duckdb-vs-sqlite-databases/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 218,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 218
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     "content": "---\ntitle: announcing-motherduck-duckdb-in-the-cloud\ncontent_type: blog\nsource_url: https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud\nindexed_at: '2025-11-25T19:57:33.138925'\ncontent_hash: a5ee88402c607603\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Announcing MotherDuck: Hybrid Execution Scales DuckDB from your Laptop into the Cloud\n\n2023/06/22 - 4 min read\n\nBY\nMotherDuck team\n\nDuckDB has become widely known as \u201cSQLite for Analytics\u201d \u2013 a powerful SQL analytics engine with broad adoption in development workflows, ad-hoc analytics on the laptop and embedded applications. MotherDuck wants to make it even easier to use, so we\u2019ve worked alongside the creators of DuckDB to build a cloud-based serverless analytics platform. Today is a large milestone in that journey \u2013 MotherDuck is now available by invitation.\n\nMotherDuck in 100 seconds (by a duck \ud83e\udd86) - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[MotherDuck in 100 seconds (by a duck \ud83e\udd86)](https://www.youtube.com/watch?list=PLIYcNkSjh-0zP7fwKzhnbx5ur1Mf80Q_p&v=BINA_ytZXMY)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\n1/1\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nFull screen is unavailable. [Learn More](https://support.google.com/youtube/answer/6276924)\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?list=PLIYcNkSjh-0zP7fwKzhnbx5ur1Mf80Q_p&v=BINA_ytZXMY&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n[Previous (SHIFT+p)](https://www.youtube.com/watch?list=PLIYcNkSjh-0zP7fwKzhnbx5ur1Mf80Q_p&v=BINA_ytZXMY \"Previous (SHIFT+p)\")\n\n0:00 / 1:51\n\n\u2022Live\n\n\u2022\n\n## Hybrid execution: cloud and laptop working together\n\nData scientists, analysts, and engineers love DuckDB because it works great no matter where their data lives. Since many data professionals have powerful laptops sitting 85% idle, they often want to bring the data to their local machine to make it even more efficient to crunch, especially when performing ad hoc analysis and development. MotherDuck lets you analyze this local data locally, while still JOINing with data processed in the cloud, giving you efficient use of all your compute resources.\n\nIn the example below, the table `yellow_cab_nyc` lives in MotherDuck in the cloud, and I have a CSV on my laptop table with currency conversions. We want to see the average cost of NYC taxi trips by passenger count in different currencies by JOINing these two tables. Yes, we\u2019re seamlessly joining data on my laptop with data in the cloud!\n\n![Post asset](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fexample_hybrid_7bb8f08e2d.png&w=3840&q=75)\n\nYou can even do hybrid query execution with data stored in s3, with MotherDuck securely storing and managing your AWS credentials.\n\n![example_s3.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fexample_s3_d97e746203.png&w=3840&q=75)\n\nNote, these examples are part of our [sample datasets and queries](https://motherduck.com/docs/category/sample-datasets--queries/), feel free to run them yourself!\n\nYou might wonder how this works under the covers. By connecting your DuckDB instance to MotherDuck, you establish a radically different type of distributed system - one, in which one node is MotherDuck in the cloud, and another node is wherever your DuckDB lives, be it your laptop or a lambda, Python or CLI, JDBC or MotherDuck\u2019s own web app. Both nodes execute queries in concert in the most optimal way, automatically routing parts of queries to the right location.\n\n## MotherDuck includes a web notebook and Git-style Collaboration\n\nWant to run some quick SQL queries without downloading and installing DuckDB? The MotherDuck web application provides a notebook-like UI. This enables you to analyze local CSVs and parquet files, upload them and manage them alongside your other data stored in MotherDuck.\n\n![app_motherduck_beta.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fapp_motherduck_beta_v2_69fbe04dcf.png%3Fupdated_at%3D2023-06-21T23%3A24%3A18.647Z&w=3840&q=75)\n\nAs a DuckDB-in-the-cloud company, naturally MotherDuck embeds DuckDB in its web application using WASM. Results of your SQL queries are cached in this DuckDB instance, enabling you to instantly sort, pivot, and filter query results!\n\nWant to share your DuckDB data with colleagues? Using SQL, you can create a shareable snapshot of your data, which your colleagues can easily attach in MotherDuck.\n![example_share.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fexample_share_cfe1d14570.png&w=3840&q=75)\n\nThis SQL command will return a shareable URL which can then be used by your colleague to access the shared database.\n![example_attach.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fexample_attach_ff22a57c32.png&w=3840&q=75)\n\n## Anywhere you can Duck, you can MotherDuck\n\nDuckDB has been starred by over 10k developers on GitHub, and it might be due to the simplicity of getting up and running with a downloadable, open source analytics engine. We want to continue (and improve!) that amazing experience as we bring DuckDB to the cloud.\n\nOne way to do this is by ensuring MotherDuck works well with many of the most popular technologies in the modern data stack, including ingestion, orchestration and BI+Visualization tools.\n\n![partner-logos.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fpartner_logos_242d207bb6.png&w=3840&q=75)\n\nWe strive to make MotherDuck as easy to adopt as DuckDB. To that end, any DuckDB instance in the world running in Python or CLI can connect to MotherDuck with a single line of code. Suddenly, by running this command your DuckDB magically becomes supercharged by MotherDuck. Such ease of onboarding could only have been possible via close collaboration with the creators of DuckDB!\n\n![d_to_md.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fd_to_md_1dfe63b09b.png&w=3840&q=75)\n\n## Continuing to Make Analytics Ducking Awesome\n\nOne of the primary reasons we were driven to build a serverless analytics platform on top of DuckDB was their fast-paced innovation. Many features in DuckDB have gone from thoughts in academic papers to committed code in a few weeks.\n\nWe\u2019re launching MotherDuck now and doing weekly releases because we admire and want to emulate this speed of execution. Thanks in advance for all the feedback you can provide to make MotherDuck a better product!\n\n## Get Started\n\n[Request an invite](https://motherduck.com/) now to get started using MotherDuck, and join the flock on [slack.motherduck.com](https://slack.motherduck.com/).\n\nAnd, if you\u2019re in San Francisco next week, don\u2019t forget to [register for the MotherDuck Party](https://motherduck-party.eventbrite.com/), watch DuckDB co-creator Hannes [keynote the Data + AI conference](https://www.databricks.com/dataaisummit/session/data-ai-summit-keynote-thursday), and join MotherDuck co-founder Ryan Boyd in his [technical session](https://www.databricks.com/dataaisummit/session/if-duck-quacks-forest-and-everyone-hears-should-you-care).\n\n### TABLE OF CONTENTS\n\n[Hybrid execution: cloud and laptop working together](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#hybrid-execution-cloud-and-laptop-working-together)\n\n[MotherDuck includes a web notebook and Git-style Collaboration](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#motherduck-includes-a-web-notebook-and-git-style-collaboration)\n\n[Anywhere you can Duck, you can MotherDuck](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#anywhere-you-can-duck-you-can-motherduck)\n\n[Continuing to Make Analytics Ducking Awesome](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#continuing-to-make-analytics-ducking-awesome)\n\n[Get Started](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#get-started)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![This Month in the DuckDB Ecosystem: June 2023](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fthumbnail_duckdb_newsletter_1_34b0dc65cf.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-seven/)\n\n[2023/06/16 - Marcos Ortiz](https://motherduck.com/blog/duckdb-ecosystem-newsletter-seven/)\n\n### [This Month in the DuckDB Ecosystem: June 2023](https://motherduck.com/blog/duckdb-ecosystem-newsletter-seven)\n\nThis Month in the DuckDB Ecosystem: June 2023\n\n[![DuckDB Tutorial For Beginners](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgetting_started_duckdb_thumbnail_70b197b1ab.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n[2024/10/31 - Mehdi Ouazza, Ryan Boyd](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n### [DuckDB Tutorial For Beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners)\n\nGet up to speed quickly with DuckDB, including installation, VSCode workflow integration and your first SQL analytics project.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 188,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 188
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     "content": "---\ntitle: hybrid-analytics-guide\ncontent_type: tutorial\nsource_url: https://motherduck.com/learn-more/hybrid-analytics-guide\nindexed_at: '2025-11-25T09:57:23.352209'\ncontent_hash: 91344eaa7ee54322\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO LEARN](https://motherduck.com/learn-more/)\n\n# Hybrid Analytics: Query Local & Cloud Data Instantly\n\n12 min readBY\n\n[Aditya Somani](https://motherduck.com/authors/aditya-aomani/)\n\n![Hybrid Analytics: Query Local & Cloud Data Instantly](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FZero_latency_e8da68f055.png&w=3840&q=75)\n\nAre your analytics queries timing out in Postgres? Does your laptop grind to a halt when you try to analyze a large CSV file? If you feel caught between the memory limits of local tools and the spiraling costs of cloud data warehouses, you're not alone. The traditional divide between local development and cloud-scale analytics creates friction, slows down projects, and drains budgets.\n\nBut there is a modern, more efficient way. Imagine running complex SQL queries on a 20 GB Parquet file directly on your laptop, without a flicker of memory strain. Picture joining a local spreadsheet with a massive table in Amazon S3 using a single, elegant command. This is the reality of the **hybrid analytics workflow**, a powerful approach that unifies your local machine and the cloud into a single, unified data environment.\n\nThis guide is your definitive resource for understanding and implementing this new workflow. We'll break down the core concepts, provide practical examples, and show you how to solve the most common data bottlenecks for good. You will learn why traditional databases fail for analytics, how to analyze datasets larger than your RAM, and how the power of \"Dual Execution\" lets you instantly join local files with cloud data, all without the cost and complexity of managing a server cluster.\n\n## Why Are My Analytics Queries on Postgres & MySQL Timing Out?\n\nIf your analytical queries on a database like PostgreSQL or MySQL are consistently slow or timing out, you've hit a fundamental architectural limit. These databases are masterpieces of engineering for **[Online Transaction Processing (OLTP)](https://estuary.dev/blog/postgres-to-motherduck/)**. These are the small, fast operations that power applications, like creating a user or updating an order. However, this same design becomes a major bottleneck for **[Online Analytical Processing (OLAP)](https://en.wikipedia.org/wiki/Online_analytical_processing)**, which involves complex queries that scan and aggregate vast amounts of data.\n\nThe root of the problem is their **[row-oriented](https://en.wikipedia.org/wiki/Data_orientation)** storage model. A row-store keeps all data for a single record together on disk, which is efficient for retrieving an entire user profile. But for an analytical query that only needs to sum the `sale_amount` column across millions of rows, a row-store is forced to read every single column for every single row. This wastes an enormous amount of I/O and CPU cycles on data it doesn't need, which is the primary reason your dashboards are slow and your queries fail.\n\nThe next logical step is to move analytical workloads to a system built on a **[columnar architecture](https://motherduck.com/learn-more/columnar-storage-guide/)**. A columnar database organizes data by column, storing all values for `sale_amount` together. When your query asks for the sum of `sale_amount`, the database reads _only_ that column, greatly reducing I/O and speeding up performance by orders of magnitude. Modern analytical engines like DuckDB couple this with **[vectorized query execution](https://15721.courses.cs.cmu.edu/spring2024/notes/06-vectorization.pdf)**, a technique that processes data in large batches or \"vectors\" instead of row-by-row. This method fully utilizes modern CPU capabilities to perform a single instruction on multiple data points simultaneously, leading to large gains in computational efficiency.\n\n| Architecture | Row-Oriented (OLTP) | Column-Oriented (OLAP) |\n| --- | --- | --- |\n| **Primary Use Case** | Application backends, frequent small updates. | Business intelligence, data analytics, large scans. |\n| **Data Storage** | All data for a single record is stored together. | All data for a single column is stored together. |\n| **Query Example** | `SELECT * FROM users WHERE user_id = 123;` (Fast) | `SELECT SUM(sale_amount) FROM sales;` (Slow) |\n| **Best For** | Fast writes and single-record lookups. | Fast aggregations and filtering on specific columns. |\n\nBy offloading analytics to a purpose-built columnar engine, you let your transactional database continue to excel at what it does best while your analytical queries run in a [high-speed, optimized environment](https://motherduck.com/blog/postgres-duckdb-options/).\n\n## How Can I Analyze a 20 GB Parquet File on My Laptop Without It Crashing?\n\nThe second major bottleneck for data professionals is local memory. Trying to load a 20 GB file into a traditional in-memory library like Pandas on a 16 GB laptop will almost certainly result in a `MemoryError`. This happens because these tools must load the entire dataset into your computer's RAM before they can begin processing.\n\nThe modern solution is **out-of-core processing**, a strategy where data is processed in manageable chunks directly from disk without ever being fully loaded into RAM. This is the key feature of engines like DuckDB, which uses a [streaming execution model to handle datasets far larger than your available memory](https://duckdb.org/2024/07/09/memory-management.html). If a complex operation requires more memory than is available (by default, 80% of your system's RAM), DuckDB can \"spill\" intermediate results to a temporary file on disk, ensuring your query completes without crashing.\n\nThis capability turns your laptop into a surprisingly powerful analytics workstation. With an out-of-core engine, analyzing that 20 GB file becomes straightforward. You can simply use SQL to query it directly from a Python script or Jupyter notebook.\n\n```python\nCopy code\n\nimport duckdb\n\n# This query runs out-of-core, never loading the full 20GB file into RAM.\n# DuckDB's streaming engine processes the file in chunks.\nresult = duckdb.sql(\"\"\"\n  SELECT\n      product_category,\n      AVG(sale_price) as avg_price\n  FROM read_parquet('large_sales_dataset.parquet')\n  WHERE region = 'North America'\n  GROUP BY product_category\n  ORDER BY avg_price DESC;\n\"\"\").arrow()\n\nprint(result)\n```\n\nThis simple, powerful approach allows you to perform heavy-duty data analysis on your local machine, iterating quickly without waiting for a cloud cluster to spin up or worrying about memory crashes.\n\n## How Can I Join Local CSVs with Cloud Data in a Single Query?\n\nThis is where the hybrid workflow really shows its strength. Joining a local CSV of fresh sales data with a historical customer table in an S3 bucket has traditionally been a major challenge. The problem is \"data gravity,\" since it's [slow and expensive to move large amounts of data](https://motherduck.com/learn-more/no-etl-query-raw-files/). A naive query would have to either upload your entire local file to the cloud or download the entire cloud table to your laptop, both of which are very inefficient.\n\nMotherDuck solves this with an innovative query planner called **Dual Execution**. It treats your laptop and the MotherDuck cloud as two nodes in a single distributed system. When you run a hybrid query, the planner intelligently breaks it down and pushes computation to where the data lives, minimizing data transfer.\n\n> \"The most innovative feature of MotherDuck's architecture is its dual execution model. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\" - [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained/)\n\nImagine you want to join a local CSV of new product pricing with a massive sales table in MotherDuck, but only for a specific product. Instead of moving entire tables, the Dual Execution planner does the following:\n\n1. **Local Scan:** It scans the small pricing CSV on your laptop.\n2. **Filter Locally:** It applies the filter for the specific product on your machine.\n3. **Transfer Minimal Data:** It sends only the single, filtered pricing row (a few bytes of data) to the cloud.\n4. **Join in the Cloud:** It performs the final join against the massive sales table in the MotherDuck cloud.\n\nThis process can reduce network traffic by orders of magnitude compared to traditional methods, turning a difficult query into an interactive one. You can see this in action with a simple `EXPLAIN` statement, which [shows which parts of the query run locally `(L)` and which run remotely `(R)`](https://motherduck.com/docs/key-tasks/running-hybrid-queries/).\n\n## Why Are Data Engineers So Excited About Dual-Execution Engines?\n\nData engineers are excited because dual-execution engines solve one of their biggest challenges: the \"it worked on my machine\" problem. In traditional setups, the tools and data used for local development are often completely different from the production cloud environment, leading to bugs and deployment failures.\n\nMotherDuck eliminates this by using the **exact same DuckDB engine** both locally and in the cloud. A query that works on your laptop is guaranteed to work in the cloud, creating a direct path from development to production. This greatly improves developer experience (DX) and accelerates iteration cycles.\n\nThis architecture provides three key benefits:\n\n1. **Faster Development:** Engineers can build and test pipelines with the zero-latency feedback of local development before scaling to the cloud.\n2. **Lower Costs:** By using the free, powerful compute on users' laptops and minimizing data transfer, this model significantly reduces cloud bills.\n3. **Better Collaboration:** It transforms the traditionally \"single-player\" DuckDB into a [\"multiplayer\" platform](https://motherduck.com/videos/339/bringing-duckdb-to-the-cloud-dual-execution-explained/) where teams can share databases and work from a single source of truth.\n\nThis powerful combination of local speed and cloud scale is backed by real-world success.\n\n> **[Case Study: Finqore's 60x Pipeline Acceleration](https://motherduck.com/case-studies/)**\n> Finqore, a financial technology company, was struggling with an 8-hour data pipeline built on Postgres. By migrating to a hybrid workflow with MotherDuck, they were able to use DuckDB's performance and MotherDuck's serverless scale to transform that pipeline into an **8-minute workflow**, a 60x improvement that unlocked real-time data exploration for their team.\n\n## How Can I Run SQL Analytics Without Managing Clusters?\n\nThe operational overhead of provisioning, scaling, and maintaining server clusters is a major drain on data teams. **[Serverless SQL analytics](https://motherduck.com/docs/concepts/architecture-and-capabilities/)** removes this burden entirely. Instead of managing infrastructure, you simply write and run your queries, and the platform handles the rest.\n\nThis model is not only simpler but also much more cost-effective for the \"spiky\" workloads typical of analytics. You pay only for the compute you actually use, avoiding the massive costs of idle clusters.\n\n| Feature / Scenario | Traditional Cloud Warehouse (e.g., Snowflake/BigQuery) | Hybrid Approach (Local + Serverless) |\n| --- | --- | --- |\n| **Development & Prototyping** | Billed for active compute time, even for small test queries. | **Free.** Uses local machine resources with no cloud costs. |\n| **Pricing Model** | Complex credits or per-TB scanned, which is hard to predict. | Simple, usage-based billing; pay only for queries you run. |\n| **Idle Compute** | Billed for provisioned clusters, even when idle, which can create a costly [idle tax on short-running queries](https://motherduck.com/learn-more/reduce-snowflake-costs-duckdb). | **No cost.** Serverless architecture has no idle compute. |\n| **Hardware Requirement** | Entirely reliant on expensive, provisioned cloud hardware. | Leverages powerful, existing local hardware (laptops, servers). |\n\nBy adopting a serverless, hybrid approach, you can reduce your cloud data warehouse costs while empowering your team with a faster, more flexible workflow.\n\n## What Lightweight SQL Solutions Can Be Integrated with Jupyter Notebooks?\n\nFor data scientists who work primarily in Jupyter, several excellent tools bring the power of SQL directly into the notebook environment. The most powerful option for serious analytics is the **DuckDB engine** itself. It can query Pandas DataFrames, Arrow tables, and large local files directly with full SQL support. Its performance on analytical queries is excellent in the embedded space.\n\nFor comparison, while Python's standard library includes **SQLite**, it's a row-based OLTP engine and is significantly slower than DuckDB for analytical queries. An independent benchmark found DuckDB to be **[12-35 times faster](https://motherduck.com/learn-more/duckdb-vs-sqlite-databases/)** for aggregations on a 100-million-row dataset. For any analytical task in a notebook, DuckDB is the clear choice for performance and scalability.\n\nAnother popular tool is **[JupySQL](https://jupysql.ploomber.io/)**, which provides convenient SQL \"magics\" (`%sql`, `%%sql`) that turn a notebook cell into a SQL editor. It connects to various backends, including DuckDB, making it great for quick, ad-hoc exploration.\n\n## How Can I Keep Queries on My Laptop in Sync With the Cloud?\n\nThe key to keeping local and cloud queries synchronized is **single-engine semantics**. Because MotherDuck uses the same DuckDB engine on your laptop and in the cloud, a query validated locally is guaranteed to behave identically in production. This eliminates the most common source of dev-prod drift.\n\nA modern dev-to-prod workflow combines this with tools like dbt (data build tool). An analyst can develop and test a dbt model on their laptop against local data for rapid iteration. Once validated, they can promote the model to run in the cloud against the full production dataset with a [single configuration change](https://motherduck.com/blog/dual-execution-dbt/), ensuring complete consistency. This workflow combines the speed of local development with the scale and reliability of a production cloud environment.\n\n## When Should I Not Use This Hybrid Approach?\n\nWhile the hybrid, in-process model is very useful for analytics, it's not a solution for every problem. It's crucial to understand its limitations to know when a different architecture is needed.\n\n1. **High-Concurrency Applications:** An in-process engine like DuckDB is not designed to be a client-server database serving hundreds of simultaneous application users. For powering a public-facing web application, a traditional OLTP database like PostgreSQL or a distributed SQL database is the right choice.\n2. **Real-Time, High-Volume Ingestion:** If your use case involves ingesting thousands of events per second in real-time, a specialized streaming platform or real-time OLAP database would be more suitable.\n3. **Truly Massive, Multi-Terabyte Workloads:** While DuckDB can handle surprisingly large datasets on a single node, if your active working set is in the tens or hundreds of terabytes, you've reached the scale where a distributed cloud data warehouse becomes necessary to parallelize work across a large cluster.\n\nThe beauty of the MotherDuck ecosystem is that it provides a smooth way to scale up. You can start with a local-first, hybrid workflow and, as your concurrency or data scale needs grow, easily push more of the workload to the MotherDuck cloud without changing your core tools or SQL logic.\n\n### TABLE OF CONTENTS\n\n[Why Are My Analytics Queries on Postgres & MySQL Timing Out?](https://motherduck.com/learn-more/hybrid-analytics-guide/#why-are-my-analytics-queries-on-postgres-mysql-timing-out)\n\n[How Can I Analyze a 20 GB Parquet File on My Laptop Without It Crashing?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-analyze-a-20-gb-parquet-file-on-my-laptop-without-it-crashing)\n\n[How Can I Join Local CSVs with Cloud Data in a Single Query?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-join-local-csvs-with-cloud-data-in-a-single-query)\n\n[Why Are Data Engineers So Excited About Dual-Execution Engines?](https://motherduck.com/learn-more/hybrid-analytics-guide/#why-are-data-engineers-so-excited-about-dual-execution-engines)\n\n[How Can I Run SQL Analytics Without Managing Clusters?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-run-sql-analytics-without-managing-clusters)\n\n[What Lightweight SQL Solutions Can Be Integrated with Jupyter Notebooks?](https://motherduck.com/learn-more/hybrid-analytics-guide/#what-lightweight-sql-solutions-can-be-integrated-with-jupyter-notebooks)\n\n[How Can I Keep Queries on My Laptop in Sync With the Cloud?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-keep-queries-on-my-laptop-in-sync-with-the-cloud)\n\n[When Should I Not Use This Hybrid Approach?](https://motherduck.com/learn-more/hybrid-analytics-guide/#when-should-i-not-use-this-hybrid-approach)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\n## FAQS\n\n### My analytics queries on Postgres / MySQL are timing out. What\u2019s the next step up?\n\nThe next step is to move your analytical workloads from a row-oriented (OLTP) database like Postgres to a purpose-built columnar (OLAP) engine like DuckDB. Columnar engines are designed for analytics and are orders of magnitude faster for large scans and aggregations because they only read the specific columns needed for a query.\n\n### How can I analyze a 20 GB Parquet file on my laptop without it crashing?\n\nUse a query engine that supports \"out-of-core\" processing, like DuckDB. It processes data in manageable chunks directly from your disk instead of loading the entire file into RAM. This allows you to analyze datasets that are significantly larger than your computer's available memory.\n\n### Why is it so hard to join local CSVs with cloud data?\n\nThe main challenge is \"data gravity\"\u2014it's slow and expensive to move large datasets across a network. Traditional tools force you to either upload your entire local file or download the entire cloud table. A modern hybrid platform with a dual-execution engine solves this by intelligently minimizing data transfer.\n\n### Can I mix local datasets and S3 data in a single query?\n\nYes. With a hybrid analytics platform like MotherDuck, you can write a single SQL query that joins local files (e.g., CSVs) with data in cloud storage (e.g., Parquet files in S3). The system treats them as if they exist in one unified environment, abstracting away their physical location.\n\n### How can I run SQL analytics without managing clusters?\n\nAdopt a serverless SQL analytics platform. These services handle all infrastructure provisioning, scaling, and maintenance for you. You simply run your queries and pay only for the compute you use, which eliminates the high cost and operational overhead of managing idle clusters.\n\n### Why are data engineers excited about dual-execution engines?\n\nDual-execution engines solve the \"it worked on my machine\" problem by using the exact same query engine on the developer's laptop and in the cloud. This guarantees consistency, speeds up development cycles, lowers costs by leveraging local compute, and enables better team collaboration on a single source of truth.\n\n### What lightweight SQL solutions can be integrated with Jupyter notebooks?\n\nDuckDB is the leading lightweight SQL solution for Jupyter. It can be installed via pip and can directly query Pandas DataFrames, Arrow tables, and large local files with full SQL support. It is significantly faster for analytical queries than other embedded options like SQLite.\n\n### How can I keep queries on my laptop in sync with the cloud?\n\nUse a platform that offers single-engine semantics, like MotherDuck. Because it runs the same DuckDB engine locally and in the cloud, a query validated on your laptop is guaranteed to behave identically in production. This eliminates dev-prod drift and ensures consistency.\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 212,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 212
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/glossary/MotherDuck extension.md",
  [2m  â”‚[0m     "content": "---\ntitle: MotherDuck extension\ncontent_type: tutorial\nsource_url: https://motherduck.com/glossary/MotherDuck extension\nindexed_at: '2025-11-25T20:08:29.934950'\ncontent_hash: 5315c271fb5d98b9\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n# MotherDuck extension\n\n_[Back to DuckDB Data Engineering Glossary](https://motherduck.com/glossary/)_\n\nThe [MotherDuck extension](https://github.com/duckdb/duckdb/tree/master/extension/motherduck) is a component of [DuckDB](https://duckdb.org/) that enables seamless integration with the [MotherDuck](https://motherduck.com/) cloud service. This extension allows users to connect their local DuckDB instance to MotherDuck's serverless analytics platform, facilitating hybrid query execution across local and cloud environments. By using the MotherDuck extension, data professionals can leverage the power of cloud computing while maintaining the flexibility of working with data on their local machine. The extension supports features such as remote data access, synchronized schema management, and the ability to push computations to the cloud when needed, enhancing DuckDB's capabilities for handling larger datasets and more complex analytics tasks.\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 22,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 22
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me write the comprehensive answer to the output file:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_2.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "create",
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_2.md",
  [2m  â”‚[0m   "content": "<4385 chars>",
  [2m  â”‚[0m   "structuredPatch": [],
  [2m  â”‚[0m   "originalFile": null
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I've written a comprehensive answer to `/tmp/answer_without_kg_2.md` that explains how MotherDuck integrates with DuckDB. 
  [94mâ”‚[0m 
  [94mâ”‚[0m **Summary of findings:**
  [94mâ”‚[0m 
  [94mâ”‚[0m MotherDuck integrates with DuckDB through:
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. **Extension-based integration** - Uses DuckDB's extension system rather than forking the project
  [94mâ”‚[0m 2. **Three-layer architecture** - Client extension, compute layer (containerized "ducklings"), and differential storage layer
  [94mâ”‚[0m 3. **Dual execution model** - The optimizer intelligently routes query parts between local client and cloud, minimizing data transfer
  [94mâ”‚[0m 4. **Same engine semantics** - Identical DuckDB engine runs locally and in cloud for consistency
  [94mâ”‚[0m 5. **Symbiotic relationship** - MotherDuck contributes improvements back to DuckDB as its largest production user
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer draws from 5 source files covering architecture details, hybrid execution, and the extension system.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 2,231 | Cost: $0.3913
     Cumulative: 2,231 tokens | $0.3913
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent has provided a detailed summary and indicated that it has completed writing the answer, which signifies task completion rather than soliciting further input.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 9 | Duration: 55.1s
â•‘    Tokens: 2,231 | Cost: $0.3913
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  Running LLM judge evaluation...
   ğŸ§  LLM Judge score: 0.97

======================================================================
â“ Question 3/10
...What SQL features from DuckDB are not yet supported in MotherDuck?
======================================================================


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 3
â”‚ Answer this question by searching through the .kurt/sources/ directory:

...What SQL features from DuckDB are not yet supported in MotherDuck?

Required Steps:
1. Search for relevant markdown files in .kurt/sources/ using glob and grep
2. Read the most relevant files (aim for 3-5 most relevant sources)
3. As you read each file, keep track of:
   - The file path
   - The file title/name
   - Key information extracted from that file
4. Synthesize a comprehensive answer based on the information found
5. Write your answer to: /tmp/answer_without_kg_3.md

IMPORTANT - Your answer MUST have this structure:
```markdown
# Answer

[Your comprehensive answer here, drawing from the sources you read]

## Sources

List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
[etc.]

DO NOT use URLs or document titles as links. Show the actual file path you read with the Read tool.
```

The Sources section is MANDATORY - you must list every file you read to answer the question.

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [96mğŸ”§ TOOL:[0m Grep â†’ not yet supported|unsupported|not supported|limitations|SQL features

  [96mğŸ”§ TOOL:[0m Grep â†’ DuckDB.*not|not.*DuckDB|difference.*DuckDB|DuckDB.*difference

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/**/*limit*.md

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/**/*support*.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "numFiles": 0
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/cultivating-growth-how-gardyn-scaled-its-data-operations-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/an-evolving-dag-for-the-llm-world-julia-schottenstein-of-langchain-at-small-data-sf-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/privacy-policy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/dataframes.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/big-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/fees-addendum.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-summary-chapter4.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/candidate-privacy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/atm-com-analytics-costs-sql-expressibility.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-simple-joys-of-scaling-up.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vector-technologies-ai-data-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg to motherduck at motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/postgres-duckdb-options.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/json-log-analysis-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/galileo-world-geospatial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/from-data-lake-to-lakehouse-duckdb-portable-catalog.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-the-great-federator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-versus-pandas-versus-polars.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-four.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineer-highlights-PyConDE-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-app-generator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 55
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/contact-us/support.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/customer-support.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "durationMs": 599,
  [2m  â”‚[0m   "numFiles": 3,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "durationMs": 739,
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ SQL.*limitation|feature.*not.*available|unsupported SQL|SQL.*unsupported

  [96mğŸ”§ TOOL:[0m Grep â†’ MotherDuck.*support|support.*MotherDuck

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/**/*duckdb*.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/privacy-policy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 10
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/faster-data-pipelines-development-with-mcp-and-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ibis-one-library-to-query-any-backend.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-dbt-accelerating-the-developer-experience-with-local-power.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-use.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-OLAP.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/product-analytics-motherduck-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/ducklake-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-lake-vs-data-warehouse-vs-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/hack-night.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/index.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/acid-transactions-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/MotherDuck extension.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/DuckDB CLI.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/superset.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/polytomic.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/contact-us.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/contact-us/support.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/customer-support.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dosomething-non-profit-tco-cost-savings.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/finqore.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/reflex-careers-gig-economy-retail.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/trunkrs-same-day-delivery-motherduck-from-redshift.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/candidate-privacy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/careers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/atm-com-analytics-costs-sql-expressibility.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dexibit.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dominik-moritz.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vector-technologies-ai-data-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-embeddings-for-semantic-meaning-in-text-and-rag.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-llm-prompt-function-gpt-models.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streaming-data-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streamkap-mysql-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/tableau-cloud-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg_duckdb-postgresql-extension-for-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-duckdb-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-mooncake-columnstore.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/preswald-health-data-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pushing-geo-boundaries-with-motherduck-geobase.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-window-functions-in-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-open-for-all-with-series-b.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-reflections-sigmod-pods-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-union-orchestration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-headquarters-seattle-opening.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-fixit-ai-sql-error-fixer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-motherduck-for-business-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/getting-started-ducklake-table-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/getting-started-gis-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/git-for-data-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-ducks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/ducklake-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/effortless-etl-unstructured-data-unstructuredio-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/fabi-ai-llm-prompt-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-versus-pandas-versus-polars.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-ten.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-nine.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/dosomething-motherduck-data-warehouse-ROI.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-110-hidden-gems.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-infrastructure-devops.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineers-answer-10-top-reddit-questions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-nov-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/building-data-applications-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/building-motherduck-partner-ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-app-generator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/big-data-is-dead.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/bigquery-to-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-data-in-azure-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-json-data-using-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/MotherDuck-Visualize-Embeddings-Marimo.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/sheila-sitaram.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/ryan-boyd.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/garrett-obrien.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/alex-monahan.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 124
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/analyze-data-in-azure-with-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/analyze-sqlite-databases-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/analyze-x-data-nodejs-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/announcing-duckdb-snippet-sets-with-motherduck-sharing-databases.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/bigquery-to-duckdb-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-110-hidden-gems.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-cognee-sql-analytics-graph-rag.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2023.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eight.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eleven.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-five.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-four.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-nine.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2023.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-one.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-seven.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-six.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-ten.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-three.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-two.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-excel-extension.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-on-edge-raspberry-pi.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-puppygraph-graph-model-on-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-python-e2e-data-engineering-project-part-1.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-text2sql-llm.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-the-great-federator.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-versus-pandas-versus-polars.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-wasm-in-browser.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-1.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/fix-outdated-llm-documentation-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/from-data-lake-to-lakehouse-duckdb-portable-catalog.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/geospatial-for-beginner-duckdb-spatial-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/getting-started-gis-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/json-log-analysis-duckdb-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/local-duckdb-ui-visual-data-analysis.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/pg-duckdb-release.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/pg_duckdb-postgresql-extension-for-duckdb-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/postgres-duckdb-options.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/python-faker-duckdb-exploration.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/scaling-duckdb-with-ducklings.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/search-using-duckdb-part-1.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/search-using-duckdb-part-2.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/search-using-duckdb-part-3.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/semantic-layer-duckdb-tutorial.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/six-reasons-duckdb-slaps.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/solving-advent-code-duckdb-dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/streamlining-ai-agents-duckdb-rag-solutions.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/the-future-of-bi-bi-as-code-duckdb-impact.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/duckdb-book-brief.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/duckdb-book-summary-chapter2.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/duckdb-book-summary-chapter3.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/duckdb-book-summary-chapter4.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/duckdb-news.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/duckdb-struct-nested-data.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "durationMs": 735,
  [2m  â”‚[0m   "numFiles": 100,
  [2m  â”‚[0m   "truncated": true
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search more specifically for content about SQL features not supported in MotherDuck:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ currently not supported|not currently supported|not yet available|coming soon|does not support|doesn't support|cannot

  [96mğŸ”§ TOOL:[0m Grep â†’ extension|attach|local|cloud.*execution|dual.*execution|hybrid
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/spatial-data-management-with-duckdb-ft-mattforrest.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": "<79846 chars>",
  [2m  â”‚[0m   "numLines": 471
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me read some of the most relevant files to find SQL features not yet supported in MotherDuck:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/glossary/ALTER TABLE statement.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/acid-transactions-sql.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     "content": "---\ntitle: 'Fix Slow BI Dashboards: A Guide to Sub-Second Analytics'\ncontent_type: guide\ndescription: Frustrated by slow BI dashboards? Learn the causes of dashboard latency,\n  from architectural bottlenecks to lakehouse issues. See how MotherDuck helps startups\n  scale.\npublished_date: '2025-10-17T00:00:00'\nsource_url: https://motherduck.com/learn-more/fix-slow-bi-dashboards\nindexed_at: '2025-11-25T10:52:12.236559'\ncontent_hash: 0e692003fc4f1f77\nhas_step_by_step: true\nhas_narrative: true\n---\n\nAs a startup, your data is your compass. But as you scale, that compass begins to spin. Business Intelligence (BI) dashboards that were once snappy now take minutes to load, turning quick checks into coffee breaks. Queries time out. Your team hesitates to ask new questions of the data because the feedback loop is agonizingly slow. You're facing a critical growth inflection point: your analytics stack can no longer keep up with your ambition.\n\nYou know you need something more powerful, but the thought of migrating to a traditional cloud data warehouse, with its complex setup, opaque pricing, and multi-year contracts, is daunting. What if there was a better way? A path that gives you the power of a massive data warehouse without the overhead, designed specifically for teams who move fast?\n\nThis guide is for the technical founders, first data hires, and lean analytics teams feeling this pain. We'll provide a clear framework for evaluating if MotherDuck is the right solution to solve your dashboard latency issues, scale your analytics, and empower your team to make faster, data-driven decisions.\n\n### What You'll Learn in This Guide\n\n**What Causes Dashboard Latency?**We'll uncover why your current database struggles with analytical queries and identify the key architectural bottlenecks.**How Do You Scale Beyond a Single Node?**Discover the limitations of single-node engines for large datasets and how MotherDuck's hybrid architecture provides a straightforward path to scale.**How Can You Achieve Real-Time Insights?**Learn how to move from stale, batch-updated data to sub-second freshness without overloading your production systems.**What Does a Migration Look Like?**Follow a practical, week-by-week plan to migrate a critical workload to MotherDuck and start seeing results in under 30 days.**Is MotherDuck the Right Fit for Your Startup?**Get a clear comparison of when MotherDuck excels and what limitations to consider for your specific use case.\n\n## Why Are My Startup's BI Dashboards So Slow?\n\nIf your dashboards are grinding to a halt, the problem usually isn't a single query but an architectural mismatch. Most startups begin by running analytics on a replica of their production database, often a system like PostgreSQL or MySQL. These are [ Online Transaction Processing (OLTP)](https://motherduck.com/learn-more/what-is-OLAP/) databases, brilliant for handling thousands of small, fast transactions like creating a user or processing an order.\n\nHowever, analytical queries are a completely different beast. They are **Online Analytical Processing (OLAP)** workloads, which involve scanning millions or billions of rows across a few columns to calculate aggregations. Asking an OLTP database to perform heavy OLAP queries is like asking a race car to haul lumber. It wasn't built for the job.\n\nThe core issue lies in how the data is stored on disk. OLTP databases are **row-oriented**, meaning they store all the data for a single record together. When your BI tool asks for the total number of users by `plan_type`\n\nfrom a table with 50 columns, a row-oriented database is forced to read all 50 columns for every single row, even though it only needs one. This wastes an enormous amount of I/O.\n\nThis is where [ columnar storage](https://motherduck.com/learn-more/columnar-storage-guide/), the foundation of modern analytical databases like DuckDB, makes a big difference. A columnar database stores all values from a single column together. When you run the same query, it reads\n\n*only*the\n\n`plan_type`\n\ncolumn, dramatically reducing the amount of data scanned. This fundamental difference is often the key to unlocking sub-second query performance.### How Do Row and Columnar Databases Compare for Analytics?\n\n| Feature | Row-Oriented (e.g., PostgreSQL) | Column-Oriented (e.g., DuckDB/MotherDuck) |\n|---|---|---|\nPrimary Use Case | OLTP: Fast reads and writes of individual records. | OLAP: Fast scans and aggregations over large datasets. |\nData Layout | Stores all values for a single record contiguously. | Stores all values for a single column contiguously. |\nQuery Performance | Slow for analytical queries that only need a few columns. | Extremely fast for analytical queries. It only reads the required columns. |\nCompression | Less effective, as it stores mixed data types in each row. | Highly effective, leading to smaller storage footprint and faster scans. |\n\n## What Happens When My Analytics Queries Need to Scan Billions of Rows?\n\nAs your startup succeeds, your data volume explodes. The single-node analytics database that was once a perfect solution starts to show its limits. While an engine like DuckDB is incredibly fast and can even process datasets larger than RAM by [\"spilling\" intermediate results to disk](https://duckdb.org/2024/07/09/memory-management.html), it is ultimately constrained by the resources of a single machine.\n\nWhen you run a complex query with multiple joins and aggregations on billions of rows, the intermediate data generated can overwhelm the system's memory. This can lead to slow queries as the engine constantly writes to and reads from disk, or worse, an [ OutOfMemoryException](https://duckdb.org/docs/stable/guides/troubleshooting/oom_errors.html) that kills the query entirely.\n\nFurthermore, a single node has a finite capacity for **concurrency**. As more team members connect with BI tools, the CPU and I/O resources get saturated, and everyone's queries slow down. This is the practical ceiling of a single-node engine. This is precisely the problem MotherDuck was built to solve. It extends the lightning-fast local experience of DuckDB with a serverless cloud backend, giving you a straightforward path to scale.\n\n## How Does MotherDuck's Hybrid Architecture Deliver Speed and Scale?\n\nMotherDuck introduces a novel architecture that gives you the best of both worlds: the zero-latency feel of local development and the on-demand power of the cloud. This is achieved through a few key concepts.\n\n### What is Dual Execution and Why Does it Matter?\n\nThe magic behind MotherDuck is its [ Dual Execution query planner](https://www.cidrdb.org/cidr2024/papers/p46-atwal.pdf). Instead of forcing you to move all your data to the cloud, it intelligently pushes the computation to where the data lives, minimizing network latency and data transfer costs.\n\n**Local Query:**If you query a CSV file on your laptop, the query runs entirely on your local DuckDB instance. The result is instantaneous.**Cloud Query:**If you query a large table stored in MotherDuck, the work is routed to a dedicated, serverless compute instance (a \"Duckling\") in the cloud.**Hybrid Query:**This is where it gets powerful. If you join a local file with a large cloud table, the planner is smart enough to push filters down to your local machine first. It processes the local file, sends only the small, filtered result to the cloud, and then performs the final join. This makes complex queries incredibly efficient.\n\n### How Do You Handle High Concurrency from BI Tools?\n\nA common and dangerous blind spot for startups is how BI tools handle concurrency. Tools like Tableau or Looker often use a single service account, funneling queries from dozens of users through one connection. This can quickly overwhelm a database.\n\n## Case Study: How Layers Solved Its Concurrency Bottleneck\n\nThe SaaS company\n\n[faced this exact problem. Their analytics, running on PostgreSQL, were overwhelmed when their BI tool masked 73 simultaneous users behind a single service account. This exhausted their connection pool and caused a high rate of query timeouts.]LayersAfter migrating to MotherDuck, they used the\n\nRead Scalingfeature. By connecting their BI tool with a special Read Scaling Token, user queries were automatically distributed across a pool of 16 dedicated, read-only DuckDB replicas. This instantly parallelized the workload, providing each user session with isolated compute. The result was a dramatic improvement in stability, with BI query timeouts dropping to virtually zero.\n\nBy issuing a unique, read-only token for each BI integration and using the `session_hint`\n\nparameter, you can ensure user queries are [intelligently load-balanced](https://motherduck.com/docs/key-tasks/authenticating-and-connecting-to-motherduck/read-scaling/), providing a consistently fast experience for everyone.\n\n## Why Might Lakehouse Architectures Have High Latency Issues?\n\nThe lakehouse architecture, which combines a data lake's flexibility with a data warehouse's performance, is a popular choice. However, startups often find that it comes with a significant \"latency tax,\" especially for interactive queries.\n\nThe primary culprit is the [ \"small files problem.\"](https://dev.to/thedanicafine/the-apache-iceberg-small-file-problem-1k2m) Streaming ingestion jobs often write thousands of tiny files to cloud object storage like Amazon S3. To run a query, the engine must make thousands of separate, high-latency HTTP requests just to read the metadata and find the right files. Each request can take 100-200 milliseconds, adding up to seconds of delay before a single byte of data is even processed.\n\nA benchmark demonstrated this vividly: a query scanning a 5 GB Apache Iceberg table with thousands of small files took **5.093 seconds** to run. The same query on MotherDuck's **DuckLake** format, which uses a database-backed index to avoid scanning S3 for metadata, returned in just **0.146 seconds**, a [ 34x speedup](https://motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats/).\n\n### How Does DuckLake Compare to Apache Iceberg?\n\n| Feature | Apache Iceberg (Standard) | MotherDuck DuckLake |\n|---|---|---|\nMetadata Storage | Stored as files (manifests) in the object store (e.g., S3). | Stored in a fast, transactional database, separate from data files. |\nQuery Planning | Requires multiple sequential S3 reads to discover which data files to scan. | A single SQL query to the catalog database returns the exact list of files. |\nSmall File Handling | Creates many small data and metadata files, requiring costly maintenance jobs (compaction). | Dramatically reduces metadata overhead and the need for frequent compaction. |\nPerformance | High latency for interactive queries due to metadata discovery overhead. |\n34x faster |\n\n## How Can I Get Real-Time Data Without Overloading My Production Database?\n\nIs your sales dashboard always 30 minutes out of date? This frustrating lag is almost always due to a reliance on traditional, batch-based ETL (Extract, Transform, Load) jobs that run on a schedule. For a startup that needs to react instantly to user behavior, this is no longer acceptable.\n\nThe modern solution is a streaming architecture built on **Change Data Capture (CDC)**. Instead of querying your production database tables (which adds heavy load), CDC tools like Estuary or [Streamkap](https://motherduck.com/blog/streamkap-mysql-to-motherduck/) monitor the database's internal transaction log. They capture every insert, update, and delete as it happens and stream these events to MotherDuck in near real-time, often with [ less than 100ms of latency](https://estuary.dev/destination/motherduck/).\n\nThis approach provides two critical benefits:\n\n**Sub-Minute Freshness:**Your analytics are always synchronized with reality.**Zero Production Impact:**It completely isolates your analytical workload, ensuring that heavy queries never slow down your customer-facing application.\n\n## What Are the Signs My Startup Has Outgrown Its Current Analytics Stack?\n\nThe signs that you've outgrown your analytics stack are both technical and cultural. Recognizing them early can save you from months of frustration and slow decision-making.\n\n**Technical Red Flags:**\n\n**High Query Latency:**Your p95 query latency for interactive dashboards consistently exceeds 2-5 seconds. Research shows that[beyond 1 second, users lose their flow of thought, and beyond 10 seconds, you lose their attention entirely](https://www.nngroup.com/articles/powers-of-10-time-scales-in-ux/).**Rising Error Rates:**You see an increase in query timeouts, connection failures, or application errors related to database load.**Stale Data:**Business teams complain that dashboards are always out of date, indicating that nightly batch jobs are no longer sufficient.\n\n**Business and Cultural Red Flags:**\n\n**Dashboard Abandonment:**Analysts and business users stop using the BI tool because it's \"too slow.\"**Rising Costs:**Your infrastructure bill is growing faster than the value you're getting from your data, often due to over-provisioning to compensate for poor performance.**Slow Product Velocity:**Engineers spend more time optimizing the database than building features, and product managers can't get timely data to inform their roadmap.\n\nCompanies that see these signs can achieve significant improvements by migrating. **Finqore** reduced 8-hour financial data pipelines to just 8 minutes, while **uDisc** cut query times from minutes to seconds, leading to [a significant lift in daily active analysts](https://motherduck.com/case-studies/).\n\n## How Do I Plan a Migration to MotherDuck?\n\nMigrating to MotherDuck doesn't have to be a massive, disruptive project. You can see value in under 30 days by following a straightforward, phased approach focused on a single, high-pain workload.\n\n### Your 30-Day Migration Roadmap\n\n| Week | Key Tasks | Success Criteria |\n|---|---|---|\nWeek 1: Connect & Ingest | - Create your MotherDuck account and generate service tokens. - Install the DuckDB SDK and connect via `ATTACH 'md:';` . - Perform an initial bulk load of a target dataset (e.g., one large, slow table) into a new MotherDuck database. | - You can successfully query your data in MotherDuck from your local machine and BI tool. - Historical data for one target workload is fully loaded. |\nWeek 2: Mirror a Pilot Workload | - Select one high-pain, low-risk dashboard to migrate. - Set up an incremental CDC pipeline (e.g., using Estuary) to keep MotherDuck in sync with the source. - Re-create the dashboard's data model to point to MotherDuck. | - The mirrored dashboard in MotherDuck is live and updating in near real-time. - p95 query latency is under 2 seconds. |\nWeek 3: Validate & Optimize | - Share the new dashboard with a small group of business users for feedback. - Monitor query performance and cost. - Use optimizations like Read Scaling tokens for the BI tool connection. | - Users confirm the new dashboard is significantly faster and accurate. - The projected cost for the pilot workload is lower than the legacy system. |\nWeek 4: Cut-Over & Expand | - Officially switch all users to the new MotherDuck-powered dashboard. - Decommission the old data models and pipelines for the migrated workload. - Plan the migration of the next set of analytical workloads. | - 100% of users for the pilot workload are using the new dashboard. - A prioritized backlog for the next migration is created. |\n\n## What Are the Limitations of MotherDuck I Should Consider?\n\nNo tool is perfect for every situation. Being honest about limitations is key to making the right choice. MotherDuck is rapidly evolving, but startups should be aware of a few current constraints:\n\n**Regional Availability:**MotherDuck currently operates in the AWS[us-east-1](https://motherduck.com/docs/concepts/architecture-and-capabilities/)and recently started in[eu-central-1](https://motherduck.com/blog/motherduck-in-europe/), hosted in Frankfurt. Teams with strict data residency requirements outside this region will need to consider this.**Partial DuckDB SQL Coverage:**MotherDuck does not yet support the full range of DuckDB's SQL features. Notably, custom Python/native User-Defined Functions (UDFs), server-side`ATTACH`\n\nto other databases (like Postgres), and custom extensions are not yet available. The common workaround is to perform these specific transformations upstream before loading data into MotherDuck.\n\n## Conclusion: Stop Waiting, Start Analyzing\n\nFor a startup, speed is everything: speed to market, speed to insight, and speed of execution. A slow analytics stack is a direct drag on all three. You don't need the complexity and cost of a legacy data warehouse, nor can you afford the performance bottlenecks of an overloaded production database.\n\nMotherDuck offers a third way, purpose-built for the scale and agility of a modern startup. By combining the raw speed of a local-first engine with the on-demand scale of the cloud, it eliminates the trade-offs that have held analytics teams back for years. If your dashboards are slow and your team is frustrated, it's time to evaluate a new approach.\n\n[Ready to experience sub-second analytics? Start building on MotherDuck's free tier today.](https://motherduck.com/startups/)\n\nStart using MotherDuck now!\n\n## FAQS\n\n### How much does MotherDuck cost for a startup?\n\nMotherDuck's pricing is designed for startups. It includes a generous free tier for smaller projects and two pay-as-you-go plans that charge only for storage used and compute-seconds executed. There are no idle cluster charges or minimums, which can lead to [ 70-90% cost savings](https://motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck/) compared to traditional data warehouses.\n\n### Can MotherDuck handle our data volume?\n\nYes. Analysis shows that [over 95% of startup databases are smaller than 1 TB](https://hemantkgupta.medium.com/insight-from-paper-motherduck-duckdb-in-the-cloud-and-in-the-client-e4a73da9dbec), a size range where MotherDuck excels. The hybrid architecture is designed to scale from megabytes on your laptop to tens of terabytes in the cloud, ensuring you have a growth path.\n\n### How does MotherDuck compare to Snowflake or BigQuery for a startup?\n\nWhile large warehouses are powerful, they often come with significant operational complexity and cost overhead that can be burdensome for a startup. MotherDuck offers [a simpler, more cost-effective path](https://motherduck.com/learn-more/modern-data-warehouse-playbook/). Its key differentiators are the serverless, per-second billing model and the unique \"local-first\" hybrid architecture, which provides an excellent development experience and eliminates network latency for many common analytical tasks.\n\n### How do startups decide if MotherDuck fits their analytics stack?\n\nA startup should consider MotherDuck if they experience slow BI dashboards, rising query timeouts, and stale data. It's a strong fit for teams that have outgrown a single database (like PostgreSQL) but want to avoid the cost and complexity of a traditional data warehouse. MotherDuck excels with data volumes from gigabytes to tens of terabytes and is ideal for building fast, interactive analytics without a large data engineering team.\n\n### Why do dashboards struggle when scanning billions of rows?\n\nWhen scanning billions of rows, single-node engines like DuckDB can be constrained by the memory and I/O of a single machine. Complex queries generate large intermediate results that can exceed available RAM, forcing the engine to \"spill\" to a slower disk, which increases latency. High user concurrency can also saturate the CPU, causing all queries to slow down.\n\n### Why do lakehouse architectures often have high query latency?\n\nLakehouse latency is often caused by the **\"small files problem.\"** Streaming jobs write thousands of tiny files to object storage like S3. To run a query, the engine must make many high-latency network requests just to read the metadata and find the right files, adding seconds of delay before the query even starts processing data.\n\n### What are the main causes of slow BI dashboards?\n\nThe primary cause is an architectural mismatch. Startups often run heavy analytical (OLAP) queries on their production (OLTP) database, like PostgreSQL. These row-oriented databases are inefficient for analytics, as they must read entire records instead of just the needed columns. As data grows, this leads to high I/O, slow queries, and dashboard latency.\n\n### How does data modeling impact dashboard performance?\n\nIt has a massive impact. Using a [ star schema](https://motherduck.com/learn-more/star-schema-data-warehouse-guide/), which organizes data into a central \"fact\" table and surrounding \"dimension\" tables, is a proven technique for accelerating analytical queries in columnar databases.",
  [2m  â”‚[0m     "numLines": 189,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 189
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/glossary/ALTER TABLE statement.md",
  [2m  â”‚[0m     "content": "---\ntitle: ALTER TABLE statement\ncontent_type: tutorial\nsource_url: https://motherduck.com/glossary/ALTER TABLE statement\nindexed_at: '2025-11-25T20:02:05.279103'\ncontent_hash: 0606fcf6209a230f\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n# ALTER TABLE statement\n\n_[Back to DuckDB Data Engineering Glossary](https://motherduck.com/glossary/)_\n\n## Overview\n\nThe `ALTER TABLE` statement allows you to modify the structure of an existing database table without having to recreate it from scratch. This is essential for maintaining and evolving database schemas as requirements change over time.\n\n## Basic Syntax in DuckDB\n\nIn DuckDB, the `ALTER TABLE` statement supports adding, dropping, and renaming columns, as well as renaming the table itself. Unlike some other databases, DuckDB does not currently support modifying column constraints or data types directly - you would need to create a new table for such changes.\n\n## Adding Columns\n\nTo add a new column to an existing table:\n\n```sql\nCopy code\n\nALTER TABLE users\nADD COLUMN email VARCHAR;\n\n-- Add column with a default value\nALTER TABLE users\nADD COLUMN status VARCHAR DEFAULT 'active';\n\n-- Add column that can't contain NULL values\nALTER TABLE users\nADD COLUMN required_field INTEGER NOT NULL;\n```\n\n## Dropping Columns\n\nTo remove an existing column from a table:\n\n```sql\nCopy code\n\nALTER TABLE users\nDROP COLUMN email;\n\n-- Drop multiple columns at once\nALTER TABLE users\nDROP COLUMN email, DROP COLUMN status;\n```\n\n## Renaming Columns\n\nTo rename an existing column:\n\n```sql\nCopy code\n\nALTER TABLE users\nRENAME COLUMN email TO contact_email;\n```\n\n## Renaming Tables\n\nTo rename an entire table:\n\n```sql\nCopy code\n\nALTER TABLE users\nRENAME TO system_users;\n```\n\n## Important Considerations\n\nUnlike more established databases like PostgreSQL, DuckDB's `ALTER TABLE` functionality is more limited. It doesn't support changing column data types, adding or removing constraints, or modifying default values of existing columns. For these operations, you would typically need to create a new table with the desired structure and migrate the data.\n\nWhen adding columns with `NOT NULL` constraints, you must either provide a default value or ensure the table is empty, as DuckDB cannot enforce the constraint on existing rows without a default value.\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 91,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 91
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/acid-transactions-sql.md",
  [2m  â”‚[0m     "content": "---\ntitle: 'ACID Transactions in Databases: A Data Engineer''s Guide'\ncontent_type: guide\ndescription: Learn how ACID transactions ensure data consistency, integrity, and reliability\n  in SQL databases like DuckDB and SQL Server. This guide covers atomicity, consistency,\n  isolation, and durability with real-world SQL examples. Perfect for data engineers\n  and analysts building robust ETL pipelines and analytics workflows. Discover how\n  modern tools like MotherDuck bring ACID guarantees to the cloud.\npublished_date: '2025-07-30T00:00:00'\nsource_url: https://motherduck.com/learn-more/acid-transactions-sql\nindexed_at: '2025-11-25T20:37:04.970349'\ncontent_hash: 96b7cbdddd203c46\nhas_code_examples: true\nhas_step_by_step: true\nhas_narrative: true\n---\n\nEver found yourself glaring at a dashboard where the numbers just don't add up? Or maybe you've watched in horror as a data pipeline crashed halfway through, leaving your tables in complete disarray? If these scenarios sound painfully familiar, you've encountered one of the fundamental challenges in the field: keeping data consistent when the world seems determined to mess it up.\n\nThis guide walks you through ACID transactions \u2013 the reliable foundation that keeps database operations dependable even when things go wrong.\n\nIn this post, you'll learn:\n\n- What ACID transactions are (beyond just a memorable acronym)\n- Why they matter specifically for data engineers and analysts\n- How they work in SQL databases like SQL Server (and we'll look at DuckDB examples)\n- Their evolving role in the NoSQL world\n- Practical code examples you can adapt to your work\n\nThink of this as your comprehensive guide to ensuring your database changes stick when they should, disappear when they shouldn't, and generally don't leave you with a mess to clean up. Let's dive in!\n\n## What Exactly Are ACID Transactions?\n\nAt its core, ACID stands for **A**tomicity, **C**onsistency, **I**solation, and **D**urability \u2013 four properties that collectively ensure your database transactions are processed reliably. A transaction in database terms is simply a sequence of operations treated as a single logical unit of work.\n\nWhen you bundle your database operations (inserts, updates, deletes) into a transaction, the database essentially makes a promise to uphold these ACID guarantees. This is critical in any system where data integrity is non-negotiable \u2013 banking, inventory management, or really any serious data application.\n\n### A: Atomicity (All or Nothing)\n\nAtomicity ensures that a transaction is treated as a single, indivisible unit. Either all operations within the transaction succeed, or none of them do. If any part fails due to errors or crashes, the entire transaction rolls back, returning the database to its previous state.\n\nWhy care? Imagine transferring $100 from Account A to Account B. This requires two operations: debiting A and crediting B. Atomicity guarantees you won't end up in a state where A is debited but B never gets credited if something goes wrong midway.\n\n#### Code Example in DuckDB\n\nCopy code\n\n```\n-- First, let's create a sample Accounts table if it doesn't exist\nCREATE TABLE IF NOT EXISTS Accounts (\nAccountID INTEGER PRIMARY KEY,\nBalance DECIMAL(10,2)\n);\n-- Insert sample data if table is empty\nINSERT OR IGNORE INTO Accounts VALUES\n(1, 500.00),\n(2, 300.00);\n-- Check balances before transfer\nSELECT * FROM Accounts WHERE AccountID IN (1, 2);\n-- Start the transaction\nBEGIN TRANSACTION;\n-- Step 1: Debit Account A\nUPDATE Accounts\nSET Balance = Balance - 100\nWHERE AccountID = 1;\n-- Step 2: Credit Account B\nUPDATE Accounts\nSET Balance = Balance + 100\nWHERE AccountID = 2;\n-- Commit the transaction to make changes permanent\nCOMMIT;\n-- Check balances after transfer\nSELECT * FROM Accounts WHERE AccountID IN (1, 2);\n-- Note: If you need to cancel the transaction, you would use:\n-- ROLLBACK;\n-- instead of COMMIT;\n```\n\n\n#### Code Explanation\n\n`BEGIN TRANSACTION;`\n\nmarks the start of our atomic unit.- The first\n`UPDATE`\n\nstatement debits money from Account A. The second`UPDATE`\n\nstatement credits money to Account B. `COMMIT;`\n\nfinalizes the transaction, making the changes permanent if both operations succeeded.`ROLLBACK;`\n\n(when needed) undoes all changes if any error occurs between`BEGIN`\n\nand`COMMIT`\n\n.\n\n**Practical Tip:** In production systems, it's typical to wrap these operations in TRY...CATCH blocks (or the equivalent in the chosen language/framework) to automatically trigger a `ROLLBACK`\n\nif any error occurs within the transaction.\n\n### C: Consistency (Keeping it Valid)\n\nConsistency ensures that a transaction brings the database from one valid state to another. This means the data not only makes logical sense (like the total money in our bank example staying the same), but also adheres to all defined database rules and constraints (`NOT NULL`\n\n, `UNIQUE`\n\n, `CHECK`\n\n, `FOREIGN KEY`\n\n).\n\nWhy care? This prevents bad data from entering your database. If you have a rule that account balances cannot be negative, a transaction attempting to overdraw an account would fail consistency checks and roll back.\n\n#### Code Example (Adding Constraints in DuckDB)\n\nCopy code\n\n```\n-- Example showing constraints in DuckDB\n-- Important: DuckDB does not support ADD CONSTRAINT and DROP CONSTRAINT in ALTER TABLE statements\n-- Since we can't add constraints after table creation, we need to create the table with all constraints\nDROP TABLE IF EXISTS Accounts;\n-- Create the table with the CHECK constraint included from the beginning\nCREATE TABLE Accounts (\nAccountID INTEGER PRIMARY KEY,\nBalance DECIMAL(10,2) CHECK (Balance >= 0) -- Constraint defined at table creation time\n);\n-- Insert sample data\nINSERT INTO Accounts VALUES\n(1, 100.00), -- Account 1 has exactly $100\n(2, 300.00);\n-- Show initial account balances\nSELECT * FROM Accounts WHERE AccountID IN (1, 2);\n-- Now, let's try a transaction that violates this consistency rule\nBEGIN TRANSACTION;\n-- Try to debit $150 from Account A which only has $100\n-- This will fail due to the CHECK constraint\nUPDATE Accounts\nSET Balance = Balance - 150\nWHERE AccountID = 1;\n-- The following line won't execute if the UPDATE fails\n-- but we include it for completeness\nSELECT 'After update attempt (this may not run if constraint fails)' AS Status,\n* FROM Accounts WHERE AccountID = 1;\n-- This COMMIT won't happen automatically if the constraint fails\n-- DuckDB will roll back automatically on constraint violation\nCOMMIT;\n-- Check final account balances - should be unchanged due to rollback\nSELECT 'Final balances after constraint violation' AS Status,\n* FROM Accounts WHERE AccountID IN (1, 2);\n-- Let's try a valid transaction now\nBEGIN TRANSACTION;\n-- Withdraw just $50 instead (which is valid)\nUPDATE Accounts\nSET Balance = Balance - 50\nWHERE AccountID = 1;\n-- Show the pending change\nSELECT 'Valid update in progress' AS Status,\n* FROM Accounts WHERE AccountID = 1;\n-- Commit the valid transaction\nCOMMIT;\n-- Show final state after valid transaction\nSELECT 'Final balances after valid transaction' AS Status,\n* FROM Accounts WHERE AccountID IN (1, 2);\n```\n\n\n#### Code Explanation\n\n`CREATE TABLE ... CHECK ...`\n\ndefines a rule at the database level when the table is created (unlike some other databases, DuckDB requires constraints to be defined at creation time, not with`ALTER TABLE`\n\n).- Our\n`TRANSACTION`\n\nattempts an operation that would result in a negative balance. - The database engine enforces the\n`CHECK`\n\nconstraint, causing the`UPDATE`\n\nstatement to fail. - Since the transaction cannot complete while satisfying all constraints, it is automatically rolled back by DuckDB.\n- A subsequent transaction with a valid withdrawal amount succeeds because it maintains the constraint.\n\n[does not currently support](https://duckdb.org/docs/stable/sql/statements/alter_table.html#add--drop-constraint)adding or dropping constraints after table creation with `ALTER TABLE` statements. All constraints must be defined during the initial `CREATE TABLE` statement, unlike some other database systems that allow constraints to be modified later.\n\n### I: Isolation (Playing Nicely with Others)\n\nIsolation ensures that concurrent transactions don't interfere with each other. From any single transaction's perspective, it should appear as if it's the only one operating on the database at that moment.\n\nThis prevents phenomena like:\n\n**Dirty Reads:**Reading data another transaction has modified but not yet committed**Non-Repeatable Reads:**Getting different results when reading the same row multiple times within a transaction**Phantom Reads:**Getting different results when querying a range of rows because another transaction inserted or deleted matching rows\n\nWhy care? Without isolation, imagine running a financial report that sums up sales while another transaction is actively adding new sales records. Your report could end up with inconsistent numbers depending on timing.\n\nDatabases implement isolation using mechanisms like locking or multiversion concurrency control (MVCC). SQL defines standard [transaction isolation levels](https://en.wikipedia.org/wiki/Isolation_(database_systems)) (`READ UNCOMMITTED`\n\n, `READ COMMITTED`\n\n, `REPEATABLE READ`\n\n, `SERIALIZABLE`\n\n) that let you balance consistency against performance.\n\n#### Code Example (Isolation in DuckDB)\n\nCopy code\n\n```\n-- Drop and recreate the Accounts table\nDROP TABLE IF EXISTS Accounts;\nCREATE TABLE Accounts (\nAccountID INTEGER PRIMARY KEY,\nBalance DECIMAL(10,2) CHECK (Balance >= 0)\n);\n-- Insert sample data\nINSERT INTO Accounts VALUES\n(1, 100.00),\n(2, 300.00);\n-- DuckDB uses snapshot isolation by default, which is equivalent to SERIALIZABLE\n-- and provides the highest isolation level\n-- Start a transaction\nBEGIN TRANSACTION;\n-- With DuckDB's snapshot isolation, if you read rows,\n-- you'll get a consistent snapshot of the database at the start of the transaction\n-- regardless of other concurrent transactions\n-- Take a snapshot of the total balance at the beginning of the transaction\nSELECT 'Initial snapshot of total balance' AS QueryInfo, SUM(Balance) AS TotalBalance\nFROM Accounts;\n-- Some other operations...\n-- Let's simulate some time passing by doing another operation\nSELECT COUNT(*) AS TotalAccounts FROM Accounts;\n-- Rerun the same query - guaranteed to get the same result\n-- even if another transaction added or modified rows in between (due to snapshot)\nSELECT 'Second snapshot (should match initial)' AS QueryInfo, SUM(Balance) AS TotalBalance\nFROM Accounts;\n-- Commit the transaction\nCOMMIT;\n```\n\n\n#### Code Explanation\n\n- In DuckDB, transaction isolation is handled automatically with snapshot isolation (equivalent to SERIALIZABLE in the SQL standard), so there's no need to explicitly set isolation levels.\n- When a transaction begins with\n`BEGIN TRANSACTION`\n\n, DuckDB creates a consistent snapshot of the database that remains stable throughout the transaction's lifetime. - This snapshot isolation guarantees that all reads within the transaction see a consistent view of the database as it existed at the start of the transaction, regardless of changes made by other concurrent transactions.\n- The database enforces these isolation rules automatically to prevent concurrency issues while providing the highest level of transaction integrity.\n\n### D: Durability (Making it Stick)\n\nDurability guarantees that once a transaction has been successfully committed, its changes are permanent and will survive system failures like power outages or server crashes.\n\nWhy care? This is your ultimate safety net. If your `COMMIT TRANSACTION`\n\ncall returns successfully, you have the database's promise that the data is safely stored and won't vanish even if the power goes out immediately afterward.\n\nDatabases typically achieve durability using techniques like [write-ahead logging (WAL)](https://en.wikipedia.org/wiki/Write-ahead_logging). Changes are written to a transaction log file on stable storage before the actual database files are modified. If a crash occurs, the database can use these logs during recovery.\n\n#### Code Example\n\nCopy code\n\n```\nBEGIN TRANSACTION;\n-- Perform critical updates\n-- Deduct $50 from Account 1\nUPDATE Accounts SET Balance = Balance - 50.00 WHERE AccountID = 1;\n-- Add $50 to Account 2\nUPDATE Accounts SET Balance = Balance + 50.00 WHERE AccountID = 2;\n-- This is the point of no return.\n-- Once COMMIT succeeds, the changes are durable.\nCOMMIT TRANSACTION;\n-- If the server crashes after this COMMIT returns success,\n-- the changes are guaranteed to persist through recovery.\n```\n\n\n#### Code Explanation\n\n- The\n`COMMIT TRANSACTION`\n\nsignals the successful end of the transaction and ensures that all updates \u2014 such as transferring funds between accounts \u2014 are made durable. - DuckDB, like other ACID-compliant systems, handles the underlying durability mechanisms (e.g., write-ahead logging) automatically. Once\n`COMMIT`\n\ncompletes, the changes are safely persisted and will survive a crash or restart.\n\n## What About ACID Transactions in NoSQL?\n\nHistorically, many NoSQL databases prioritized availability and scalability over strict ACID guarantees, often following BASE (Basically Available, Soft state, Eventually consistent) principles.\n\nHowever, the landscape has evolved significantly:\n\n- Many NoSQL databases now offer ACID guarantees for operations within a single document or record\n- Several modern NoSQL systems (MongoDB, RavenDB, FaunaDB, certain DynamoDB configurations) support multi-document ACID transactions\n- Implementation details and performance characteristics may differ from traditional relational databases\n\nSo, can NoSQL be ACID-compliant? Increasingly, yes \u2013 but check the specific capabilities of your chosen database.\n\n## MotherDuck and ACID Compliance\n\nSpeaking of modern database solutions, [MotherDuck](https://motherduck.com/) \u2013 the cloud service for [DuckDB](https://duckdb.org/) \u2013 brings ACID transaction support to analytics workloads. As a managed service built on DuckDB's foundation, MotherDuck maintains DuckDB's ACID compliance while adding cloud-native features. This means data engineers can confidently run their analytical queries and transformations with the same transactional guarantees we've discussed, even at scale. When you're processing large analytical datasets, having these ACID guarantees prevents the kind of partial updates or inconsistent results that can waddle their way into your dashboards and reports. For data teams looking to maintain data integrity across local and cloud environments, MotherDuck's approach ensures your analytical pipelines don't sacrifice reliability for performance.\n\n## Why Should Data Engineers and Analysts Care About ACID?\n\nUnderstanding ACID is essential for data professionals:\n\n**Data Integrity:**Understanding ACID helps you design data pipelines and transformation logic that preserves correctness and reliability. No more mysterious inconsistencies from partial updates in failed batch jobs.**Troubleshooting:**When things go wrong (and they will), knowledge of transactions provides a framework for debugging. Was data lost? Check durability. Seeing weird intermediate values? Suspect isolation issues. Pipeline failed mid-way? Thank atomicity for rolling things back cleanly.**System Design:**When choosing databases or designing data flows, understanding trade-offs between ACID and other models helps you select the right tool based on consistency needs versus performance requirements.**Writing Robust Code:**Explicitly using`BEGIN`\n\n,`COMMIT`\n\n, and`ROLLBACK`\n\nconstructs makes your code significantly more reliable and easier to reason about.\n\n## Wrapping Up\n\nACID transactions aren't just theoretical database concepts \u2013 they're fundamental pillars ensuring the reliability of data we work with daily. Whether you're building ETL pipelines, modifying data in SQL databases like DuckDB or SQL Server, or even working with modern NoSQL databases, understanding Atomicity, Consistency, Isolation, and Durability helps you build more robust systems and avoid those frustrating data anomalies.\n\nKeep these ACID properties in your toolkit, and your data will remain as organized as ducks in a row. Your future self (and your data consumers) will thank you!\n\n**Frequently Asked Questions (FAQ)**\n\n**1. What is ACID in simple terms?**\n\nACID stands for Atomicity, Consistency, Isolation, and Durability. It's a set of four properties that guarantee database transactions are processed reliably. In simple terms, it's a contract that ensures your data remains correct and uncorrupted, even when multiple operations happen at once or if the system crashes.\n\n**2. Why are ACID transactions important for data integrity?**\n\nACID transactions are the foundation of data integrity in relational databases. They prevent common data corruption scenarios:\n\n**Atomicity**stops partial updates from failed pipelines. This means that a transaction is an \"all or nothing\" event; either the entire transaction completes successfully, or it leaves the database unchanged.**Consistency**enforces data quality rules (e.g., a product must have a price), ensuring the database transitions from one valid state to another.**Isolation**prevents analytics queries from reading incomplete or \"dirty\" data by making sure concurrent transactions do not interfere with each other.**Durability**ensures that once data is written, it's saved permanently, even in the event of a system failure.\n\nWithout these guarantees, it would be nearly impossible to trust the data in your dashboards, reports, or applications.\n\n**3. Is DuckDB fully ACID compliant?**\n\nYes, [DuckDB is fully ACID compliant](https://duckdb.org/2024/09/25/changing-data-with-confidence-and-acid.html). It supports serializable transactions, which is the highest level of isolation. This is achieved through a custom, bulk-optimized Multi-Version Concurrency Control (MVCC) system. This makes it a reliable tool for data professionals who need to perform complex, multi-step analytical queries and transformations with the same data integrity guarantees found in larger server-based databases like PostgreSQL or SQL Server.\n\n**4. Do NoSQL databases use ACID?**\n\nIt's a mixed bag. Historically, many NoSQL databases sacrificed strict ACID compliance for higher availability and scalability, a model often described by the acronym BASE (Basically Available, Soft state, Eventual consistency). However, many modern NoSQL databases, such as [MongoDB](https://www.mongodb.com/resources/products/capabilities/acid-compliance), now offer full ACID compliance, especially for multi-document transactions, as the demand for data reliability has grown.\n\n**5. What's a real-world example of an ACID transaction?**\n\nA classic example is an e-commerce order placement. The process involves multiple steps that must be treated as a single transaction:\n\n- Decrease the product's stock level in the inventory table.\n- Create a new order in the orders table.\n- Process the payment via a payment gateway.\n\nIf the payment fails (Step 3), **atomicity** ensures the inventory is restocked and the order is canceled (rolled back). **Consistency** ensures the stock level can't go below zero. **Isolation** prevents another customer from buying the last item while your order is being processed. **Durability** ensures that once your order is confirmed, it remains in the system even if the server reboots.\n\nStart using MotherDuck now!",
  [2m  â”‚[0m     "numLines": 361,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 361
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me search for more specific information about MotherDuck limitations and DuckDB features not supported in MotherDuck:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ UDF|user.defined|extension|ATTACH|server.side|Python function

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     "content": "---\ntitle: bringing-duckdb-to-the-cloud-dual-execution-explained\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained\nindexed_at: '2025-11-25T20:45:20.295187'\ncontent_hash: ab4030806c409492\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nBringing DuckDB to the Cloud: Dual Execution Explained - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Bringing DuckDB to the Cloud: Dual Execution Explained](https://www.youtube.com/watch?v=2eVNm4A8sLw)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=2eVNm4A8sLw&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 1:08:11\n\n\u2022Live\n\n\u2022\n\nYouTubeQuack & CodeMotherDuck Features\n\n# Bringing DuckDB to the Cloud: Dual Execution Explained\n\n2024/06/28\n\nBringing DuckDB's analytical power to the cloud requires more than just running it on a server. While DuckDB excels at processing data on a local machine, the dynamics of data analysis change when dealing with cloud-scale data, collaboration, and shared resources. This shift introduces challenges around [security](https://motherduck.com/learn-more/secure-startup-data-warehouse/), concurrent access, and performance, which is where a purpose-built cloud architecture becomes essential.\n\nMotherDuck, a modern cloud data warehouse, is built on DuckDB to solve these challenges. It extends DuckDB\u2019s capabilities without forking the open source project, creating a hybrid system that intelligently balances local and cloud computing. In a conversation with MotherDuck founding engineer Stephanie, she explained the architecture and the innovative query model, known as **dual execution**, that makes this possible.\n\n* * *\n\n## The MotherDuck Architecture: Beyond Hosted DuckDB\n\nA common misconception is that MotherDuck is simply DuckDB hosted in the cloud. The reality is a more sophisticated system designed to overcome the limitations of using a locally-optimized engine in a distributed environment. MotherDuck\u2019s architecture is built on three key components and a core philosophy of not forking DuckDB.\n\nInstead of maintaining a separate version, MotherDuck leverages DuckDB\u2019s powerful **extension system**. This allows MotherDuck to add new capabilities at multiple layers, including the SQL parser, optimizer, and storage interface, while staying current with the latest open source DuckDB releases. This tight integration means users benefit from the rapid innovation of the DuckDB community almost immediately.\n\nThe architecture can be broken down into three main layers:\n\n1. **The Client Extension**: This is how MotherDuck integrates with the DuckDB ecosystem. Whether a user is working in the CLI, a Python script, or a JDBC connection, a simple `ATTACH` command connects their local DuckDB instance to the MotherDuck cloud. This extension is even used to run DuckDB in the browser via WASM for the MotherDuck UI, enabling client-side processing to reduce latency and cloud compute costs.\n2. **The Compute Layer**: In the cloud, queries are processed by containerized DuckDB instances, fondly called \"ducklings.\" These compute resources are scaled based on user needs, providing the necessary CPU and memory to handle complex analytical queries on large datasets.\n3. **The Storage Layer**: DuckDB's native file format is optimized for a single writer on a local file system. This model is not well-suited for the cloud, where multiple users need to read and write to the same database concurrently. To solve this, MotherDuck implemented a differential storage system that maps a logical database file to append-only snapshot layers in cloud object storage. This design is cloud-friendly, enabling efficient in-place updates and forming the foundation for features like database sharing and time travel.\n\n* * *\n\n## Enabling Collaboration with Database Sharing and Secure Credentials\n\nThis unique architecture transforms DuckDB from a \"single-player\" tool into a collaborative \"multiplayer\" platform. One of the most significant advantages is **database sharing**. Instead of emailing SQL scripts or passing around large database files, team members within the same organization can grant query access to their databases. This streamlines collaboration, ensuring everyone works from a consistent and up-to-date version of the data.\n\nSecurity is another critical aspect of any cloud data platform. MotherDuck provides a **centralized Secret Manager**, co-designed with the DuckDB team. Users can create persistent, encrypted secrets for accessing external data sources like AWS S3, Google Cloud Storage, or Delta Lake. Once a secret is created in MotherDuck, it can be reused across different clients and sessions without needing to expose or reconfigure credentials on each local machine.\n\n* * *\n\n## Understanding Dual Execution\n\nThe most innovative feature of MotherDuck's architecture is its **dual execution model**. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\n\nThe process was demonstrated with a query joining two tables. When both tables reside in a MotherDuck database, the query is executed entirely in the cloud. An `EXPLAIN` plan for this query reveals that all operators, from the table scans to the final join, are marked as **remote**. The only local operation is downloading the final result set to the client.\n\nThe real power of dual execution becomes apparent when a query involves both local and cloud data. In the demonstration, one of the remote tables was copied to a local Parquet file. The query was then modified to join this local file with the table that remained in the cloud.\n\nThe `EXPLAIN` plan for this new query showed a mix of local and remote operations. The scan of the Parquet file and its associated processing happened **locally**, leveraging the user's machine. The scan of the cloud table happened **remotely** on MotherDuck's compute. The system then efficiently transferred only the necessary intermediate data to complete the join. This hybrid approach avoids needlessly uploading the local file to the cloud or downloading the entire remote table, resulting in faster and more efficient queries.\n\n* * *\n\n## Taking Control with Manual Execution Overrides\n\nWhile MotherDuck's optimizer is designed to make the most efficient choice automatically, some situations require manual control. For this, users can use the `md_run` parameter within scan functions like `read_parquet()` or `read_csv()`.\n\nBy setting `md_run = 'local'`, a user can force the scan of a remote S3 file to be executed on their local client. This involves downloading the data, which might be desirable if the data needs to be processed near the user's location or if they prefer using local credentials. Conversely, setting `md_run = 'remote'` forces the operation to run in the MotherDuck cloud.\n\nThis control is especially useful for performance tuning. A direct comparison showed that querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds). By pushing the computation to the data's location, MotherDuck minimizes network I/O and delivers results more quickly.\n\n* * *\n\n## A Symbiotic Future for Local and Cloud Analytics\n\nMotherDuck's architecture is a thoughtful extension of DuckDB's core principles, adapting its local-first power for the demands of the cloud. The dual execution model provides a flexible and efficient bridge between a user's laptop and cloud data, optimizing workloads in a way that pure-cloud or pure-local systems cannot.\n\nThis relationship is symbiotic. As the largest production user of DuckDB, MotherDuck continuously pushes the engine to its limits, uncovering opportunities for improvement and contributing enhancements back to the open source project. This collaboration ensures that both DuckDB and MotherDuck will continue to evolve, offering a powerful and seamless analytical experience that spans from local exploration to cloud-scale production.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Ficeberg_video_thumbnail_adfdabe3d6.jpg&w=3840&q=75)\\\\\n\\\\\n46:26](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n[2025-06-12](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n### [Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse)\n\nBuilding a data stack means choosing between easy SaaS and complex open-source. Apache Iceberg is a middle ground, but its catalog is a hurdle. New tools now simplify using Iceberg with DuckDB to create a flexible, local data lakehouse.\n\nMotherDuck Features\n\n[![\"Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FG_Se_B_Sox_AW_Fg_HD_f4fddaa9ab.jpg&w=3840&q=75)](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n[2025-04-23](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n### [Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly)\n\nHamilton Ulmer shares insights from MotherDuck's Instant SQL Mode, exploring how real-time query result previews eliminate the traditional write-run-debug cycle through client-side parsing and DuckDB-WASM caching.\n\nSQL\n\nTalk\n\nMotherDuck Features\n\n[![\"DuckDB: Run dbt build with sub-second execution times\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_03_26_at_3_40_16_PM_c6e3a8096f.png&w=3840&q=75)\\\\\n\\\\\n24:06](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n[2025-03-13](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n### [DuckDB: Run dbt build with sub-second execution times](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times)\n\nWhether you're new to DuckDB or looking to optimize your workflows, this session will provide practical insights to help you leverage its full potential.\n\nYouTube\n\nData Pipelines\n\ndbt\n\nMotherDuck Features\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 184,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 184
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/spatial-data-management-with-duckdb-ft-mattforrest.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/panel-fixing-the-data-engineering-lifecycle-coalesce-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md",
  [2m  â”‚[0m     "content": "---\ntitle: 'DuckDB Python Quickstart (Part 2): Pandas, Arrow, Polars & Python UDFs'\ncontent_type: tutorial\ndescription: Take your DuckDB and Python skills further by learning how to query Pandas\n  DataFrames directly with SQL. This guide shows you how to integrate with Arrow and\n  Polars and extend DuckDB by writing your own custom Python UDFs.\npublished_date: '2025-11-21T00:00:00'\nsource_url: https://motherduck.com/learn-more/duckdb-python-quickstart-part2\nindexed_at: '2025-11-25T10:52:39.097304'\ncontent_hash: 92a22a3264093f33\nhas_code_examples: true\nhas_step_by_step: true\n---\n\n# DuckDB Python Quickstart (Part 2): Pandas, Arrow, Polars & Python UDFs\n\n15 min readBYWelcome back to your DuckDB Python quickstart series! In [Part 1](https://motherduck.com/learn-more/duckdb-python-quickstart-part1/), you covered the essentials: getting DuckDB installed, making connections, executing basic SQL queries using the `.sql()`\n\nand `.execute()`\n\nmethods, ingesting data directly from files, and leveraging the powerful Relational API for programmatic query building, including set operations and joins.\n\nIn this second part, you'll explore the features that truly make DuckDB a first-class citizen in the Python data ecosystem. You'll see how DuckDB seamlessly integrates with popular libraries like Pandas, Apache Arrow, and Polars, and how you can extend DuckDB's functionality by writing your own functions in Python.\n\nNow you can pick up where you left off and see how DuckDB interacts with your existing Python data structures! And like last time, you can follow this tutorial through [this notebook](https://github.com/adisomani/duckdb-notebooks/blob/main/duckdb_python_quickstart-part2.ipynb) also.\n\n## Integrating with the Python Data Ecosystem: Pandas, Arrow, Polars\n\nThis is where DuckDB really shines for data professionals living in the Python world. DuckDB is designed to work *with* your existing data structures, not just separate from them.\n\n### Querying Pandas DataFrames\n\nGot a Pandas DataFrame loaded in memory? You can query it directly as if it were a table.\n\nCopy code\n\n```\nimport pandas as pd\nimport duckdb\n# Create a sample Pandas DataFrame\ndata = {'col1': [1, 2, 3, 4],\n'col2': ['A', 'B', 'C', 'A']}\ndf = pd.DataFrame(data)\n# Connect to DuckDB (in-memory for this example)\ncon = duckdb.connect(database=':memory:')\n# Query the DataFrame using duckdb.sql() via the connection\nresult_relation = con.sql(\"SELECT * FROM df WHERE col2 = 'A'\")\nresult_relation.show()\n```\n\n\nCopy code\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col1 \u2502 col2 \u2502\n\u2502 int64 \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1 \u2502 A \u2502\n\u2502 4 \u2502 A \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n\nThat's it! DuckDB automatically recognizes the DataFrame variable `df`\n\nin your Python environment (when using a connection or the default global connection) and makes it available as if it were a table in the `FROM`\n\nclause. This is incredibly powerful for quickly querying, filtering, joining, or aggregating DataFrames using familiar SQL syntax, which can often be much faster than equivalent Pandas operations for certain types of queries, especially aggregates and complex joins on larger DataFrames.\n\nYou can also query the DataFrame using the relational API:\n\nCopy code\n\n```\n# Query the DataFrame using the relational API via the connection\n(con.sql(\"FROM df\") # Start with the DataFrame as a relation\n.filter(\"col2 = 'A'\")\n.show()\n)\n```\n\n\nCopy code\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col1 \u2502 col2 \u2502\n\u2502 int64 \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1 \u2502 A \u2502\n\u2502 4 \u2502 A \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n\nINFO: SQLAlchemy Integration\nFor more complex applications requiring an Object-Relational Mapper (ORM) or a standard database interface like SQLAlchemy, you can use the `duckdb-engine` package (`pip install duckdb-engine`). This provides a SQLAlchemy dialect that allows you to interact with DuckDB using the full power of SQLAlchemy, including querying DataFrames registered with the engine.\n### Getting Results as Pandas DataFrames\n\nGoing the other way is just as easy. Any `DuckDBPyRelation`\n\ncan be converted into a Pandas DataFrame using `.df()`\n\nor `.fetchdf()`\n\n.\n\nCopy code\n\n```\nimport duckdb\nimport pandas as pd\n# Connect to DuckDB (or reuse connection)\ncon = duckdb.connect(database=':memory:')\ncon.sql(\"INSTALL httpfs\")\ncon.sql(\"LOAD httpfs\")\n# Query a file or table to get a relation\npopulation_relation = con.sql(\"SELECT Country, Population FROM read_csv_auto('https://bit.ly/3KoiZR0')\")\n# Convert the result relation to a Pandas DataFrame\npopulation_df = population_relation.df()\nprint(type(population_df))\nprint(population_df.head())\n```\n\n\nCopy code\n\n```\n<class 'pandas.core.frame.DataFrame'>\nCountry Population\n0 Afghanistan 31056997\n1 Albania 3581655\n2 Algeria 32930091\n3 American Samoa 57794\n4 Andorra 71201\n```\n\n\nThis seamless back-and-forth between DuckDB relations and Pandas DataFrames removes a lot of the impedance mismatch you might face with other databases.\n\n### Working with Apache Arrow and Polars\n\nDuckDB also has deep integration with Apache Arrow, the standard for in-memory columnar data. Arrow enables zero-copy data transfer between DuckDB and other libraries that support Arrow, like Pandas (under the hood with newer versions), PyArrow itself, and the rapidly growing Polars library.\n\nYou can convert a DuckDB relation to an Arrow Table using `.arrow()`\n\nor `.fetcharrow()`\n\n, or the equivalent `.to_arrow_table()`\n\n:\n\nCopy code\n\n```\nimport duckdb\nimport pyarrow as pa\n# Make sure you have pyarrow installed: pip install pyarrow\n# Connect to DuckDB (or reuse connection)\ncon = duckdb.connect(database=':memory:')\ncon.sql(\"INSTALL httpfs\")\ncon.sql(\"LOAD httpfs\")\n# Get a relation (from a file, table, or query)\ncountries_relation = con.sql(\"SELECT Country, Region FROM read_csv_auto('https://bit.ly/3KoiZR0') LIMIT 10\")\n# Convert to an Apache Arrow Table\narrow_table = countries_relation.arrow() # Or countries_relation.to_arrow_table()\nprint(type(arrow_table))\nprint(arrow_table)\n```\n\n\nCopy code\n\n```\n<class 'pyarrow.lib.RecordBatchReader'>\n<pyarrow.lib.RecordBatchReader object at 0x123a7b630>\n```\n\n\nSimilarly, converting to a Polars DataFrame is just as easy with `.pl()`\n\n(requires the `polars`\n\nlibrary installed: `pip install polars`\n\n):\n\nCopy code\n\n```\nimport duckdb\nimport polars as pl\n# Connect to DuckDB (or reuse connection)\ncon = duckdb.connect(database=':memory:')\ncon.sql(\"INSTALL httpfs\")\ncon.sql(\"LOAD httpfs\")\n# Get a relation\ncountries_relation = con.sql(\"SELECT Country, Region FROM read_csv_auto('https://bit.ly/3KoiZR0') LIMIT 10\")\n# Convert to a Polars DataFrame\npolars_df = countries_relation.pl()\nprint(type(polars_df))\nprint(polars_df)\n```\n\n\nCopy code\n\n```\n<class 'polars.dataframe.frame.DataFrame'>\nshape: (10, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Country \u2506 Region \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 str \u2506 str \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Afghanistan \u2506 ASIA (EX. NEAR EAST) \u2502\n\u2502 Albania \u2506 EASTERN EUROPE \u2026 \u2502\n\u2502 Algeria \u2506 NORTHERN AFRICA \u2026 \u2502\n\u2502 American Samoa \u2506 OCEANIA \u2026 \u2502\n\u2502 Andorra \u2506 WESTERN EUROPE \u2026 \u2502\n\u2502 Angola \u2506 SUB-SAHARAN AFRICA \u2026 \u2502\n\u2502 Anguilla \u2506 LATIN AMER. & CARIB \u2502\n\u2502 Antigua & Barbuda \u2506 LATIN AMER. & CARIB \u2502\n\u2502 Argentina \u2506 LATIN AMER. & CARIB \u2502\n\u2502 Armenia \u2506 C.W. OF IND. STATES \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n\nINFO: Deferring Materialization\nConverting a DuckDB relation to a Python DataFrame/Table (`.df()`, `.arrow()`, `.pl()`) materializes the entire result set in Python memory. For performance, it's generally recommended that you perform as many filtering, projection, aggregation, and joining steps as possible using DuckDB's SQL or Relational API *before* converting to a Python object. This allows DuckDB's optimized query engine to process the data efficiently, often without bringing everything into Python memory until the final result is needed.\nOnce you have data as an Arrow table, you can use `pyarrow.compute`\n\nfor further operations directly within Arrow if needed, though DuckDB often remains faster for many analytical queries:\n\nCopy code\n\n```\nimport pyarrow.compute as pc\nimport pyarrow as pa\n# Convert RecordBatchReader to Table\narrow_table = arrow_table.read_all()\n# Now you can use filter and select\nfiltered_arrow = arrow_table.filter(pc.match_substring(arrow_table['Country'], 'America'))\nselected_arrow = filtered_arrow.select(['Country', 'Region'])\nprint(\"Filtered and Selected Arrow Table:\")\nprint(selected_arrow)\n```\n\n\nCopy code\n\n```\nFiltered and Selected Arrow Table:\npyarrow.Table\nCountry: string\nRegion: string\n----\nCountry: [[\"American Samoa \"]]\nRegion: [[\"OCEANIA \"]]\n```\n\n\nThis interoperability makes DuckDB a fantastic glue layer for data pipelines involving various Python libraries.\n\n## Extending DuckDB with Python: User-Defined Functions (UDFs)\n\nWARNING: UDFs and MotherDuck Please note that User-Defined Functions (UDFs), as described in this section, are a feature of the local, embedded DuckDB Python library and**do not work on MotherDuck**. The reason is that UDFs require a Python runtime to execute the function's code. MotherDuck is a serverless platform that provides SQL execution but does not run user-provided Python code on its servers. The examples below are for local DuckDB usage within a Python environment.\n\nSometimes you need to perform an operation within your SQL query that's simply not available in standard SQL or DuckDB's built-in functions, but it's easy to do in Python. This is where User-Defined Functions (UDFs) come in. DuckDB lets you define Python functions and call them directly from your SQL queries.\n\nLet's revisit the population data. Looking at the `Region`\n\ncolumn, there seem to be some extra spaces (padding) that make grouping or filtering tricky.\n\nCopy code\n\n```\nimport duckdb\n# Connect to DuckDB and load population data\ncon = duckdb.connect(database=':memory:') # Or use the persistent DB file\ncon.sql(\"INSTALL httpfs\")\ncon.sql(\"LOAD httpfs\")\n# Load the data into a table if it doesn't exist in this session\ntry:\ncon.sql(\"SELECT COUNT(*) FROM population\")\nexcept duckdb.CatalogException:\nprint(\"Loading population data from URL...\")\ncon.sql(\"SELECT * FROM read_csv_auto('https://bit.ly/3KoiZR0')\").to_table(\"population\")\nprint(\"Population data loaded.\")\ncon.sql(\"\"\"\nSELECT DISTINCT Region, length(Region) AS numChars\nFROM population\n\"\"\").show()\n```\n\n\nCopy code\n\n```\nLoading population data from URL...\nPopulation data loaded.\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Region \u2502 numChars \u2502\n\u2502 varchar \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 WESTERN EUROPE \u2502 35 \u2502\n\u2502 SUB-SAHARAN AFRICA \u2502 35 \u2502\n\u2502 NEAR EAST \u2502 35 \u2502\n\u2502 C.W. OF IND. STATES \u2502 20 \u2502\n\u2502 BALTICS \u2502 35 \u2502\n\u2502 ASIA (EX. NEAR EAST) \u2502 29 \u2502\n\u2502 OCEANIA \u2502 35 \u2502\n\u2502 NORTHERN AMERICA \u2502 35 \u2502\n\u2502 NORTHERN AFRICA \u2502 35 \u2502\n\u2502 EASTERN EUROPE \u2502 35 \u2502\n\u2502 LATIN AMER. & CARIB \u2502 23 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 11 rows 2 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n\nSee those character counts? 23 for \"LATIN AMER. & CARIB\" looks about right, but 35 for \"BALTICS\" (which is 7 characters long)? Definitely trailing spaces.\n\nDuckDB has a built-in `trim()`\n\nfunction, but for the sake of demonstration, you can write a Python UDF to remove leading/trailing spaces.\n\nCopy code\n\n```\ndef remove_spaces_py(field: str) -> str:\n\"\"\"Removes leading/trailing spaces from a string.\"\"\"\nif field is not None:\n# Use Python's strip()\nreturn field.strip() # Python's strip() removes both leading/trailing\n# Or use lstrip() and rstrip() specifically\n# return field.lstrip().rstrip()\nreturn field\n# Register the Python function as a SQL function in DuckDB\ncon.create_function('remove_spaces_py', remove_spaces_py)\n```\n\n\nYou defined a simple Python function `remove_spaces_py`\n\n. You used type hints (`str`\n\nfor input and output), which helps DuckDB infer the SQL types (VARCHAR). Then, `con.create_function()`\n\nregisters this Python function under a name you can use in SQL (`remove_spaces_py`\n\n).\n\n### Introspecting Registered Functions\n\nAfter registering a UDF, you can query DuckDB's built-in `duckdb_functions()`\n\ntable function to see information about all available functions, including your new one:\n\nCopy code\n\n```\ncon.sql(\"\"\"\nSELECT function_name, function_type, parameters, parameter_types, return_type\nFROM duckdb_functions()\nWHERE function_name = 'remove_spaces_py'\n\"\"\").show()\n```\n\n\nCopy code\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 function_name \u2502 function_type \u2502 parameters \u2502 parameter_types \u2502 return_type \u2502\n\u2502 varchar \u2502 varchar \u2502 varchar[] \u2502 varchar[] \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 remove_spaces_py \u2502 scalar \u2502 [col0] \u2502 [VARCHAR] \u2502 VARCHAR \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518```\nThis introspection confirms your function is registered correctly with the inferred types.\nNow, try using it in a query:\n```python\ncon.sql(\"\"\"\nSELECT\nRegion AS original_region,\nlength(Region) AS len1,\nremove_spaces_py(Region) AS cleaned_region,\nlength(remove_spaces_py(Region)) AS len2\nFROM population\nWHERE length(Region) > length(remove_spaces_py(Region)) -- Only show rows where trimming actually happened\nLIMIT 3\n\"\"\").show()\n```\n\n\nCopy code\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 original_region \u2502 len1 \u2502 cleaned_region \u2502 len2 \u2502\n\u2502 varchar \u2502 int64 \u2502 varchar \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 ASIA (EX. NEAR EAST) \u2502 29 \u2502 ASIA (EX. NEAR EAST) \u2502 20 \u2502\n\u2502 EASTERN EUROPE \u2502 35 \u2502 EASTERN EUROPE \u2502 14 \u2502\n\u2502 NORTHERN AFRICA \u2502 35 \u2502 NORTHERN AFRICA \u2502 15 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n\nSuccess! Your Python UDF `remove_spaces_py`\n\nis correctly callable from SQL and does its job.\n\nDuckDB usually does a good job inferring types from Python type hints. However, for clarity or if type hints are missing, you can explicitly specify the input and return types when registering the function:\n\nCopy code\n\n```\nfrom duckdb.sqltypes import VARCHAR\n# Remove the old function first (optional, but good practice if redefining)\ntry:\ncon.remove_function('remove_spaces_py')\nexcept duckdb.InvalidInputException: # Function might not exist yet\npass\n# Register again, explicitly specifying types\ncon.create_function(\n'remove_spaces_py',\nremove_spaces_py, # Use the Python function object\n[VARCHAR], # List of input types (a single VARCHAR parameter)\nVARCHAR # Return type (VARCHAR)\n)\n# Now you can use it just as before\n```\n\n\n### Real-World Data Cleaning: Handling Locale-Specific Decimals\n\nA common data cleaning task is handling numbers formatted according to different regional conventions, such as using a comma (`,`\n\n) as a decimal separator instead of a period (`.`\n\n). If ingested without proper handling, these numbers might be treated as strings.\n\nThe population dataset you are using includes columns like `\"Coastline (coast/area ratio)\"`\n\nand `\"Pop. Density (per sq. mi.)\"`\n\nwhich appear to use the European comma format. You can define a Python UDF using the `locale`\n\nmodule to convert these strings to numeric types.\n\nFirst, make sure the `locale`\n\nmodule is available and you have a locale installed that uses comma as a decimal separator (like 'de_DE' for German). You might need to configure your operating system's locales if they aren't available by default.\n\nCopy code\n\n```\nimport locale\nfrom duckdb.sqltypes import DOUBLE, VARCHAR\n# Define the Python function to convert locale-specific strings to float\ndef convert_locale_py(field: str) -> float:\n\"\"\"Converts a locale-specific string (e.g., using comma decimal) to a float.\"\"\"\nif field is None:\nreturn None\ntry:\n# Set locale temporarily (consider thread safety in multi-threaded apps)\n# You might need to adjust the locale string based on your system setup\noriginal_locale = locale.getlocale(locale.LC_NUMERIC)\n# The locale string can vary. 'de_DE.UTF-8' is common on Linux.\n# On Windows, it might be 'German_Germany.1252' or just 'de'.\n# On MacOS, it might be 'de_DE.UTF-8'.\n# We'll try a few common ones.\nlocales_to_try = ['de_DE.UTF-8', 'de_DE', 'de', 'German']\nfor loc in locales_to_try:\ntry:\nlocale.setlocale(locale.LC_NUMERIC, loc)\nbreak\nexcept locale.Error:\ncontinue\nelse:\n# If no locale worked, we can't do the conversion this way.\n# A more robust solution might be a simple string replace.\nreturn float(field.replace(',', '.'))\n# Use locale.atof to convert string to float based on locale settings\nresult = locale.atof(field)\n# Restore original locale\nlocale.setlocale(locale.LC_NUMERIC, original_locale)\nreturn result\nexcept (ValueError, TypeError):\nreturn None # Return None for conversion errors\n# Register the function with DuckDB, specifying input and output types\ncon.create_function(\n'convert_locale_py',\nconvert_locale_py,\n[VARCHAR], # Expecting a VARCHAR input\nDOUBLE # Returning a DOUBLE\n)\n```\n\n\n*(Note: Handling locales can be system-dependent. The locale string 'de_DE.UTF-8' might need adjustment. Setting and restoring the locale is important in applications to avoid side effects. The code above includes fallback logic for robustness.)*\n\nNow, use your `convert_locale_py`\n\nfunction in a query to see how it transforms the data:\n\nCopy code\n\n```\ncon.sql(\"\"\"\nSELECT\n\"Coastline (coast/area ratio)\" AS original_coastline,\nconvert_locale_py(\"Coastline (coast/area ratio)\") AS cleaned_coastline_double,\n\"Pop. Density (per sq. mi.)\" AS original_pop_density,\nconvert_locale_py(\"Pop. Density (per sq. mi.)\") AS cleaned_pop_density_double\nFROM population\nLIMIT 5\n\"\"\").show()\n```\n\n\nCopy code\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 original_coastline \u2502 cleaned_coastline_double \u2502 original_pop_density \u2502 cleaned_pop_density_double \u2502\n\u2502 varchar \u2502 double \u2502 varchar \u2502 double \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0,00 \u2502 0.0 \u2502 48,0 \u2502 48.0 \u2502\n\u2502 1,26 \u2502 1.26 \u2502 124,6 \u2502 124.6 \u2502\n\u2502 0,04 \u2502 0.04 \u2502 13,8 \u2502 13.8 \u2502\n\u2502 58,29 \u2502 58.29 \u2502 290,4 \u2502 290.4 \u2502\n\u2502 0,00 \u2502 0.0 \u2502 152,1 \u2502 152.1 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n\nExcellent! The UDF successfully converted the comma-separated strings to standard double-precision floating-point numbers.\n\nThis UDF capability opens the door to using any Python library within your SQL queries, from complex string manipulations with `re`\n\nto mathematical functions with `numpy`\n\nor `scipy`\n\n, or even calling external APIs (though be mindful of performance implications for row-by-row processing).\n\nOnce you've verified the conversion, you can use `ALTER TABLE`\n\nto change the column's data type permanently and apply the UDF to update the values in place:\n\nCopy code\n\n```\ncon.sql(\"\"\"\nALTER TABLE population\nALTER \"Coastline (coast/area ratio)\"\nSET DATA TYPE DOUBLE\nUSING convert_locale_py(\"Coastline (coast/area ratio)\")\n\"\"\")\ncon.sql(\"\"\"\nALTER TABLE population\nALTER \"Pop. Density (per sq. mi.)\"\nSET DATA TYPE DOUBLE\nUSING convert_locale_py(\"Pop. Density (per sq. mi.)\")\n\"\"\")\n# You would repeat this for other columns needing locale conversion like\n# \"Birthrate\", \"Deathrate\" etc.\n# Verify the column type change\ncon.sql(\"DESCRIBE population\").show()\n```\n\n\nCopy code\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_name \u2502 column_type \u2502 null \u2502 key \u2502 default \u2502 extra \u2502\n\u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Country \u2502 VARCHAR \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u2502 Region \u2502 VARCHAR \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u2502 Population \u2502 BIGINT \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u2502 Area (sq. mi.) \u2502 BIGINT \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u2502 Pop. Density (per sq. mi.) \u2502 DOUBLE \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u2502 Coastline (coast/area ratio) \u2502 DOUBLE \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u2502 Net migration \u2502 VARCHAR \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u2502 Infant mortality (per 1000 births) \u2502 VARCHAR \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u2502 GDP ($ per capita) \u2502 BIGINT \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u2502 Literacy (%) \u2502 VARCHAR \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u2502 Phones (per 1000) \u2502 VARCHAR \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u2502 Arable (%) \u2502 VARCHAR \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u2502 Crops (%) \u2502 VARCHAR \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u2502 Other (%) \u2502 VARCHAR \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u2502 Climate \u2502 VARCHAR \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u2502 Birthrate \u2502 VARCHAR \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u2502 Deathrate \u2502 VARCHAR \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u2502 Agriculture \u2502 VARCHAR \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u2502 Industry \u2502 VARCHAR \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u2502 Service \u2502 VARCHAR \u2502 YES \u2502 NULL \u2502 NULL \u2502 NULL \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 20 rows 6 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n\nINFO: UDF Performance Considerations\nWhile Python UDFs are incredibly flexible, they typically execute row-by-row and involve context switching between DuckDB's C++ execution engine and the Python interpreter. This can be slower than DuckDB's highly optimized vectorized native functions. You should use UDFs when a necessary operation *cannot* be done efficiently or at all in SQL, but prefer native SQL functions (`trim()`, `replace()`, etc.) for common tasks when available. For the locale conversion, using a UDF might be necessary if a suitable built-in function or reader option isn't available.\n## Closing Time\n\nWhen you're finished with a persistent database connection, it's good practice to close it:\n\nCopy code\n\n`con.close()`\n\n\nThis ensures any pending writes are flushed and resources are released. For in-memory databases created with `:memory:`\n\nor the default `duckdb.sql()`\n\nconnection, this isn't strictly necessary as they live and die with the Python process or script execution, but it doesn't hurt.\n\n## Wrapping Up\n\nThis two-part DuckDB Python quickstart has covered the essential features that make DuckDB such a powerful tool for data engineers, analysts, and scientists. Its embedded nature eliminates the overhead of managing a separate database server for local work. Its columnar architecture and vectorized execution make analytical queries on large datasets surprisingly fast.\n\nBut perhaps its biggest win is the deep integration with the Python data ecosystem. The ability to query Pandas DataFrames directly, seamlessly convert results to and from DataFrames, and leverage Arrow for efficient data transfer makes DuckDB feel like a natural extension of your Python data stack. The Relational API provides a robust, programmatic way to build queries, complementing standard SQL and enabling safer, more maintainable code. Add in the flexibility of Python UDFs for tackling custom cleaning and transformation tasks, and you have a powerful, high-performance tool that fits snugly into modern data workflows.\n\nSo next time you're faced with a pile of data files or a large DataFrame and need to slice and dice it with SQL or relational operations, don't groan about setting up a server. Just remember this DuckDB Python quickstart guide, and get quacking on your analysis!\n\nStart using MotherDuck now!",
  [2m  â”‚[0m     "numLines": 636,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 636
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md",
  [2m  â”‚[0m     "content": "---\ntitle: 'The Star Schema: Making Your Data Warehouse Shine'\ncontent_type: guide\ndescription: Learn how to implement Star Schema in your data warehouse for faster\n  analytics queries. Complete guide with SQL examples, best practices, and performance\n  tips for dimensional data modeling\npublished_date: '2025-07-30T00:00:00'\nsource_url: https://motherduck.com/learn-more/star-schema-data-warehouse-guide\nindexed_at: '2025-11-25T20:37:11.199206'\ncontent_hash: b327777e02e19c93\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nIn the world of data warehousing, complexity often creeps in where simplicity should reign. Data models that begin with elegant simplicity frequently evolve into labyrinthine structures of interconnected tables that confuse even their original architects. It's a familiar tale: what starts as a straightforward customer database somehow transforms into a puzzle spread across five normalized tables, leaving analysts scratching their heads and queries running longer than a coffee break.\n\nEnter the Star Schema - a [dimensional modeling](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/) approach that brings clarity back to analytical databases. This proven technique has become the backbone of successful data warehouses, offering a perfect balance between query performance, data organization, and user comprehension.\n\nIn this comprehensive guide, we'll explore:\n\n- What a Star Schema is and why its structure resembles a star\n- The compelling reasons it excels for analytical workloads\n- Step-by-step implementation with practical SQL examples\n- Important trade-offs to consider in your design decisions\n- Battle-tested tips that have proven invaluable in production environments\n\nBy the end of this guide, you'll have mastered a fundamental data warehousing technique that can transform how your organization approaches analytical data modeling - and dramatically improve query performance in the process.\n\n## The Star Schema: Astronomy for Data Engineers\n\nAt its core, a [Star Schema](https://en.wikipedia.org/wiki/Star_schema) is a dimensional data modeling technique that organizes data into two main types of tables:\n\n**Fact Table:** This is the center of your star. It contains the quantitative metrics or events you're measuring - your sales transactions, website visits, sensor readings, or other numeric data points. It's mostly composed of:\n\n- Foreign keys connecting to dimension tables\n- Numeric measures like quantity, price, duration, etc.\n\n**Dimension Tables:** These are the points radiating out from the center, providing context to the facts. They answer the who, what, where, when, and why questions. Each dimension table typically includes:\n\n- A primary key that the fact table references\n- Descriptive attributes that provide context\n\nThe key insight here is that dimension tables in a star schema are usually denormalized. Instead of splitting product information across multiple tables (categories, brands, etc.) as you might in a transactional database, you keep all related attributes in a single dimension table to minimize joins.\n\n## Meet the Cast: The Tables in Our Star Schema\n\nLet's look at a concrete example that's familiar for many businesses: online sales data.\n\n### The Fact Table: FactSales\n\nThis table records each sales line item:\n\n**Purpose:** Capture quantitative data about each sale\n**Grain:** One row per product line item per sales transaction\n**Columns:**\n\n- DateKey (FK to DimDate)\n- CustomerKey (FK to DimCustomer)\n- ProductKey (FK to DimProduct)\n- StoreKey (FK to DimStore)\n- QuantitySold (Measure)\n- UnitPrice (Measure)\n- TotalAmount (Measure)\n\n### The Dimension Tables\n\nThese provide the rich context around each sale:\n\n**DimDate**\n\n- Purpose: Slice sales by time periods\n- Primary Key: DateKey (often an integer like YYYYMMDD)\n- Attributes: FullDate, DayOfWeek, MonthName, Quarter, Year, IsWeekend, etc.\n\n**DimCustomer**\n\n- Purpose: Describe who made the purchase\n- Primary Key: CustomerKey\n- Attributes: CustomerName, Email, City, State, Country, Segment, JoinDate\n\n**DimProduct**\n\n- Purpose: Describe what was purchased\n- Primary Key: ProductKey\n- Attributes: ProductName, SKU, Category, Subcategory, Brand, Color, Size\n\n**DimStore**\n\n- Purpose: Describe where the sale occurred\n- Primary Key: StoreKey\n- Attributes: StoreName, City, State, Country, Region, StoreType\n\n## Why Star Schemas Matter: The Benefits\n\nSo why do so many data professionals swear by this approach?\n\n**Simplicity & Intuitiveness:** A star schema maps well to how business users think about their data. When an analyst asks, \"What's our revenue by product category for each region last month?\" they're intuitively thinking in dimensions (time, product, geography) and measures (revenue). This makes it easier for them to write queries and understand the data model.\n\n**Query Performance:** This is the big one. Star schemas are optimized for analytical queries. With fewer tables and direct join paths, queries that slice and dice across dimensions tend to run faster. Modern columnar databases like [DuckDB](https://duckdb.org/) can really fly with properly designed star schemas - they quack through aggregations and joins at remarkable speeds.\n\n**BI Tool Compatibility:** Most BI tools (Tableau, Power BI, Looker, etc.) are designed with star schemas in mind. They can often automatically detect relationships between facts and dimensions, making drag-and-drop report building much easier for end users.\n\nMigrating reporting workloads from complex normalized structures to well-designed star schemas often results in query performance improvements of an order of magnitude. Dashboards can go from unusably slow to responsive in real-time after such restructuring.\n\n## The Trade-offs: Nothing's Perfect\n\nLike any modeling technique, star schemas come with trade-offs:\n\n**Data Redundancy:** Since we're denormalizing data, we end up storing repeated values. If 100 products share the same category, that category name is stored 100 times. This increases storage requirements.\n\n**Data Maintenance:** Updating values in denormalized tables requires more care. If a brand name changes, you need to update it across potentially many product rows. This is where solid ETL/ELT processes become crucial.\n\n**Not for OLTP:** Star schemas are optimized for read-heavy analytical workloads (OLAP), not for high-frequency transactional writes (OLTP). The structure and redundancy don't align with the needs of applications requiring rapid inserts and updates with strict normalization.\n\n## Let's Build a Star Schema: SQL Examples\n\nEnough theory - let's get our hands dirty with some SQL. Here's how to implement our online sales star schema using standard SQL syntax.\n\n### Step 1: Create the Dimension Tables\n\nLet's start with defining the tables that provide context:\n\nCopy code\n\n```\n-- Dimension Table for Dates\nCREATE TABLE DimDate (\nDateKey INT PRIMARY KEY, -- Example: 20240424\nFullDate DATE NOT NULL,\nDayOfMonth INT NOT NULL,\nDayOfWeek VARCHAR(10) NOT NULL,\nMonthOfYear INT NOT NULL,\nMonthName VARCHAR(10) NOT NULL,\nQuarterOfYear INT NOT NULL,\nYear INT NOT NULL,\nIsWeekend BOOLEAN NOT NULL\n);\n-- Sequence for DimCustomer Primary Key\nCREATE SEQUENCE customer_key_seq START 1;\n-- Dimension Table for Customers\nCREATE TABLE DimCustomer (\nCustomerKey INTEGER PRIMARY KEY DEFAULT nextval('customer_key_seq'), -- Auto-incrementing key via sequence\nCustomerID VARCHAR(50) UNIQUE, -- Business key from source system\nCustomerName VARCHAR(255) NOT NULL,\nEmail VARCHAR(255),\nCity VARCHAR(100),\nState VARCHAR(100),\nCountry VARCHAR(100),\nSegment VARCHAR(50) -- e.g., 'Retail', 'Wholesale'\n);\n-- Sequence for DimProduct Primary Key\nCREATE SEQUENCE product_key_seq START 1;\n-- Dimension Table for Products\nCREATE TABLE DimProduct (\nProductKey INTEGER PRIMARY KEY DEFAULT nextval('product_key_seq'), -- Auto-incrementing key via sequence\nProductID VARCHAR(50) UNIQUE, -- Business key from source system\nProductName VARCHAR(255) NOT NULL,\nCategory VARCHAR(100),\nSubcategory VARCHAR(100),\nBrand VARCHAR(100),\nColor VARCHAR(50),\nStandardCost DECIMAL(10, 2) -- Cost price might live here\n);\n-- Sequence for DimStore Primary Key\nCREATE SEQUENCE store_key_seq START 1;\n-- Dimension Table for Stores (Optional, if relevant)\nCREATE TABLE DimStore (\nStoreKey INTEGER PRIMARY KEY DEFAULT nextval('store_key_seq'), -- Auto-incrementing key via sequence\nStoreID VARCHAR(50) UNIQUE, -- Business key from source system\nStoreName VARCHAR(255) NOT NULL,\nCity VARCHAR(100),\nState VARCHAR(100),\nCountry VARCHAR(100)\n);\n```\n\n\n**Code Explanation:**\n\n`CREATE TABLE Dim...`\n\n: We're defining each dimension table with a clear naming convention.`CREATE SEQUENCE ...`\n\n: We create a sequence object for each table where we need an auto-generated key. This sequence will manage the increasing numbers.`...Key INT PRIMARY KEY`\n\nor`...Key INTEGER PRIMARY KEY DEFAULT nextval('sequence_name')`\n\n: Each dimension needs a unique identifier.`INT PRIMARY KEY`\n\nrequires you to manage the key values (like for DimDate), while`INTEGER PRIMARY KEY DEFAULT nextval('sequence_name')`\n\nuses the sequence to automatically generate a sequential integer key when a new row is inserted without specifying a value for that column. This is DuckDB's way of achieving the functionality of`SERIAL`\n\n.`VARCHAR`\n\n,`INT`\n\n,`DATE`\n\n,`BOOLEAN`\n\n,`DECIMAL`\n\n: Standard data types for storing text, numbers, dates, true/false values, and precise decimals.`NOT NULL`\n\n: Ensures that a value must be provided for these columns.`UNIQUE`\n\n: For business keys like CustomerID, this ensures we don't have duplicates, which is important during the ETL lookup process.\n\nUnlike some other SQL databases (like PostgreSQL's `SERIAL`\n\nor MySQL's `AUTO_INCREMENT`\n\n), DuckDB does not support the `SERIAL`\n\ntype or a simple `AUTOINCREMENT`\n\nkeyword directly on a column definition for auto-generating primary keys.\n\nInstead, the standard method in DuckDB for creating an auto-incrementing integer primary key is to:\n\n-\nDefine a sequence using\n\n`CREATE SEQUENCE sequence_name START 1;`\n\n-\nSet the primary key column's default value to the next value from that sequence using\n\n`column_name INTEGER PRIMARY KEY DEFAULT nextval('sequence_name')`\n\n.\n\nThis pattern achieves the same result as `SERIAL`\n\nor other `AUTO_INCREMENT`\n\nsyntax, automatically assigning a unique, sequential integer to new rows when the primary key is not explicitly provided during insert. You'll need to create a separate sequence for each table that requires an auto-generated primary key.\n\n### Step 2: Create the Fact Table\n\nNow for the central table that links everything together and holds our metrics:\n\nCopy code\n\n```\n-- Sequence for FactSales Primary Key\nCREATE SEQUENCE sales_key_seq START 1;\n-- Fact Table for Sales\nCREATE TABLE FactSales (\nSalesKey INTEGER PRIMARY KEY DEFAULT nextval('sales_key_seq'), -- Unique key for the fact row itself, auto-generated via sequence\nDateKey INT NOT NULL,\nCustomerKey INT NOT NULL,\nProductKey INT NOT NULL,\nStoreKey INT NOT NULL, -- Use a placeholder key if not applicable, e.g., -1 for 'Online'\nQuantitySold INT NOT NULL,\nUnitPrice DECIMAL(10, 2) NOT NULL,\nTotalAmount DECIMAL(12, 2) NOT NULL, -- Often calculated as Quantity * UnitPrice\n-- Foreign Key Constraints\nFOREIGN KEY (DateKey) REFERENCES DimDate(DateKey),\nFOREIGN KEY (CustomerKey) REFERENCES DimCustomer(CustomerKey),\nFOREIGN KEY (ProductKey) REFERENCES DimProduct(ProductKey),\nFOREIGN KEY (StoreKey) REFERENCES DimStore(StoreKey)\n);\n-- Optional: Create indexes on foreign keys for better join performance\nCREATE INDEX idx_factsales_date ON FactSales(DateKey);\nCREATE INDEX idx_factsales_customer ON FactSales(CustomerKey);\nCREATE INDEX idx_factsales_product ON FactSales(ProductKey);\nCREATE INDEX idx_factsales_store ON FactSales(StoreKey);\n```\n\n\n**Code Explanation:**\n\n`DateKey INT NOT NULL, CustomerKey INT NOT NULL, ...`\n\n: These columns hold the integer primary keys from the dimension tables, forming the foreign key relationships.`QuantitySold INT NOT NULL, UnitPrice DECIMAL(10, 2) NOT NULL, ...`\n\n: These are our numeric measures - the core facts we want to analyze.`FOREIGN KEY (...) REFERENCES Dim...`\n\n: These constraints explicitly define the relationships between the fact table and dimension tables, ensuring referential integrity.`CREATE INDEX ...`\n\n: Indexes on the foreign key columns in the fact table can significantly speed up join operations. For large fact tables, these indexes are crucial for performance.\n\nWhen running this on DuckDB or MotherDuck, you'll find that even without explicit indexing, the query optimizer often handles star schema joins very efficiently due to the columnar storage format, which allows for high-performance filtering and joining, especially on the dimension keys.\n\n### Step 3: ETL Process (Conceptually)\n\nWhile the actual ETL (Extract, Transform, Load) or ELT process depends on your specific tools and data sources, conceptually it involves:\n\n**Extract**: Pull raw data from source systems**Transform**:- Clean the data\n- Look up or create surrogate keys for dimension tables\n- Perform calculations\n\n**Load**: Insert transformed data into fact and dimension tables\n\nModern data tools like dbt (data build tool) make managing this process much more maintainable, especially when working with a data warehouse or data lake solution like MotherDuck, which provides the scalability of the cloud with the simplicity of DuckDB.\n\n### Step 4: Querying the Star Schema\n\nNow for the fun part! Let's write a query to answer a business question: \"What were the total sales amounts for each product category in January 2024?\"\n\nCopy code\n\n```\nSELECT\ndp.Category,\nSUM(fs.TotalAmount) AS TotalSalesAmount\nFROM\nFactSales fs\nJOIN\nDimDate dd ON fs.DateKey = dd.DateKey\nJOIN\nDimProduct dp ON fs.ProductKey = dp.ProductKey\nWHERE\ndd.Year = 2024\nAND dd.MonthOfYear = 1 -- January\nGROUP BY\ndp.Category\nORDER BY\nTotalSalesAmount DESC;\n```\n\n\n**Code Explanation:**\n\n`FROM FactSales fs`\n\n: We start with the central fact table.`JOIN DimDate dd ON fs.DateKey = dd.DateKey`\n\n: Link to the Date dimension using the DateKey.`JOIN DimProduct dp ON fs.ProductKey = dp.ProductKey`\n\n: Link to the Product dimension.`WHERE dd.Year = 2024 AND dd.MonthOfYear = 1`\n\n: Filter using pre-calculated date attributes - no messy date functions needed!`SELECT dp.Category, SUM(fs.TotalAmount)`\n\n: Select the category from the dimension and aggregate the measure from the fact table.`GROUP BY dp.Category`\n\n: Aggregate by category.`ORDER BY TotalSalesAmount DESC`\n\n: Show highest-selling categories first.\n\nThis query is clean, intuitive, and typically performs well because the join paths are direct and the filter conditions are applied to pre-computed dimension attributes. A well-designed star schema makes these kinds of queries fly - they won't quack under pressure!\n\n## Star vs. Snowflake: A Quick Comparison\n\nYou might also hear about [\"Snowflake Schemas\"](https://en.wikipedia.org/wiki/Snowflake_schema) - no, not the cloud data platform, but a variation of the Star Schema where dimension tables are normalized. For instance, instead of storing Category directly in DimProduct, a Snowflake might have DimProduct link to a separate DimCategory table. The biggest pro of this schema is that it reduces redundancy, potentially making maintenance easier for some attributes. This doesn't come for free though and requires more joins, potentially impacting query performance and increasing complexity.\n\nThe choice often depends on your specific use case, query patterns, and tooling. Stars are typically preferred for their simplicity and performance, especially when dimension tables aren't excessively large or complex.\n\n## A Practical Tip from the Trenches\n\nOne of the most valuable lessons in data engineering is the immense value of a dedicated DimDate table. Don't rely on using date functions directly on fact table date columns in every query. Instead, pre-populate a DimDate table with all relevant date attributes (day of week, month name, fiscal periods, holidays, etc.).\n\nThis approach offers several benefits:\n\n- Consistent date calculations across all queries\n- Better performance (no repeated function calls)\n- Easy filtering by business-specific time concepts\n- Support for fiscal calendars, custom seasons, etc.\n\nCreate it once, populate it for a few decades (past and future), and use it everywhere. It's a simple technique that pays dividends in both performance and consistency.\n\n## Wrapping Up\n\nThe Star Schema isn't flashy or trendy, but it's a workhorse in data warehousing for good reason. Its focus on query performance, user comprehension, and analytical insights makes it a powerful foundation for turning complex data into actionable information.\n\nBy organizing your analytical data around central fact tables and descriptive dimension tables, you create a structure that's intuitive for users, performs well for analytical queries, and integrates seamlessly with modern BI tools.\n\nSo the next time you find yourself drowning in complex joins or watching your queries paddle laboriously through a normalized database, consider whether a star schema might be the right approach. Sometimes the oldest techniques are still around because they work - and the star schema certainly fits that bill!\n\n## Frequently Asked Questions\n\n### What's the difference between a Star Schema and a traditional normalized database?\n\nThe primary difference lies in structure and purpose. Traditional normalized databases (3NF) minimize data redundancy by splitting related information across multiple tables, which is ideal for transactional systems. Star schemas intentionally denormalize dimension data into fewer tables to optimize for analytical queries. This means faster joins and more intuitive query writing, but with some data redundancy trade-offs.\n\n### When should I use a Star Schema instead of a Snowflake Schema?\n\nChoose a Star Schema when query performance and simplicity are your primary concerns. Star schemas work best when your dimension tables aren't excessively large and when you want to minimize the number of joins in analytical queries. Opt for a Snowflake Schema when you need to reduce data redundancy significantly or when your dimension hierarchies are complex and frequently changing.\n\n### How do I handle slowly changing dimensions in a Star Schema?\n\n[Slowly changing dimensions (SCDs)](https://en.wikipedia.org/wiki/Slowly_changing_dimension) are handled through various techniques depending on your business requirements. Type 1 SCD overwrites old values with new ones (losing history), Type 2 creates new rows with effective dates to preserve history, and Type 3 adds columns to track limited historical changes. Most data warehouses use Type 2 for critical business dimensions where historical accuracy matters.\n\n### Can Star Schemas work with modern cloud data warehouses?\n\nAbsolutely! Star schemas are particularly well-suited for modern cloud data warehouses like Snowflake, BigQuery, and Redshift. These platforms' columnar storage and distributed computing capabilities make star schema queries extremely fast. The simple join patterns in star schemas also work excellently with modern query optimizers and parallel processing engines.\n\n### How large can fact tables get in a Star Schema before performance degrades?\n\nModern data warehouses can handle fact tables with billions or even trillions of rows when properly designed. Performance depends more on query patterns, indexing strategy, and hardware than raw size. Key factors include partitioning strategies (often by date), appropriate indexing on foreign keys, and query optimization. Cloud data warehouses with columnar storage can maintain excellent performance even with very large fact tables.\n\n### Is it worth migrating from a normalized schema to a Star Schema?\n\nMigration is typically worthwhile if you're experiencing slow analytical query performance, complex reporting requirements, or difficulty with BI tool integration. The benefits include dramatically faster query performance (often 10x improvements), simplified report development, and better end-user adoption. However, consider the migration effort, data transformation complexity, and ongoing ETL process changes. Start with a pilot project focusing on your most critical reporting areas to validate the benefits before full migration.\n\nStart using MotherDuck now!",
  [2m  â”‚[0m     "numLines": 398,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 398
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now I have comprehensive information. Let me write the answer to the file:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_3.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "create",
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_3.md",
  [2m  â”‚[0m   "content": "<3934 chars>",
  [2m  â”‚[0m   "structuredPatch": [],
  [2m  â”‚[0m   "originalFile": null
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I've completed the task. Here's a summary of what I found:
  [94mâ”‚[0m 
  [94mâ”‚[0m ## SQL Features from DuckDB Not Yet Supported in MotherDuck:
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. **User-Defined Functions (UDFs)** - Python UDFs cannot run on MotherDuck because it's a serverless platform without a Python runtime
  [94mâ”‚[0m 2. **Server-Side ATTACH to External Databases** - Cannot attach to databases like PostgreSQL from within MotherDuck
  [94mâ”‚[0m 3. **Custom Extensions** - Third-party or custom DuckDB extensions are not available on MotherDuck
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer has been written to `/tmp/answer_without_kg_3.md` with full details and source citations from 6 different documentation files in the `.kurt/sources/` directory.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 7,770 | Cost: $0.6687
     Cumulative: 7,770 tokens | $0.6687
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent clearly indicates task completion and provides information without soliciting further user input.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 17 | Duration: 88.0s
â•‘    Tokens: 7,770 | Cost: $0.6687
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  Running LLM judge evaluation...
   ğŸ§  LLM Judge score: 0.82

======================================================================
â“ Question 4/10
...Why might a query run slower on MotherDuck than locally?
======================================================================


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 4
â”‚ Answer this question by searching through the .kurt/sources/ directory:

...Why might a query run slower on MotherDuck than locally?

Required Steps:
1. Search for relevant markdown files in .kurt/sources/ using glob and grep
2. Read the most relevant files (aim for 3-5 most relevant sources)
3. As you read each file, keep track of:
   - The file path
   - The file title/name
   - Key information extracted from that file
4. Synthesize a comprehensive answer based on the information found
5. Write your answer to: /tmp/answer_without_kg_4.md

IMPORTANT - Your answer MUST have this structure:
```markdown
# Answer

[Your comprehensive answer here, drawing from the sources you read]

## Sources

List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
[etc.]

DO NOT use URLs or document titles as links. Show the actual file path you read with the Read tool.
```

The Sources section is MANDATORY - you must list every file you read to answer the question.

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [96mğŸ”§ TOOL:[0m Grep â†’ slower|performance|latency|speed

  [96mğŸ”§ TOOL:[0m Grep â†’ query.*slow|slow.*query

  [96mğŸ”§ TOOL:[0m Grep â†’ local|locally|duckdb
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/take-flight-with-the-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-if-sql-queries-returned-results-instantly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/ducklake-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/scaling-duckdb-panel-ondemand.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/spatial-data-management-with-duckdb-ft-mattforrest.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/speed-querying-stackoverflow-data-with-duckdb-ft-michael-hunger.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/sql-ide-safari-harlequin-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/take-flight-with-dbt-and-duckdb-dropping-dev-warehouse-costs-to-zero.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 30,
  [2m  â”‚[0m   "appliedLimit": 30
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search more specifically for content about performance comparison between local DuckDB and MotherDuck:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     "content": "---\ntitle: hybrid-analytics-guide\ncontent_type: tutorial\nsource_url: https://motherduck.com/learn-more/hybrid-analytics-guide\nindexed_at: '2025-11-25T09:57:23.352209'\ncontent_hash: 91344eaa7ee54322\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO LEARN](https://motherduck.com/learn-more/)\n\n# Hybrid Analytics: Query Local & Cloud Data Instantly\n\n12 min readBY\n\n[Aditya Somani](https://motherduck.com/authors/aditya-aomani/)\n\n![Hybrid Analytics: Query Local & Cloud Data Instantly](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FZero_latency_e8da68f055.png&w=3840&q=75)\n\nAre your analytics queries timing out in Postgres? Does your laptop grind to a halt when you try to analyze a large CSV file? If you feel caught between the memory limits of local tools and the spiraling costs of cloud data warehouses, you're not alone. The traditional divide between local development and cloud-scale analytics creates friction, slows down projects, and drains budgets.\n\nBut there is a modern, more efficient way. Imagine running complex SQL queries on a 20 GB Parquet file directly on your laptop, without a flicker of memory strain. Picture joining a local spreadsheet with a massive table in Amazon S3 using a single, elegant command. This is the reality of the **hybrid analytics workflow**, a powerful approach that unifies your local machine and the cloud into a single, unified data environment.\n\nThis guide is your definitive resource for understanding and implementing this new workflow. We'll break down the core concepts, provide practical examples, and show you how to solve the most common data bottlenecks for good. You will learn why traditional databases fail for analytics, how to analyze datasets larger than your RAM, and how the power of \"Dual Execution\" lets you instantly join local files with cloud data, all without the cost and complexity of managing a server cluster.\n\n## Why Are My Analytics Queries on Postgres & MySQL Timing Out?\n\nIf your analytical queries on a database like PostgreSQL or MySQL are consistently slow or timing out, you've hit a fundamental architectural limit. These databases are masterpieces of engineering for **[Online Transaction Processing (OLTP)](https://estuary.dev/blog/postgres-to-motherduck/)**. These are the small, fast operations that power applications, like creating a user or updating an order. However, this same design becomes a major bottleneck for **[Online Analytical Processing (OLAP)](https://en.wikipedia.org/wiki/Online_analytical_processing)**, which involves complex queries that scan and aggregate vast amounts of data.\n\nThe root of the problem is their **[row-oriented](https://en.wikipedia.org/wiki/Data_orientation)** storage model. A row-store keeps all data for a single record together on disk, which is efficient for retrieving an entire user profile. But for an analytical query that only needs to sum the `sale_amount` column across millions of rows, a row-store is forced to read every single column for every single row. This wastes an enormous amount of I/O and CPU cycles on data it doesn't need, which is the primary reason your dashboards are slow and your queries fail.\n\nThe next logical step is to move analytical workloads to a system built on a **[columnar architecture](https://motherduck.com/learn-more/columnar-storage-guide/)**. A columnar database organizes data by column, storing all values for `sale_amount` together. When your query asks for the sum of `sale_amount`, the database reads _only_ that column, greatly reducing I/O and speeding up performance by orders of magnitude. Modern analytical engines like DuckDB couple this with **[vectorized query execution](https://15721.courses.cs.cmu.edu/spring2024/notes/06-vectorization.pdf)**, a technique that processes data in large batches or \"vectors\" instead of row-by-row. This method fully utilizes modern CPU capabilities to perform a single instruction on multiple data points simultaneously, leading to large gains in computational efficiency.\n\n| Architecture | Row-Oriented (OLTP) | Column-Oriented (OLAP) |\n| --- | --- | --- |\n| **Primary Use Case** | Application backends, frequent small updates. | Business intelligence, data analytics, large scans. |\n| **Data Storage** | All data for a single record is stored together. | All data for a single column is stored together. |\n| **Query Example** | `SELECT * FROM users WHERE user_id = 123;` (Fast) | `SELECT SUM(sale_amount) FROM sales;` (Slow) |\n| **Best For** | Fast writes and single-record lookups. | Fast aggregations and filtering on specific columns. |\n\nBy offloading analytics to a purpose-built columnar engine, you let your transactional database continue to excel at what it does best while your analytical queries run in a [high-speed, optimized environment](https://motherduck.com/blog/postgres-duckdb-options/).\n\n## How Can I Analyze a 20 GB Parquet File on My Laptop Without It Crashing?\n\nThe second major bottleneck for data professionals is local memory. Trying to load a 20 GB file into a traditional in-memory library like Pandas on a 16 GB laptop will almost certainly result in a `MemoryError`. This happens because these tools must load the entire dataset into your computer's RAM before they can begin processing.\n\nThe modern solution is **out-of-core processing**, a strategy where data is processed in manageable chunks directly from disk without ever being fully loaded into RAM. This is the key feature of engines like DuckDB, which uses a [streaming execution model to handle datasets far larger than your available memory](https://duckdb.org/2024/07/09/memory-management.html). If a complex operation requires more memory than is available (by default, 80% of your system's RAM), DuckDB can \"spill\" intermediate results to a temporary file on disk, ensuring your query completes without crashing.\n\nThis capability turns your laptop into a surprisingly powerful analytics workstation. With an out-of-core engine, analyzing that 20 GB file becomes straightforward. You can simply use SQL to query it directly from a Python script or Jupyter notebook.\n\n```python\nCopy code\n\nimport duckdb\n\n# This query runs out-of-core, never loading the full 20GB file into RAM.\n# DuckDB's streaming engine processes the file in chunks.\nresult = duckdb.sql(\"\"\"\n  SELECT\n      product_category,\n      AVG(sale_price) as avg_price\n  FROM read_parquet('large_sales_dataset.parquet')\n  WHERE region = 'North America'\n  GROUP BY product_category\n  ORDER BY avg_price DESC;\n\"\"\").arrow()\n\nprint(result)\n```\n\nThis simple, powerful approach allows you to perform heavy-duty data analysis on your local machine, iterating quickly without waiting for a cloud cluster to spin up or worrying about memory crashes.\n\n## How Can I Join Local CSVs with Cloud Data in a Single Query?\n\nThis is where the hybrid workflow really shows its strength. Joining a local CSV of fresh sales data with a historical customer table in an S3 bucket has traditionally been a major challenge. The problem is \"data gravity,\" since it's [slow and expensive to move large amounts of data](https://motherduck.com/learn-more/no-etl-query-raw-files/). A naive query would have to either upload your entire local file to the cloud or download the entire cloud table to your laptop, both of which are very inefficient.\n\nMotherDuck solves this with an innovative query planner called **Dual Execution**. It treats your laptop and the MotherDuck cloud as two nodes in a single distributed system. When you run a hybrid query, the planner intelligently breaks it down and pushes computation to where the data lives, minimizing data transfer.\n\n> \"The most innovative feature of MotherDuck's architecture is its dual execution model. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\" - [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained/)\n\nImagine you want to join a local CSV of new product pricing with a massive sales table in MotherDuck, but only for a specific product. Instead of moving entire tables, the Dual Execution planner does the following:\n\n1. **Local Scan:** It scans the small pricing CSV on your laptop.\n2. **Filter Locally:** It applies the filter for the specific product on your machine.\n3. **Transfer Minimal Data:** It sends only the single, filtered pricing row (a few bytes of data) to the cloud.\n4. **Join in the Cloud:** It performs the final join against the massive sales table in the MotherDuck cloud.\n\nThis process can reduce network traffic by orders of magnitude compared to traditional methods, turning a difficult query into an interactive one. You can see this in action with a simple `EXPLAIN` statement, which [shows which parts of the query run locally `(L)` and which run remotely `(R)`](https://motherduck.com/docs/key-tasks/running-hybrid-queries/).\n\n## Why Are Data Engineers So Excited About Dual-Execution Engines?\n\nData engineers are excited because dual-execution engines solve one of their biggest challenges: the \"it worked on my machine\" problem. In traditional setups, the tools and data used for local development are often completely different from the production cloud environment, leading to bugs and deployment failures.\n\nMotherDuck eliminates this by using the **exact same DuckDB engine** both locally and in the cloud. A query that works on your laptop is guaranteed to work in the cloud, creating a direct path from development to production. This greatly improves developer experience (DX) and accelerates iteration cycles.\n\nThis architecture provides three key benefits:\n\n1. **Faster Development:** Engineers can build and test pipelines with the zero-latency feedback of local development before scaling to the cloud.\n2. **Lower Costs:** By using the free, powerful compute on users' laptops and minimizing data transfer, this model significantly reduces cloud bills.\n3. **Better Collaboration:** It transforms the traditionally \"single-player\" DuckDB into a [\"multiplayer\" platform](https://motherduck.com/videos/339/bringing-duckdb-to-the-cloud-dual-execution-explained/) where teams can share databases and work from a single source of truth.\n\nThis powerful combination of local speed and cloud scale is backed by real-world success.\n\n> **[Case Study: Finqore's 60x Pipeline Acceleration](https://motherduck.com/case-studies/)**\n> Finqore, a financial technology company, was struggling with an 8-hour data pipeline built on Postgres. By migrating to a hybrid workflow with MotherDuck, they were able to use DuckDB's performance and MotherDuck's serverless scale to transform that pipeline into an **8-minute workflow**, a 60x improvement that unlocked real-time data exploration for their team.\n\n## How Can I Run SQL Analytics Without Managing Clusters?\n\nThe operational overhead of provisioning, scaling, and maintaining server clusters is a major drain on data teams. **[Serverless SQL analytics](https://motherduck.com/docs/concepts/architecture-and-capabilities/)** removes this burden entirely. Instead of managing infrastructure, you simply write and run your queries, and the platform handles the rest.\n\nThis model is not only simpler but also much more cost-effective for the \"spiky\" workloads typical of analytics. You pay only for the compute you actually use, avoiding the massive costs of idle clusters.\n\n| Feature / Scenario | Traditional Cloud Warehouse (e.g., Snowflake/BigQuery) | Hybrid Approach (Local + Serverless) |\n| --- | --- | --- |\n| **Development & Prototyping** | Billed for active compute time, even for small test queries. | **Free.** Uses local machine resources with no cloud costs. |\n| **Pricing Model** | Complex credits or per-TB scanned, which is hard to predict. | Simple, usage-based billing; pay only for queries you run. |\n| **Idle Compute** | Billed for provisioned clusters, even when idle, which can create a costly [idle tax on short-running queries](https://motherduck.com/learn-more/reduce-snowflake-costs-duckdb). | **No cost.** Serverless architecture has no idle compute. |\n| **Hardware Requirement** | Entirely reliant on expensive, provisioned cloud hardware. | Leverages powerful, existing local hardware (laptops, servers). |\n\nBy adopting a serverless, hybrid approach, you can reduce your cloud data warehouse costs while empowering your team with a faster, more flexible workflow.\n\n## What Lightweight SQL Solutions Can Be Integrated with Jupyter Notebooks?\n\nFor data scientists who work primarily in Jupyter, several excellent tools bring the power of SQL directly into the notebook environment. The most powerful option for serious analytics is the **DuckDB engine** itself. It can query Pandas DataFrames, Arrow tables, and large local files directly with full SQL support. Its performance on analytical queries is excellent in the embedded space.\n\nFor comparison, while Python's standard library includes **SQLite**, it's a row-based OLTP engine and is significantly slower than DuckDB for analytical queries. An independent benchmark found DuckDB to be **[12-35 times faster](https://motherduck.com/learn-more/duckdb-vs-sqlite-databases/)** for aggregations on a 100-million-row dataset. For any analytical task in a notebook, DuckDB is the clear choice for performance and scalability.\n\nAnother popular tool is **[JupySQL](https://jupysql.ploomber.io/)**, which provides convenient SQL \"magics\" (`%sql`, `%%sql`) that turn a notebook cell into a SQL editor. It connects to various backends, including DuckDB, making it great for quick, ad-hoc exploration.\n\n## How Can I Keep Queries on My Laptop in Sync With the Cloud?\n\nThe key to keeping local and cloud queries synchronized is **single-engine semantics**. Because MotherDuck uses the same DuckDB engine on your laptop and in the cloud, a query validated locally is guaranteed to behave identically in production. This eliminates the most common source of dev-prod drift.\n\nA modern dev-to-prod workflow combines this with tools like dbt (data build tool). An analyst can develop and test a dbt model on their laptop against local data for rapid iteration. Once validated, they can promote the model to run in the cloud against the full production dataset with a [single configuration change](https://motherduck.com/blog/dual-execution-dbt/), ensuring complete consistency. This workflow combines the speed of local development with the scale and reliability of a production cloud environment.\n\n## When Should I Not Use This Hybrid Approach?\n\nWhile the hybrid, in-process model is very useful for analytics, it's not a solution for every problem. It's crucial to understand its limitations to know when a different architecture is needed.\n\n1. **High-Concurrency Applications:** An in-process engine like DuckDB is not designed to be a client-server database serving hundreds of simultaneous application users. For powering a public-facing web application, a traditional OLTP database like PostgreSQL or a distributed SQL database is the right choice.\n2. **Real-Time, High-Volume Ingestion:** If your use case involves ingesting thousands of events per second in real-time, a specialized streaming platform or real-time OLAP database would be more suitable.\n3. **Truly Massive, Multi-Terabyte Workloads:** While DuckDB can handle surprisingly large datasets on a single node, if your active working set is in the tens or hundreds of terabytes, you've reached the scale where a distributed cloud data warehouse becomes necessary to parallelize work across a large cluster.\n\nThe beauty of the MotherDuck ecosystem is that it provides a smooth way to scale up. You can start with a local-first, hybrid workflow and, as your concurrency or data scale needs grow, easily push more of the workload to the MotherDuck cloud without changing your core tools or SQL logic.\n\n### TABLE OF CONTENTS\n\n[Why Are My Analytics Queries on Postgres & MySQL Timing Out?](https://motherduck.com/learn-more/hybrid-analytics-guide/#why-are-my-analytics-queries-on-postgres-mysql-timing-out)\n\n[How Can I Analyze a 20 GB Parquet File on My Laptop Without It Crashing?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-analyze-a-20-gb-parquet-file-on-my-laptop-without-it-crashing)\n\n[How Can I Join Local CSVs with Cloud Data in a Single Query?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-join-local-csvs-with-cloud-data-in-a-single-query)\n\n[Why Are Data Engineers So Excited About Dual-Execution Engines?](https://motherduck.com/learn-more/hybrid-analytics-guide/#why-are-data-engineers-so-excited-about-dual-execution-engines)\n\n[How Can I Run SQL Analytics Without Managing Clusters?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-run-sql-analytics-without-managing-clusters)\n\n[What Lightweight SQL Solutions Can Be Integrated with Jupyter Notebooks?](https://motherduck.com/learn-more/hybrid-analytics-guide/#what-lightweight-sql-solutions-can-be-integrated-with-jupyter-notebooks)\n\n[How Can I Keep Queries on My Laptop in Sync With the Cloud?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-keep-queries-on-my-laptop-in-sync-with-the-cloud)\n\n[When Should I Not Use This Hybrid Approach?](https://motherduck.com/learn-more/hybrid-analytics-guide/#when-should-i-not-use-this-hybrid-approach)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\n## FAQS\n\n### My analytics queries on Postgres / MySQL are timing out. What\u2019s the next step up?\n\nThe next step is to move your analytical workloads from a row-oriented (OLTP) database like Postgres to a purpose-built columnar (OLAP) engine like DuckDB. Columnar engines are designed for analytics and are orders of magnitude faster for large scans and aggregations because they only read the specific columns needed for a query.\n\n### How can I analyze a 20 GB Parquet file on my laptop without it crashing?\n\nUse a query engine that supports \"out-of-core\" processing, like DuckDB. It processes data in manageable chunks directly from your disk instead of loading the entire file into RAM. This allows you to analyze datasets that are significantly larger than your computer's available memory.\n\n### Why is it so hard to join local CSVs with cloud data?\n\nThe main challenge is \"data gravity\"\u2014it's slow and expensive to move large datasets across a network. Traditional tools force you to either upload your entire local file or download the entire cloud table. A modern hybrid platform with a dual-execution engine solves this by intelligently minimizing data transfer.\n\n### Can I mix local datasets and S3 data in a single query?\n\nYes. With a hybrid analytics platform like MotherDuck, you can write a single SQL query that joins local files (e.g., CSVs) with data in cloud storage (e.g., Parquet files in S3). The system treats them as if they exist in one unified environment, abstracting away their physical location.\n\n### How can I run SQL analytics without managing clusters?\n\nAdopt a serverless SQL analytics platform. These services handle all infrastructure provisioning, scaling, and maintenance for you. You simply run your queries and pay only for the compute you use, which eliminates the high cost and operational overhead of managing idle clusters.\n\n### Why are data engineers excited about dual-execution engines?\n\nDual-execution engines solve the \"it worked on my machine\" problem by using the exact same query engine on the developer's laptop and in the cloud. This guarantees consistency, speeds up development cycles, lowers costs by leveraging local compute, and enables better team collaboration on a single source of truth.\n\n### What lightweight SQL solutions can be integrated with Jupyter notebooks?\n\nDuckDB is the leading lightweight SQL solution for Jupyter. It can be installed via pip and can directly query Pandas DataFrames, Arrow tables, and large local files with full SQL support. It is significantly faster for analytical queries than other embedded options like SQLite.\n\n### How can I keep queries on my laptop in sync with the cloud?\n\nUse a platform that offers single-engine semantics, like MotherDuck. Because it runs the same DuckDB engine locally and in the cloud, a query validated on your laptop is guaranteed to behave identically in production. This eliminates dev-prod drift and ensures consistency.\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 212,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 212
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     "content": "---\ntitle: 'Fix Slow BI Dashboards: A Guide to Sub-Second Analytics'\ncontent_type: guide\ndescription: Frustrated by slow BI dashboards? Learn the causes of dashboard latency,\n  from architectural bottlenecks to lakehouse issues. See how MotherDuck helps startups\n  scale.\npublished_date: '2025-10-17T00:00:00'\nsource_url: https://motherduck.com/learn-more/fix-slow-bi-dashboards\nindexed_at: '2025-11-25T10:52:12.236559'\ncontent_hash: 0e692003fc4f1f77\nhas_step_by_step: true\nhas_narrative: true\n---\n\nAs a startup, your data is your compass. But as you scale, that compass begins to spin. Business Intelligence (BI) dashboards that were once snappy now take minutes to load, turning quick checks into coffee breaks. Queries time out. Your team hesitates to ask new questions of the data because the feedback loop is agonizingly slow. You're facing a critical growth inflection point: your analytics stack can no longer keep up with your ambition.\n\nYou know you need something more powerful, but the thought of migrating to a traditional cloud data warehouse, with its complex setup, opaque pricing, and multi-year contracts, is daunting. What if there was a better way? A path that gives you the power of a massive data warehouse without the overhead, designed specifically for teams who move fast?\n\nThis guide is for the technical founders, first data hires, and lean analytics teams feeling this pain. We'll provide a clear framework for evaluating if MotherDuck is the right solution to solve your dashboard latency issues, scale your analytics, and empower your team to make faster, data-driven decisions.\n\n### What You'll Learn in This Guide\n\n**What Causes Dashboard Latency?**We'll uncover why your current database struggles with analytical queries and identify the key architectural bottlenecks.**How Do You Scale Beyond a Single Node?**Discover the limitations of single-node engines for large datasets and how MotherDuck's hybrid architecture provides a straightforward path to scale.**How Can You Achieve Real-Time Insights?**Learn how to move from stale, batch-updated data to sub-second freshness without overloading your production systems.**What Does a Migration Look Like?**Follow a practical, week-by-week plan to migrate a critical workload to MotherDuck and start seeing results in under 30 days.**Is MotherDuck the Right Fit for Your Startup?**Get a clear comparison of when MotherDuck excels and what limitations to consider for your specific use case.\n\n## Why Are My Startup's BI Dashboards So Slow?\n\nIf your dashboards are grinding to a halt, the problem usually isn't a single query but an architectural mismatch. Most startups begin by running analytics on a replica of their production database, often a system like PostgreSQL or MySQL. These are [ Online Transaction Processing (OLTP)](https://motherduck.com/learn-more/what-is-OLAP/) databases, brilliant for handling thousands of small, fast transactions like creating a user or processing an order.\n\nHowever, analytical queries are a completely different beast. They are **Online Analytical Processing (OLAP)** workloads, which involve scanning millions or billions of rows across a few columns to calculate aggregations. Asking an OLTP database to perform heavy OLAP queries is like asking a race car to haul lumber. It wasn't built for the job.\n\nThe core issue lies in how the data is stored on disk. OLTP databases are **row-oriented**, meaning they store all the data for a single record together. When your BI tool asks for the total number of users by `plan_type`\n\nfrom a table with 50 columns, a row-oriented database is forced to read all 50 columns for every single row, even though it only needs one. This wastes an enormous amount of I/O.\n\nThis is where [ columnar storage](https://motherduck.com/learn-more/columnar-storage-guide/), the foundation of modern analytical databases like DuckDB, makes a big difference. A columnar database stores all values from a single column together. When you run the same query, it reads\n\n*only*the\n\n`plan_type`\n\ncolumn, dramatically reducing the amount of data scanned. This fundamental difference is often the key to unlocking sub-second query performance.### How Do Row and Columnar Databases Compare for Analytics?\n\n| Feature | Row-Oriented (e.g., PostgreSQL) | Column-Oriented (e.g., DuckDB/MotherDuck) |\n|---|---|---|\nPrimary Use Case | OLTP: Fast reads and writes of individual records. | OLAP: Fast scans and aggregations over large datasets. |\nData Layout | Stores all values for a single record contiguously. | Stores all values for a single column contiguously. |\nQuery Performance | Slow for analytical queries that only need a few columns. | Extremely fast for analytical queries. It only reads the required columns. |\nCompression | Less effective, as it stores mixed data types in each row. | Highly effective, leading to smaller storage footprint and faster scans. |\n\n## What Happens When My Analytics Queries Need to Scan Billions of Rows?\n\nAs your startup succeeds, your data volume explodes. The single-node analytics database that was once a perfect solution starts to show its limits. While an engine like DuckDB is incredibly fast and can even process datasets larger than RAM by [\"spilling\" intermediate results to disk](https://duckdb.org/2024/07/09/memory-management.html), it is ultimately constrained by the resources of a single machine.\n\nWhen you run a complex query with multiple joins and aggregations on billions of rows, the intermediate data generated can overwhelm the system's memory. This can lead to slow queries as the engine constantly writes to and reads from disk, or worse, an [ OutOfMemoryException](https://duckdb.org/docs/stable/guides/troubleshooting/oom_errors.html) that kills the query entirely.\n\nFurthermore, a single node has a finite capacity for **concurrency**. As more team members connect with BI tools, the CPU and I/O resources get saturated, and everyone's queries slow down. This is the practical ceiling of a single-node engine. This is precisely the problem MotherDuck was built to solve. It extends the lightning-fast local experience of DuckDB with a serverless cloud backend, giving you a straightforward path to scale.\n\n## How Does MotherDuck's Hybrid Architecture Deliver Speed and Scale?\n\nMotherDuck introduces a novel architecture that gives you the best of both worlds: the zero-latency feel of local development and the on-demand power of the cloud. This is achieved through a few key concepts.\n\n### What is Dual Execution and Why Does it Matter?\n\nThe magic behind MotherDuck is its [ Dual Execution query planner](https://www.cidrdb.org/cidr2024/papers/p46-atwal.pdf). Instead of forcing you to move all your data to the cloud, it intelligently pushes the computation to where the data lives, minimizing network latency and data transfer costs.\n\n**Local Query:**If you query a CSV file on your laptop, the query runs entirely on your local DuckDB instance. The result is instantaneous.**Cloud Query:**If you query a large table stored in MotherDuck, the work is routed to a dedicated, serverless compute instance (a \"Duckling\") in the cloud.**Hybrid Query:**This is where it gets powerful. If you join a local file with a large cloud table, the planner is smart enough to push filters down to your local machine first. It processes the local file, sends only the small, filtered result to the cloud, and then performs the final join. This makes complex queries incredibly efficient.\n\n### How Do You Handle High Concurrency from BI Tools?\n\nA common and dangerous blind spot for startups is how BI tools handle concurrency. Tools like Tableau or Looker often use a single service account, funneling queries from dozens of users through one connection. This can quickly overwhelm a database.\n\n## Case Study: How Layers Solved Its Concurrency Bottleneck\n\nThe SaaS company\n\n[faced this exact problem. Their analytics, running on PostgreSQL, were overwhelmed when their BI tool masked 73 simultaneous users behind a single service account. This exhausted their connection pool and caused a high rate of query timeouts.]LayersAfter migrating to MotherDuck, they used the\n\nRead Scalingfeature. By connecting their BI tool with a special Read Scaling Token, user queries were automatically distributed across a pool of 16 dedicated, read-only DuckDB replicas. This instantly parallelized the workload, providing each user session with isolated compute. The result was a dramatic improvement in stability, with BI query timeouts dropping to virtually zero.\n\nBy issuing a unique, read-only token for each BI integration and using the `session_hint`\n\nparameter, you can ensure user queries are [intelligently load-balanced](https://motherduck.com/docs/key-tasks/authenticating-and-connecting-to-motherduck/read-scaling/), providing a consistently fast experience for everyone.\n\n## Why Might Lakehouse Architectures Have High Latency Issues?\n\nThe lakehouse architecture, which combines a data lake's flexibility with a data warehouse's performance, is a popular choice. However, startups often find that it comes with a significant \"latency tax,\" especially for interactive queries.\n\nThe primary culprit is the [ \"small files problem.\"](https://dev.to/thedanicafine/the-apache-iceberg-small-file-problem-1k2m) Streaming ingestion jobs often write thousands of tiny files to cloud object storage like Amazon S3. To run a query, the engine must make thousands of separate, high-latency HTTP requests just to read the metadata and find the right files. Each request can take 100-200 milliseconds, adding up to seconds of delay before a single byte of data is even processed.\n\nA benchmark demonstrated this vividly: a query scanning a 5 GB Apache Iceberg table with thousands of small files took **5.093 seconds** to run. The same query on MotherDuck's **DuckLake** format, which uses a database-backed index to avoid scanning S3 for metadata, returned in just **0.146 seconds**, a [ 34x speedup](https://motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats/).\n\n### How Does DuckLake Compare to Apache Iceberg?\n\n| Feature | Apache Iceberg (Standard) | MotherDuck DuckLake |\n|---|---|---|\nMetadata Storage | Stored as files (manifests) in the object store (e.g., S3). | Stored in a fast, transactional database, separate from data files. |\nQuery Planning | Requires multiple sequential S3 reads to discover which data files to scan. | A single SQL query to the catalog database returns the exact list of files. |\nSmall File Handling | Creates many small data and metadata files, requiring costly maintenance jobs (compaction). | Dramatically reduces metadata overhead and the need for frequent compaction. |\nPerformance | High latency for interactive queries due to metadata discovery overhead. |\n34x faster |\n\n## How Can I Get Real-Time Data Without Overloading My Production Database?\n\nIs your sales dashboard always 30 minutes out of date? This frustrating lag is almost always due to a reliance on traditional, batch-based ETL (Extract, Transform, Load) jobs that run on a schedule. For a startup that needs to react instantly to user behavior, this is no longer acceptable.\n\nThe modern solution is a streaming architecture built on **Change Data Capture (CDC)**. Instead of querying your production database tables (which adds heavy load), CDC tools like Estuary or [Streamkap](https://motherduck.com/blog/streamkap-mysql-to-motherduck/) monitor the database's internal transaction log. They capture every insert, update, and delete as it happens and stream these events to MotherDuck in near real-time, often with [ less than 100ms of latency](https://estuary.dev/destination/motherduck/).\n\nThis approach provides two critical benefits:\n\n**Sub-Minute Freshness:**Your analytics are always synchronized with reality.**Zero Production Impact:**It completely isolates your analytical workload, ensuring that heavy queries never slow down your customer-facing application.\n\n## What Are the Signs My Startup Has Outgrown Its Current Analytics Stack?\n\nThe signs that you've outgrown your analytics stack are both technical and cultural. Recognizing them early can save you from months of frustration and slow decision-making.\n\n**Technical Red Flags:**\n\n**High Query Latency:**Your p95 query latency for interactive dashboards consistently exceeds 2-5 seconds. Research shows that[beyond 1 second, users lose their flow of thought, and beyond 10 seconds, you lose their attention entirely](https://www.nngroup.com/articles/powers-of-10-time-scales-in-ux/).**Rising Error Rates:**You see an increase in query timeouts, connection failures, or application errors related to database load.**Stale Data:**Business teams complain that dashboards are always out of date, indicating that nightly batch jobs are no longer sufficient.\n\n**Business and Cultural Red Flags:**\n\n**Dashboard Abandonment:**Analysts and business users stop using the BI tool because it's \"too slow.\"**Rising Costs:**Your infrastructure bill is growing faster than the value you're getting from your data, often due to over-provisioning to compensate for poor performance.**Slow Product Velocity:**Engineers spend more time optimizing the database than building features, and product managers can't get timely data to inform their roadmap.\n\nCompanies that see these signs can achieve significant improvements by migrating. **Finqore** reduced 8-hour financial data pipelines to just 8 minutes, while **uDisc** cut query times from minutes to seconds, leading to [a significant lift in daily active analysts](https://motherduck.com/case-studies/).\n\n## How Do I Plan a Migration to MotherDuck?\n\nMigrating to MotherDuck doesn't have to be a massive, disruptive project. You can see value in under 30 days by following a straightforward, phased approach focused on a single, high-pain workload.\n\n### Your 30-Day Migration Roadmap\n\n| Week | Key Tasks | Success Criteria |\n|---|---|---|\nWeek 1: Connect & Ingest | - Create your MotherDuck account and generate service tokens. - Install the DuckDB SDK and connect via `ATTACH 'md:';` . - Perform an initial bulk load of a target dataset (e.g., one large, slow table) into a new MotherDuck database. | - You can successfully query your data in MotherDuck from your local machine and BI tool. - Historical data for one target workload is fully loaded. |\nWeek 2: Mirror a Pilot Workload | - Select one high-pain, low-risk dashboard to migrate. - Set up an incremental CDC pipeline (e.g., using Estuary) to keep MotherDuck in sync with the source. - Re-create the dashboard's data model to point to MotherDuck. | - The mirrored dashboard in MotherDuck is live and updating in near real-time. - p95 query latency is under 2 seconds. |\nWeek 3: Validate & Optimize | - Share the new dashboard with a small group of business users for feedback. - Monitor query performance and cost. - Use optimizations like Read Scaling tokens for the BI tool connection. | - Users confirm the new dashboard is significantly faster and accurate. - The projected cost for the pilot workload is lower than the legacy system. |\nWeek 4: Cut-Over & Expand | - Officially switch all users to the new MotherDuck-powered dashboard. - Decommission the old data models and pipelines for the migrated workload. - Plan the migration of the next set of analytical workloads. | - 100% of users for the pilot workload are using the new dashboard. - A prioritized backlog for the next migration is created. |\n\n## What Are the Limitations of MotherDuck I Should Consider?\n\nNo tool is perfect for every situation. Being honest about limitations is key to making the right choice. MotherDuck is rapidly evolving, but startups should be aware of a few current constraints:\n\n**Regional Availability:**MotherDuck currently operates in the AWS[us-east-1](https://motherduck.com/docs/concepts/architecture-and-capabilities/)and recently started in[eu-central-1](https://motherduck.com/blog/motherduck-in-europe/), hosted in Frankfurt. Teams with strict data residency requirements outside this region will need to consider this.**Partial DuckDB SQL Coverage:**MotherDuck does not yet support the full range of DuckDB's SQL features. Notably, custom Python/native User-Defined Functions (UDFs), server-side`ATTACH`\n\nto other databases (like Postgres), and custom extensions are not yet available. The common workaround is to perform these specific transformations upstream before loading data into MotherDuck.\n\n## Conclusion: Stop Waiting, Start Analyzing\n\nFor a startup, speed is everything: speed to market, speed to insight, and speed of execution. A slow analytics stack is a direct drag on all three. You don't need the complexity and cost of a legacy data warehouse, nor can you afford the performance bottlenecks of an overloaded production database.\n\nMotherDuck offers a third way, purpose-built for the scale and agility of a modern startup. By combining the raw speed of a local-first engine with the on-demand scale of the cloud, it eliminates the trade-offs that have held analytics teams back for years. If your dashboards are slow and your team is frustrated, it's time to evaluate a new approach.\n\n[Ready to experience sub-second analytics? Start building on MotherDuck's free tier today.](https://motherduck.com/startups/)\n\nStart using MotherDuck now!\n\n## FAQS\n\n### How much does MotherDuck cost for a startup?\n\nMotherDuck's pricing is designed for startups. It includes a generous free tier for smaller projects and two pay-as-you-go plans that charge only for storage used and compute-seconds executed. There are no idle cluster charges or minimums, which can lead to [ 70-90% cost savings](https://motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck/) compared to traditional data warehouses.\n\n### Can MotherDuck handle our data volume?\n\nYes. Analysis shows that [over 95% of startup databases are smaller than 1 TB](https://hemantkgupta.medium.com/insight-from-paper-motherduck-duckdb-in-the-cloud-and-in-the-client-e4a73da9dbec), a size range where MotherDuck excels. The hybrid architecture is designed to scale from megabytes on your laptop to tens of terabytes in the cloud, ensuring you have a growth path.\n\n### How does MotherDuck compare to Snowflake or BigQuery for a startup?\n\nWhile large warehouses are powerful, they often come with significant operational complexity and cost overhead that can be burdensome for a startup. MotherDuck offers [a simpler, more cost-effective path](https://motherduck.com/learn-more/modern-data-warehouse-playbook/). Its key differentiators are the serverless, per-second billing model and the unique \"local-first\" hybrid architecture, which provides an excellent development experience and eliminates network latency for many common analytical tasks.\n\n### How do startups decide if MotherDuck fits their analytics stack?\n\nA startup should consider MotherDuck if they experience slow BI dashboards, rising query timeouts, and stale data. It's a strong fit for teams that have outgrown a single database (like PostgreSQL) but want to avoid the cost and complexity of a traditional data warehouse. MotherDuck excels with data volumes from gigabytes to tens of terabytes and is ideal for building fast, interactive analytics without a large data engineering team.\n\n### Why do dashboards struggle when scanning billions of rows?\n\nWhen scanning billions of rows, single-node engines like DuckDB can be constrained by the memory and I/O of a single machine. Complex queries generate large intermediate results that can exceed available RAM, forcing the engine to \"spill\" to a slower disk, which increases latency. High user concurrency can also saturate the CPU, causing all queries to slow down.\n\n### Why do lakehouse architectures often have high query latency?\n\nLakehouse latency is often caused by the **\"small files problem.\"** Streaming jobs write thousands of tiny files to object storage like S3. To run a query, the engine must make many high-latency network requests just to read the metadata and find the right files, adding seconds of delay before the query even starts processing data.\n\n### What are the main causes of slow BI dashboards?\n\nThe primary cause is an architectural mismatch. Startups often run heavy analytical (OLAP) queries on their production (OLTP) database, like PostgreSQL. These row-oriented databases are inefficient for analytics, as they must read entire records instead of just the needed columns. As data grows, this leads to high I/O, slow queries, and dashboard latency.\n\n### How does data modeling impact dashboard performance?\n\nIt has a massive impact. Using a [ star schema](https://motherduck.com/learn-more/star-schema-data-warehouse-guide/), which organizes data into a central \"fact\" table and surrounding \"dimension\" tables, is a proven technique for accelerating analytical queries in columnar databases.",
  [2m  â”‚[0m     "numLines": 189,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 189
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md",
  [2m  â”‚[0m     "content": "---\ntitle: 'Fix Slow Query: A Developer''s Guide to Data Warehouse Performance'\ncontent_type: guide\ndescription: 'Diagnose and fix slow queries by targeting the true bottlenecks: I/O,\n  Network, & CPU. This developer''s guide helps you optimize data layout & joins for\n  faster apps.'\npublished_date: '2025-11-09T00:00:00'\nsource_url: https://motherduck.com/learn-more/diagnose-fix-slow-queries\nindexed_at: '2025-11-25T10:52:22.487248'\ncontent_hash: 3b9a8730ded678a1\nhas_step_by_step: true\n---\n\n# Fix Slow Query: A Developer's Guide to Data Warehouse Performance\n\n17 min readBYA developer pushes a new feature powered by a data warehouse query. In staging, it is snappy. In production, the user-facing dashboard takes five seconds to load, generating user complaints and performance alerts. This scenario is painfully common. The modern data stack promised speed and scale, yet developers constantly find themselves fighting inscrutable latency. Slow queries are not a vendor problem. They are [a physics problem](https://en.wikipedia.org/wiki/Amdahl%27s_law). Performance is governed by a predictable hierarchy of bottlenecks: reading data from storage (I/O), moving it across a network for operations like joins (Shuffle), and finally, processing it (CPU).\n\nWithout understanding this hierarchy, developers waste time optimizing the wrong things, such as rewriting SQL when the data layout is the issue. They burn money on oversized compute clusters and deliver poor user experiences. This article provides a developer-centric mental model to diagnose and fix latency at its source. By understanding the physical constraints of storage, network, and compute, you can build data systems that are not just fast, but predictably and efficiently so.\n\n## TL;DR\n\n- Query performance is a physics problem, with bottlenecks occurring in a specific order: I/O (storage), then Network (shuffle), then CPU (compute). Fixing them in this order is the most effective approach.\n- Your data layout strategy is your performance strategy. Columnar formats, optimal file sizes, partitioning, and sorting can cut the amount of data scanned by over 90%, directly targeting the largest bottleneck.\n- Distributed systems impose a \"shuffle tax.\" The most expensive operations are large joins and aggregations that move terabytes of data between nodes. Avoiding the shuffle is the key to fast distributed queries.\n- There is no one-size-fits-all warehouse. A \"Workload-Fit Architecture\" matches the engine to the job's specific concurrency and latency needs, often leading to simpler, faster, and cheaper solutions for interactive workloads.\n\n## The Three-Layer Bottleneck Model: Why Queries Crawl\n\nLatency is almost always I/O-bound first, then network-bound, then CPU-bound. A slow query is the result of a traffic jam in the data processing pipeline, and this congestion nearly always occurs in a predictable sequence across three fundamental layers. Developers often jump to optimizing SQL logic or scaling up compute clusters, which are CPU-level concerns. This is ineffective because the real bottleneck lies much earlier in the process: in the physical access of data from disk (I/O).\n\nThe hierarchy of pain begins with I/O. Reading data from cloud object storage like [Amazon S3 is the slowest part of any query](https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-guidelines.html). An unoptimized storage layer can force an engine to read 100 times more data than necessary, a problem known as read amplification. Fixing data layout can yield greater performance gains than doubling compute resources.\n\nNext comes the Network. In distributed systems, operations like joins and aggregations often require moving massive amounts of data between compute nodes in a process called the shuffle. This involves serialization, network transit, and potential spills to disk, making it orders of magnitude slower than memory access. The shuffle is a tax on distributed computing that must be minimized.\n\nFinally, once the necessary data is located and moved into memory, the bottleneck becomes the CPU. At this stage, efficiency is determined by the engine's architecture. Modern analytical engines use [vectorized execution](https://www.cidrdb.org/cidr2005/papers/P19.pdf), processing data in batches of thousands of values at a time instead of row-by-row, which dramatically improves computational throughput. Optimizing SQL is only impactful once the I/O and network bottlenecks have been resolved.\n\n## Scenario 1: Optimizing I/O for Slow Dashboards with Partitioning and Clustering\n\nWhen a user-facing dashboard needs to fetch a small amount of data, such as sales for a single user, the query should be nearly instant. If it takes several seconds, the cause is almost always an I/O problem. The engine is being forced to perform a massive, brute-force scan to find a few relevant rows, a classic \"needle in a haystack\" problem. This occurs when the physical layout of the data on disk does not align with the query's access pattern.\n\nThe main culprits are partition and clustering misses. For example, a query filtering by `user_id`\n\non a table partitioned by `date`\n\nforces the engine to scan every single date partition. Similarly, if data for a single user is scattered across hundreds of files, the engine must perform hundreds of separate read operations. The first time this data is read, it is a \"cold cache\" read from slow object storage, which carries the highest latency penalty.\n\nThe fix is to enable data skipping, where the engine uses metadata to avoid reading irrelevant data. Partitioning allows the engine to skip entire folders of data, while clustering (sorting) ensures that data for the same entity (like a `user_id`\n\n) is co-located in the same files. This allows the min/max statistics within file headers to be highly effective, letting the engine prune most files from the scan. This is addressed with features like ** Snowflake's Clustering Keys**,\n\n**,**\n\n[BigQuery's Clustered Tables](https://cloud.google.com/bigquery/docs/clustered-tables)**, or**\n\n[Databricks' Z-Ordering](https://docs.databricks.com/aws/en/delta/data-skipping)**. Warehouses may also offer managed features to aid this, such as Snowflake's Search Optimization Service, which create index-like structures to accelerate these lookups at a cost.**\n\n[Redshift's Sort Keys](https://docs.aws.amazon.com/redshift/latest/dg/t_Sorting_data.html)### From Theory to Practice: Implementing Data Layout\n\nUnderstanding the need for a good data layout is the first step. Implementing it is the next. The most direct way to enforce clustering is to sort the data on write. Using SQL, you can create a new, optimized table by ordering the data by the columns you frequently filter on.\n\nFor example, to create a clustered version of a `page_views`\n\ntable for fast user lookups:\n\nCopy code\n\n```\nCREATE TABLE page_views_clustered AS\nSELECT * FROM page_views\nORDER BY user_id, event_timestamp;\n```\n\n\nThis physical ordering ensures that all data for a given `user_id`\n\nis stored contiguously, dramatically reducing the number of files the engine needs to read for a query like `WHERE user_id = 'abc-123'`\n\n.\n\nFor teams using dbt, this can be managed directly within a [model's configuration block](https://docs.getdbt.com/reference/resource-configs/resource-configs). This approach automates the process and keeps the data layout logic version-controlled alongside the rest of the data transformations.\n\nCopy code\n\n```\n-- in models/marts/core/page_views.sql\n{{\nconfig(\nmaterialized='table',\npartition_by={\n\"field\": \"event_date\",\n\"data_type\": \"date\",\n\"granularity\": \"day\"\n},\ncluster_by = [\"user_id\"]\n)\n}}\nSELECT\n...\nFROM\n{{ ref('stg_page_views') }}\n```\n\n\nThis configuration tells the warehouse to partition the final table by day and then cluster (sort) the data within each partition by `user_id`\n\n, providing a highly efficient layout for user-facing dashboards.\n\n## Scenario 2: Fixing Slow Joins by Minimizing Network Shuffle\n\nLarge joins in distributed systems are slow because of the massive data movement required. This network bottleneck, known as the shuffle, is the tax paid for distributed processing. When joining two large tables, the engine must redistribute the data across the cluster so that rows with the same join key end up on the same machine. This involves expensive serialization, network transfer, and potential spills to disk if the data exceeds memory.\n\nDistributed engines primarily use two join strategies. A **Broadcast Join** is used when one table is small (e.g., under a 10 MB default in Spark). The engine copies the small table to every node, allowing the join to occur locally without shuffling the large table. This is highly efficient. A **Shuffle Join** is used when both tables are large. Both tables are repartitioned across the network based on the join key. This is brutally expensive and is often the cause of a slow query. This is known as [a Broadcast Join in Spark](https://spark.apache.org/docs/latest/sql-performance-tuning.html), but the concept of distributing a small dimension table to all compute nodes is a fundamental optimization in all MPP systems, including [Snowflake](https://docs.snowflake.com/en/user-guide/querying-joins) and Redshift.\n\nThe performance of a shuffle join is further degraded by two killers: data skew and disk spills. Data skew occurs when one join key contains a disproportionate amount of data, creating a \"straggler\" task that bottlenecks the entire job. Disk spills happen when a node runs out of memory and is forced to write intermediate data to slow storage, turning a memory-bound operation into a disk-bound one.\n\n### From Theory to Practice: Reading an Execution Plan\n\nDiagnosing a slow join requires inspecting the query's execution plan, which is the primary diagnostic tool. You can find this in ** Snowflake's Query Profile**,\n\n**, or by running an**\n\n[BigQuery's Query execution details](https://cloud.google.com/bigquery/docs/query-plan-explanation)`EXPLAIN`\n\ncommand in **Databricks**. While graphical plans are helpful, understanding the textual output is a critical skill. Consider a simplified plan for a shuffle join:\n\nCopy code\n\n```\n== Physical Plan ==\nSortMergeJoin [left_key], [right_key], Inner\n:- *(2) Sort [left_key ASC], false, 0\n: +- Exchange hashpartitioning(left_key, 200)\n: +- *(1) FileScan parquet table_A[left_key] Batched: true, DataFilters: [], Format: Parquet\n+- *(4) Sort [right_key ASC], false, 0\n+- Exchange hashpartitioning(right_key, 200)\n+- *(3) FileScan parquet table_B[right_key] Batched: true, DataFilters: [], Format: Parquet\n```\n\n\nHere is how to interpret it:\n\n**Spot the Shuffle**: The`Exchange`\n\noperator is the shuffle. It indicates that data is being repartitioned and sent across the network. If you see an`Exchange`\n\non both sides of a join, it is a shuffle join. The absence of an`Exchange`\n\non one side would suggest a more efficient broadcast join.**Identify the Scan**: The`FileScan`\n\noperator shows where data is being read from storage. A well-optimized query will show partition filters here (e.g.,`PartitionFilters: [isnotnull(date), (date = 2024-10-26)]`\n\n), confirming that[partition pruning](https://cloud.google.com/bigquery/docs/partitioned-tables)is working.**Find the Join Algorithm**: The`SortMergeJoin`\n\nindicates the specific type of shuffle join. Another common type is`ShuffleHashJoin`\n\n. The choice of algorithm can have performance implications, but the presence of the`Exchange`\n\nis the bigger red flag.\n\nWhen a query is slow, look for large `bytes shuffled`\n\nor `time spent in shuffle`\n\nmetrics associated with the `Exchange`\n\noperator. If one task within the `Exchange`\n\nstage takes much longer than others, it is a clear sign of data skew.\n\nFor cases where you know a table is small enough to be broadcast but the optimizer fails to choose that strategy, you can often provide a hint in the SQL.\n\nCopy code\n\n```\nSELECT /*+ BROADCAST(country_lookup) */\ne.event_id,\nc.country_name\nFROM\nevents AS e\nJOIN\ncountry_lookup AS c\nON e.country_code = c.country_code;\n```\n\n\nThis hint forces the engine to broadcast the `country_lookup`\n\ntable, avoiding a costly shuffle of the massive `events`\n\ntable.\n\n## Scenario 3: Solving Read Amplification with Columnar Formats like Parquet\n\nReading an entire file to answer a query that needs only one column is the most wasteful I/O operation and a clear sign of a poorly chosen file format. This happens with row-oriented formats like CSV or JSON, which store data in rows. To get the value from a single column, the engine must read and discard all other columns in that row. This is a primary cause of read amplification.\n\nThe solution is to standardize on columnar formats like [Apache Parquet](https://parquet.apache.org/). Parquet stores data in columns, not rows, which immediately enables **column pruning**. If a query is `SELECT avg(price) FROM sales`\n\n, the engine reads only the `price`\n\ncolumn and ignores all others. This can reduce storage footprints by up to 75% compared to raw formats and is a cornerstone of modern analytics performance.\n\nParquet's efficiency goes deeper, with a metadata hierarchy that enables further data skipping. Files are divided into **row groups** (e.g., 128 MB chunks), and the file footer contains min/max statistics for every column in each row group. When a query contains a filter like `WHERE product_category = 'electronics'`\n\n, the engine first reads the footer. If the min/max statistics for a row group show it only contains 'books' and 'clothing', the engine can skip reading that entire 128 MB chunk of data. For this to be effective, data should be sorted by frequently filtered columns before being written, which makes the min/max ranges tighter and more precise.\n\n### From Theory to Practice: Writing Optimized Parquet\n\nCreating an optimized data layout is a data engineering task performed during ETL/ELT. For teams using frameworks like Apache Spark, the write logic is the control point for implementing partitioning, sorting, and file compaction. A common pattern is to repartition the data by a low-cardinality key (like date) and then sort within those partitions by a higher-cardinality key (like user ID).\n\nHere is a PySpark example demonstrating this pattern:\n\nCopy code\n\n```\n# Assuming 'df' is a Spark DataFrame with page view events\n# Define output path and keys\noutput_path = \"s3://my-bucket/page_views_optimized\"\npartition_key = \"event_date\"\ncluster_key = \"user_id\"\n(df\n.repartition(partition_key)\n.sortWithinPartitions(cluster_key)\n.write\n.mode(\"overwrite\")\n.partitionBy(partition_key)\n.parquet(output_path)\n)\n```\n\n\nThis code snippet does three critical things:\n\n`repartition(partition_key)`\n\n: Groups data by the partition key, ensuring all data for a given date ends up on the same worker node before writing.`sortWithinPartitions(cluster_key)`\n\n: Sorts the data on each worker by`user_id`\n\n, making the min/max statistics in the resulting Parquet files highly effective for pruning.`partitionBy(partition_key)`\n\n: Writes the data out to a directory structure like`/event_date=2024-10-26/`\n\n, which enables partition pruning at the folder level.\n\nThis approach produces well-structured, skippable Parquet files that form the foundation of a high-performance data lakehouse.\n\n## The Economics of Speed: Cost vs. Performance Trade-offs\n\nIn the real world, performance is not an absolute goal. It is an economic decision. Engineers constantly balance query speed, compute cost, storage cost, and their own time. Without this context, performance advice remains academic and is insufficient for making business-justified architectural choices. Every optimization is a trade-off between paying now or paying later.\n\nThe most fundamental trade-off is between compute and storage. Optimizing data layout by sorting and compacting files is not free. It requires an upfront compute cost to perform the ETL/ELT job. This, in turn, may slightly increase storage costs if less efficient compression is used to favor faster reads. However, this one-time investment pays dividends over time by dramatically reducing the compute costs for every subsequent query that reads the data. A well-clustered table might cost $50 in compute to write but save thousands of dollars in query compute over its lifetime.\n\nThis economic model extends to managed features. When you enable a feature like [Snowflake's Search Optimization Service](https://docs.snowflake.com/en/user-guide/search-optimization-service) or [BigQuery's Clustering](https://cloud.google.com/bigquery/docs/clustered-tables), you are making a conscious financial decision. You are agreeing to pay for the managed compute required to maintain an index-like structure and for the additional storage that structure consumes. In return, you avoid paying for massive, recurring compute costs from brute-force table scans. This is a sensible trade-off for high-value, frequently executed queries, but a poor one for ad-hoc analysis on cold data.\n\nFinally, the human cost must be considered. An engineer's time is often the most expensive resource. Spending two weeks manually optimizing a data pipeline to shave 10% off a query's runtime might not be worth it if simply scaling up the virtual warehouse for ten minutes a day would achieve the same result for a fraction of the cost. The goal is to find the right balance, investing engineering effort in foundational layout optimizations that provide compounding returns and using compute resources flexibly to handle spiky or unpredictable workloads.\n\nThis economic reality leads to a crucial insight: if the primary performance killers for interactive queries are I/O latency from object storage and network shuffle, what if we could architect a system that bypasses them entirely for certain workloads? This is the central idea behind a modern, Workload-Fit Architecture.\n\n## The Solution: Adopting a Workload-Fit Architecture\n\nFixing common performance scenarios reveals a pattern: most problems are symptoms of an architectural mismatch. The era of using one massive, monolithic MPP warehouse for every job is over. It is often too complex and expensive for the task at hand. This leads to a more modern approach: **Workload-Fit Architecture**, which is the principle of matching the tool to the job's specific concurrency, latency, and cost requirements.\n\nThis approach explicitly re-evaluates the I/O, Network, and CPU trade-offs for a given workload.\n\n**I/O**: An in-process engine like DuckDB, running on a developer's laptop or a cloud VM, can use the local operating system's page cache and achieve extremely low-latency I/O from local SSDs. For \"hot\" data that fits on a single machine, this is orders of magnitude faster than fetching data from remote object storage.**Network**: The single biggest advantage of an in-process or single-node architecture is the complete elimination of the network shuffle tax. Joins and aggregations happen entirely in-memory or with spills to local disk, avoiding the expensive serialization and network transit inherent in distributed systems.**CPU**: Without the overhead of network serialization and deserialization, more CPU cycles are spent on productive, vectorized computation. This allows in-process engines to achieve incredible single-threaded performance.\n\nMotherDuck is a prime example of this workload-fit philosophy. It combines the speed of DuckDB's local-first, in-process vectorized engine with the persistence and scalability of a serverless cloud backend. It is not designed for petabyte-scale ETL. Instead, it excels at the vast majority of workloads: powering interactive dashboards, enabling ad-hoc analysis, and serving data apps on datasets from gigabytes to a few terabytes, where low latency is critical and the overhead of a distributed MPP system is unnecessary. Read more in our documentation about [MotherDuck's Architecture](https://motherduck.com/docs/concepts/architecture-and-capabilities/).\n\n## Decision Matrix: Matching Your Workload to the Right Engine\n\nChoosing the right architecture requires evaluating your workload along two critical axes: the number of simultaneous users or queries (**Concurrency**) and the required response time (**Latency SLA**). This matrix provides a framework for selecting the appropriate engine type.\n\nSub-Second (<1s) | Interactive (1-10s) | Reporting (>10s) | |\n|---|---|---|---|\nVery High (1000+ users) |\nSpecialized engines for user-facing analytics with high concurrency. | MPP Warehouse (Snowflake, BigQuery)Designed for enterprise BI with elastic scaling for thousands of users. | MPP Warehouse (Snowflake, BigQuery)Can scale out compute to handle massive batch reporting workloads. |\nMedium (10-100 users) | MotherDuck, ClickHouseExcellent for internal dashboards and data apps where latency is key for a team. | MotherDuck, DuckDB (large server)Ideal for interactive analysis by a team, providing speed without MPP overhead. | All EnginesMost modern warehouses can handle this. Choice depends on cost and specific features. |\nLow (1-10 users) | DuckDB (local), MotherDuckUnparalleled speed for local analysis or embedded apps, with cloud persistence. | DuckDB, MotherDuckPerfect for individual data scientists or small teams exploring data. Fast and simple. | DuckDB, All Cloud WarehousesFor a few users running long queries, any engine will work. DuckDB offers simplicity. |\n\n## Conclusion: Performance is a Data Engineering Choice\n\nSlow queries are not a mystery but a result of understandable physical principles. The path to performance is through disciplined data engineering: fixing I/O first by optimizing data layout, then minimizing network shuffles, and finally, choosing an architecture that fits the workload's economic and technical requirements. Performance is not a feature you buy from a vendor. It is a characteristic you design into your system. By addressing bottlenecks in the right order, I/O, then Network, then CPU, you can systematically build data applications that are fast, efficient, and cost-effective.\n\n### Path Forward\n\n**Analyze Your Own Query**: Pick one of your slow queries and inspect its execution plan. Can you identify the bottleneck using the I/O-Network-CPU model? Look for signs of full table scans, large data shuffles, or disk spills.**Audit Your Data Layout**: Examine the physical layout of your most frequently queried table. Is it stored in Parquet? Are file sizes optimized between 128MB and 1GB? Is the data sorted by columns commonly used in filters?**Consider Your Architecture**: For your next interactive dashboard or data application project, evaluate if a Workload-Fit architecture could provide better performance and lower complexity than a traditional MPP warehouse. For many medium-data workloads, the answer is yes.\n\nStart using MotherDuck now!\n\n## FAQS\n\n### Why does it take so long to show sales or page hits for a user?\n\nThis \"needle in a haystack\" problem is typically an I/O bottleneck, forcing the query engine to scan massive amounts of data just to find a few relevant rows for a single user. Optimizing your data layout with clustering and partitioning is the first step to enable data skipping and speed up these lookups. For workloads that demand consistently fast, interactive analytics, a modern data warehouse like MotherDuck leverages the power of DuckDB to deliver near-instant results for such queries.\n\n### How can we improve the speed of our data warehouse reports?\n\nThe most effective way to improve report speed is to tackle bottlenecks in order, starting with I/O by optimizing your data layout through partitioning and sorting. This dramatically reduces the amount of data scanned, which is the most common cause of slowness. Adopting a workload-fit architecture with a platform like MotherDuck can also provide a simpler, faster, and more cost-effective solution specifically for interactive reporting and analytics.\n\n### Why do I need to read entire files when querying large datasets?\n\nYour query engine reads entire files when the data layout isn't optimized, preventing it from \"skipping\" irrelevant data blocks. Without proper sorting or clustering, the engine's metadata can't effectively prune files, forcing a brute-force scan. Platforms like MotherDuck, built on the high-performance DuckDB engine, are architected to maximize the benefits of a good data layout and minimize costly I/O operations.\n\n### How can I optimize performance when using direct queries in a data warehouse environment?\n\nFor direct queries in data apps, performance hinges on minimizing I/O latency by aligning your physical data layout with common query patterns. Using techniques like partitioning and clustering allows the engine to skip most of the data and return results in milliseconds. This is where a serverless data warehouse like MotherDuck excels, providing the low-latency query engine needed to power snappy, user-facing applications without complex infrastructure management.\n\n### Why are my large range joins taking so long to complete?\n\nLarge joins are often slow due to the \"shuffle tax\" in distributed data warehouses, where massive amounts of data must be moved across the network between compute nodes. This network transit is a major bottleneck, often orders of magnitude slower than in-memory processing. Modern architectures like MotherDuck, which leverages DuckDB's state-of-the-art join algorithms, can process these complex operations with greater efficiency and less reliance on costly data shuffling.\n\n### How does query latency affect product-led teams building data apps?\n\nHigh query latency directly translates to a poor user experience, with slow dashboards and unresponsive features leading to user frustration and abandonment. For product-led teams, this lag undermines the value of their data apps and can trigger a flood of performance alerts. Building on a fast, serverless platform like MotherDuck is crucial for these teams, as it provides the performance foundation needed to deliver the snappy, interactive experiences users expect.\n\n### What are some ways to improve data system performance and handle multiple users efficiently?\n\nBeyond optimizing data layout, a key strategy is to adopt a \"Workload-Fit Architecture\" that matches your query engine to your specific needs. For interactive analytics with many concurrent users, traditional batch-oriented warehouses can struggle. A modern, serverless solution like MotherDuck is specifically designed for these high-concurrency, low-latency workloads, ensuring consistently fast performance for all users without manual scaling.\n\n### Why might lakehouse architectures have high latency issues?\n\nLakehouse architectures often suffer from high latency because they query data directly on cloud object storage like S3, which is inherently slow for I/O. If the data layout isn't meticulously optimized with partitioning and sorting, queries are forced to scan far more data than necessary, causing significant delays. To overcome this, you can use a high-performance query engine like MotherDuck, which can sit on top of your data lake to provide the speed needed for interactive analysis.\n\n### What solutions perform well when joining large datasets across S3 or other cloud storage?\n\nHigh-performance joins on cloud storage require an engine that can both minimize I/O and execute joins efficiently without a massive \"shuffle tax.\" The key is an architecture that can intelligently read only the necessary data and process joins using modern, vectorized algorithms. Cloud data warehouses like MotherDuck are built on DuckDB's powerful engine, which excels at running complex analytical queries and joins directly against data in S3 with remarkable speed.\n\n### Why do analysts complain about slow \u201ctime-to-first-insight\u201d?\n\nA slow \"time-to-first-insight\" is a direct symptom of query latency caused by I/O and network bottlenecks in the data warehouse. When analysts wait minutes for queries to run, their creative and exploratory workflow is constantly interrupted, leading to frustration and reduced productivity. By using a platform like MotherDuck, which is optimized for rapid, interactive queries, teams can eliminate these delays and empower analysts to find insights in seconds, not minutes.",
  [2m  â”‚[0m     "numLines": 330,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 330
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me search for more specific content about network latency, data transfer, and cloud vs local comparison:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ network.*latency|data.*transfer|cloud.*local|remote.*query

  [96mğŸ”§ TOOL:[0m Grep â†’ MotherDuck.*slower|slower.*MotherDuck|cloud.*slower
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 15,
  [2m  â”‚[0m   "appliedLimit": 15
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/summer-data-engineering-roadmap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-ducks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/bigquery-to-duckdb-motherduck.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 8,
  [2m  â”‚[0m   "appliedLimit": 10
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/faster-ducks.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     "content": "---\ntitle: bringing-duckdb-to-the-cloud-dual-execution-explained\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained\nindexed_at: '2025-11-25T20:45:20.295187'\ncontent_hash: ab4030806c409492\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nBringing DuckDB to the Cloud: Dual Execution Explained - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Bringing DuckDB to the Cloud: Dual Execution Explained](https://www.youtube.com/watch?v=2eVNm4A8sLw)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=2eVNm4A8sLw&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 1:08:11\n\n\u2022Live\n\n\u2022\n\nYouTubeQuack & CodeMotherDuck Features\n\n# Bringing DuckDB to the Cloud: Dual Execution Explained\n\n2024/06/28\n\nBringing DuckDB's analytical power to the cloud requires more than just running it on a server. While DuckDB excels at processing data on a local machine, the dynamics of data analysis change when dealing with cloud-scale data, collaboration, and shared resources. This shift introduces challenges around [security](https://motherduck.com/learn-more/secure-startup-data-warehouse/), concurrent access, and performance, which is where a purpose-built cloud architecture becomes essential.\n\nMotherDuck, a modern cloud data warehouse, is built on DuckDB to solve these challenges. It extends DuckDB\u2019s capabilities without forking the open source project, creating a hybrid system that intelligently balances local and cloud computing. In a conversation with MotherDuck founding engineer Stephanie, she explained the architecture and the innovative query model, known as **dual execution**, that makes this possible.\n\n* * *\n\n## The MotherDuck Architecture: Beyond Hosted DuckDB\n\nA common misconception is that MotherDuck is simply DuckDB hosted in the cloud. The reality is a more sophisticated system designed to overcome the limitations of using a locally-optimized engine in a distributed environment. MotherDuck\u2019s architecture is built on three key components and a core philosophy of not forking DuckDB.\n\nInstead of maintaining a separate version, MotherDuck leverages DuckDB\u2019s powerful **extension system**. This allows MotherDuck to add new capabilities at multiple layers, including the SQL parser, optimizer, and storage interface, while staying current with the latest open source DuckDB releases. This tight integration means users benefit from the rapid innovation of the DuckDB community almost immediately.\n\nThe architecture can be broken down into three main layers:\n\n1. **The Client Extension**: This is how MotherDuck integrates with the DuckDB ecosystem. Whether a user is working in the CLI, a Python script, or a JDBC connection, a simple `ATTACH` command connects their local DuckDB instance to the MotherDuck cloud. This extension is even used to run DuckDB in the browser via WASM for the MotherDuck UI, enabling client-side processing to reduce latency and cloud compute costs.\n2. **The Compute Layer**: In the cloud, queries are processed by containerized DuckDB instances, fondly called \"ducklings.\" These compute resources are scaled based on user needs, providing the necessary CPU and memory to handle complex analytical queries on large datasets.\n3. **The Storage Layer**: DuckDB's native file format is optimized for a single writer on a local file system. This model is not well-suited for the cloud, where multiple users need to read and write to the same database concurrently. To solve this, MotherDuck implemented a differential storage system that maps a logical database file to append-only snapshot layers in cloud object storage. This design is cloud-friendly, enabling efficient in-place updates and forming the foundation for features like database sharing and time travel.\n\n* * *\n\n## Enabling Collaboration with Database Sharing and Secure Credentials\n\nThis unique architecture transforms DuckDB from a \"single-player\" tool into a collaborative \"multiplayer\" platform. One of the most significant advantages is **database sharing**. Instead of emailing SQL scripts or passing around large database files, team members within the same organization can grant query access to their databases. This streamlines collaboration, ensuring everyone works from a consistent and up-to-date version of the data.\n\nSecurity is another critical aspect of any cloud data platform. MotherDuck provides a **centralized Secret Manager**, co-designed with the DuckDB team. Users can create persistent, encrypted secrets for accessing external data sources like AWS S3, Google Cloud Storage, or Delta Lake. Once a secret is created in MotherDuck, it can be reused across different clients and sessions without needing to expose or reconfigure credentials on each local machine.\n\n* * *\n\n## Understanding Dual Execution\n\nThe most innovative feature of MotherDuck's architecture is its **dual execution model**. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\n\nThe process was demonstrated with a query joining two tables. When both tables reside in a MotherDuck database, the query is executed entirely in the cloud. An `EXPLAIN` plan for this query reveals that all operators, from the table scans to the final join, are marked as **remote**. The only local operation is downloading the final result set to the client.\n\nThe real power of dual execution becomes apparent when a query involves both local and cloud data. In the demonstration, one of the remote tables was copied to a local Parquet file. The query was then modified to join this local file with the table that remained in the cloud.\n\nThe `EXPLAIN` plan for this new query showed a mix of local and remote operations. The scan of the Parquet file and its associated processing happened **locally**, leveraging the user's machine. The scan of the cloud table happened **remotely** on MotherDuck's compute. The system then efficiently transferred only the necessary intermediate data to complete the join. This hybrid approach avoids needlessly uploading the local file to the cloud or downloading the entire remote table, resulting in faster and more efficient queries.\n\n* * *\n\n## Taking Control with Manual Execution Overrides\n\nWhile MotherDuck's optimizer is designed to make the most efficient choice automatically, some situations require manual control. For this, users can use the `md_run` parameter within scan functions like `read_parquet()` or `read_csv()`.\n\nBy setting `md_run = 'local'`, a user can force the scan of a remote S3 file to be executed on their local client. This involves downloading the data, which might be desirable if the data needs to be processed near the user's location or if they prefer using local credentials. Conversely, setting `md_run = 'remote'` forces the operation to run in the MotherDuck cloud.\n\nThis control is especially useful for performance tuning. A direct comparison showed that querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds). By pushing the computation to the data's location, MotherDuck minimizes network I/O and delivers results more quickly.\n\n* * *\n\n## A Symbiotic Future for Local and Cloud Analytics\n\nMotherDuck's architecture is a thoughtful extension of DuckDB's core principles, adapting its local-first power for the demands of the cloud. The dual execution model provides a flexible and efficient bridge between a user's laptop and cloud data, optimizing workloads in a way that pure-cloud or pure-local systems cannot.\n\nThis relationship is symbiotic. As the largest production user of DuckDB, MotherDuck continuously pushes the engine to its limits, uncovering opportunities for improvement and contributing enhancements back to the open source project. This collaboration ensures that both DuckDB and MotherDuck will continue to evolve, offering a powerful and seamless analytical experience that spans from local exploration to cloud-scale production.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Ficeberg_video_thumbnail_adfdabe3d6.jpg&w=3840&q=75)\\\\\n\\\\\n46:26](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n[2025-06-12](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n### [Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse)\n\nBuilding a data stack means choosing between easy SaaS and complex open-source. Apache Iceberg is a middle ground, but its catalog is a hurdle. New tools now simplify using Iceberg with DuckDB to create a flexible, local data lakehouse.\n\nMotherDuck Features\n\n[![\"Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FG_Se_B_Sox_AW_Fg_HD_f4fddaa9ab.jpg&w=3840&q=75)](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n[2025-04-23](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n### [Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly)\n\nHamilton Ulmer shares insights from MotherDuck's Instant SQL Mode, exploring how real-time query result previews eliminate the traditional write-run-debug cycle through client-side parsing and DuckDB-WASM caching.\n\nSQL\n\nTalk\n\nMotherDuck Features\n\n[![\"DuckDB: Run dbt build with sub-second execution times\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_03_26_at_3_40_16_PM_c6e3a8096f.png&w=3840&q=75)\\\\\n\\\\\n24:06](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n[2025-03-13](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n### [DuckDB: Run dbt build with sub-second execution times](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times)\n\nWhether you're new to DuckDB or looking to optimize your workflows, this session will provide practical insights to help you leverage its full potential.\n\nYouTube\n\nData Pipelines\n\ndbt\n\nMotherDuck Features\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 184,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 184
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/faster-ducks.md",
  [2m  â”‚[0m     "content": "---\ntitle: faster-ducks\ncontent_type: blog\nsource_url: https://motherduck.com/blog/faster-ducks\nindexed_at: '2025-11-25T19:56:38.989923'\ncontent_hash: 228fba6935ede5a6\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Faster Ducks\n\n2025/10/28 - 8 min read\n\nBY\n\n[Jordan Tigani](https://motherduck.com/authors/jordan-tigani/)\n\nYou\u2019ve probably heard the old Henry Ford [quotation](https://hbr.org/2011/08/henry-ford-never-said-the-fast) about customers wanting faster horses. Clearly he was full of horse-shit; what everyone needs is faster ducks. And with the recent DuckDB 1.4 release, we\u2019re seeing an average of nearly 20% improvement in performance; that\u2019s some faster ducks indeed.\n\nWhen we started MotherDuck, we made a huge bet on DuckDB; it was already an amazing analytics engine, but what was even more impressive was how quickly it was getting better. You\u2019d assume that after a while the pace of improvement would slow down, but three and a half years later, if anything they\u2019re moving even faster.\n\nAt MotherDuck, we operate the largest, most complex fleet of DuckDB instances in the world. We push DuckDB hard, know where it reaches its limits, and work very closely with Hannes & Mark (the creators of DuckDB) and the rest of the DuckDB team to pinpoint where people run into problems. Every DuckDB release has gotten harder to break, thanks to improvements from memory management to concurrency.\n\nThere used to be a disclaimer on the DuckDB website about how they didn\u2019t really care about performance; the goal was first to make a database that was correct, and then they\u2019d make it fast. That disclaimer isn\u2019t on the website anymore, because they\u2019ve finally gotten around to working on performance. And, without ruining the surprise, they\u2019ve made DuckDB damn fast.\n\n## Lies, Damn Lies, and Benchmarks\n\nIt is always a good idea to take database benchmarks with a grain of salt, especially when a vendor is sharing the results. Hannes and Mark even wrote a [paper](https://hannes.muehleisen.org/publications/DBTEST2018-performance-testing.pdf) about how fair database benchmarking is difficult to do, which includes this famous satirical graph:\n\n![Satirical image showing benchmark results.](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Ffigure1_53048bde56.png&w=3840&q=75)\n\nOne way to get slightly more valid benchmarks is to look at benchmarks created by someone else. Hannes likes to call these \u201cAway benchmarks\u201d, since it is a lot harder to win when you\u2019re playing on someone else\u2019s home turf rather than your own. When your competitor creates a benchmark, it generally is done to make them look good vs their competition, and when things go well for you using that benchmark, it is probably a very good sign.\n\nOne such \u201caway benchmark\u201d is [ClickBench](https://benchmark.clickhouse.com/). It was created by the folks at ClickHouse and includes a bunch of queries of the type that ClickHouse is good at. That said, for a vendor benchmark, it is pretty good at representing the types of queries that people actually run. It doesn\u2019t use a huge amount of data, but then most people don\u2019t actually use a ton of data in their day-to-day queries (see [this](https://motherduck.com/blog/redshift-files-hunt-for-big-data/) analysis we did of public datasets). Database people tend to favor the TPC-H and TPC-DS benchmarks, but those are pretty [well-known](https://assets.amazon.science/24/3b/04b31ef64c83acf98fe3fdca9107/why-tpc-is-not-enough-an-analysis-of-the-amazon-redshift-fleet.pdf) to be non-representative of real-world workloads. The other nice thing about ClickBench is that anyone can submit results, so dozens of vendors have tried their hands at claiming the top spot.\n\nAs of this morning, the MotherDuck Mega instance is #1 overall in ClickBench. While this is a nice result, there are a handful of systems that are only a few percent slower, and the rankings will almost certainly change over time. We try not to put too much stock in this kind of thing.\n\nWhat is interesting to us, however, is that if you limit the results to the main Cloud Data Warehouses (BigQuery, Snowflake, Redshift, MotherDuck), the results are dramatic, and less likely to be overturned with a clever hack or tweak to the scoring.\n\n![Clickbench results chart.](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Ffigure2_4696ce015a.png&w=3840&q=75)\n\nLet\u2019s take a look at the MotherDuck Standard, at $1.80/hour, and see how it stacks up against the other vendors. The fastest Redshift cluster is the 4x ra3.16xlarge (that really rolls off the tongue), which costs 29 times as much, at $52/hour, and is just a little bit slower than the MotherDuck Standard. MotherDuck Standard is also faster than a Snowflake 3XL at only 1/100 of the price. This last comparison isn\u2019t super fair because Snowflake doesn\u2019t really get much faster after you get to the XL instance. However, a Snowflake XL at $32/hour is still 18 times more expensive than a MotherDuck Standard, while being half the performance.\n\nSay we wanted to compare similarly priced options and how they score in the benchmark. MotherDuck Jumbo instances, at $3.20, are a little bit less expensive than a Snowflake S ($4), but are 6x faster. MotherDuck Mega instances at $10.80 are a little bit more expensive than a Snowflake M ($8), but are 7 times faster. If we\u2019re looking at Redshift, the 4x ra3.xlplus costs $4.34 an hour, not too much more than a MotherDuck Jumbo at $3.20, but with less than 1/7th the performance. The Redshift 2x dc2.8xlarge is $9.60/hour, about 10% less expensive than a MotherDuck Mega, but 1/11th the performance.\n\nHere is another way to look at it; let\u2019s say you want to run the Clickbench workload, how much does it cost you to run it in MotherDuck, Snowflake, and Redshift? Let\u2019s say we want to run it 100 times, and the first time we\u2019ll use the time it took the \u2018cold\u2019 run, and the remaining times we\u2019ll use the time for the hot run. After downloading the raw data from the [results](https://github.com/ClickHouse/ClickBench/tree/main), I\u2019ve summarized the cost to run this workload in the following chart (in dollars, lower is better unless you like spending more money):\n\n![Clickbench benchmark costs by warehouse vendor.](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Ffigure3_b6da1ee098.png&w=3840&q=75)\n\nIn general, database vendors give you the ability to \u201cpay more to make it go faster\u201d. That is, you can run on a larger instance and, in general, your performance will be better. In a perfect world, you could pay 2x more and get 2x the performance, so the actual cost wouldn\u2019t change since it would run in half the time. In that case, the bars in this graph would be flat. The only one of these that looks mostly flat is MotherDuck; not only is it much less expensive to run, but it also scales nearly linearly. So if you pay 2x more, you can run your workload roughly 2x faster.\n\nWhat about BigQuery? I spent a decade of my career working on BigQuery, so it pains me more than a little bit to see it not showing up better in the results. Looking at the code for the benchmark, my guess is that if someone from the BigQuery team updated the method of running the benchmark slightly, the results would look a lot better.\n\nThis goes to show that you don\u2019t want to put too much credence on one benchmark. After all, benchmarks are not the real world. And I think it is always more useful to benchmark against past versions of yourself; if you\u2019re accelerating faster than everyone else, then at the end of the day, you\u2019ll end up in first place, no matter how you measure or where you started. And this is where we can really shine.\n\n## Keep on Flocking in the Real World\n\nAt MotherDuck, we track query statistics across our fleet. Since we rolled out DuckDB 1.4 a few weeks ago, we\u2019ve been looking at the before and after performance to determine, in the real world, how much faster DuckDB 1.4 has gotten. And it is a lot.\n\nWe looked at a sample of around 100 million queries from before and after we released the new DuckDB version on our servers. We compared the performance of successful queries from paying users running in our cloud-hosted DuckDB instances.\n\nThe results are summarized below, with all times in seconds.\n\n|  | average | median | 90%-ile | 99%-ile | 99.9%-ile | 99.99%-ile |\n| --- | --: | --: | --: | --: | --: | --: |\n| **DuckDB 1.4.x** | 0.42 | 0.011 | 0.342 | 5.47 | 43.53 | 283.69 |\n| **DuckDB 1.3.x** | 0.50 | 0.011 | 0.375 | 6.22 | 51.94 | 412.22 |\n| **% change** | 19% | 0% | 10% | 14% | 19% | 45% |\n\n**The average query got 19% faster.** Of course, the average tends to be dominated by slower queries. The median query wasn\u2019t faster but the median queries were already only 11 milliseconds; there wasn\u2019t a whole lot of point in making them faster. Where you really start to see major improvements is when you look at the higher percentiles: the 99th percentile query got 14% faster, and the 99.99 percentile query got 45% faster.\n\nThis is all amazing news for users of DuckDB and MotherDuck, because typically, user experience is driven by the slowest queries. Most people won\u2019t really notice performance improvements when queries are already under 100 milliseconds or so. But if one of your queries takes 4 minutes instead of 7, that\u2019s a big difference.\n\nAnother way of looking at query performance is to ask, \u201cWhat percentage of queries appear to be instantaneous?\u201d Human reaction time is around 200 ms, so queries faster than that appear to be instant. When running DuckDB 1.3 on MotherDuck, 94% of queries were sub-200 ms. With DuckDB 1.4, more than 96% of queries were under 200 ms. This means that there was a 1/3 reduction in the likelihood a user had to wait for a query, and 24/25 of all queries appeared to be instantaneous.\n\n## The Pond Ahead\n\nAt MotherDuck, we strive to increase value for our customers; they get value when they can do more work faster for less money. In the last few weeks, their queries have taken less time to run, and in particular, their slowest ones have been a lot less slow. People have had to do a lot less waiting for queries to complete. This means they can spend more time figuring out what kinds of queries to run, or what to do with the results.\n\nThe exciting thing is that these improvements aren\u2019t a one-time event; every release of DuckDB has both a bunch of new features as well as improved performance. That makes MotherDuck better and faster, too. We estimate that since DuckDB 1.0, MotherDuck performance has doubled. While we still [believe](https://motherduck.com/blog/perf-is-not-enough/) that performance should not be the only criterion you use to choose a database, it certainly helps when your database keeps getting faster.\n\n### TABLE OF CONTENTS\n\n[Lies, Damn Lies, and Benchmarks](https://motherduck.com/blog/faster-ducks/#lies-damn-lies-and-benchmarks)\n\n[Keep on Flocking in the Real World](https://motherduck.com/blog/faster-ducks/#keep-on-flocking-in-the-real-world)\n\n[The Pond Ahead](https://motherduck.com/blog/faster-ducks/#the-pond-ahead)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![Why Python Developers Need DuckDB (And Not Just Another DataFrame Library)](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Fwhy_pythondev_1_22167e31bf.png&w=3840&q=75)](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/)\n\n[2025/10/08 - Mehdi Ouazza](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/)\n\n### [Why Python Developers Need DuckDB (And Not Just Another DataFrame Library)](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries)\n\nUnderstand why a database is much more than just a dataframe library\n\n[![DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_4_1_b6209aca06.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n[2025/10/09 - Alex Monahan, Garrett O'Brien](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n### [DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/blog/announcing-duckdb-141-motherduck)\n\nMotherDuck now supports DuckDB 1.4.1 and DuckLake 0.3, with new SQL syntax, faster sorting, Iceberg interoperability, and more. Read on for the highlights from these major releases.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 134,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 134
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me look for more content about architecture and why queries might be slower on MotherDuck vs local:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ latency|network|round.?trip|data.?transfer|overhead

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/motherduck.com/docs/**/*.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "durationMs": 369,
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ latency|round.?trip|overhead
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/build-bigger-with-small-ai-running-small-models-locally.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/building-data-apps-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-new-paradigm-for-data-visualization-with-just-sql-markdown.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     "content": "---\ntitle: duckfooding-at-motherduck\ncontent_type: event\nsource_url: https://motherduck.com/videos/duckfooding-at-motherduck\nindexed_at: '2025-11-25T20:45:02.848161'\ncontent_hash: 83fa33d7aa0c8ab6\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nDuckfooding at MotherDuck - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Duckfooding at MotherDuck](https://www.youtube.com/watch?v=z5P6Qa2OP6Y)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=z5P6Qa2OP6Y&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 40:09\n\n\u2022Live\n\n\u2022\n\nYouTubeData PipelinesMeetupTalk\n\n# Duckfooding at MotherDuck\n\n2024/10/14\n\n## **Lessons from Building MotherDuck with MotherDuck**\n\nWhen I joined MotherDuck as a Founding Engineer, I took on an unusual role: becoming the company's first internal customer. \"Dogfooding\" \u2013 the practice of using your own product extensively before releasing it to customers \u2013 has long been a staple of software development. For us at MotherDuck, a company building a cloud service based on the popular DuckDB analytical database, this approach has proved invaluable in uncovering real-world issues and accelerating product development.\n\n## **38 Seconds of Nothing**\n\nMy journey with data systems began around 2010 when I inherited the management of what was then the largest east coast Hadoop cluster, a 300-terabyte behemoth. This early experience revealed a fundamental inefficiency in distributed systems that would later influence MotherDuck's design philosophy.\n\nI ran a MapReduce job that took 42 seconds. But when I ran the same computation locally on my machine, it completed in just 4 seconds. I wondered: where were the other 38 seconds going?\n\nThe answer lies in the overhead of distributed computing. When data is processed across multiple machines, it must be serialized, sent over the network, deserialized, and reassembled \u2013 a process called \"shuffling.\" This creates significant latency that doesn't exist when processing data on a single machine.\n\nIn distributed systems, every shuffle operation causes this network storm where each node must communicate with every other node. You're exposed to long tail latency because the second part of your query plan cannot proceed until all partitions have been received.\n\nMeanwhile, hardware advancements have dramatically increased the processing capabilities of individual machines. Modern servers can have dozens of cores, hundreds of gigabytes of RAM, and terabytes of fast SSD storage. A single Graviton3 instance at $1.30 per hour delivers 300 gigabytes per second of aggregate memory bandwidth \u2013 comparable to an entire mid-sized Hadoop cluster from a decade ago.\n\nWe built these distributed systems to handle the top 1% of workloads, but we pay this distribution overhead for everything, even when it's unnecessary.\n\n## **No Overkill, No Lag**\n\nThe inefficiencies of traditional big data systems created an opportunity for a different approach. DuckDB emerged as a response to the need for efficient local analytical processing, particularly for data scientists working with tools like pandas.\n\nDuckDB takes a fundamentally different approach from cloud data warehouses. As an embedded analytical database, it runs in-process, eliminating network overhead entirely. It offers columnar storage, vectorized execution, and aggressive pipelining \u2013 essentially implementing state-of-the-art analytical processing techniques in a library that can run anywhere.\n\nDuckDB became relevant because it addressed the inefficiency of traditional approaches. It applies cutting-edge research on pipelining data and parallelizing operators to a market segment that larger companies had ignored \u2013 local analytics on your own machine.\n\nUnlike traditional data warehouses, DuckDB has no authentication system, no network interface, and no user management. It's designed for single-user, local processing. While this makes it incredibly efficient, it also limits its usefulness for collaborative work and larger datasets.\n\n## **Tiny Queries Everywhere**\n\nOur hypothesis at MotherDuck that most analytical workloads don't require massive distributed systems was supported by real-world data. Amazon published a paper with aggregated statistics from Redshift users, revealing that approximately 95% of queries scan less than 10GB of data, and 90% scan less than 1GB.\n\nEven more surprisingly, 30% of queries in production data warehouses scan just 1-100MB of data, and about 10% of Redshift customers are running entire warehouses on datasets between 1-10MB \u2013 data that would fit comfortably in a laptop's memory.\n\nPeople are over-provisioning relative to what one big machine could handle. There are benefits to having a shared, cloud-based system, but clearly many organizations are paying for more distributed processing power than they actually need.\n\n## **We are Our Own First Customer**\n\nFrom day one, I positioned myself as MotherDuck's first customer, implementing our internal analytics infrastructure using our own product. It would be hypocritical not to use the product we recommend to others ourselves.\n\nOur internal analytics stack at MotherDuck is relatively straightforward \u2013 using Airflow as a scheduler, mostly Python for data processing, and growing to employ partners for specific needs. The system now handles about 6,000 queries daily across 40 scheduled jobs, making it a substantial enough workload to thoroughly test the product.\n\nBy being deliberate about data types and optimizing for the vector execution engine, I keep our datasets efficiently sized \u2013 about 1 terabyte in total. This allows us to run the entire analytics operation at remarkably low cost, demonstrating the efficiency that careful engineering can achieve with DuckDB.\n\nOur billing is ridiculously tight. We only charge for when queries are actively working on something, not even billing for time when they're blocked on I/O. Following good practices with DuckDB results in a really cost-effective solution.\n\n## **Real World Discoveries**\n\nThe real value of dogfooding emerged when I began encountering issues that might not have been caught through conventional testing:\n\n**File Descriptor Limits**: We discovered a slow leak of connections when using `SELECT FROM s3://` commands that would eventually hit the 1024 file descriptor limit after about a week of running. This would never appear in short unit tests but became apparent during continuous operation of our analytics stack. Without dogfooding, customers might have hit this in production before we caught it.\n\n**Concurrent Schema Change Issues**: We found that when schema changes occurred, our system would pessimistically abort running queries due to out-of-sync local catalogs. This seemed reasonable in theory, but with tools like dbt where every materialization is a DDL statement (CREATE TABLE AS), it made the system practically unusable for real data engineering work. We had to revise this approach for better usability.\n\n**Long SQL in UI vs Error Location**: Our initial UI design placed SQL error messages below the query. This worked fine for simple examples, but I work with 1300+ line SQL queries that require scrolling back and forth between errors and the code that caused them. We shifted to inline error highlighting, similar to what dbt does, making it much more practical for real-world complex queries.\n\n**Validating Differential Storage**: We implemented a feature called differential storage, essentially writing our own file system layer. This was an extremely risky change that affects the core of data persistence. By enabling it for my production workloads for a month and a half before rolling it out to customers, we were able to catch edge cases and ensure stability for this critical component.\n\n**Bind Timeout During Slow Source Scans**: When connecting to external systems like Postgres, we initially expected at least one row of data every 15 seconds to keep connections fresh. But I had queries that scanned billions of rows yet returned only 40 rows after 10 minutes due to high selectivity. These timed out unnecessarily, teaching us that timeouts need to account for query selectivity, not just overall duration.\n\n**Deeply Nested JSON Memory Usage**: While typical JSON test data might be 30 fields with one nesting level, I showed up with real documents from MongoDB that were 80 levels deep per record. DuckDB wasn't prepared for this and used about 500 times the memory of the string just to parse it. Real-world data is far messier than test data, and dogfooding helped us identify and fix these memory issues.\n\n**Slow Extension Load Times**: As our binary grew over time, we weren't closely monitoring its size. The security checksumming process (which is crucial and can't be skipped) was taking 7+ seconds on each extension load. This delay became very noticeable in my daily workflow and prompted us to optimize our binary size and loading performance.\n\n## **Continuous Learning by Running Stuff**\n\nOur dogfooding approach created a tight feedback loop between product development and real-world usage. Over time, my role evolved from primarily testing basic functionality to actually using MotherDuck as the company's data engineer.\n\nWhen we first started, I was just making sure that basic operations like loading data and running aggregations worked properly. Now, about two-thirds of my time is actually doing data work for the company because the product is no longer giving me trouble.\n\nThis transition happened because issues were identified and fixed early. By the time MotherDuck reached general availability, the product had already been battle-tested through months of internal use.\n\nWe also gained valuable insights about scalability and resource management on a single node. While distributed systems spread workloads across multiple machines, a scale-up approach requires careful attention to memory allocation, core utilization, and I/O management. These learnings informed improvements to DuckDB's resource handling capabilities.\n\nLooking ahead, we continue to enhance our hybrid execution model, which intelligently processes queries partly on the client and partly in the cloud based on data location. This architecture provides the best of both worlds \u2013 local processing speed with cloud scalability when needed.\n\n## **If You Don\u2019t Use It, Nobody Should**\n\nFor us at MotherDuck, dogfooding has proven essential in building a product that truly meets user needs. By using our own service for actual data engineering work, we uncovered issues that synthetic testing would have missed and gained firsthand experience with the product's strengths and limitations.\n\nThis approach has yielded three main benefits: identifying bugs earlier in the development cycle, ensuring feature designs match real-world workflows, and building empathy for the customer experience. It has also reinforced our core thesis that most analytical workloads can be handled efficiently on a single node with modern hardware.\n\nFor engineers and data teams considering new analytical solutions, my experience suggests a few key takeaways. First, many workloads don't need the complexity of distributed systems. Second, the efficiency gains from eliminating network overhead and intermediate data materialization can be substantial. Finally, a product built by a team that uses it daily is likely to evolve in ways that address real pain points rather than theoretical concerns.\n\nWhether you're building or buying a data solution, the principle remains the same: the best way to understand if something works in practice is to make it part of your daily workflow. I've essentially been the sacrificial data engineer, running into issues before our customers do, which has made the product better for everyone.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"DuckLake: Making BIG DATA feel small (Coalesce 2025)\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Fmaxresdefault_1_7f1e9ebbca.jpg&w=3840&q=75)](https://motherduck.com/videos/ducklake-big-data-small-coalesce-2025/)\n\n[2025-10-14](https://motherduck.com/videos/ducklake-big-data-small-coalesce-2025/)\n\n### [DuckLake: Making BIG DATA feel small (Coalesce 2025)](https://motherduck.com/videos/ducklake-big-data-small-coalesce-2025)\n\nMotherDuck\u2019s managed DuckLake data lakehouse blends the cost efficiency, scale, and openness of a lakehouse with the speed of a warehouse for truly joyful dbt pipelines. They will show you how!\n\nTalk\n\n[![\"What can Postgres learn from DuckDB? (PGConf.dev 2025)\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_06_13_at_3_52_19_PM_470b0f71b1.png&w=3840&q=75)\\\\\n\\\\\n20:44](https://motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025/)\n\n[2025-06-13](https://motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025/)\n\n### [What can Postgres learn from DuckDB? (PGConf.dev 2025)](https://motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025)\n\nDuckDB an open source SQL analytics engine that is quickly growing in popularity. This begs the question: What can Postgres learn from DuckDB?\n\nYouTube\n\nEcosystem\n\nTalk\n\n[![\" pg_duckdb: Ducking awesome analytics in Postgres\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2F0kc_W5o0tcg_HD_03880f61fb.jpg&w=3840&q=75)](https://motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres/)\n\n[2025-06-12](https://motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres/)\n\n### [pg\\_duckdb: Ducking awesome analytics in Postgres](https://motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres)\n\nSupercharge your Postgres analytics! This talk shows how the pg\\_duckdb extension accelerates your slowest queries instantly, often with zero code changes. Learn practical tips and how to use remote columnar storage for even more speed.\n\nTalk\n\nSources\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 208,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 208
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m This is very helpful. Now I have a comprehensive understanding. Let me write the answer to the specified file:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_4.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "create",
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_4.md",
  [2m  â”‚[0m   "content": "<5484 chars>",
  [2m  â”‚[0m   "structuredPatch": [],
  [2m  â”‚[0m   "originalFile": null
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I've completed the task. Here's a summary of what I found:
  [94mâ”‚[0m 
  [94mâ”‚[0m ## Answer Summary
  [94mâ”‚[0m 
  [94mâ”‚[0m A query might run slower on MotherDuck than locally due to several key factors:
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. **Network Latency & Overhead** - Cloud execution involves network round-trips for sending queries and receiving results, plus data serialization/deserialization costs
  [94mâ”‚[0m 
  [94mâ”‚[0m 2. **Distributed System Overhead** - The "shuffle tax" from distributed computing (illustrated by the "38 seconds of nothing" example where a 4-second local computation took 42 seconds in a distributed system)
  [94mâ”‚[0m 
  [94mâ”‚[0m 3. **Local I/O is Faster** - Local SSDs and OS page caches offer dramatically lower latency than cloud object storage (S3 requests can take 100-200ms each)
  [94mâ”‚[0m 
  [94mâ”‚[0m 4. **In-Process Execution Eliminates Network** - DuckDB running locally has zero network overhead since it runs in-process
  [94mâ”‚[0m 
  [94mâ”‚[0m 5. **Small Data Scenarios Favor Local** - Most queries (95%) scan <10GB of data, which local machines handle efficiently
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer has been written to `/tmp/answer_without_kg_4.md` with full source attribution from 6 relevant files in the `.kurt/sources/` directory.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 15,220 | Cost: $0.6295
     Cumulative: 15,220 tokens | $0.6295
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent clearly states that the task is completed and provides a summary, indicating that it is not asking for further input but rather concluding the interaction.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 15 | Duration: 92.7s
â•‘    Tokens: 15,220 | Cost: $0.6295
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  Running LLM judge evaluation...
   ğŸ§  LLM Judge score: 0.95

======================================================================
â“ Question 5/10
...How do I set up MotherDuck to work with dbt?
======================================================================


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 5
â”‚ Answer this question by searching through the .kurt/sources/ directory:

...How do I set up MotherDuck to work with dbt?

Required Steps:
1. Search for relevant markdown files in .kurt/sources/ using glob and grep
2. Read the most relevant files (aim for 3-5 most relevant sources)
3. As you read each file, keep track of:
   - The file path
   - The file title/name
   - Key information extracted from that file
4. Synthesize a comprehensive answer based on the information found
5. Write your answer to: /tmp/answer_without_kg_5.md

IMPORTANT - Your answer MUST have this structure:
```markdown
# Answer

[Your comprehensive answer here, drawing from the sources you read]

## Sources

List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
[etc.]

DO NOT use URLs or document titles as links. Show the actual file path you read with the Read tool.
```

The Sources section is MANDATORY - you must list every file you read to answer the question.

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [96mğŸ”§ TOOL:[0m Grep â†’ dbt

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/**/*dbt*.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/llm-data-pipelines-prompt-motherduck-dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/solving-advent-code-duckdb-dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/ecosystem/dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/glossary/data build tool (dbt).md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/duckdb-dbt-accelerating-the-developer-experience-with-local-power.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/duckdb-dbt-end-to-end-data-engineering-project-23.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/take-flight-with-dbt-and-duckdb-dropping-dev-warehouse-costs-to-zero.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "durationMs": 415,
  [2m  â”‚[0m   "numFiles": 15,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/data-discoverability-secoda-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/sql-ide-safari-harlequin-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/take-flight-with-dbt-and-duckdb-dropping-dev-warehouse-costs-to-zero.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-in-100-seconds-by-a-duck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/no-sql-standard-for-7-years.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/panel-fixing-the-data-engineering-lifecycle-coalesce-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/is-duckdb-the-secret-to-unlocking-your-gis-potential.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/monte-carlo-simulations-with-duckdb-ft-james-mcneill.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-ga-produck-tour.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/faster-data-pipelines-development-with-mcp-and-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/fixing-sql-spaghetti-effective-refactoring-techniques.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/friendly-sql-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-bootstrap-a-data-warehouse-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ducklake-big-data-small-coalesce-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-dbt-accelerating-the-developer-experience-with-local-power.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-dbt-end-to-end-data-engineering-project-23.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-experiments-peeking-into-the-future-of-analytics-ft-christophe-blefari.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/big-data-is-dead-long-live-hot-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/building-data-apps-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/cultivating-growth-how-gardyn-scaled-its-data-operations-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/beyond-data-engineering-ft-joe-reis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/react-components-testing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-use-cases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-lake-vs-data-warehouse-vs-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/index.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data build tool (dbt).md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data model.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data pipeline.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/SQL analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Dagster.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/ELT.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/ETL.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/events.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/orchestra.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/paradime.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/contact-us/product-expert.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-brief.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dosomething-non-profit-tco-cost-savings.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/gardyn.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/reflex-careers-gig-economy-retail.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/trunkrs-same-day-delivery-motherduck-from-redshift.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/udisc-motherduck-sports-management.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/community-and-open-source.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-everybody-hates-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-embeddings-for-semantic-meaning-in-text-and-rag.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/summer-data-engineering-roadmap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-recap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/solving-advent-code-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/spark-ducklake-getting-started.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/semantic-layer-duckdb-tutorial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-kestra-etl-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-open-for-all-with-series-b.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-reflections-sigmod-pods-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/llm-data-pipelines-prompt-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/json-log-analysis-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/guide-to-coalesce-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/geospatial-for-beginner-duckdb-spatial-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/fake-duck-game.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-python-e2e-data-engineering-project-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-three.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-two.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-puppygraph-graph-model-on-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-seven.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-six.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-ten.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-nine.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eight.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eleven.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-five.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-four.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-essential-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-infrastructure-devops.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineers-answer-10-top-reddit-questions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-dec-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/building-motherduck-partner-ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/sung-won-chung.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/jacob-matson.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/marcos-ortiz.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/graham-wetzler.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/adithya-krishnan.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 156
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me read the most relevant files - starting with the ecosystem dbt page and the key blog posts about dbt setup:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/ecosystem/dbt.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/glossary/data build tool (dbt).md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/dual-execution-dbt.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/ecosystem/dbt.md",
  [2m  â”‚[0m     "content": "---\ntitle: dbt\ncontent_type: tutorial\nsource_url: https://motherduck.com/ecosystem/dbt\nindexed_at: '2025-11-25T20:37:37.218572'\ncontent_hash: 596a296ccc2dc11d\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO ECOSYSTEM](https://motherduck.com/ecosystem/)\n\n# dbt Core\n\nTRANSFORMATION\n\ndbt (data build tool) is a command-line tool that enables data analysts and engineers to transform data in their warehouse more effectively. It allows users to write modular SQL queries, test data integrity, and document data transformations, streamlining the analytics engineering workflow. dbt supports version control and collaboration, making it easier for teams to manage and deploy data models.\n\n## dbt + MotherDuck\n\ndbt integrates with MotherDuck by allowing users to perform data transformations and analytics directly within the MotherDuck cloud data warehouse. By connecting dbt to MotherDuck, users can leverage DuckDB's efficient query execution engine for transforming and modeling data, ensuring seamless and optimized data workflows. This integration empowers teams to build, test, and document their data pipelines efficiently while utilizing MotherDuck's scalable and performant data infrastructure.\n\nSince both dbt and DuckDB can run locally, users can do development and testing using the same technologies they run in production.\n\nBlog\n\nVideo\n\nDocs\n\n![dbt Core's logo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdbt_59d5e63eaf.svg&w=3840&q=75)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 38,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 38
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     "content": "---\ntitle: motherduck-duckdb-dbt\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/motherduck-duckdb-dbt\nindexed_at: '2025-11-25T19:58:31.660396'\ncontent_hash: 4233f76fbdf40552\nhas_code_examples: true\nhas_step_by_step: true\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# MotherDuck + dbt: Better Together\n\n2023/09/07 - 5 min read\n\nBY\n\n[Sung Won Chung](https://motherduck.com/authors/sung-won-chung--/)\n\n## My Personal DuckDB Story\n\nDuckDB has been charming to me ever since I wrote [about it a year ago](https://roundup.getdbt.com/p/dbt-learning-to-love-software-engineers).\n\nIt gave me the glimmers of something I\u2019ve been begging for a long time: fast should be measured in seconds, not minutes.\n\n[I kicked the tires a lot when working at dbt Labs](https://github.com/dbt-labs/jaffle_shop_duckdb).\n\n- [And here](https://www.loom.com/share/ed4a6f59957e43158837eb4ba0c5ed67)\n\n- [And most recently here](https://www.loom.com/share/e213768457094a3187663a6cff76a61d?sid=29d6d696-0581-4b50-af45-7132dfb65f80)\n\n\nAnd in all the tire kicking, it has remained true to the glimmers it gave me and so much more. It\u2019s fast, easy, and cheap. And if it\u2019s running on your local computer, it\u2019s free.\n\nI\u2019ve had incredibly asymmetric expectations of how much money, time, and work it takes to make data fast and easy that I think to myself, \u201cOh, of course you\u2019re supposed to pay lots of dollars to run queries on millions/billions of rows per month.\u201d This has pleasantly disrupted that inner anchoring point. I see something more charming at play. Data teams can be productive with data bigger and work faster and save more money than they could have dreamed of 5 years ago. Heck! Even a year ago. So let\u2019s get into it.\n\n## Why use MotherDuck + dbt?\n\nWell, DuckDB and Motherduck\u2019s primary use case is solving analytical problems fast. Because of its columnar design, it\u2019s able to do just that. Even more so, the creators were smart about making integrations with adjacent data tools a first class experience. We see this with reading S3 files without copying them over and querying postgres directly without needing to extract and load it into DuckDB. And you don\u2019t need to define schemas or tedious configurations to make it work! Motherduck enables the multiplayer experience that having a single file on your machine is too tedious to pass around and synchronize with your teammates. Motherduck runs DuckDB on your behalf AND uses your local computer if the query you\u2019re running makes more sense to run there. You get dynamic execution out of the box. And that\u2019s pretty sweet.\n\nBut more than platitudes, let\u2019s get hands-on with working code so you can taste and see for yourself!\n\n## Get Started\n\nYou can follow along with [this repo](https://github.com/sungchun12/jaffle_shop_duckdb/tree/blog-guide):\n\n1. Signup for a [MotherDuck account!](https://motherduck.com/)![signup](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmotherduck_signup_33d4e9cf54.png%3Fupdated_at%3D2023-09-06T12%3A58%3A42.172Z&w=3840&q=75)\nNote : MotherDuck is still under private beta, but I heard you could get an invite if you join their [community slack](https://slack.motherduck.com/) with a good duck pun.\n\n2. Sign in and your screen should look like this minus some of the stuff you\u2019ll be building in the rest of this guide.\n![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fsignin_14a92fa2b0.png%3Fupdated_at%3D2023-09-06T12%3A58%3A41.340Z&w=3840&q=75)\n\n3. Click on the settings in the upper right hand corner and copy your Service Token to the clipboard.\n![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fservice_token_8fec7f0642.png%3Fupdated_at%3D2023-09-06T12%3A58%3A39.304Z&w=3840&q=75)\n\n4. Clone the repo and change directories into it.\n\n\n```bash\nCopy code\n\ngit clone -b blog-guide https://github.com/sungchun12/jaffle_shop_duckdb.git\ncd jaffle_shop_duckdb\n```\n\n5. Follow the detailed instructions to setup your [free AWS account and use S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/GetStartedWithS3.html):\n\n_Note: Feel free to skip this step if you already have an AWS account with S3 setup! Plus, MotherDuck has these data under their public S3 bucket at s3://us-prd-motherduck-open-datasets/jaffle\\_shop/csv/_\n\n6. Take the csv files stored in the git repo [here](https://github.com/sungchun12/jaffle_shop_duckdb/tree/blog-guide/seeds) and upload them into S3:\n![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fseeds_9b3753fd5d.png%3Fupdated_at%3D2023-09-06T12%3A58%3A41.504Z&w=3840&q=75)![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fs3_seeds_adf0454153.png%3Fupdated_at%3D2023-09-06T12%3A58%3A41.673Z&w=3840&q=75)\n\n7. [Copy the AWS S3 access keys to authenticate](https://docs.aws.amazon.com/powershell/latest/userguide/pstools-appendix-sign-up.html) your dbt project for later.\n\n\n## Configure Your dbt project\n\n_Note: Huge thanks to Josh Wills for creating the dbt-duckdb adapter and it works great with both DuckDB and MotherDuck: [https://github.com/jwills/dbt-duckdb](https://github.com/jwills/dbt-duckdb). This demo only works with DuckDB version 0.8.1: [https://motherduck.com/docs/intro](https://motherduck.com/docs/intro)_\n\n1. Adjust your `profiles.yml` for the naming conventions that make sense to you. Specifically, focus on schema.\n\n```yaml\nCopy code\n\njaffle_shop:\n\n  target: dev\n  outputs:\n    dev:\n      type: duckdb\n      schema: dev_sung\n      path: 'md:jaffle_shop'\n      threads: 16\n      extensions:\n        - httpfs\n      settings:\n        s3_region: \"{{ env_var('S3_REGION', 'us-west-1') }}\"\n        s3_access_key_id: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n        s3_secret_access_key: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n\n    dev_public_s3:\n      type: duckdb\n      schema: dev_sung\n      path: 'md:jaffle_shop'\n      threads: 16\n      extensions:\n        - httpfs\n      settings:\n        s3_region: \"{{ env_var('S3_REGION', 'us-east-1') }}\" # default region to make hello_public_s3.sql work correctly!\n        s3_access_key_id: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n        s3_secret_access_key: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n\n    prod:\n      type: duckdb\n      schema: prod_sung\n      path: 'md:jaffle_shop'\n      threads: 16\n      extensions:\n        - httpfs\n      settings:\n        s3_region: us-west-1\n        s3_access_key_id: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n        s3_secret_access_key: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n```\n\n2. Export your motherduck and S3 credentials to the terminal session, so your dbt project can authenticate to both\n\n```shell\nCopy code\n\n# all examples are fake\nexport motherduck_token=<your motherduck token> # aouiweh98229g193g1rb9u1\nexport S3_REGION=<your region> # us-west-1\nexport S3_ACCESS_KEY_ID=<your access key id> # haoiwehfpoiahpwohf\nexport S3_SECRET_ACCESS_KEY=<your secret access key> # jiaowhefa998333\n```\n\n3. Create a python virtual environment and install the packages to run this dbt project\n\n```shell\nCopy code\n\npython3 -m venv venv\nsource venv/bin/activate\npython3 -m pip install --upgrade pip\npython3 -m pip install -r requirements.txt\n```\n\n4. Run `dbt debug` to verify dbt can connect to motherduck and S3\n\n```shell\nCopy code\n\ndbt debug\n```\n\n5. Run `dbt build` to run and test the project!\n\n```shell\nCopy code\n\ndbt build\n```\n\n6. If you're feeling adventurous, run the below to reference a public s3 bucket provided by MotherDuck!\n\nImpacted dbt model\n\n```sql\nCopy code\n\n--filename: hello_public_s3.sql\n{% if target.name == 'dev_public_s3' %}\n\nSELECT * FROM 's3://us-prd-motherduck-open-datasets/jaffle_shop/csv/raw_customers.csv'\n\n{% else %}\n\nselect 1 as id\n\n{% endif %}\n```\n\n```shell\nCopy code\n\ndbt build --target dev_public_s3\n```\n\n7. Now, you should see everything ran with green font everywhere and you should see this in the UI! Including the S3 data you built a dbt model on top of!\n\n![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgreen_logs_ad3ec33dd1.png%3Fupdated_at%3D2023-09-06T12%3A58%3A39.718Z&w=3840&q=75)![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmotherduck_success_5f718670be.png%3Fupdated_at%3D2023-09-06T12%3A58%3A42.460Z&w=3840&q=75)\n\nThat\u2019s it! Ruffle up those feathers and start quacking and slapping those juicy SQL queries together to solve your analytics problems faster and cheaper than ever before!\n\n## Conclusion\n\nWe\u2019re at a really cool place where all I had to give you was a couple instructions to get you up and running with MotherDuck. I really hope the data industry gets to a place where we brag about the things we do NOT have to do vs. pride ourselves on complexity for its own sake. What matters is that we solve problems and spend time, money, and energy doing it where it\u2019s actually worth it to solve those problems. I\u2019m excited to see you all build MotherDuck guides far superior to mine. That\u2019s why this is so fun. We get to sharpen each other!\n\n_Want to know more about MotherDuck and dbt ? Checkout [MotherDuck & dbt documentation](https://motherduck.com/docs/integrations/transformation/dbt/) and have a look at their YouTube tutorial about DuckDB & dbt \ud83d\udc47_\n\nUnleashing DuckDB & dbt for local analytics triumphs - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Unleashing DuckDB & dbt for local analytics triumphs](https://www.youtube.com/watch?v=asxGh2TrNyI)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nFull screen is unavailable. [Learn More](https://support.google.com/youtube/answer/6276924)\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nMore videos\n\n## More videos\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=asxGh2TrNyI&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 8:54\n\n\u2022Live\n\n\u2022\n\n### TABLE OF CONTENTS\n\n[My Personal DuckDB Story](https://motherduck.com/blog/motherduck-duckdb-dbt/#my-personal-duckdb-story)\n\n[Why use MotherDuck + dbt?](https://motherduck.com/blog/motherduck-duckdb-dbt/#why-use-motherduck-dbt)\n\n[Get Started](https://motherduck.com/blog/motherduck-duckdb-dbt/#get-started)\n\n[Configure Your dbt project](https://motherduck.com/blog/motherduck-duckdb-dbt/#configure-your-dbt-project)\n\n[Conclusion](https://motherduck.com/blog/motherduck-duckdb-dbt/#conclusion)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![This Month in the DuckDB Ecosystem: August 2023](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fthumbnail_duckdb_newsletter_1_feb51165aa.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-nine/)\n\n[2023/08/21 - Marcos Ortiz](https://motherduck.com/blog/duckdb-ecosystem-newsletter-nine/)\n\n### [This Month in the DuckDB Ecosystem: August 2023](https://motherduck.com/blog/duckdb-ecosystem-newsletter-nine)\n\nThis Month in the DuckDB Ecosystem: August 2023\n\n[![DuckDB Tutorial For Beginners](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgetting_started_duckdb_thumbnail_70b197b1ab.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n[2024/10/31 - Mehdi Ouazza, Ryan Boyd](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n### [DuckDB Tutorial For Beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners)\n\nGet up to speed quickly with DuckDB, including installation, VSCode workflow integration and your first SQL analytics project.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 314,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 314
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/glossary/data build tool (dbt).md",
  [2m  â”‚[0m     "content": "---\ntitle: data build tool (dbt)\ncontent_type: event\nsource_url: https://motherduck.com/glossary/data build tool (dbt)\nindexed_at: '2025-11-25T20:02:28.271062'\ncontent_hash: 02a47aefb763154f\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n# data build tool (dbt)\n\n_[Back to DuckDB Data Engineering Glossary](https://motherduck.com/glossary/)_\n\n[dbt](https://www.getdbt.com/) is an open-source command-line tool that enables data analysts and engineers to transform data in their warehouses more effectively. It allows users to write modular SQL queries, which can be version-controlled and tested, bringing software engineering best practices to the world of data analytics. dbt works by compiling these SQL models into executable queries and running them against your data warehouse. It supports a wide range of data platforms, including Snowflake, BigQuery, and Redshift. With dbt, you can define dependencies between models, document your data transformations, and even generate data lineage diagrams. This tool is particularly useful for implementing the ELT (Extract, Load, Transform) paradigm, where transformations occur within the data warehouse itself. By using dbt, data teams can collaborate more efficiently, maintain cleaner code, and build more reliable data pipelines.\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 21,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 21
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     "content": "---\ntitle: dual-execution-dbt\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/dual-execution-dbt\nindexed_at: '2025-11-25T19:57:41.215232'\ncontent_hash: 2b6c5ae395798479\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Local dev and cloud prod for faster dbt development\n\n2025/01/16 - 8 min read\n\nBY\n\n[Jacob Matson](https://motherduck.com/authors/jacob-matson/)\n\n## Introducktion\n\nI hate waiting for slow pipelines to run, so I am delighted to share some strategies to iterate on your data problems at maximum speed - MotherDuck even gave a talk on this concept at [dbt Coalesce in 2024](https://www.youtube.com/watch?v=oqwIHvSfOVQ). By harnessing the capabilities of DuckDB locally, backed by MotherDuck in the cloud, we can unlock an incredibly fast and efficient development cycle. We'll explore how to configure your dbt profile for dual execution and share some tips on how much data to bring local. By implementing these techniques, you can significantly accelerate your data pipeline development and iterate even faster to solve business problems.\n\nLooking to following along in the code?\n\nCheck out the [example repo](https://github.com/motherduckdb/motherduck-examples/tree/main/dbt-dual-execution)!\n\n![Instant feedback loop](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FInstant_feedback_loop_b99078a679.png&w=3840&q=75)\n\n## Setting up your Profile\n\nIn order to take advantage of these capabilities, we need to configure our dbt profile to execute in the correct place, as well as define the behavior that we want in our sources. In the example dbt profile below, `prod` runs entirely in the cloud, while `local` runs mostly on local but is also linked to MotherDuck for reading data into your local database.\n\n```yml\nCopy code\n\ndual_execution:\n  outputs:\n    local:\n      type: duckdb\n      path: local.db\n      attach:\n        - path: \"md:\"\t# attaches all MotherDuck databases\n    prod:\n      type: duckdb\n      path: \"md:jdw\"\n  target: local\n```\n\n## Sources & Models\n\nWith your sources, you need to define which ones to replicate entirely, which ones are ok as views, and which ones to sample. Keep in mind for sampling, you need to think about your data model and make sure that related samples are hydrated (i.e. if you only bring in 100 customers, you need to make sure you also bring in their orders too).\n\nIn my example project using TPC-DS as the source data, I am sampling 1% of the data when running locally on the large tables. In general, I am aiming to keep the datasets less than a million rows per table, although there is no hard limit. For the remaining tables, I am replicating the entire data set locally since they are so small.\n\nThe way that we conditionally sample our models is by using the [\u2018target\u2019 variable](https://docs.getdbt.com/reference/dbt-jinja-functions/target). You can add this parameter by checking your `target` and running it conditionally on your model.\n\nAn example sql snippet is below (using jinja).\n\n```sql\nCopy code\n\nfrom {{ source(\"tpc-ds\", \"catalog_sales\") }}\n{% if target.name == 'local' %} using sample 1 % {% endif %}\n```\n\nAs an example of a simple \u201ccreate local table from cloud\u201d, consider the following query plan. The \u201cL\u201d indicates Local and the \u201cR\u201d indicates Remote (i.e. MotherDuck).\n\n```bash\nCopy code\n\n\ud83e\udd86 explain create table\n        \"local\".\"main\".\"call_center\"\n      as (\n        from \"jdw_dev\".\"jdw_tpcds\".\"call_center\"\n      );\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502\u2502       Physical Plan       \u2502\u2502\n\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 BATCH_CREATE_TABLE_AS (L) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    DOWNLOAD_SOURCE (L)    \u2502\n\u2502    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n\u2502        bridge_id: 1       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  BATCH_DOWNLOAD_SINK (R)  \u2502\n\u2502    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n\u2502        bridge_id: 1       \u2502\n\u2502       parallel: true      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       SEQ_SCAN  (R)       \u2502\n\u2502    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n\u2502        call_center        \u2502\n\u2502                           \u2502\n\u2502        Projections:       \u2502\n\u2502     cc_call_center_sk     \u2502\n\u2502     cc_call_center_id     \u2502\n\u2502     cc_rec_start_date     \u2502\n\u2502      cc_rec_end_date      \u2502\n\u2502     cc_closed_date_sk     \u2502\n\u2502      cc_open_date_sk      \u2502\n\u2502          cc_name          \u2502\n\u2502          cc_class         \u2502\n\u2502        cc_employees       \u2502\n\u2502          cc_sq_ft         \u2502\n\u2502          cc_hours         \u2502\n\u2502         cc_manager        \u2502\n\u2502         cc_mkt_id         \u2502\n\u2502        cc_mkt_class       \u2502\n\u2502        cc_mkt_desc        \u2502\n\u2502     cc_market_manager     \u2502\n\u2502        cc_division        \u2502\n\u2502      cc_division_name     \u2502\n\u2502         cc_company        \u2502\n\u2502      cc_company_name      \u2502\n\u2502      cc_street_number     \u2502\n\u2502       cc_street_name      \u2502\n\u2502       cc_street_type      \u2502\n\u2502      cc_suite_number      \u2502\n\u2502          cc_city          \u2502\n\u2502         cc_county         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nThis can also be extended to your `sources.yml` if necessary for testing local datasets (i.e. json or parquet on experimental pipelines that have not yet made it to your data lake). Configuring these is similar:\n\n```ini\nCopy code\n\n{%if- target.name == 'local' -%}\n   meta:\n      external_location:\n        data/tpcds/{name}.parquet\n{%- endif -%}\n```\n\n## Running your pipeline\n\nOnce you have this configuration in place, you can simply run your pipeline as normal, although for ease of use, you may want to add tags to the models that you are working on so you can avoid going back to the cloud data set too often. This can be set simply in the `dbt_project.yml` like this:\n\n```yml\nCopy code\n\nmodels:\n  dual_execution:\n    tpcds:\n      raw:\n        +tags: ['raw']\n        +materialized: table\n      queries:\n        +materialized: view\n        +tags: ['queries']\n```\n\nFrom there, it is as simple as running `dbt build -s tag:raw` to load your raw data and then for subsequent query iteration, run `dbt build -s tag:queries` in the CLI. The subsequent runs can be visualized like this:\n\n![data flow cloud to local](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdata_flow_cloud_to_local_792a293673.png&w=3840&q=75)\n\n## Shipping dev to the cloud\n\nCertain tables may need to be available in your cloud data warehouse for testing even in the local workflow. This may be something like a BI tool, that is connected to your cloud instance and is difficult to run locally. This can be accomplished by setting the database attribute in your model, so that after the model is run, it is available in the cloud as well.\n\n```yml\nCopy code\n\n{{ config(\n    database=\"jdw_dev\",\n    schema=\"local_to_prod\"\n    materialized=\"table\"\n) }}\n```\n\nIt should be noted that this is a static configuration that is best used for testing. If you don\u2019t want to manually flip models between dev / prod destinations, you can define the database as an attribute of a specific model in your `dbt_project.yml` file.\n\n## Wrapping up\n\nAs you can see from this example, using MotherDuck\u2019s dual execution allows us to leverage the unique value proposition of DuckDB to run an accelerated development cycle on your local machine. With some basic optimization, we can get ~5x faster dbt runs by making the data smaller and using local compute. This is a very powerful combination for rapidly iterating on your pipeline and then pushing a high quality change back into your production environment.\n\nWant to learn more? Join our webinar about Local Dev & Cloud Prod on [February 13th, 2025](https://lu.ma/0die8ual?utm_source=blog).\n\n### TABLE OF CONTENTS\n\n[Introducktion](https://motherduck.com/blog/dual-execution-dbt/#introducktion)\n\n[Setting up your Profile](https://motherduck.com/blog/dual-execution-dbt/#setting-up-your-profile)\n\n[Sources & Models](https://motherduck.com/blog/dual-execution-dbt/#sources-models)\n\n[Running your pipeline](https://motherduck.com/blog/dual-execution-dbt/#running-your-pipeline)\n\n[Shipping dev to the cloud](https://motherduck.com/blog/dual-execution-dbt/#shipping-dev-to-the-cloud)\n\n[Wrapping up](https://motherduck.com/blog/dual-execution-dbt/#wrapping-up)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![What\u2019s New: Streamlined User Management, Metadata, and UI Enhancements](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FMother_Duck_Feature_Roundup_2_47f5d902c0.png&w=3840&q=75)](https://motherduck.com/blog/data-warehouse-feature-roundup-dec-2024/)\n\n[2024/12/21 - Sheila Sitaram](https://motherduck.com/blog/data-warehouse-feature-roundup-dec-2024/)\n\n### [What\u2019s New: Streamlined User Management, Metadata, and UI Enhancements](https://motherduck.com/blog/data-warehouse-feature-roundup-dec-2024)\n\nDecember\u2019s feature roundup is focused on improving the user experience on multiple fronts. Introducing the User Management REST API, the Table Summary, and a read-only MD\\_INFORMATION\\_SCHEMA for metadata.\n\n[![DuckDB Ecosystem: January 2025](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fnewsletter_a65cff5430.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025/)\n\n[2025/01/10 - Simon Sp\u00e4ti](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025/)\n\n### [DuckDB Ecosystem: January 2025](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025)\n\nDuckDB Monthly #25: PyIceberg, 0$ data distribution and more!\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 245,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 245
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me read one more relevant file - the dbt-duckdb end-to-end project guide which likely has more setup details:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     "content": "---\ntitle: duckdb-dbt-e2e-data-engineering-project-part-2\ncontent_type: blog\nsource_url: https://motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2\nindexed_at: '2025-11-25T19:58:03.158001'\ncontent_hash: df5e80b6258156d7\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# DuckDB & dbt \\| End-To-End Data Engineering Project (2/3)\n\n2024/03/22 - 10 min read\n\nBY\n\n[Mehdi Ouazza](https://motherduck.com/authors/mehdi-ouazza/)\n\ndbt is a great and straightforward tool for building production-ready data pipelines with SQL. It acts as a toolkit that assists in packaging, testing, and deploying your SQL pipelines. However, there's a common misconception that dbt itself processes data. In reality, it's a client that sends SQL commands to a cloud data warehouse, where the actual computing occurs. As a result, you always depend on this cloud service, and the development loop can sometimes be challenging.\n\nIn this blog, we'll explore how the development experience can be significantly improved through the use of DuckDB and dbt. We'll learn how to streamline your architecture, accelerate certain pipelines, and finally allow you to write genuine unit tests. We'll also cover some best practices for AWS S3 authentication and managing incremental pipelines.\n\nAll the source code is available on [GitHub](https://github.com/mehd-io/pypi-duck-flow). And for those who prefer watching over reading, I've got a video for you.\n\nDuckDB & dbt \\| End-To-End Data Engineering Project (2/3) - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[DuckDB & dbt \\| End-To-End Data Engineering Project (2/3)](https://www.youtube.com/watch?v=SpfEQQXBGMQ)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nFull screen is unavailable. [Learn More](https://support.google.com/youtube/answer/6276924)\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=SpfEQQXBGMQ&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 37:25\n\n\u2022Live\n\n\u2022\n\n## Quick recap on part 1 : ingestion\n\n[In the first part of our end-to-end data engineering project](https://motherduck.com/blog/duckdb-python-e2e-data-engineering-project-part-1/), we gathered data from PyPi to obtain download statistics for a specific Python library, DuckDB, using Python. In this second part, we'll transform this raw data using dbt and DuckDB to prepare a dataset ready for data visualization, which will be the focus of part three in this series.\n\nDon't worry if you haven't completed the first part of the project; we've got you covered. We have some sample raw data available in a public AWS S3 bucket that you can use as input for the transformation pipeline.\n\n## dbt & DuckDB Integration\n\nIn dbt, we connect to various databases through [adapters](https://docs.getdbt.com/reference/dbt-jinja-functions/adapter), which are defined in a YAML file. These adapters make it easy to switch quickly between different environments. Typically, your Python process (dbt) would send the query to the target database.\n\nHowever, since DuckDB is an embedded database and just another Python library to install (without any cloud dependency), we can run the computation within the same Python dbt process!\n\nIn this dbt project, we'll look at two setups (aka targets):\n\n1. Reading and writing from S3 when using dbt and DuckDB locally (our \"dev\" setup)\n2. Reading from S3 and pushing the result back to MotherDuck (our \"prod\" setup)\n\n![arch](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdbt_duckdb_md_excalidraw_11600df402.png&w=3840&q=75)\n\nSince MotherDuck is DuckDB in the cloud, you benefit from a seamless transition from working locally to scaling in the cloud. Moreover, for part 3, as we aim to create a dashboard with a BI tool, which mostly relies on a SQL engine to fetch data, MotherDuck will prove to be very useful.\n\nLet's dive into the code.\n\n## Building the SQL pipeline\n\n### Setup\n\nOur initial repository has a monolithic structure with the first part of the series located under `/ingestion`. We'll create a new folder under `/transform` for the code discussed in this blog.\n\nFirst off, we need to add the dbt package dependency. As of now, MotherDuck supports only one version of DuckDB. We're using Poetry as our package manager, so to install dbt and the appropriate DuckDB version, simply execute:\n\n```csharp\nCopy code\n\npoetry add dbt-duckdb[md]\n```\n\nNext, initiate the dbt repository under `./transform` with:\n\n```csharp\nCopy code\n\ndbt init pypi_metrics\n```\n\nYou should now see a structure with some folders pre-created for you:\n\n```csharp\nCopy code\n\n.\n\u251c\u2500\u2500 analyses\n\u251c\u2500\u2500 dbt_project.yml\n\u251c\u2500\u2500 macros\n\u251c\u2500\u2500 models\n\u251c\u2500\u2500 package-lock.yml\n\u251c\u2500\u2500 packages.yml\n\u251c\u2500\u2500 profiles.yml\n\u251c\u2500\u2500 seeds\n\u251c\u2500\u2500 snapshots\n\u251c\u2500\u2500 target\n\u2514\u2500\u2500 tests\n```\n\n### Exploring the Data and Building the Model\n\nTo start, I want to explore the raw data. You can access a free public sample here: `s3://us-prd-motherduck-open-datasets/pypi/sample_tutorial/pypi_file_downloads/*/*/*.parquet`\n\nA straightforward way to begin is by using the DuckDB CLI. You can [find the installation steps online](https://duckdb.org/docs/installation/). A useful setup I recommend -if you are using VSCode- is opening a terminal in VSCode and configuring a shortcut to send commands from the editor to the terminal (the opened DuckDB CLI).\nI assigned the `cmd+k` shortcut to this specific command in my JSON Keyboard Shortcuts settings.\n\n```json\nCopy code\n\n  {\n    \"key\": \"cmd+k\",\n    \"command\": \"workbench.action.terminal.runSelectedText\"\n  },\n```\n\nThat way, you are building your SQL query directly at the right place, in a SQL file\n![](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fim1_89602c1b13.png&w=3840&q=75)\n\nAs you can see on the above screenshot, you can easily describe a remote parquet file using :\n\n```sql\nCopy code\n\nDESCRIBE TABLE 's3://us-prd-motherduck-open-datasets/pypi/sample_tutorial/pypi_file_downloads/*/*/*.parquet';\n```\n\n![](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2024_03_22_at_16_11_36_0b37ea8ccb.png&w=3840&q=75)\n\nThis data shows each row as a download of a specific Python project, already filtered for the `duckdb` project.\n\nOur transformations should include:\n\n- Selecting only relevant columns and unnesting as necessary.\n- Converting the Python version to include only minor versions (e.g., 3.9.1 -> 3.9) for more meaningful aggregation.\n- Aggregating the download count per day to streamline our insights.\n- Adding a `load_id` (based on a hash) for incremental loading.\n\nThe final model is as follows:\n\n```vbnet\nCopy code\n\nWITH pre_aggregated_data AS (\n    SELECT\n        timestamp :: date as download_date,\n        details.system.name AS system_name,\n        details.system.release AS system_release,\n        file.version AS version,\n        project,\n        country_code,\n        details.cpu,\n        CASE\n            WHEN details.python IS NULL THEN NULL\n            ELSE CONCAT(\n                SPLIT_PART(details.python, '.', 1),\n                '.',\n                SPLIT_PART(details.python, '.', 2)\n            )\n        END AS python_version\n    FROM\n          {{ dbt_unit_testing.source('external_source', 'pypi_file_downloads') }}\n    WHERE\n        download_date >= '{{ var(\"start_date\") }}'\n        AND download_date < '{{ var(\"end_date\") }}'\n)\n\nSELECT\n    MD5(CONCAT_WS('|', download_date, system_name, system_release, version, project, country_code, cpu, python_version)) AS load_id,\n    download_date,\n    system_name,\n    system_release,\n    version,\n    project,\n    country_code,\n    cpu,\n    python_version,\n    COUNT(*) AS daily_download_sum\nFROM\n    pre_aggregated_data\nGROUP BY\n    ALL\n```\n\nNotable points include:\n\n- Filtering is always done between a dbt variable including `start_date` and `end_date` for easy data reprocessing.\n- The source table is abstracted with `{{ dbt_unit_testing.source('external_source', 'pypi_file_downloads') }}` for unit testing purposes (more on that further in the blog).\n\nBefore we get to unit testing, let's review our configuration files, mainly `sources.yml` and `dbt_project.yml` and `profiles.yml`.\n\n### YAML configurations files\n\nSources are defined in `sources.yml` in `/transform/pypi_metrics/models/sources.yml`\n\n```yaml\nCopy code\n\nversion: 2\n\nsources:\n  - name: external_source\n    meta:\n      external_location: \"{{ env_var('TRANSFORM_S3_PATH_INPUT') }}\"\n    tables:\n      - name: pypi_file_downloads\n```\n\nWe're using an external location (AWS S3) with a nickname that we referred to in our model's `FROM` statement earlier.\n\nWe've also made the S3 path flexible so it can be provided through environment variables.\n\nTo manage these environment variables smoothly, we use a `Makefile` along with a `.env` file. At the beginning of the `Makefile`, you'll see:\n\n```bash\nCopy code\n\ninclude .env\nexport\n```\n\nIn the code repository, there's an `env.template` file. You can copy this to create a `.env` file and enter the necessary values.\n\nNext, we initiate the dbt run through an entry in the Makefile named `pypi-transform`:\n\n```makefile\nCopy code\n\npypi-transform:\n\tcd $$DBT_FOLDER && \\\n\tdbt run \\\n\t\t--target $$DBT_TARGET \\\n\t\t--vars '{\"start_date\": \"$(START_DATE)\", \"end_date\": \"$(END_DATE)\"}'\n```\n\nLet's have a look now on our `dbt_project.yml`\n\n```ini\nCopy code\n\nmodels:\n  pypi_metrics:\n    pypi_daily_stats:\n      +materialized: \"{{ 'incremental' if target.name == 'prod' else 'table' }}\"\n      +unique_key: load_id\n      +pre-hook: \"{% if target.name == 'dev' %}CALL load_aws_credentials(){% endif %}\"\n      +post-hook: \"{% if target.name == 'dev' %}{{ export_partition_data('download_date', this.name ) }}{% endif %}\"\n```\n\nAs mentioned before, we have two setups: one for local running and read/writing to AWS S3, and another using MotherDuck, designated as `dev` and `prod` targets, respectively.\n\nThese settings are outlined in our `profiles.yml`:\n\n```yaml\nCopy code\n\npypi_metrics:\n  outputs:\n    dev:\n      type: duckdb\n      path: /tmp/dbt.duckdb\n    prod:\n      type: duckdb\n      path: \"md:\"\n  target: dev\n```\n\nThe only difference between running locally and using MotherDuck is the `path` setting. Using `md:` triggers authentication with MotherDuck, which checks for a token in the `motherduck_token` environment variable. You can get this token from your MotherDuck account settings page.\n\nWe face a few challenges:\n\n- dbt doesn't support incremental loading when writing to an external source like AWS S3.\n- We need to authenticate with AWS S3.\n\nThankfully, DuckDB offers extensions that simplify authentication and read/write operations to AWS S3. To address the first challenge, we write to AWS S3 with partitions, allowing us to process within a specific time frame and overwrite any existing partitions.\n\nWe use a simple macro, `export_partition_data.sql`, for this:\n\n```sql\nCopy code\n\n{% macro export_partition_data(date_column, table) %}\n{% set s3_path = env_var('TRANSFORM_S3_PATH_OUTPUT', 'my-bucket-path') %}\n    COPY (\n        SELECT *,\n            YEAR({{ date_column }}) AS year,\n            MONTH({{ date_column }}) AS month\n        FROM {{ table }}\n    )\n    TO '{{ s3_path }}/{{ table }}'\n     (FORMAT PARQUET, PARTITION_BY (year, month), OVERWRITE_OR_IGNORE 1, COMPRESSION 'ZSTD', ROW_GROUP_SIZE 1000000);\n{% endmacro %}\n```\n\nWith dbt running DuckDB, it creates an internal table from the model, allowing us to easily export this data to any format and remote storage (AWS S3/GCP Cloud storage) using the `COPY` command.\n\nNotable points include:\n\n- The AWS S3 path is set as an environment variable.\n- We use a date column for partition generation. For instance, our data will be stored as `s3://my-bucket/my_data/year=2024/month=04`.\n\nFor authentication, we use another extension and invoke `CALL load_aws_credentials()` as a pre-hook in the `dbt_project.yml`, looking for the default profile under `~/.aws`.\n\nWith all configurations set for different environments, let's dive into unit testing.\n\n## Unit Testing the Model\n\nDuckDB operates in-process, allowing us to iterate quickly on our model since computation occurs locally within the same dbt process. dbt is improving unit tests in its April `1.8` release, but currently, it's challenging to run tests without cloud dependencies. While you could install Postgres locally, it's an additional step.\n\nFor unit testing, we use the `dbt-unit-testing` dbt package, added to a `packages.yml` file at the root of your dbt directory:\n\n```yaml\nCopy code\n\npackages:\n  - git: \"https://github.com/EqualExperts/dbt-unit-testing\"\n    revision: v0.4.12\n```\n\nFirst, install the package by running `dbt deps`. This step allows us to use SQL for defining our mock data, both the input and the expected outcome, and then run the model using `dbt-duckdb` right on our local machine.\n\nNext, dive into the tests folder and craft a new SQL file named `test_pypi_daily_stats.sql`:\n\n```sql\nCopy code\n\n{{ config(tags=['unit-test']) }}\n\n{% call dbt_unit_testing.test ('pypi_daily_stats','check_duckdb_downloads_on_20230402') %}\n\n  {% call dbt_unit_testing.mock_source('external_source', 'pypi_file_downloads') %}\n    SELECT\n      '2023-04-02 14:49:15+02'::timestamp AS timestamp,\n      'US' AS country_code,\n      '/packages/38/5b/...' AS url,\n      'duckdb' AS project,\n      NULL AS file, -- Assuming the 'file' struct is not essential for this test\n      STRUCT_PACK(\n          installer := NULL,\n          python := '3.8.2',\n          implementation := NULL,\n          distro := NULL,\n          system := STRUCT_PACK(name := 'Linux', release := '4.15.0-66-generic'),\n          cpu := 'x86_64',\n          openssl_version := NULL,\n          setuptools_version := NULL,\n          rustc_version := NULL\n      ) AS details,\n      'TLSv1.2' AS tls_protocol,\n      'ECDHE-RSA-AES128-GCM-SHA256' AS tls_cipher\n    UNION ALL\n    SELECT\n      '2023-04-02 14:49:15+02'::timestamp AS timestamp,\n      'US' AS country_code,\n      '/packages/38/5b/...' AS url,\n      'duckdb' AS project,\n      NULL AS file, -- Assuming the 'file' struct is not essential for this test\n      STRUCT_PACK(\n          installer := NULL,\n          python := '3.9.1',\n          implementation := NULL,\n          distro := NULL,\n          system := STRUCT_PACK(name := 'Linux', release := '4.15.0-66-generic'),\n          cpu := 'x86_64',\n          openssl_version := NULL,\n          setuptools_version := NULL,\n          rustc_version := NULL\n      ) AS details,\n      'TLSv1.2' AS tls_protocol,\n      'ECDHE-RSA-AES128-GCM-SHA256' AS tls_cipher\n    -- Add more rows as needed for your test\n  {% endcall %}\n\n{% call dbt_unit_testing.expect() %}\n    SELECT\n      '2023-04-02'::date AS download_date,\n      'duckdb' AS project,\n      '3.8' AS python_version,\n      'x86_64' AS cpu,\n      'Linux' AS system_name,\n      2 AS daily_download_sum -- Adjust this based on the expected outcome of your test\n  {% endcall %}\n\n{% endcall %}\n```\n\nThis test is structured in three key parts:\n\n1. Specifying which model we're testing with `{% call dbt_unit_testing.test('pypi_daily_stats', 'check_duckdb_downloads_on_20230402') %}`.\n2. Creating mock source data using `{% call dbt_unit_testing.mock_source('external_source', 'pypi_file_downloads') %}`, which uses SQL to simulate the data. This method allows for the easy definition of complex data structures, perfect for working with DuckDB.\n3. Defining the expected results with `{% call dbt_unit_testing.expect() %}` to verify our model's output.\n\nRun the test by executing:\n\n```bash\nCopy code\n\ndbt test\n```\n\nOr, use the Makefile shortcut `make pypi-transform-test` to initiate testing directly from the project's root folder.\n![](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2024_03_22_at_13_27_00_abea94f8c1.png&w=3840&q=75)\n\nThe testing process is swift, typically taking less than two seconds!\n\n## A New Developer Experience\n\nThis blog has highlighted the dbt-duckdb adapter's contributions, showcasing it as more than a new dbt destination. It introduces a revitalized developer experience, enabling local prototyping, cloud-independent unit testing, and smooth transitions to cloud deployments with MotherDuck.\nUp next in this series, we'll breathe life into our PyPi dataset by creating a dashboard.\n\nIn the meantimes, keep quacking and keep coding.\n\n### TABLE OF CONTENTS\n\n[Quick recap on part 1 : ingestion](https://motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2/#quick-recap-on-part-1-ingestion)\n\n[dbt & DuckDB Integration](https://motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2/#dbt-duckdb-integration)\n\n[Building the SQL pipeline](https://motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2/#building-the-sql-pipeline)\n\n[Unit Testing the Model](https://motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2/#unit-testing-the-model)\n\n[A New Developer Experience](https://motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2/#a-new-developer-experience)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![Differential Storage: A Key Building Block For A DuckDB-Based Data Warehouse](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdiff_storage_121958b7eb.png&w=3840&q=75)](https://motherduck.com/blog/differential-storage-building-block-for-data-warehouse/)\n\n[2024/03/11 - Joseph Hwang](https://motherduck.com/blog/differential-storage-building-block-for-data-warehouse/)\n\n### [Differential Storage: A Key Building Block For A DuckDB-Based Data Warehouse](https://motherduck.com/blog/differential-storage-building-block-for-data-warehouse)\n\nDifferential Storage: A Key Building Block For A DuckDB-Based Data Warehouse\n\n[![DuckDB Tutorial For Beginners](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgetting_started_duckdb_thumbnail_70b197b1ab.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n[2024/10/31 - Mehdi Ouazza, Ryan Boyd](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n### [DuckDB Tutorial For Beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners)\n\nGet up to speed quickly with DuckDB, including installation, VSCode workflow integration and your first SQL analytics project.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 513,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 513
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now I have gathered comprehensive information from the sources. Let me write the answer:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_5.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "create",
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_5.md",
  [2m  â”‚[0m   "content": "<3729 chars>",
  [2m  â”‚[0m   "structuredPatch": [],
  [2m  â”‚[0m   "originalFile": null
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I've written a comprehensive answer to `/tmp/answer_without_kg_5.md` based on 5 source files from the `.kurt/sources/` directory. The answer covers:
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. **Installing the dbt-duckdb adapter** with MotherDuck support
  [94mâ”‚[0m 2. **Getting and configuring the MotherDuck service token**
  [94mâ”‚[0m 3. **Multiple profiles.yml configurations** including:
  [94mâ”‚[0m    - Basic MotherDuck connection
  [94mâ”‚[0m    - Local dev + cloud prod (dual execution) setup
  [94mâ”‚[0m    - S3 integration setup
  [94mâ”‚[0m 4. **Testing the connection** with dbt debug and dbt build
  [94mâ”‚[0m 5. **Key benefits** of using MotherDuck with dbt
  [94mâ”‚[0m 
  [94mâ”‚[0m All sources are properly cited with their exact local file paths in the Sources section.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 3,297 | Cost: $0.3565
     Cumulative: 3,297 tokens | $0.3565
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent has clearly signaled task completion by summarizing the information and stating that it has written a comprehensive answer, without asking for any user input.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 8 | Duration: 54.5s
â•‘    Tokens: 3,297 | Cost: $0.3565
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  Running LLM judge evaluation...
   ğŸ§  LLM Judge score: 0.00
