{
  "question": "How do I migrate data from a local DuckDB database to MotherDuck?",
  "answer": "# How to Migrate Data from a Local DuckDB Database to MotherDuck\n\nThere are several methods to migrate data from a local DuckDB database to MotherDuck, ranging from simple one-command uploads to more granular table-by-table approaches.\n\n## Method 1: Upload Entire Local Database (Simplest)\n\nThe easiest way to migrate your local DuckDB database to MotherDuck is using a single command:\n\n```sql\nCREATE DATABASE remote_database_name FROM CURRENT_DATABASE();\n\n-- or more generally\nCREATE DATABASE remote_database_name FROM '<local database name>';\n```\n\n**Important considerations:**\n- The local and remote database names **must be different**, otherwise you'll get an error: `Catalog Error: error while importing share: Schema with name <local-database-name> does not exist!`\n- For large databases, this can take considerable time (e.g., ~1 hour for an 11GB database, transferring 15GB of data)\n- This method uploads all tables and data from your local database to MotherDuck\n\n## Method 2: Connect and Attach Local Database\n\nFirst, connect to MotherDuck by attaching it to your local DuckDB session:\n\n```sql\n-- From DuckDB CLI\n.open md:\n\n-- Or attach MotherDuck\nATTACH 'md:';\n```\n\nWhen you first connect, you'll be prompted to authenticate via SSO. The system will provide a motherduck_token that you should save as an environment variable:\n\n```bash\nexport motherduck_token='eyJhbGciOiJI..._Jfo'\n```\n\n## Method 3: Create Database and Copy Tables from Files\n\nIf you have your data exported to files (like Parquet or CSV), you can create tables directly in MotherDuck from those files:\n\n```sql\n-- Create the database in MotherDuck\nCREATE DATABASE so;\n\n-- Create tables from Parquet files on S3\nCREATE TABLE users AS\nFROM 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/users.parquet';\n\nCREATE TABLE posts AS\nFROM 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/posts.parquet';\n\n-- Or from local files\nCREATE TABLE population AS\nFROM 'local_file.parquet';\n\n-- Repeat for other tables\n```\n\nThis approach works well when:\n- You have data already exported to cloud storage (S3, GCS, etc.)\n- You want to selectively migrate specific tables\n- You want to transform data during migration\n\n## Method 4: Copy Data Table-by-Table Using ATTACH\n\nYou can also use the standard SQL `CREATE TABLE AS SELECT` (CTAS) pattern to copy tables:\n\n```sql\n-- Attach your local DuckDB database\nATTACH 'my_local_data.duckdb' AS local_db;\n\n-- Connect to MotherDuck\nATTACH 'md:' AS motherduck;\n\n-- Create database in MotherDuck (if needed)\nCREATE DATABASE my_remote_db;\nUSE my_remote_db;\n\n-- Copy tables from local to MotherDuck\nCREATE TABLE population AS SELECT * FROM local_db.population;\nCREATE TABLE sales AS SELECT * FROM local_db.sales;\n```\n\n## Understanding DuckDB's Dual Execution Model\n\nMotherDuck uses a \"dual execution\" model that intelligently decides whether to run queries locally or in the cloud:\n\n- When both tables reside in MotherDuck, queries run entirely in the cloud\n- When joining local files with cloud tables, DuckDB processes data where it makes most sense\n- The system minimizes data movement by transferring only necessary intermediate data\n- You can manually control execution location using the `md_run` parameter:\n\n```sql\n-- Force local execution\nSELECT * FROM read_parquet('s3://bucket/file.parquet', md_run='local');\n\n-- Force remote execution in MotherDuck\nSELECT * FROM read_parquet('s3://bucket/file.parquet', md_run='remote');\n```\n\n**Performance example:** Querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to local execution that had to download the data first (36 seconds).\n\n## Data Persistence Options\n\nDuckDB offers two modes:\n1. **In-memory** (default): Data disappears when session ends - use `duckdb.connect(database=':memory:')`\n2. **Persistent**: Data saved to a database file\n\nTo create a persistent local database before uploading:\n\n```bash\n# Start DuckDB with a database file\nduckdb my_local_data.duckdb\n\n# Or from Python\nimport duckdb\ncon = duckdb.connect(database='my_local_data.duckdb')\n```\n\nOr attach an existing database if DuckDB is already running:\n\n```sql\nATTACH DATABASE '/path/to/your/database.db' AS mydb;\n```\n\n## Python Example\n\nHere's a complete Python example for migrating data using the relational API:\n\n```python\nimport duckdb\n\n# Connect to persistent local database\ncon = duckdb.connect(database='my_local_data.duckdb')\n\n# Load data into local database\npopulation_relation = con.read_csv(\"https://bit.ly/3KoiZR0\")\npopulation_relation.to_table(\"population\")  # Persist as table\n\n# Connect to MotherDuck\ncon.sql(\"ATTACH 'md:my_cloud_db'\")\ncon.sql(\"USE my_cloud_db\")\n\n# Copy table to MotherDuck\ncon.sql(\"CREATE TABLE population AS SELECT * FROM my_local_data.population\")\n```\n\n## Best Practices\n\n1. **For small to medium databases**: Use Method 1 (single command upload) for simplicity\n2. **For large databases**: Consider exporting to Parquet files on cloud storage first, then use Method 3 to leverage MotherDuck's high-bandwidth cloud connections\n3. **For selective migration**: Use Method 4 to copy only specific tables\n4. **Set environment variable**: Always export your `motherduck_token` to avoid repeated authentication\n5. **Test connection first**: Verify you can connect to MotherDuck before starting large migrations\n6. **Monitor progress**: Large uploads can take time; plan accordingly\n7. **Leverage cloud proximity**: MotherDuck's cloud infrastructure provides high-bandwidth connections to cloud storage, making it faster to load from S3/GCS than from local files\n\n## Working with Extensions\n\nIf you need to read from cloud storage, make sure to install and load the necessary extensions:\n\n```sql\n-- Install httpfs extension for S3 access\nINSTALL httpfs;\nLOAD httpfs;\n\n-- Configure for public S3 bucket\nSET s3_region='us-east-1';\n\n-- For private buckets, use CREATE SECRET\nCREATE SECRET (\n    TYPE S3,\n    PROVIDER credential_chain\n);\n```",
  "answer_file": "/tmp/answer_without_kg_6.md",
  "token_usage": {
    "total_tokens": 1485,
    "duration_seconds": 147.1244821548462
  },
  "cached_response": false,
  "llm_judge": {
    "overall_score": 1.0,
    "component_scores": {
      "accuracy": 1.0,
      "completeness": 1.0,
      "relevance": 1.0,
      "clarity": 1.0
    },
    "feedback": "The generated answer is highly accurate, complete, relevant, and clear. It effectively covers the migration process with multiple methods and detailed explanations, which enhances the user's understanding. The clarity of the writing and the structured format make it easy to follow, ensuring that all required topics are addressed thoroughly."
  },
  "command": "question:q6",
  "returncode": 0
}