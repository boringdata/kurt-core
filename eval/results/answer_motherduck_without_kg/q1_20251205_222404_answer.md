# Answer

When loading data into MotherDuck, **Parquet is the most efficient file format**, followed by other columnar formats. Here's what makes different formats efficient or inefficient:

## Most Efficient: Parquet

Parquet is the optimal choice for loading data into MotherDuck due to several key advantages:

1. **Columnar Storage**: Parquet organizes data by columns rather than rows, which enables faster retrieval when analytical queries focus on specific columns. All values for a particular column are stored contiguously on disk, enabling better compression ratios.

2. **Superior Compression**: Parquet uses specialized encoding techniques such as dictionary encoding, run-length encoding, and delta encoding to optimize storage. This reduces the data footprint, translating to cost savings and improved access speeds by minimizing I/O operations.

3. **Query Optimization Features**:
   - **Predicate Pushdown**: Filtering operations are pushed closer to the storage layer, reducing the amount of data that needs processing
   - **Column Pruning**: Only the columns needed for a query are read from disk
   - **Built-in Statistics**: Parquet files contain min/max values per column chunk (row groups) that help DuckDB skip reading parts of files that don't match query predicates

4. **Schema Evolution Support**: Parquet handles evolving data structures with ease, allowing seamless schema modifications like adding or altering columns without disrupting existing workflows.

5. **DuckDB/MotherDuck Optimization**: DuckDB's architecture is purpose-built to leverage Parquet's column-oriented design. The platform can directly query Parquet files without importing them first, and supports efficient SQL-driven exports to Parquet with advanced compression options.

## Other Format Considerations

### CSV Files
- **Pros**: Simple, universal, human-readable, and easy to edit with no dependencies
- **Cons**:
  - Lacks compression and schema management
  - Row-based format (not optimized for analytical queries)
  - No built-in statistics for query optimization
  - Requires more storage space
  - Can have encoding and parsing issues
- **Use Case**: Good for data exchange and initial data loading, but should be converted to Parquet for efficient querying

### Apache ORC
- **Pros**: Columnar storage with efficient compression, ACID transaction support
- **Cons**: Primarily optimized for Hive/Hadoop ecosystems
- **Efficiency**: Similar to Parquet but less universally supported

### Apache Avro
- **Pros**: Flexible row-based format with good schema evolution support
- **Cons**: Row-based design makes it less efficient for analytical queries compared to columnar formats
- **Use Case**: Better for streaming and cross-system data exchange than analytics

### Delta Lake and Apache Iceberg
- **Note**: These are table formats, not file formats. They typically use Parquet as their underlying file format while adding transaction management, versioning, and metadata capabilities.

## Performance Best Practices

When loading data into MotherDuck:

1. **Use Partitioned Writes**: For large datasets, partition Parquet files using `COPY TO ... PARTITION_BY` with appropriate partition keys (e.g., by date) to improve query performance through partition pruning.

2. **Avoid Too Many Small Files**: Aim for partition sizes of at least 100MB each. Thousands of tiny files slow down file listing operations, especially on cloud storage.

3. **Consider Compression**: DuckDB supports compression options like GZIP and ZSTD when writing Parquet files, which can further reduce storage costs.

4. **Direct Parquet Queries**: DuckDB can query Parquet files directly without loading them into a table first, which is faster for exploratory analysis.

5. **DuckLake for Advanced Features**: For petabyte-scale workloads requiring open format compatibility with Spark and other engines, consider DuckLake. However, MotherDuck's native storage offers 2-10x faster query performance for most workloads.

## Storage Trade-offs

**DuckLake (with Parquet)**:
- Open format with broader ecosystem compatibility
- Support for Spark and other compute engines
- Better suited for petabyte-scale workloads
- Supports encryption for secure data lakes

**MotherDuck Native Storage**:
- 2-10x faster query performance than DuckLake
- Optimized for read/write throughput
- Better caching and regional performance
- Best for most analytical workloads

## Practical Loading Examples

**Convert CSV to Parquet for efficient loading:**
```bash
duckdb -c "COPY (SELECT * FROM 'data.csv') TO 'data.parquet';"
```

**Query Parquet files directly:**
```sql
SELECT * FROM 'data.parquet';
```

**Partitioned writes for large datasets:**
```sql
COPY orders TO 'output_orders_parquet' (
  FORMAT PARQUET,
  PARTITION_BY (year, month)
);
```

## Sources

- .kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md - Comprehensive guide on Parquet format benefits including columnar storage, compression techniques, predicate pushdown, column pruning, schema evolution, and DuckDB integration
- .kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md - Tutorial on converting between CSV and Parquet formats using DuckDB's COPY command with practical examples
- .kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md - Discussion on CSV file format characteristics and limitations compared to columnar formats
- .kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md - Details on efficient data loading into DuckLake, performance optimization with MotherDuck, and storage trade-offs between DuckLake and MotherDuck native storage (2-10x performance difference)
- .kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md - In-depth guide on partitioned writes with COPY TO PARTITION_BY, Hive partitioning, compression options, filename patterns, and performance best practices including avoiding too many small files
- .kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md - Real-world example of loading CSV files with DuckDB, comparison with Parquet efficiency, and detailed explanation of why Parquet is superior for analytical workloads
