# Raw Transcript: answer_motherduck_without_kg

**Status**: âŒ FAILED

**Tokens Used**: 11591

```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ SCENARIO: answer_motherduck_without_kg
   Answer MotherDuck questions by searching sources directly (no KG)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ” Verifying .claude installation...
   âœ“ .claude folder exists at /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.claude
   âœ“ Skills: 0 found
   âœ“ Commands: 1 found

ğŸ§ª Running 10 question(s) defined in questions_motherduck.yaml

======================================================================
â“ Question 1/10
...... What file formats are most efficient for loading data into MotherDuck?
======================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 1
â”‚ Answer this question by searching through the .kurt/sources/ directory:

...... What file formats are most efficient for loading data into MotherDuck?

Required Steps:
1. Search for relevant markdown files in .kurt/sources/ using glob and grep
2. Read the most relevant files (aim for 3-5 most relevant sources)
3. As you read each file, keep track of:
   - The file path
   - The file title/name
   - Key information extracted from that file
4. Synthesize a comprehensive answer based on the information found
5. Write your answer to: /tmp/answer_without_kg_1.md

IMPORTANT - Your answer MUST have this structure:
```markdown
# Answer

[Your comprehensive answer here, drawing from the sources you read]

## Sources

List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
[etc.]

DO NOT use URLs or document titles as links. Show the actual file path you read with the Read tool.
```

The Sources section is MANDATORY - you must list every file you read to answer the question.
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I'll search through the .kurt/sources/ directory to find information about efficient file formats for loading data into MotherDuck.
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/**/*.md

  [96mğŸ”§ TOOL:[0m Grep â†’ file format|data format|parquet|csv|json|loading data|import|efficient
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/about-us.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/acceptable-use-policy.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/adithya-krishnan.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/aditya-aomani.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/alex-monahan.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/antony-courtney.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/amrutha-gujjar.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/brett-griffin.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/anna-geller.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/christophe-oudar.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/doug-raymond.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/andrew-mcewen.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/daniel-palma.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/emily-lucek.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/andrew-witten.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/faraz-hameed.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/diptanu-gon-choudhury.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/boaz-leskes.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/manveer-chawla.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/joseph-hwang.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/leslie-thomas.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/garrett-obrien.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/luciano-galvao-filho.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/graham-wetzler.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/jacob-matson.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/hamilton-ulmer.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/jelte-fennema-nio.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/jessica-libman.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/jordan-tigani.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/peter-boncz.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/nick-ursa.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/nouras-haddad.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/mehdi-ouazza.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/marc-d.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/mike-spaulding.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/oli-dinov.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/michael-hunger.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/mike-xu.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/marcos-ortiz.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/thomas-tfmv-mcgeehan.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/simon-spati.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/ryan-boyd.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/stephanie-wang.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/sung-won-chung.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/tino-tereshko.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/sheila-sitaram.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/sahil-gupta.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/sean-m-tracey.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/till-dohmen.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/saqib-rasul.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/pranav-aurora.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/yannick-welsch.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/zhenni-wu.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/yves-le-maout.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/vasilije-markovic.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/analyze-json-data-using-sql.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/analyze-sqlite-databases-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/MotherDuck-Visualize-Embeddings-Marimo.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/analyze-data-in-azure-with-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-duckdb-snippet-sets-with-motherduck-sharing-databases.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/analyze-x-data-nodejs-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-mega-giga-instance-sizes-huge-scale.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-series-seed-and-a.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/bigquery-to-duckdb-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/building-motherduck-partner-ecosystem.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/big-data-is-dead.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-small-data-sf-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/building-data-applications-with-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-engineering-toolkit-infrastructure-devops.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-engineers-answer-10-top-reddit-questions.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-app-generator.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-engineering-toolkit-essential-tools.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-engineer-highlights-PyConDE-2023.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-cognee-sql-analytics-graph-rag.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/differential-storage-building-block-for-data-warehouse.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-dec-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-110-hidden-gems.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/dosomething-motherduck-data-warehouse-ROI.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-oct-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-nov-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2023.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-five.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "durationMs": 11,
  [2m  â”‚[0m   "numFiles": 100,
  [2m  â”‚[0m   "truncated": true
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-if-sql-queries-returned-results-instantly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/spatial-data-management-with-duckdb-ft-mattforrest.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/leveraging-duckdb-and-delta-lake-together.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ parquet|csv|load|import|insert|copy|ingest
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-power-of-wasm-for-analytics-duckdb-in-the-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-surprising-birth-of-duckdb-ft-co-creator-hannes-muhleisen.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-if-sql-queries-returned-results-instantly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/spatial-data-management-with-duckdb-ft-mattforrest.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/speed-querying-stackoverflow-data-with-duckdb-ft-michael-hunger.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/sql-ide-safari-harlequin-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/take-flight-with-dbt-and-duckdb-dropping-dev-warehouse-costs-to-zero.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-bad-the-good-and-the-ugly-navigating-the-new-waves-of-machine-learning-ft-adi-polak.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-future-of-bi-exploring-the-impact-of-bi-as-code-tools-with-duckdb.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 30,
  [2m  â”‚[0m   "appliedLimit": 30
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search more specifically for documentation about data loading and file formats:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ performance|efficient|speed|faster|optimize

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/motherduck.com/docs/**/*load*.md

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/motherduck.com/docs/**/*import*.md

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/motherduck.com/docs/**/*parquet*.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "durationMs": 2,
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "durationMs": 3,
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "durationMs": 3,
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ find .kurt/sources/motherduck.com -type f -name "*.md" | head -20
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m .kurt/sources/motherduck.com/research/execution-guided-sql-generation.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/research/ssdtrain-framework-paper.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/research/accelerating-machine-learning-lifecycle.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/customer-support.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/datalab.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/secoda.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/cube.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/airbyte.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/marimo.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/zenlytic.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/bruin.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/ascend.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/hex.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/OneSignal.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/datawisp.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/Shearwater.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/astronomer.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/streamkap.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/puppygraph.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/codecentric.md
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ ls -la .kurt/sources/motherduck.com/
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m total 1176
  [2m  â”‚[0m drwxr-xr-x   51 julien  staff   1632 Dec  1 08:32 .
  [2m  â”‚[0m drwxr-xr-x    3 julien  staff     96 Dec  1 08:32 ..
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   4118 Nov 26 10:30 about-us.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   2154 Nov 26 10:30 acceptable-use-policy.md
  [2m  â”‚[0m drwxr-xr-x   55 julien  staff   1760 Dec  1 08:32 authors
  [2m  â”‚[0m drwxr-xr-x  178 julien  staff   5696 Dec  1 08:32 blog
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  12059 Nov 26 10:30 blog.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  15483 Nov 26 10:30 candidate-privacy.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  27511 Nov 26 10:30 careers.md
  [2m  â”‚[0m drwxr-xr-x   13 julien  staff    416 Dec  1 08:32 case-studies
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  15955 Nov 26 10:30 case-studies.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  20608 Nov 26 10:30 community-and-open-source.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   1734 Nov 26 10:30 community-code.md
  [2m  â”‚[0m drwxr-xr-x    4 julien  staff    128 Dec  1 08:32 contact-us
  [2m  â”‚[0m -rw-r--r--    1 julien  staff    901 Nov 26 10:30 contact-us.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   2380 Nov 26 10:30 customer-support.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   6132 Nov 26 10:30 duckdb-book-brief.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   4571 Nov 26 10:30 duckdb-book-summary-chapter2.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   4014 Nov 26 10:30 duckdb-book-summary-chapter3.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   4370 Nov 26 10:30 duckdb-book-summary-chapter4.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  11557 Nov 26 10:30 duckdb-news.md
  [2m  â”‚[0m drwxr-xr-x   68 julien  staff   2176 Dec  1 08:32 ecosystem
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  66511 Nov 26 10:30 ecosystem.md
  [2m  â”‚[0m drwxr-xr-x    3 julien  staff     96 Dec  1 08:32 events
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   2525 Nov 26 10:30 events.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   7249 Nov 26 10:30 fees-addendum.md
  [2m  â”‚[0m drwxr-xr-x    3 julien  staff     96 Dec  1 08:32 forms
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   1392 Nov 26 10:30 getting-started-with-motherduck.md
  [2m  â”‚[0m drwxr-xr-x   71 julien  staff   2272 Dec  1 08:32 glossary
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   5549 Nov 26 10:30 hack-night.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  49247 Nov 26 10:30 index.md
  [2m  â”‚[0m drwxr-xr-x   37 julien  staff   1184 Dec  1 08:32 learn-more
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  20455 Nov 26 10:30 learn-more.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   1720 Nov 26 10:30 local-dev-cloud-prod-dagster-motherduck.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff    524 Nov 26 10:30 onboarding-sessions.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  17998 Nov 26 10:30 privacy-policy.md
  [2m  â”‚[0m drwxr-xr-x    8 julien  staff    256 Dec  1 08:32 product
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  26113 Nov 26 10:30 product.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  16979 Nov 26 10:30 quacking-around-the-web.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   4924 Nov 26 10:30 react-components-testing.md
  [2m  â”‚[0m drwxr-xr-x    5 julien  staff    160 Dec  1 08:32 research
  [2m  â”‚[0m -rw-r--r--    1 julien  staff    550 Nov 26 10:30 research.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   2381 Nov 26 10:30 sql-duckdb-book-form.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  12172 Nov 26 10:30 startups.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff    406 Nov 26 10:30 terms-of-service-app.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  39072 Nov 26 10:30 terms-of-service.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  22162 Nov 26 10:30 terms-of-use.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   2528 Nov 26 10:30 trust-and-security.md
  [2m  â”‚[0m drwxr-xr-x   99 julien  staff   3168 Dec  1 08:32 videos
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  86237 Nov 26 10:30 videos.md
  [2m  â”‚[0m drwxr-xr-x    6 julien  staff    192 Dec  1 08:32 webinar
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ parquet|CSV|file format|data format
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-learn-sql-in-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/tableau-cloud-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vector-technologies-ai-data-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/solving-advent-code-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/spark-ducklake-getting-started.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streaming-oracle-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/semantic-layer-duckdb-tutorial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/six-reasons-duckdb-slaps.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-recap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pushing-geo-boundaries-with-motherduck-geobase.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-faker-duckdb-exploration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/redshift-files-hunt-for-big-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-duckdb-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-mooncake-columnstore.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/postgres-duckdb-options.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/preswald-health-data-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-kestra-etl-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-open-for-all-with-series-b.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-union-orchestration.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 25,
  [2m  â”‚[0m   "appliedLimit": 25
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me read some of the most relevant files about file formats and data loading:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md",
  [2m  â”‚[0m     "content": "---\ntitle: taming-wild-csvs-with-duckdb-data-engineering\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering\nindexed_at: '2025-11-25T19:57:41.542448'\ncontent_hash: 6594cfafde3c18de\nhas_code_examples: true\nhas_step_by_step: true\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Taming Wild CSVs: Advanced DuckDB Techniques for Data Engineers\n\n2025/05/17 - 12 min read\n\nBY\n\n[Ryan Boyd](https://motherduck.com/authors/ryan-boyd/)\n\nIt's 2:30 AM. The office is empty. Your coffee's gone cold, and you're staring blankly at your screen as it mockingly displays: `Error: Could not convert string 'N/A' to INTEGER` on line 56,789. All you wanted was to import a \"simple\" CSV export from that upstream system. Sound familiar?\n\nWe've all been in CSV purgatory. That moment when what should be a five-minute task turns into a multi-hour ordeal because somewhere, buried deep in that innocent-looking file, lurks an inconsistent delimiter, a rogue quote, or my personal favorite\u2014columns that mysteriously appear and disappear like fish fry diving underwater to avoid being eaten by our duck friends.\n\nI've spent countless hours wrestling with problematic CSVs, but after discovering some of DuckDB's lesser-known features, those late-night CSV battles have become far less common. While DuckDB's automatic CSV reader is already impressively smart, knowing a few advanced techniques can save you from writing custom preprocessing scripts when things get messy.\n\nIn this guide, I'll share the DuckDB techniques that have repeatedly saved me from CSV hell:\n\n- How to diagnose what DuckDB actually thinks your [**CSV looks like**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#peeking-under-the-hood-sniffcsv)\n- Deep dive into the [**CSV sniffer**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#how-the-sniffer-works) and how it works under the hood\n- Ways to [**handle problematic rows**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#wrangling-the-errors) without aborting your entire import\n- Strategies for [**dealing with inconsistent schemas**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#handling-shifting-schemas-unionbyname) across files\n- [**Fine-tuning options**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#fine-tuning-detection-and-overrides) when auto-detection needs a helping hand\n- Real-world robustness: how DuckDB performs on a [**benchmark of messy CSVs**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#duckdbs-csv-parser-champion-of-the-pollock-benchmark)\n\nLet's dive in and see if we can make your next CSV import session a little less...quackers.\n\n## **Peeking Under the Hood: sniff\\_csv**\n\nBefore attempting to load the data using DuckDB\u2019s auto-detection capabilities, it's incredibly useful to understand what DuckDB _thinks_ it's dealing with. Is it guessing the delimiter correctly? Did it detect the header? What types is it inferring? The [sniff\\_csv()](https://duckdb.org/docs/stable/data/csv/auto_detection.html) function is your reconnaissance tool here.\n\nInstead of blindly running `read_csv` and potentially hitting errors, run `sniff_csv` first. It analyzes a sample of the file and reports back the detected dialect, types, header presence, and more.\n\n**Let\u2019s imagine a file [events.csv](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/events.csv):**\n\n```txt\nCopy code\n\nEventDate|Organizer|City|Venue\n2022-03-01|OpenTech|San Francisco, CA|Moscone Center, Hall A\n2022-03-02|OpenTech|San Francisco, CA|Moscone Center, Hall B\n2022-03-03|OpenTech|San Francisco, CA|Moscone Center, Hall C\n```\n\n**Let's see what DuckDB makes of this:**\n\n```sql\nCopy code\n\nFROM sniff_csv('events.csv');\n```\n\nINFO: Using full http URLs\nFor brevity, we'll just include the filename in each of the example commands below. You can download the example files using the links (eg, how events.csv is linked above). However, you can also specify the full http URL instead because DuckDB supports the http protocol for retrieving CSV files.\n\n```sql\nCopy code\n\nFROM sniff_csv('http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/events.csv');\n```\n\n**You can also control how much of the file it samples:**\n\n```sql\nCopy code\n\nFROM sniff_csv('events.csv', sample_size=10000); -- Sample 10k rows\n```\n\n**Or sample the whole file (careful with huge files!):**\n\n```sql\nCopy code\n\nFROM sniff_csv('events.csv', sample_size=-1);\n```\n\n**The output provides a wealth of information in a single row table:**\n\n- `Delimiter`, `Quote`, `Escape`, `NewLineDelimiter`: The detected structural characters.\n- `SkipRows`: Number of rows it thinks should be skipped at the start.\n- `HasHeader`: Boolean flag if a header is detected.\n- `Columns`: A list of structs showing detected column names and types (e.g., `{'name': 'VARCHAR', 'age': 'BIGINT'}`).\n- `DateFormat`, `TimestampFormat`: Any special date/time formats it detected.\n- `Prompt`: This is extremely useful! It gives you a `read_csv` command _with_ all the detected options explicitly set. You can copy, paste, and modify this as needed.\n\nRunning `sniff_csv` first can save you significant guesswork when an import fails. If the detected `Delimiter` is wrong, or it thinks `HasHeader` is true when it isn't, you know exactly which options to override in your `read_csv` call.\n\n### **How the Sniffer Works**\n\nDuckDB's CSV sniffer actually operates through multiple phases to determine the best way to read your file:\n\n1. **Dialect Detection**: At the time of writing, sniffer tests 24 different combinations of dialect configurations (delimiters, quotes, escapes, newlines) to see which one creates the most consistent number of columns across rows.\n\n2. **Type Detection**: After determining the dialect, the sniffer analyzes the first chunk of data (2048 rows by default) to detect column types, trying to cast values from most to least specific types (SQLNULL \u2192 BOOLEAN \u2192 BIGINT \u2192 DOUBLE \u2192 TIME \u2192 DATE \u2192 TIMESTAMP \u2192 VARCHAR).\n\n3. **Header Detection**: The sniffer checks if the first valid line can be cast to the detected column types. If it can't, that line is considered a header.\n\n4. **Type Replacement**: If you specified column types, they override the sniffer's detected types.\n\n5. **Type Refinement**: The sniffer validates the detected types on more data using vectorized casting.\n\n\n**Here's a code example showing how to inspect what the sniffer sees in a more complex scenario:**\n\n```sql\nCopy code\n\n-- Examine what the sniffer detects with a larger sample size\nSELECT\n    Delimiter, Quote, Escape, SkipRows, HasHeader, DateFormat, TimestampFormat\nFROM sniff_csv('events.csv',  sample_size=50000);\n\n-- To see the detected column types\nSELECT unnest(Columns)\nFROM sniff_csv('events.csv');\n```\n\nWhen I was working with a dataset containing 20+ columns of mixed types, the `unnest(Columns)` trick was particularly helpful to see exactly which columns were being detected as which types, saving a ton of back-and-forth troubleshooting.\n\n## **Wrangling the Errors: ignore\\_errors, store\\_rejects, and More**\n\nSo `sniff_csv` looks good, but your file _still_ has issues\u2014maybe just a few problematic rows scattered throughout millions of good ones. By default, DuckDB will halt the import on the first error. But often, you just want the valid data and to deal with the bad rows separately.\n\n### **Option 1: Just Skip 'Em (`ignore_errors`)**\n\nThe simplest approach is to tell DuckDB to skip rows that cause parsing or casting errors using `ignore_errors = true`.\n\nLet's imagine a file [`faulty_folks.csv`](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/faulty_folks.csv):\n\n```txt\nCopy code\n\nName,Age\nAlice,30\nBob,forty-two\nCharlie,35\n```\n\nTrying to read this normally with explicit types will fail on Bob's age:\n\n```sql\nCopy code\n\n-- This will error out!\nSELECT * FROM read_csv('faulty_folks.csv', header=true, columns={'Name':'VARCHAR', 'Age':'INTEGER'});\n```\n\nBut if we just want Alice and Charlie:\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('faulty_folks.csv',\n    header = true,\n    -- Specify expected types\n    columns = {'Name': 'VARCHAR', 'Age': 'INTEGER'},\n    ignore_errors = true  -- The key part!\n    );\n```\n\n**Explanation:**\n\n- We define the `columns` we expect, including the `INTEGER` type for `Age`.\n- `ignore_errors = true` tells the reader: if you hit a row where 'Age' can't become an `INTEGER` (like \"forty-two\"), just drop that row and keep going.\n\n**Output:**\n\n| Name | Age |\n| --- | --- |\n| Alice | 30 |\n| Charlie | 35 |\n\nBob gets left behind, but the import succeeds with the valid rows. This approach skips rows with various issues: casting errors, wrong number of columns, unescaped quotes, etc.\n\n### **Option 2: Skip and Store (store\\_rejects)**\n\nIgnoring errors is okay, but generally, you need to know _what_ went wrong and _which_ rows were rejected. Maybe you need to fix the source data or report the issues. This is where `store_rejects = true` becomes invaluable.\n\nWhen you use [`store_rejects`](https://duckdb.org/docs/stable/data/csv/overview.html), DuckDB still skips the bad rows (like `ignore_errors`), but it also logs detailed information about each rejected row and the error encountered into two temporary tables: `reject_scans` and `reject_errors`.\n\n```sql\nCopy code\n\n-- Read the file, storing rejected rows\nSELECT * FROM read_csv(\n    'faulty_folks.csv',\n    header = true,\n    columns = {'Name': 'VARCHAR', 'Age': 'INTEGER'},\n    store_rejects = true -- Store info about errors\n    -- Optional: Customize table names and limit\n    -- rejects_scan = 'my_scan_info',\n    -- rejects_table = 'my_rejected_rows',\n    -- rejects_limit = 100 -- Store max 100 errors per file\n);\n\n-- Now, let's see what was rejected\nFROM reject_errors;\n-- And details about the scan itself\nFROM reject_scans;\n```\n\n**Explanation:**\n\n1. The `read_csv` call runs, skips Bob's row, and returns Alice and Charlie just like before.\n2. The key difference: `store_rejects = true` populates the temporary tables.\n3. `FROM reject_errors;` shows details about the failed rows:\n\n   - `scan_id`, `file_id`: Link back to the specific scan/file.\n   - `line`: The original line number in the CSV.\n   - `column_idx`, `column_name`: Which column had the issue (if applicable).\n   - `error_type`: The category of error (e.g., `CAST`, `TOO_MANY_COLUMNS`).\n   - `csv_line`: The actual content of the rejected line.\n   - `error_message`: The specific error message DuckDB generated.\n4. `FROM reject_scans;` gives metadata about the `read_csv` operation itself (delimiter, quote rule, schema used, file path, etc.).\n\nI've found this incredibly useful for debugging dirty data. You get the clean data loaded _and_ a detailed report on the rejects, all within DuckDB. No more grep-ing through massive files trying to find that one problematic line!\n\n### **Option 3: Relaxing the Rules (strict\\_mode=false and null\\_padding=true)**\n\nSometimes, you just want to _get the data in_, even if it\u2019s a little messy. That\u2019s where DuckDB's more forgiving CSV parsing options can help you out. **`strict_mode = false`** option tells DuckDB to loosen up its parsing expectations. It will _try_ to read rows even if they contain typical formatting problems like:\n\n- Unescaped quote characters in fields (e.g., `\"15\" Laptop\"`).\n- Rows with _more_ columns than defined (DuckDB just drops the extras).\n- Mixed newline formats (like mixing `\\n` and `\\r\\n`).\n\nWARNING: fast, but not precise\nWhen you set `strict_mode=false`, you\u2019re trusting DuckDB to make its best guess. That works great when you want results fast\u2014but double-check the output if data precision matters!\n\nAnother commonly used option is **`null_padding = true,`** which handles rows that come up _short_, meaning they have fewer columns than expected. Instead of throwing an error, DuckDB just fills in the blanks with `NULL`.\n\nLet\u2019s look at an example. Here's a messy CSV file named [`inventory.csv`](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/inventory.csv):\n\n```txt\nCopy code\n\nItemID,Description,Price\n101,\"15\" Laptop\",999.99\n102,\"Wireless Mouse\"\n103,\"Mechanical Keyboard\",129.99,ExtraField\n```\n\nThis file includes:\n\n- An unescaped quote in the first row\u2019s description\n- A missing price in the second row\n- An extra column in the third row\n\nTry reading it normally:\n\n```sql\nCopy code\n\nFROM read_csv('inventory.csv');\n```\n\nDuckDB will skip all lines except the last.\n\n![DuckDB CLI image](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimage1_7ef2565c8e.png&w=3840&q=75)\n\nBut with relaxed settings:\n\n```sql\nCopy code\n\n-- Parsing a messy CSV while gracefully handling missing and extra fields\nFROM read_csv('inventory.csv',\n    strict_mode = false,   -- Forgive formatting quirks\n    null_padding = true   -- Fill missing columns with NULLs\n);\n```\n\n**Resulting Table:**\n\n| ItemID | Description | Price | column3 |\n| --- | --- | --- | --- |\n| 101 | 15 Laptop | 999.99 | NULL |\n| 102 | Wireless Mouse | NULL | NULL |\n| 103 | Mechanical Keyboard | 129.99 | ExtraField |\n\n## **Handling Shifting Schemas: union\\_by\\_name**\n\nHere's another common headache: you have multiple CSV files to load (e.g., monthly reports), but the columns aren't _quite_ the same across files. Maybe a column was added in March, or the order changed in June. Trying to read them together with a simple `read_csv('monthly_report_*.csv')` might fail or produce misaligned data.\n\nDuckDB's `union_by_name = true` option handles this elegantly. When reading multiple files (using globs or lists), it aligns columns based on their _header names_ rather than their position. If a file is missing a column found in other files, it fills it with `NULL`.\n\nImagine [`report_jan.csv`](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/report_jan.csv):\n\n```txt\nCopy code\n\nUserID,MetricA,MetricB\n1,10,100\n2,15,110\n```\n\nAnd [`report_feb.csv`](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/report_feb.csv):\n\n```txt\nCopy code\n\nUserID,MetricB,MetricC,MetricA\n3,120,xyz,20\n4,125,abc,25\n```\n\nNotice the different order and the new `MetricC` in February.\n\n```sql\nCopy code\n\nSELECT *\nFROM read_csv(\n    ['report_jan.csv', 'report_feb.csv'], -- List of files (or glob)\n    union_by_name = true -- The magic!\n);\n```\n\n**Explanation:**\n\n- DuckDB reads the headers from all files involved.\n- It creates a combined schema containing _all_ unique column names (`UserID`, `MetricA`, `MetricB`, `MetricC`).\n- For each file, it matches the data to the combined schema based on the header names found _in that specific file_.\n- Where a column doesn't exist in a file (like `MetricC` in `report_jan.csv`), it inserts `NULL`.\n\n**Output:**\n\n| UserID | MetricA | MetricB | MetricC |\n| --- | --- | --- | --- |\n| 1 | 10 | 100 | NULL |\n| 2 | 15 | 110 | NULL |\n| 3 | 20 | 120 | xyz |\n| 4 | 25 | 125 | Abc |\n\n## **Fine-Tuning Detection and Overrides**\n\nWhile auto-detection is great, sometimes you know better, or the sample DuckDB takes isn't quite representative. Here are some ways to fine-tune the process:\n\n**Bigger Sample:** If type detection seems off (e.g., a column that's mostly integers but has a few floats later gets detected as `BIGINT`), try increasing the sample size:\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('file.csv', sample_size = 50000);\n\n-- Or scan the whole file (can be slow for huge files)\nSELECT * FROM read_csv('file.csv', sample_size = -1);\n```\n\n**Manual Types:** Override specific column types if detection gets it wrong or if you want a different type:\n\n```sql\nCopy code\n\n-- Override by name\nSELECT * FROM read_csv('file.csv',\n  types = {'UserID': 'VARCHAR', 'TransactionAmount': 'DOUBLE'});\n\n-- Or by position if no headers\nSELECT * FROM read_csv('file.csv', header = false,\n  types = ['VARCHAR', 'DOUBLE', 'DATE']);\n```\n\n**Force Header/No Header:** If header detection fails (common if all columns look like strings):\n\n```sql\nCopy code\n\n-- Force header presence\nSELECT * FROM read_csv('file.csv', header = true);\n\n-- Or no header with custom names\nSELECT * FROM read_csv('file.csv',\n header = false,\n names = ['colA', 'colB', 'colC']);\n```\n\n**Date/Timestamp Formats:** If dates aren't ISO 8601 (`YYYY-MM-DD`) or times aren't standard:\n\n```sql\nCopy code\n\n SELECT * FROM read_csv('file.csv',\n  dateformat = '%m/%d/%Y',\n  timestampformat = '%Y-%m-%dT%H:%M:%S.%f');\n```\n\n**Everything is a String:** If you want to load _everything_ as `VARCHAR` and deal with types later:\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('file.csv', all_varchar = true);\n```\n\n**Which Columns Can Be NULL?:** By default, an empty field is treated as `NULL`. If empty strings should be valid values:\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('file.csv',\n  force_not_null = ['column_name1', 'column_name2']);\n```\n\n**Clean Up Names:** Got headers with spaces or weird characters?\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('file.csv', normalize_names = true);\n```\n\nThis will [automatically clean them up](https://duckdb.org/docs/stable/data/csv/overview.html#parameters) (replacing non-alphanumeric with `_`, etc.) during import.\n\n## DuckDB's CSV Parser: Champion of the Pollock Benchmark\n\nFor those really interested in CSV robustness, there's an intriguing benchmark called [Pollock](https://hpi.de/naumann/projects/data-preparation/pollock.html) that evaluates how well different systems handle non-standard CSV files. The creators studied over 245,000 public CSV datasets to identify common violations of the RFC-4180 standard, then created test files with these issues.\n\nIn recent testing, DuckDB [ranked #1](https://github.com/HPI-Information-Systems/Pollock) in the benchmark when configured to handle problematic files, correctly reading 99.61% of the data across all test files. Even in auto-detect mode with minimal configuration, DuckDB still managed to read about 90.75% of the data correctly.\n\nThis is practical validation that the approaches we've covered in this article can handle the vast majority of real-world CSV issues you'll encounter.\n\n| System under test | Pollock score<br>(simple) | Pollock score<br>(weighted) |\n| --- | --- | --- |\n| DuckDB 1.2 | 9.961 | 9.599 |\n| SQLite 3.39.0 | 9.955 | 9.375 |\n| UniVocity 2.9.1 | 9.939 | 7.936 |\n| LibreOffice Calc 7.3.6 | 9.925 | 7.833 |\n| SpreadDesktop | 9.929 | 9.597 |\n| SpreadWeb | 9.721 | 9.431 |\n| Python native csv 3.10.5 | 9.721 | 9.436 |\n| Pandas 1.4.3 | 9.895 | 9.431 |\n| MySQL 8.0.31 | 9.587 | 7.484 |\n| Mariadb 10.9.3 | 9.585 | 7.483 |\n\n## **Taking Flight Beyond the Basics**\n\nWe've covered quite a bit in our journey through DuckDB's CSV capabilities\u2014from diagnosing issues with `sniff_csv` to handling errors with `ignore_errors` and `store_rejects`, merging inconsistent schemas with `union_by_name`, and fine-tuning the whole process with various overrides.\n\nWhat I've come to appreciate about DuckDB is that its CSV reader isn't just a basic loader\u2014it's a sophisticated tool designed to handle real-world data messiness directly within SQL. Most data tools can handle the perfect CSV file, but it's how they deal with the imperfect ones that really matters in day-to-day work.\n\nBy understanding these slightly more advanced options, you can often avoid external preprocessing steps, keeping your data loading logic right within your SQL workflow. The result is cleaner pipelines that are less likely to waddle when faced with unexpected CSV quirks.\n\nThe next time a tricky CSV lands on your desk, remember these techniques. They might just save you some time and frustration, letting you get back to the more interesting parts of data analysis sooner. Happy querying!\n\n### TABLE OF CONTENTS\n\n[Peeking Under the Hood: sniff\\_csv](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#peeking-under-the-hood-sniffcsv)\n\n[Wrangling the Errors: ignore\\_errors, store\\_rejects, and More](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#wrangling-the-errors-ignoreerrors-storerejects-and-more)\n\n[Handling Shifting Schemas: union\\_by\\_name](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#handling-shifting-schemas-unionbyname)\n\n[Fine-Tuning Detection and Overrides](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#fine-tuning-detection-and-overrides)\n\n[DuckDB's CSV Parser: Champion of the Pollock Benchmark](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#duckdbs-csv-parser-champion-of-the-pollock-benchmark)\n\n[Taking Flight Beyond the Basics](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#taking-flight-beyond-the-basics)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![MotherDuck lands on Tableau Cloud: Live, Fast Analytics Unleashed](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FTableau_Cloud_52bd53b821.png&w=3840&q=75)](https://motherduck.com/blog/tableau-cloud-motherduck/)\n\n[2025/05/06 - Jacob Matson](https://motherduck.com/blog/tableau-cloud-motherduck/)\n\n### [MotherDuck lands on Tableau Cloud: Live, Fast Analytics Unleashed](https://motherduck.com/blog/tableau-cloud-motherduck)\n\nUse MotherDuck to power your Tableau Cloud, Server, and Desktop dashboards.\n\n[![DuckDB Ecosystem: May 2025](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fduckdbeco_may_ca294a4d7f.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025/)\n\n[2025/05/08 - Simon Sp\u00e4ti](https://motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025/)\n\n### [DuckDB Ecosystem: May 2025](https://motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025)\n\nDuckDB Monthly #29: From Metabase to Doom, DuckDB powers everything.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 513,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 513
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     "content": "---\ntitle: simple-way-to-convert-csv-and-parquet-files\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files\nindexed_at: '2025-11-25T20:44:27.252956'\ncontent_hash: 0215f394eabfe4ec\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nSimple way to convert CSV - Parquet files - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Simple way to convert CSV - Parquet files](https://www.youtube.com/watch?v=Y_GXdbet9Gk)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=Y_GXdbet9Gk&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 1:00\n\n\u2022Live\n\n\u2022\n\nYouTubeShortTutorial\n\n# Simple way to convert CSV and Parquet files\n\n2024/01/17\n\nAs developers, we live in the terminal. It\u2019s our command center for everything from Git commits to running applications. But we all have those small, annoying tasks that break our flow\u2014the little papercuts of the development cycle. One of the most common? File format conversion.\n\nYou have a Parquet file, but a legacy tool or a colleague needs it in CSV format. Or maybe you have a massive CSV that you want to compress into the more efficient Parquet format. What's your go-to move? Do you spin up a Jupyter notebook and `import pandas`? Do you write a quick, one-off Python script? Or do you resort to a sketchy online file converter?\n\nThese interruptions, while small, add up. But what if the perfect tool for the job was already on your machine, ready to go?\n\nEnter DuckDB. While you might know it as a powerful embedded analytical database, it's also a versatile Swiss army knife for your data. Because it's a lightweight, serverless tool that speaks fluent SQL and natively understands formats like Parquet, CSV, and JSON, it's the perfect utility for lightning-fast file conversions directly from your command line.\n\nLet's build a powerful, reusable conversion utility in just a few minutes.\n\n### Instant File Conversion with a DuckDB One-Liner\n\nFirst things first, you need the `duckdb` CLI. If you're on macOS, installation is a single command with Homebrew.\n\n```bash\nCopy code\n\nbrew install duckdb\n```\n\nFor other operating systems, check out the [official installation documentation](https://duckdb.org/docs/installation/index).\n\nOnce installed, you have everything you need. Let's say you have a file named `data.parquet` and you want to convert it to `data.csv`. The magic is a single command that leverages DuckDB's powerful `COPY` statement.\n\n```bash\nCopy code\n\nduckdb -c \"COPY (SELECT * FROM 'data.parquet') TO 'data.csv' (HEADER, DELIMITER ',');\"\n```\n\nLet's break down this command to see what's happening:\n\n- `duckdb -c \"...\"`: This is the key to using DuckDB as a scripting tool. The `-c` flag tells DuckDB to execute the SQL command that follows and then immediately exit. No interactive shell, no fuss\u2014just pure, scriptable execution.\n- `COPY (...) TO 'data.csv'`: This is the workhorse. The `COPY` command is incredibly efficient at moving data into and out of DuckDB.\n- `(SELECT * FROM 'data.parquet')`: Instead of copying from a table, we're telling DuckDB to copy the result of a query. The magic here is that DuckDB can query files like Parquet or CSV directly, _as if they were database tables_. It automatically infers the file type and schema from the file extension.\n- `(HEADER, DELIMITER ',')`: These are options specific to the output format. Here, we're telling DuckDB to include a header row in our final CSV file.\n\nAnd that's it. In the time it would have taken you to open a new editor tab, you've converted your file.\n\n### Building a Reusable Conversion Script\n\nThat one-liner is great, but we can make it even better. The real power of the command line comes from creating reusable, generic tools.\n\nLet's wrap this logic into a simple Bash script. Create a file named `file-converter.sh` somewhere convenient, like `~/scripts/`.\n\n```bash\nCopy code\n\n#!/bin/bash\n# file-converter.sh\n\n# Check if two arguments are provided\nif [ \"$#\" -ne 2 ]; then\n    echo \"Usage: $0 <input_file> <output_file>\"\n    exit 1\nfi\n\nINPUT_FILE=$1\nOUTPUT_FILE=$2\n\nduckdb -c \"COPY (SELECT * FROM '${INPUT_FILE}') TO '${OUTPUT_FILE}';\"\n\necho \"Successfully converted ${INPUT_FILE} to ${OUTPUT_FILE}\"\n```\n\nMake the script executable:\n\n```bash\nCopy code\n\nchmod +x ~/scripts/file-converter.sh\n```\n\nNow, you have a generic script that takes an input file and an output file as arguments. The final step is to create a shell alias for ultimate convenience. Open your `.zshrc`, `.bashrc`, or equivalent shell configuration file and add this line:\n\n```bash\nCopy code\n\n# Add to your .zshrc or .bashrc\nalias dconvert='~/scripts/file-converter.sh'\n```\n\nRestart your terminal or run `source ~/.zshrc` to apply the changes. Now, witness your new superpower. You can convert files back and forth with a simple, memorable command.\n\n**Convert Parquet to CSV:**\n\n```bash\nCopy code\n\ndconvert data.parquet data.csv\n# Successfully converted data.parquet to data.csv\n```\n\n**Convert CSV to Parquet:**\n\n```bash\nCopy code\n\ndconvert data.csv data.parquet\n# Successfully converted data.csv to data.parquet\n```\n\nIt doesn't get simpler than that. You've just built a universal file conversion utility that is faster and more reliable than a custom script and safer than any online tool.\n\n### Conclusion: More Than Just Conversion\n\nWe started with a simple problem and ended with an elegant, reusable solution. With one small script, you've added a powerful tool to your developer toolkit, powered by DuckDB.\n\nBut don't forget what's happening inside that command. You aren't just copying bytes; you're running a full-fledged SQL query. This opens up a world of possibilities that go far beyond the simple 1-to-1 conversion our `dconvert` alias handles.\n\nWhat if you only wanted a subset of the data? For more complex tasks, you can bypass the alias and use the `duckdb -c` command directly to run a more powerful query.\n\n```bash\nCopy code\n\n# Filter for specific rows before converting\nduckdb -c \"COPY (SELECT * FROM 'data.parquet' WHERE category = 'A') TO 'filtered_data.csv' (HEADER, DELIMITER ',');\"\n```\n\nWhat if you only needed a few columns?\n\n```bash\nCopy code\n\n# Select specific columns\nduckdb -c \"COPY (SELECT user_id, event_timestamp FROM 'logs.parquet') TO 'events.csv' (HEADER, DELIMITER ',');\"\n```\n\nThe `SELECT` statement is your playground. You can perform filtering, transformations, and even simple aggregations as part of your conversion pipeline, all within that single command.\n\n* * *\n\n### Get Started Today\n\n- Try this out and share your favorite DuckDB one-liners with us on [Twitter](https://twitter.com/motherduckcorp)!\n- For more details on the options available, check out the official DuckDB documentation for the [`COPY` command](https://duckdb.org/docs/sql/data_loading/copy).\n- When your challenges go beyond local files, see how [MotherDuck](https://motherduck.com/) brings the power of DuckDB to the cloud for serverless, collaborative analytics.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Is DuckDB the Secret to Unlocking Your GIS Potential?\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmaxresdefault_1_c988e40ed0.jpg&w=3840&q=75)\\\\\n\\\\\n14:49](https://motherduck.com/videos/is-duckdb-the-secret-to-unlocking-your-gis-potential/)\n\n[2024-08-29](https://motherduck.com/videos/is-duckdb-the-secret-to-unlocking-your-gis-potential/)\n\n### [Is DuckDB the Secret to Unlocking Your GIS Potential?](https://motherduck.com/videos/is-duckdb-the-secret-to-unlocking-your-gis-potential)\n\nIn this video, \u202aMehdi walks you through the basics of working with geospatial data and introduces the DuckDB spatial extension. By the end, you will create your own heatmap using DuckDB, Python, and MotherDuck for sharing and scalability.\n\nYouTube\n\nTutorial\n\n[![\"DuckDB & dataviz | End-To-End Data Engineering Project (3/3)\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fvideo_ta_Pzc2_EE_Eo_23e0b0a9d0.jpg&w=3840&q=75)\\\\\n\\\\\n0:21:46](https://motherduck.com/videos/duckdb-dataviz-end-to-end-data-engineering-project-33/)\n\n[2024-06-27](https://motherduck.com/videos/duckdb-dataviz-end-to-end-data-engineering-project-33/)\n\n### [DuckDB & dataviz \\| End-To-End Data Engineering Project (3/3)](https://motherduck.com/videos/duckdb-dataviz-end-to-end-data-engineering-project-33)\n\nIn this part 3 of the project, @mehdio explores how to build a Dashboard with Evidence using MotherDuck/DuckDb as a data source.\n\nYouTube\n\nBI & Visualization\n\nTutorial\n\n[![\"One data tool with all its dependencies: DuckDB and extensions\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fvideo_we_RMT_Aj_Udic_9e9f5c6d41.jpg&w=3840&q=75)\\\\\n\\\\\n0:00:55](https://motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions/)\n\n[2024-06-21](https://motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions/)\n\n### [One data tool with all its dependencies: DuckDB and extensions](https://motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions)\n\nLearn about DuckDB extensions, including the ability to query data in your AWS S3-powered data lake.\n\nYouTube\n\nShort\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 267,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 267
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     "content": "---\ntitle: why-csvs-still-matter-the-indispensable-file-format\ncontent_type: event\nsource_url: https://motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format\nindexed_at: '2025-11-25T20:44:48.426444'\ncontent_hash: 3788e1cc125b23f9\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nWhy CSVs Still Matter: The Indispensable File Format - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Why CSVs Still Matter: The Indispensable File Format](https://www.youtube.com/watch?v=I07qV2hij4E)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=I07qV2hij4E&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 58:06\n\n\u2022Live\n\n\u2022\n\nYouTubeSources\n\n# Why CSVs Still Matter: The Indispensable File Format\n\n2024/05/07\n\nIn this discussion with Mehdi Ouazza and Pedro Holanda from DuckDB, they talk about all things CSVs and dive into a pragmatic example of how the CSV parser works in DuckDB. Enjoy!\n\n## Related Videos\n\n[![\" pg_duckdb: Ducking awesome analytics in Postgres\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2F0kc_W5o0tcg_HD_03880f61fb.jpg&w=3840&q=75)](https://motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres/)\n\n[2025-06-12](https://motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres/)\n\n### [pg\\_duckdb: Ducking awesome analytics in Postgres](https://motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres)\n\nSupercharge your Postgres analytics! This talk shows how the pg\\_duckdb extension accelerates your slowest queries instantly, often with zero code changes. Learn practical tips and how to use remote columnar storage for even more speed.\n\nTalk\n\nSources\n\n[![\"Can DuckDB revolutionize the data lake experience?\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmaxresdefault_2074c1c09e.jpg&w=3840&q=75)\\\\\n\\\\\n16:37](https://motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience/)\n\n[2024-11-22](https://motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience/)\n\n### [Can DuckDB revolutionize the data lake experience?](https://motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience)\n\nMehdi explores DuckDB as a catalog for Data Lake and Lakehouse pattern. He'll define what we mean by \"data catalog\", gives clear examples on how they work and dive into a pragmatic use case with DuckDB & MotherDuck.\n\nYouTube\n\nData Pipelines\n\nSources\n\n[![\"Leveraging DuckDB and Delta Lake Together\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmaxresdefault_2_426f95538d.jpg&w=3840&q=75)\\\\\n\\\\\n1:02:10](https://motherduck.com/videos/leveraging-duckdb-and-delta-lake-together/)\n\n[2024-07-24](https://motherduck.com/videos/leveraging-duckdb-and-delta-lake-together/)\n\n### [Leveraging DuckDB and Delta Lake Together](https://motherduck.com/videos/leveraging-duckdb-and-delta-lake-together)\n\nQuack & Code on table formats and Delta Lake with Holly Smith from Databricks. Can you use DuckDB and MotherDuck to query your data lakehouse based on Delta Lake? Yes! And Delta Lake provides a better format than Parquet.\n\nYouTube\n\nSources\n\nQuack & Code\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 127,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 127
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search for more specific information about Parquet and data loading performance:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ parquet.*efficient|parquet.*performance|parquet.*fast|columnar.*format

  [96mğŸ”§ TOOL:[0m Grep â†’ compress|compression|size|storage
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-everybody-hates-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streamlining-ai-agents-duckdb-rag-solutions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/summer-data-engineering-roadmap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-simple-joys-of-scaling-up.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vector-technologies-ai-data-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-workshops-agenda.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/spark-ducklake-getting-started.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streamkap-mysql-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/semantic-layer-duckdb-tutorial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/simplifying-iot-analytics-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/six-reasons-duckdb-slaps.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-recap-2025.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 15,
  [2m  â”‚[0m   "appliedLimit": 15
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vector-technologies-ai-data-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/six-reasons-duckdb-slaps.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-recap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-faker-duckdb-exploration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-mooncake-columnstore.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/postgres-duckdb-options.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/just-enough-sql-for-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/getting-started-ducklake-table-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/estuary-streaming-cdc-replication.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-versus-pandas-versus-polars.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 15,
  [2m  â”‚[0m   "appliedLimit": 15
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/six-reasons-duckdb-slaps.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/six-reasons-duckdb-slaps.md",
  [2m  â”‚[0m     "content": "---\ntitle: six-reasons-duckdb-slaps\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/six-reasons-duckdb-slaps\nindexed_at: '2025-11-25T19:58:49.657325'\ncontent_hash: 3261ff1a5abbdacc\nhas_code_examples: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Why Use DuckDB for Analytics?\n\n2022/11/11 - 5 min read\n\nBY\n\n[Tino Tereshko](https://motherduck.com/authors/tino-tereshko/)\n,\n[Ryan Boyd](https://motherduck.com/authors/ryan-boyd/)\n\nIndustries transform on the back of momentous technological change. For example, the modern cloud data warehouse arose a decade ago on a foundation of powerful cloud storage, compute, and networking. When we founded MotherDuck we recognized that DuckDB might just be the next major game changer thanks to its ease of use, portability, lightning-fast performance, and a rapid pace of community-driven innovation.\n\n## First, What is DuckDB?\n\n[DuckDB](https://duckdb.org/) is an open source in-process SQL [OLAP](https://en.wikipedia.org/wiki/Online_analytical_processing) database management system. DuckDB can be thought of as \u201cSQLite for analytics\u201d - you can embed it in virtually any codebase and run it in virtually any environment with minimal complexity.\n\nAs an in-process database, DuckDB is a storage and compute engine that enables developers, data scientists and data analysts to power their code with extremely fast analyses using plain SQL. Additionally, DuckDB can analyze data wherever it lives, be it on your laptop or in the cloud.\n\nDuckDB comes with a [command-line interface](https://duckdb.org/docs/api/cli) for rapid prototyping, and you can try DuckDB right now using the [hosted DuckDB shell](https://shell.duckdb.org/).\n\n## Runs Anywhere\n\nThanks to DuckDB, practically any CPU in the world can now be mobilized to perform powerful analytics. DuckDB is portable and modular, with no external dependencies. Thus you can run DuckDB on your laptop, in the browser, on a cloud VM, in a cloud function, and even in a CDN edge point-of-presence.\n\nYou can use DuckDB in Python notebooks, R scripts, Javascript data apps, or Java backends. DuckDB is universally useful for data scientists, analysts, data engineers, and application developers.\n\n## Simplified Data Access\n\nAnalysts often tell us that they wish to analyze data that lives in disparate places - CSV files on their laptops, Parquet files on S3, dataframes in their Python notebooks, and even tables in relational databases. DuckDB challenges the current status quo that needlessly complicates access to these diverse data sources. With DuckDB, you\u2019re at most one or two commands away from querying data where it lies, whether it\u2019s on your local hard drive, in the cloud, or in another database.\n\nThese are all valid SQL statements in DuckDB:\n\n```sql\nCopy code\n\nSELECT AVG(trip_distance) FROM 's3://yellow_tripdata_20[12]*.parquet'\n\nSELECT * FROM '~/local/files/file.parquet'\n\nSELECT * FROM dataframe\n\nSELECT * FROM 'https://shell.duckdb.org/data/tpch/0_01/parquet/lineitem.parquet'\n```\n\nDo you have Arrow tables, PostgreSQL databases or SQLite databases? DuckDB can directly query those too; no import required!\n\n## Use with Popular Tools and Frameworks\n\nDuckDB rose in prominence thanks to its ease of use in Python alongside pandas, a hugely popular library for data science. While pandas enables rich and powerful data science transformations, DuckDB dramatically accelerates analytical workloads, with the added benefit of using a standard SQL interface. DuckDB can even treat pandas dataframes as DuckDB tables and query them directly.\n\n```python\nCopy code\n\nimport pandas as pd\n\nimport duckdb\n\nmydf = pd.DataFrame({'a' : [1, 2, 3]})\n\nprint(duckdb.query(\"SELECT sum(a) FROM mydf;\").fetchall())\n```\n\nDuckDB enables users to connect to powerful BI tools like Tableau, Looker, or Superset with standard ODBC or JDBC drivers. Additionally, DuckDB is available in Python, R, Javan, node.JS, Julia, C/C++, and WASM.\n\n## Fast Aggregation and Excellent SQL Support, the Key to Analytics\n\nDuckDB is designed as an analytics database from the bottoms up \u2013 aiming to squeeze every ounce of performance while also allowing you to perform complex analytics queries using standardized SQL.\n\nAs an analytics database, DuckDB is optimized for read operations and can also perform updates in a transactional ACID-compliant fashion. It stores data in a compressed columnar format, which provides the best performance for large-scale aggregations. This is in contrast to a transactional database, which is optimized for high-frequency writes and typically stores data as rows (tuples) to support that.\n\nAdditionally, DuckDB has a vectorized query engine, enabling small batches of data to be analyzed simultaneously via processors supporting SIMD (Simultaneous Instruction on Multiple Data). These small batches are optimized for locality to the CPU, utilizing the L1/L2/L3 caches which have the lowest latency, as opposed to only using main memory.\n\nThe SQL engine is extremely thoroughly tested and aims to support PostgreSQL-style SQL, along with some special analytical functions and custom syntax that\u2019s helpful for analysts. You get [window functions](https://duckdb.org/docs/sql/window_functions), [statistical sampling](https://duckdb.org/docs/sql/samples), a good [math library](https://duckdb.org/docs/sql/functions/numeric), and even support for [working with nested data](https://duckdb.org/docs/sql/functions/nested).\n\n## Open Source Community that Flocks Together\n\nWith hundreds of contributors and 7.1k GitHub stars at time of publication, DuckDB is home to a vibrant and rapidly expanding open source community. Contributors are working on core database functionality, improved integrations with external data formats and tooling, improved documentation and all other aspects of the project. The community [flocks together on Discord](https://discord.com/invite/tcvwpjfnZx), with over 1,100 members, supported by a [growing DuckDB foundation](https://duckdb.org/foundation/).\n\n## Innovation at an Incredible Pace\n\nThe DuckDB project came out of academic research, so naturally its code base is very clean. Moreover, DuckDB is based on a very simple scale-up architecture, which enables an unparalleled velocity of innovation, and the DuckDB team habitually implements cutting edge academic research (eg [compression algorithms](https://duckdb.org/2022/10/28/lightweight-compression.html)). As a consequence, DuckDB is getting faster, more efficient, and easier to use every single month.\n\n## Next Steps\n\nAt MotherDuck, we want to help the community, the DuckDB Foundation and DuckDB Labs build greater awareness and adoption of DuckDB, whether users are working locally or want a serverless always-on way to execute their SQL.\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POST\n\n[![Hello, World! Quack. Quack.](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmotherduck_team_34b2dcac8d.jpg&w=3840&q=75)](https://motherduck.com/blog/hello-world/)\n\n[2022/11/08 - MotherDuck team](https://motherduck.com/blog/hello-world/)\n\n### [Hello, World! Quack. Quack.](https://motherduck.com/blog/hello-world)\n\nMotherDuck is building a serverless SQL analytics platform to use as a data warehouse and backend to data apps. We believe that big data is dead and we should be focused on making data analysis easier with DuckDB.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 132,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 132
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     "content": "---\ntitle: duckdb-tutorial-for-beginners\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/duckdb-tutorial-for-beginners\nindexed_at: '2025-11-25T19:57:39.343742'\ncontent_hash: 9666dd2fd1ca1c68\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# DuckDB Tutorial For Beginners\n\n2024/10/31 - 12 min read\n\nBY\n\n[Mehdi Ouazza](https://motherduck.com/authors/mehdi-ouazza/)\n,\n[Ryan Boyd](https://motherduck.com/authors/ryan-boyd/)\n\nIf you haven't had the chance to get up to speed with DuckDB, this tutorial is for you! We'll go over the essentials, from installation to workflow, getting to know the command-line interface (CLI), and diving into your first analytics project. If are too lazy to read, I also made a video for this tutorial.\n\nDuckDB Tutorial For Beginners In 12 min - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[DuckDB Tutorial For Beginners In 12 min](https://www.youtube.com/watch?v=ZX5FdqzGT1E)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nFull screen is unavailable. [Learn More](https://support.google.com/youtube/answer/6276924)\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Why am I seeing this?](https://support.google.com/youtube/answer/9004474?hl=en)\n\n[Watch on](https://www.youtube.com/watch?v=ZX5FdqzGT1E&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 11:26\n\n\u2022Live\n\n\u2022\n\nLet's start quacking some code!\n\nEditor's note: this tutorial was originally published 2023-04-26 by Mehdi and has been updated by Ryan to reflect advancements in DuckDB.\n\n\n## What is DuckDB?\n\nDuckDB is an in-process SQL [OLAP](https://motherduck.com/learn-more/what-is-OLAP/) database, which means it is a database optimized for analytics and runs within the same process as the application using it. This unique feature allows DuckDB to offer the advantages of a database without the complexities of managing one. But, as with any software concept, the best way to learn is to dive in and get your hands dirty.\n\nWe\u2019ll be showing examples using the DuckDB command-line client (CLI), but you can also use DuckDB from within Python, R, and other languages, or any tool supporting JDBC or ODBC drivers. There is a community-contributed selection of example queries and code for many of these languages on the [DuckDB Snippets](https://duckdbsnippets.com/) website.\n\n_In the below snippets, any code example prefixed with `$` means that it\u2019s a bash command. Otherwise we assume that these would run within a DuckDB process, which uses a `D` prompt._\n\n## Installation\n\nInstalling DuckDB is a breeze. Visit the [DuckDB documentation](https://duckdb.org/docs/installation/index) and download the binary for your operating system.\n\nFor MacOS and Windows users, you can leverage package managers to make the DuckDB CLI directly available in your PATH, simplifying upgrades and installations.\n\nTo install DuckDB on MacOS using Homebrew, run the following command:\n\n```bash\nCopy code\n\n$ brew install duckdb\n```\n\nTo install DuckDB on Windows using winget, run the following command:\n\n```bash\nCopy code\n\nC:\\> winget install DuckDB.cli\n```\n\nYou can now launch DuckDB by simply calling the `duckdb` CLI command.\n\n```jsx\nCopy code\n\n$ duckdb\nv1.0.0 1f98600c2c\nEnter \".help\" for usage hints.\nConnected to a transient in-memory database.\nUse \".open FILENAME\" to reopen on a persistent database.\nD\n```\n\n## Workflow with VSCode\n\nTo follow along with our exploration of DuckDB, check out this [GitHub repository](https://github.com/mehd-io/duckdb-playground-tutorial). I recommend working with an editor, a SQL file, and sending commands to the terminal for a lightweight setup. This approach offers visibility on all commands, enables you to safely version control them, and allows you to leverage formatting tools and AI friends like Copilot.\n\nIn our example, we'll use Visual Studio Code (VSCode). To configure a custom shortcut to send commands from the editor to the terminal, open the keyboard shortcuts JSON file and add a key binding to the following command :\n\n```jsx\nCopy code\n\n{\n    \"key\": \"shift+enter\",\n    \"command\": \"workbench.action.terminal.runSelectedText\"\n}\n```\n\nOf course, this workflow can be pretty easily replicated with any editor or IDE!\n\n### Data Persistence with DuckDB: Overview\n\nBy default, DuckDB is an in-memory process and won't persist any data. To demonstrate this, let's create a simple table based on a query result:\n\n```sql\nCopy code\n\n$ duckdb\nD CREATE TABLE ducks AS SELECT 3 AS age, 'mandarin' AS breed;\nFROM ducks;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  age  \u2502  breed   \u2502\n\u2502 int32 \u2502 varchar  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     3 \u2502 mandarin \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nThis query creates and populates a \"ducks\" table. However, if we exit the CLI and reopen it, the table will be gone.\n\nUnlike standard SQL, the query above uses the `FROM` statement without any `SELECT *` statement. This is a neat shortcut in DuckDB and there are plenty more [DuckDB SQL shortcuts](https://duckdb.org/2022/05/04/friendlier-sql.html)!\n\n\n### Data Persistence with DuckDB: Creating a Database\n\nTo persist data, you have two options:\n\n1. Provide a path to a database file when starting DuckDB. The file can have any extension, but common choices are `.db`, `.duckdb`, or `.ddb`. If no database exists at the specified path, DuckDB will create one.\n\n\n```bash\nCopy code\n\n\n$ duckdb /data/myawesomedb.db\n```\n\n\nYou can also launch DuckDB with a database in read-only mode to avoid modifying the database:\n\n\n```bash\nCopy code\n\n\n$ duckdb -readonly /data/myawesomedb.db\n```\n\n2. If DuckDB is already running, use the `attach` command to connect to a database at the specified file path.\n\n\n```arduino\nCopy code\n\n\nATTACH DATABASE '/path/to/your/database.db' AS mydb;\n```\n\n\nThe database file uses DuckDB's custom single-file format (all tables are included), which supports transactional ACID compliance and stores data in a compressed columnar format for optimal aggregation performance. DuckDB is [regularly adding](https://duckdb.org/2022/10/28/lightweight-compression.html) new compression algorithms to improve performance.\n\nWhile the DuckDB team often improves the file format with new releases, it is [backward compatible](https://duckdb.org/docs/internals/storage.html) as of DuckDB 1.0, meaning that new releases are able to read files produced by early releases of DuckDB.\n\nIf you use MotherDuck as your cloud data warehouse, it automatically manages the DuckDB databases for you, so you create a MotherDuck database using the familiar [`CREATE DATABASE`](https://motherduck.com/docs/sql-reference/motherduck-sql-reference/create-database/) SQL statement.\n\n## Reading and Displaying Data\n\nNext, let's explore reading and writing data in CSV and Parquet formats. We'll use a small dataset from Kaggle containing daily Netflix Top 10 Movie/TV Show data for the United States from 2020 to March 2022.\n\nTo load the CSV dataset, use the [read\\_csv\\_auto](https://duckdb.org/docs/data/csv/overview#read_csv_auto-function) command, which infers the schema and detects the delimiter. You can also use the `read_csv` command and pass the schema and delimiter as parameters.\n\n```sql\nCopy code\n\nSELECT * FROM read_csv_auto('path/to/your/file.csv');\n```\n\nWhen you use this command, the dataset is read, but an actual table is not created in your DuckDB database. To create a table, use a `CREATE TABLE x AS` (CTAS) statement:\n\n```sql\nCopy code\n\nCREATE TABLE netflix_top10 AS SELECT * FROM read_csv_auto('path/to/your/file.csv');\n```\n\nTo write data to a CSV file, use the `COPY` command and specify the delimiter. For Parquet files, simply specify the file format:\n\n```sql\nCopy code\n\nCOPY ./data/netflix_top10.csv TO 'path/to/your/output/file.csv' WITH (FORMAT 'CSV', DELIMITER ',');\nCOPY ./data/netflix_top10.csv TO 'path/to/your/output/file.parquet' WITH (FORMAT 'PARQUET');\n```\n\nTo read data from a Parquet file, use the `read_parquet` command:\n\n```sql\nCopy code\n\nSELECT * FROM read_parquet('path/to/your/file.parquet');\n```\n\nDuckDB supports a wide variety of different file formats, including the native DuckDB database file used above, CSV, [JSON](https://motherduck.com/blog/analyze-json-data-using-sql/), Parquet, [Iceberg](https://motherduck.com/docs/integrations/file-formats/apache-iceberg/), [Delta Lake](https://motherduck.com/docs/integrations/file-formats/delta-lake/) and more. You can read these files from your local filesystem, a http endpoint or a cloud blob store like AWS S3, Cloudflare R2, Azure Blob Storage or Google Cloud Storage.\n\n## Display Modes, Output Options\n\nDuckDB CLI offers various ways to enhance your experience by customizing the data display and output options.\n\nYou can use the `.mode` command to change the appearance of tables returned in the terminal output. For instance, if you are dealing with long nested JSON, you can change the mode to `line` or `JSON` to have a better view of your data.\n\n```jsx\nCopy code\n\n.mode line\nSELECT * FROM './data/sales.json';\nsales_data = [{'order_id': 1, 'customer': {'id': 101, 'name': John Doe, 'email': john.doe@example.com}, 'items': [{'product_id': 301, 'product_name': Laptop, 'quantity': 1, 'price': 1200}, {'product_id': 302, 'product_name': Mouse, 'quantity': 1, 'price': 25}], 'total_amount': 1225, 'date': 2023-03-24}, {'order_id': 2, 'customer': {'id': 102, 'name': Jane Smith, 'email': jane.smith@example.com}, 'items': [{'product_id': 303, 'product_name': Keyboard, 'quantity': 1, 'price': 50}, {'product_id': 304, 'product_name': Monitor, 'quantity': 1, 'price': 200}], 'total_amount': 250, 'date': 2023-03-25}]\n```\n\nNext to that, you can output elsewhere the data by redirecting the terminal output to a file.\n\nLet\u2019s say you would like to output the result to a Markdown file, you can set the display mode to Markdown with `.mode markdown`. Combine this with the `.output` or `.once` command to write the result directly to a specific file. The `.output` command writes all the output of the different results you run, while `.once` does it just once.\n\n```lua\nCopy code\n\n.mode markdown\n.output myfile.md\n```\n\n## Running Commands and Exiting\n\nDuckDB CLI allows you to run a SQL statement and exit using the `-c` option parameter. For example, if you use a `SELECT` statement to read a Parquet file:\n\n```jsx\nCopy code\n\n$ duckdb -c \"SELECT * FROM read_parquet('path/to/your/file.parquet');\"\n```\n\nThis feature is lightweight, fast, and easy. You can even build your own [bash functions](https://duckdbsnippets.com/snippets/6/quickly-convert-a-csv-to-parquet-bash-function) using the DuckDB CLI for various operations on CSV/Parquet files, such as converting a CSV to Parquet.\n\nDuckDB also offers flags for configuration that you can fine-tune, such as setting the thread count, memory limits, ordering of null values and more. You can find the full list of flag options and their current values from the `duckdb_settings()` table function.\n\n```csharp\nCopy code\n\nFROM duckdb_settings();\n```\n\n## Working with Extensions\n\nExtensions are like packages that you can install within DuckDB to enjoy specific feature. DuckDB supports a number of core extensions. Not all are included by default, but DuckDB has a mechanism for remote extension installation. To view the available core extensions, execute the following statement:\n\n```csharp\nCopy code\n\nFROM duckdb_extensions();\n```\n\nTo install an extension, such as the popular `httpfs` extension that allows reading/writing remote files over HTTPS and S3, use the `INSTALL` command followed by the extension name. Once installed, DuckDB downloads the extension to the `$HOME/.duckdb/` folder (modifiable by setting the `extension_directory` parameter).\n\nNext, load the extension in the DuckDB process with the `LOAD` command.\n\n```ini\nCopy code\n\nINSTALL httpfs;\nLOAD httpfs;\n```\n\nDuckDB supports autoloading of the core extensions, so you often do not need to manually load these. As an example. if you read from a CSV file with a \\`https://\\` scheme, the \\`httpfs\\` extension will be autoloaded.\n\n\nIf you're using a third-party extension or your own extension not bundled by default, set the `allow_unsigned_extensions` flag to `True`, or use the `-unsigned` flag parameter when launching DuckDB.\n\n```jsx\nCopy code\n\n$ duckdb -unsigned\n```\n\nExtensions are powerful and versatile. You can create your own using the [template](https://github.com/duckdb/extension-template) provided by the DuckDB Labs team to kickstart your extension development journey.\n\nThere is now a [Community Extensions repository](https://duckdb.org/docs/extensions/community_extensions.html) for you to share any custom extensions with the wider DuckDB community for easy installation.\n\n## First analytics project\n\nWe have the mentioned Netflix dataset hosted on a public AWS S3 bucket. In this simple project, we will answer the most existential question : what were people in the US binge-watching during the COVID lockdown?\n\nAs the data is sitting on AWS S3, we\u2019ll start by installing the extension httpfs.\n\n```jsx\nCopy code\n\n-- Install extensions\nINSTALL httpfs;\nLOAD httpfs;\n-- Minimum configuration for loading S3 dataset if the bucket is public\nSET s3_region='us-east-1';\n```\n\nWe can now read our dataset :\n\n```jsx\nCopy code\n\nD CREATE TABLE netflix AS SELECT * FROM read_parquet('s3://us-prd-motherduck-open-datasets/netflix/netflix_daily_top_10.parquet');\nFROM netflix;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   As of    \u2502 Rank  \u2502 Year to Date Rank \u2502 \u2026 \u2502 Days In Top 10 \u2502 Viewership Score \u2502\n\u2502    date    \u2502 int64 \u2502      varchar      \u2502   \u2502     int64      \u2502      int64       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2020-04-01 \u2502     1 \u2502 1                 \u2502 \u2026 \u2502              9 \u2502               90 \u2502\n\u2502 2020-04-01 \u2502     2 \u2502 2                 \u2502 \u2026 \u2502              5 \u2502               45 \u2502\n\u2502 2020-04-01 \u2502     3 \u2502 3                 \u2502 \u2026 \u2502              9 \u2502               76 \u2502\n\u2502 2020-04-01 \u2502     4 \u2502 4                 \u2502 \u2026 \u2502              5 \u2502               30 \u2502\n\u2502 2020-04-01 \u2502     5 \u2502 5                 \u2502 \u2026 \u2502              9 \u2502               55 \u2502\n\u2502 2020-04-01 \u2502     6 \u2502 6                 \u2502 \u2026 \u2502              4 \u2502               14 \u2502\n```\n\nFinally, getting the top watched movies as follow :\n\n```jsx\nCopy code\n\n-- Display the most popular TV Shows\nSELECT Title, max(\"Days In Top 10\") from netflix\nwhere Type='Movie'\nGROUP BY Title\nORDER BY max(\"Days In Top 10\") desc\nlimit 5;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502             Title              \u2502 max(\"Days In Top 10\") \u2502\n\u2502            varchar             \u2502         int64         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 The Mitchells vs. The Machines \u2502                    31 \u2502\n\u2502 How the Grinch Stole Christmas \u2502                    29 \u2502\n\u2502 Vivo                           \u2502                    29 \u2502\n\u2502 365 Days                       \u2502                    28 \u2502\n\u2502 Despicable Me 2                \u2502                    27 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n-- Copy the result to CSV\nCOPY (\nSELECT Title, max(\"Days In Top 10\") from netflix\nwhere Type='TV Show'\nGROUP BY Title\nORDER BY max(\"Days In Top 10\") desc\nlimit 5\n) TO 'output.csv' (HEADER, DELIMITER ',');\n```\n\nWhat\u2019s fun is that for both Movies and TV shows, the top 5 mostly include kids show. We all know that kids doesn\u2019t bother to see multiple time the same thing\u2026\n\nDuckDB and MotherDuck also support accessing private S3 buckets by using [`CREATE SECRET`](https://motherduck.com/docs/integrations/cloud-storage/amazon-s3/) to specify and store your credentials.\n\n## Exploring Beyond the Pond\n\nThat\u2019s it for this tutorial! If you're interested in delving deeper into DuckDB, check out these resources:\n\n- The official DuckDB docs : [https://duckdb.org/](https://duckdb.org/)\n- The DuckDB discord : [https://discord.com/invite/tcvwpjfnZx](https://discord.com/invite/tcvwpjfnZx)\n\nTo elevate your experience with DuckDB and scale it with a cloud data warehouse, explore [MotherDuck](https://motherduck.com/product/)! Dive into our [end-to-end tutorial](https://motherduck.com/docs/getting-started/e2e-tutorial) to discover the user-friendly web interface, AI-based SQL query fixing, global and organization-wide data sharing capabilities, and more.\n\nAdditionally, stay tuned to our [monthly newsletter](https://motherduck.com/duckdb-news/) and [YouTube channel](https://youtube.com/@motherduckdb/), where we'll continue to share more DuckDB-related content!\n\nKeep quacking, keep coding.\n\n!['DuckDB In Action' book cover](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduckdb-book-full-cover.68e4f598.png&w=3840&q=75)\n\nGet your free book!\n\nE-mail\n\nSubscribe to other MotherDuck news\n\nSubmit\n\nFree Book!\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![This Month in the DuckDB Ecosystem: March 2023](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fduckdb_ecosystem_monthly_feb_2023_352e669717.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-four/)\n\n[2023/03/23 - Marcos Ortiz](https://motherduck.com/blog/duckdb-ecosystem-newsletter-four/)\n\n### [This Month in the DuckDB Ecosystem: March 2023](https://motherduck.com/blog/duckdb-ecosystem-newsletter-four)\n\nThis month in the DuckDB Ecosystem, by Marcos Ortiz. Includes featured community member Elliana May, Python ecosystem, top links, upcoming events and more.\n\n[![This Month in the DuckDB Ecosystem: April 2023](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fduckdb_ecosystem_monthly_april_2023_bb2015c778.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-five/)\n\n[2023/04/17 - Marcos Ortiz](https://motherduck.com/blog/duckdb-ecosystem-newsletter-five/)\n\n### [This Month in the DuckDB Ecosystem: April 2023](https://motherduck.com/blog/duckdb-ecosystem-newsletter-five)\n\nThis month in the DuckDB Ecosystem, by Marcos Ortiz. Latest updates, including featured community member Josh Wills, upcoming events like webinars and top links.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 465,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 465
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md",
  [2m  â”‚[0m     "content": "---\ntitle: python-duckdb-vs-dataframe-libraries\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries\nindexed_at: '2025-11-25T19:56:18.692453'\ncontent_hash: a69eeb7d9fe08a97\nhas_code_examples: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Why Python Developers Need DuckDB (And Not Just Another DataFrame Library)\n\n2025/10/08 - 6 min read\n\nBY\n\n[Mehdi Ouazza](https://motherduck.com/authors/mehdi-ouazza/)\n\nIf you're working with Python and building data pipelines, you've probably used pandas or Polars. They're great, right? But here's the thing - DuckDB is different, and not just because it's faster.\n\nIt's an in-process database that you can literally `pip install duckdb` and start using immediately. So what does a database bring to the table that your DataFrame library doesn't?\n\nLet's talk about **6 pragmatic reasons** why DuckDB might become your new best friend or pet.\n\nBut first, a quick history lesson on why dataframe became so popular and what they are missing today.\n\nDuckDB for Python Devs: 6 Reasons It Beats DataFrames - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[DuckDB for Python Devs: 6 Reasons It Beats DataFrames](https://www.youtube.com/watch?v=XRhw4B8Esms)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nFull screen is unavailable. [Learn More](https://support.google.com/youtube/answer/6276924)\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Why am I seeing this?](https://support.google.com/youtube/answer/9004474?hl=en)\n\n[Watch on](https://www.youtube.com/watch?v=XRhw4B8Esms&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 6:26\n\n\u2022Live\n\n\u2022\n\n## THE DATAFRAME ERA\n\nBack in the 2000s, if you wanted to do analytics, you'd install Oracle or SQL Server. Expensive licenses, complex setup, DBAs to manage connections... it was a nightmare for quick analysis.\n\nThen Python exploded in popularity. Pandas came along and changed everything. Suddenly you could:\n\n- `pip install pandas`\n- Write a few lines of code\n- Get immediate results\n\nNo DBA, no licenses, no infrastructure headaches. Just pure analysis in a Python process. Beautiful, right?\n\n## THE PROBLEM\n\nHere's where things get messy. We've pushed DataFrames way beyond their original design. They were built for:\n\n- Quick experimentation\n- In-memory computation\n- One-off analysis\n\nAnd they are still great for this use case.\n\nBut DataFrame libraries give you one slice of what a database does, and then you end up stiching together a bunch of other Python libraries to fill the gaps. It works... but it's fragile.\n\nSo what if you could get the simplicity of DataFrames with the power of a real database? That's DuckDB.\n\n## REASON 1: ACID TRANSACTIONS\n\nLet's start with the obvious - **it's an actual database**. That means ACID transactions.\n\n```sql\nCopy code\n\nBEGIN TRANSACTION;\n  CREATE TABLE staging AS SELECT * FROM source;\n  INSERT INTO prod SELECT * FROM staging WHERE valid = true;\nCOMMIT;\n```\n\nIf anything fails into this pipeline? Automatic rollback. Your data stays intact. No more corrupted parquet files because your pipeline crashed halfway through a write.\n\nWe've all been there - you're writing to a CSV or parquet file, something breaks, and now you've got half-written garbage data. With DuckDB, that's not a problem because, there's an actual file format from DuckDB aside from the supports to read/write to classic json,csv,parquet.\n\nINFO: ACID transactions ? Quick recap **ACID transactions** are database guarantees that keep data reliable. They make sure updates happen **all or nothing** (Atomicity), follow the rules (Consistency), don\u2019t interfere with each other (Isolation), and stay permanent once confirmed (Durability). Unlike Pandas or Polars, which don\u2019t provide full ACID guarantees, databases ensure every change is complete, consistent, and durable. This matters because it keeps your analysis safe from half-finished updates or conflicting edits, so the numbers you see truly reflect reality.\n\n## REASON 2: ACTUAL DATA PERSISTENCE\n\nSecond point - DuckDB has its own database file format.\n\n```python\nCopy code\n\nimport duckdb\nconn = duckdb.connect('my_analytics.db')\n```\n\nWhen you create a DuckDB connection - you just provide a path to a file and that's it. Everything you create is persisted in that file. It's a one single database file that contains Real schemas, metadata, ACID guarantees - all in one portable file.\n\nYou know that mess where you've got CSV files scattered everywhere, some parquet files over there, JSON from an API somewhere else? Yeah, that. With DuckDB, you can consolidate everything into a single database file with proper schemas and relationships.\n\n![screenshot](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_10_07_at_11_12_26_AM_d26cc3e905.png&w=3840&q=75)_Every analytics project - [source](https://youtu.be/hoyQnP8CiXE?t=397)_\n\n## REASON 3: BATTERIES INCLUDED\n\nThird - DuckDB has a [**built-in ecosystem** of features](https://duckdb.org/community_extensions/).\n\nWith DataFrames, you need different Python packages for everything:\n\n- S3 access? Install `boto3`\n- Parquet files? Install `pyarrow`\n- PostgreSQL? Install `psycopg2`\n\nWelcome to dependency hell! Good luck when one of those updates breaks everything.\n\nDuckDB's extensions are built in C++ (so lightweight footprint!), maintained by the core team, and just work. Watch this:\n\n```python\nCopy code\n\nimport duckdb\nconn = duckdb.connect()\n# Read from public AWS S3 - one line, no setup\nconn.sql(\"SELECT * FROM 's3://bucket/data.parquet'\")\n\n# Connect to Postgres\nconn.sql(\"ATTACH 'postgresql://user:pass@host/db' AS pg\")\nconn.sql(\"SELECT * FROM pg.my_pg_table\")\n```\n\nBehind the scenes, DuckDB loads the core extensions automatically. No configuration, no dependency management. It just works.\n\n![battery](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_ecosystem_e480dd6b85.png&w=3840&q=75)_DuckDB ecosystem_\n\n## REASON 4: NOT JUST FOR PYTHON\n\nHere's something important for Python users - **DuckDB isn't locked into Python**.\n\nYes, you can hang out with your Javascript friends. Or whatever your friends use.\n\nYou process data in Python, sure. But eventually you need to serve it somewhere - maybe a web app, a dashboard, whatever.\n\nBecause DuckDB is in-process, it can run anywhere:\n\n- [JavaScript in the browser (via WebAssembly)](https://motherduck.com/blog/duckdb-wasm-in-browser/)\n- Java backend services\n- Rust applications\n- Even the command line\n\nAnd here's the cool part - they can all read the same DuckDB file format. Everyone speaks SQL, and you can even offload compute to the client side if needed.\n\nYour Python pipeline creates the database, and your JavaScript frontend queries it directly.\n\nEasy peasy\n\n## REASON 5: SQL AS A FEATURE\n\nI know some of you are thinking \"but DataFrames look cleaner!\"\n\nLook, this is partly syntax preference and debate.\n\nBut SQL is **universal**. Your data analyst knows it. Your backend engineer knows it. Your future self will thank you when you come back to this code in six months.\n\nPlus, DuckDB has \"friendly SQL\" that makes common tasks ridiculously easy:\n\n```sql\nCopy code\n\n-- Exclude specific columns\nSELECT * EXCLUDE (password, ssn) FROM users;\n\n-- Select columns by pattern\nSELECT COLUMNS('sales_*') FROM revenue;\n\n-- Built-in functions for everything\nSELECT * FROM read_json_auto('api_response.json');\n```\n\nCheck the [DuckDB docs](https://duckdb.org/docs/stable/sql/dialect/friendly_sql.html#:~:text=Friendly%20SQL%20%E2%80%93%20DuckDB&text=DuckDB%20offers%20several%20advanced%20SQL,(currently)%20exclusive%20to%20DuckDB.) for the full list of friendly SQL features\n\n## REASON 6: SCALE TO THE CLOUD\n\nBecause DuckDB can run anywhere, **scaling to the cloud is trivial**.\n\nWith MotherDuck (DuckDB in the cloud), moving your workflow requires literally one line:\n\n```python\nCopy code\n\nimport duckdb\n\n# Local\nconn = duckdb.connect('local.db')\n\n# Cloud - same code, one extra line\nconn = duckdb.connect('md:my_database?motherduck_token=...')\n\n# That's it. Same queries, now running in the cloud.\nconn.sql(\"SELECT * FROM 's3://bucket/data.parquet'\")\n```\n\nYour code doesn't change. Your SQL doesn't change. You just get cloud scale when you need it.\n\n## GETTING STARTED\n\nHere's the best part - you can **start today without rewriting everything.**\n\nThanks to Apache Arrow, DuckDB has zero-copy integration with [pandas](https://duckdb.org/docs/stable/guides/python/import_pandas) and [Polars](https://duckdb.org/docs/stable/guides/python/polars.html):\n\n```python\nCopy code\n\nimport pandas as pd\nimport duckdb\n\ndf = pd.read_csv('data.csv')\n\n# Query your DataFrame directly with SQL and export back as a dataframe\nresult = duckdb.sql(\"\"\"\n    SELECT category, AVG(price)\n    FROM df\n    GROUP BY category\n\"\"\").df()\n```\n\nNo conversion overhead. Start small, refactor what makes sense, and gradually adopt more DuckDB features!\n\nSo yeah, DuckDB is way more than just another DataFrame library. It's **a full database** that's as easy to use as pandas, but with actual database features when you need them.\n\n### TABLE OF CONTENTS\n\n[THE DATAFRAME ERA](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/#the-dataframe-era)\n\n[THE PROBLEM](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/#the-problem)\n\n[REASON 1: ACID TRANSACTIONS](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/#reason-1-acid-transactions)\n\n[REASON 2: ACTUAL DATA PERSISTENCE](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/#reason-2-actual-data-persistence)\n\n[REASON 3: BATTERIES INCLUDED](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/#reason-3-batteries-included)\n\n[REASON 4: NOT JUST FOR PYTHON](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/#reason-4-not-just-for-python)\n\n[REASON 5: SQL AS A FEATURE](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/#reason-5-sql-as-a-feature)\n\n[REASON 6: SCALE TO THE CLOUD](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/#reason-6-scale-to-the-cloud)\n\n[GETTING STARTED](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/#getting-started)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![MotherDuck is Landing in Europe! Announcing our EU Region](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Feu_launch_blog_b165ff2751.png&w=3840&q=75)](https://motherduck.com/blog/motherduck-in-europe/)\n\n[2025/09/24 - Garrett O'Brien, Sheila Sitaram](https://motherduck.com/blog/motherduck-in-europe/)\n\n### [MotherDuck is Landing in Europe! Announcing our EU Region](https://motherduck.com/blog/motherduck-in-europe)\n\nServerless analytics built on DuckDB, running entirely in the EU.\n\n[![DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_4_1_b6209aca06.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n[2025/10/09 - Alex Monahan, Garrett O'Brien](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n### [DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/blog/announcing-duckdb-141-motherduck)\n\nMotherDuck now supports DuckDB 1.4.1 and DuckLake 0.3, with new SQL syntax, faster sorting, Iceberg interoperability, and more. Read on for the highlights from these major releases.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 336,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 336
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search for one more file specifically about data loading efficiency:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ load.*data|insert.*data|COPY.*INTO
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 10,
  [2m  â”‚[0m   "appliedLimit": 10
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! Let me read the file about efficiently loading data:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     "content": "---\ntitle: how-to-efficiently-load-data-into-ducklake-with-estuary\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary\nindexed_at: '2025-11-25T20:44:50.287804'\ncontent_hash: 54211f956cd60bed\nhas_step_by_step: true\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nHow to Efficiently Load Data into DuckLake with Estuary - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[How to Efficiently Load Data into DuckLake with Estuary](https://www.youtube.com/watch?v=8uce9V9VnjY)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Why am I seeing this?](https://support.google.com/youtube/answer/9004474?hl=en)\n\n[Watch on](https://www.youtube.com/watch?v=8uce9V9VnjY&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 50:08\n\n\u2022Live\n\n\u2022\n\nYouTube\n\n# How to Efficiently Load Data into DuckLake with Estuary\n\n2025/07/26\n\n## Introduction to DuckLake and Real-Time Data Integration\n\nDuckLake represents a new open table format created by the developers of DuckDB. Unlike traditional data lake formats that store metadata in JSON or Avro files within blob storage, DuckLake takes a different approach by storing metadata in a relational database. This architectural decision significantly speeds up metadata transactions, making operations faster and more efficient.\n\n## Understanding MotherDuck's Cloud Data Warehouse\n\nMotherDuck is a cloud data warehouse designed to make big data feel small. Built on top of DuckDB, it focuses on three key principles:\n\n- **Eliminating complexity** from traditional distributed data warehouses\n- **Enabling fast insights** while keeping developers in their workflow\n- **Leveraging DuckDB's performance** for cost-efficient operations\n\nThe platform integrates seamlessly with existing data stack tools and supports standard SQL queries with enhanced features.\n\n## Estuary's Real-Time Data Integration Platform\n\nEstuary provides a real-time data integration platform that supports both streaming and batch data movement. The platform features:\n\n### Architecture Components\n\n- **Capture connectors** that extract data from source systems using change data capture (CDC)\n- **Collections** stored in object storage (S3, GCS, or compatible systems)\n- **Materialization connectors** that load data into destinations\n\n### Key Capabilities\n\n- Support for hundreds of source and destination systems\n- Native CDC for databases like PostgreSQL, MySQL, and Oracle\n- Real-time extraction from SaaS applications like Salesforce and HubSpot\n- No-code setup with managed service\n\n## Setting Up a Real-Time Pipeline to DuckLake\n\nThe process of loading data into DuckLake involves several straightforward steps:\n\n### Source Configuration\n\n1. Connect to your source database (PostgreSQL, MySQL, etc.)\n2. Enable change data capture to track real-time changes\n3. Configure schema evolution settings for automatic handling of schema changes\n\n### DuckLake Setup\n\n1. Create a DuckLake database in MotherDuck\n2. Configure access to your S3 bucket where data will be stored\n3. Set up appropriate access tokens for read/write permissions\n\n### Pipeline Configuration\n\n- Choose sync frequency (from real-time to scheduled batches)\n- Select specific fields to materialize\n- Configure merge queries for maintaining latest state\n\n## Performance Optimization with MotherDuck\n\n### Instant SQL Feature\n\nMotherDuck introduces Instant SQL, which provides query results at the speed of typing by:\n\n- Pre-caching data for immediate feedback\n- Validating SQL syntax in real-time\n- Enabling rapid iteration on complex queries\n\n### Storage Trade-offs\n\nWhen deciding between DuckLake and MotherDuck native storage:\n\n**DuckLake advantages:**\n\n- Open format with broader ecosystem compatibility\n- Support for Spark and other compute engines\n- Better suited for petabyte-scale workloads\n\n**MotherDuck storage advantages:**\n\n- 2-10x faster query performance\n- Optimized for read/write throughput\n- Better caching and regional performance\n\n## Scaling Considerations\n\nMotherDuck now offers larger instance sizes (Mega and Giga) to support intensive data lake operations. These instances are comparable to Snowflake 3XL configurations and enable:\n\n- Terabyte to petabyte-scale operations\n- Complex aggregations and sorting\n- Efficient medallion architecture implementations\n\n## Best Practices for Implementation\n\n### Data Architecture\n\n- Keep raw data in DuckLake for openness and flexibility\n- Move silver/gold layer data to MotherDuck storage for performance\n- Use Estuary's intermediate storage for reliability and replay capabilities\n\n### Partitioning Strategy\n\nWhile Estuary doesn't natively configure partitions, you can:\n\n1. Allow Estuary to create initial tables\n2. Use `ALTER TABLE` commands to add partitions\n3. Subsequent writes will respect partition configuration\n\n### Error Handling and Reliability\n\nThe architecture prevents common streaming issues:\n\n- Intermediate storage prevents message loss\n- Automatic handling of destination unavailability\n- Support for backfills without re-querying sources\n\n## Integration with Modern Data Stack\n\nThe combination of Estuary and MotherDuck integrates with existing tools:\n\n- dbt support through DuckDB adapter\n- AI-powered SQL writing assistance\n- Automatic error detection and fixing\n- Support for multiple materialization targets from single source\n\nThis architecture enables organizations to implement real-time data pipelines without the traditional complexity of streaming systems, making the difference between batch and streaming simply a configuration toggle rather than an architectural decision.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Data-based: Going Beyond the Dataframe\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FData_based_f32745b461.png&w=3840&q=75)](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n[2025-11-20](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n### [Data-based: Going Beyond the Dataframe](https://motherduck.com/videos/going-beyond-the-dataframe)\n\nLearn how to turbocharge your Python data work using DuckDB and MotherDuck with Pandas. We walk through performance comparisons, exploratory data analysis on bigger datasets, and an end-to-end ML feature engineering pipeline.\n\nWebinar\n\nPython\n\nAI, ML and LLMs\n\n[![\"Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FHex_Webinar_778e3959e4.png&w=3840&q=75)](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n[2025-11-14](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n### [Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck)\n\nAI isn't here to replace data work, it's here to make it better. Watch this webinar to see how Hex and MotherDuck build AI workflows that prioritize context, iteration, and real-world impact.\n\nWebinar\n\nAI, ML and LLMs\n\n[![\"Lies, Damn Lies, and Benchmarks\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FLies_Damn_Lies_and_Benchmarks_Thumbnail_404db1bf46.png&w=3840&q=75)](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n[2025-10-31](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n### [Lies, Damn Lies, and Benchmarks](https://motherduck.com/videos/lies-damn-lies-and-benchmarks)\n\nWhy do database benchmarks so often mislead? MotherDuck CEO Jordan Tigani discusses the pitfalls of performance benchmarking, lessons from BigQuery, and why your own workload is the only benchmark that truly matters.\n\nStream\n\nInterview\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 247,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 247
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now I'll write the comprehensive answer to the file:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_1.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "create",
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_1.md",
  [2m  â”‚[0m   "content": "<4602 chars>",
  [2m  â”‚[0m   "structuredPatch": []
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! I've completed the research task and written the answer to `/tmp/answer_without_kg_1.md`.
  [94mâ”‚[0m 
  [94mâ”‚[0m ## Summary
  [94mâ”‚[0m 
  [94mâ”‚[0m I searched through the .kurt/sources/ directory and found comprehensive information about file formats for loading data into MotherDuck. After reading 6 key source files, I determined that:
  [94mâ”‚[0m 
  [94mâ”‚[0m **Parquet is the most efficient file format for loading data into MotherDuck** because:
  [94mâ”‚[0m - It uses compressed columnar storage, matching DuckDB's internal architecture
  [94mâ”‚[0m - It provides the best performance for analytical aggregations
  [94mâ”‚[0m - DuckDB can query Parquet files directly without importing
  [94mâ”‚[0m - It offers significant compression and I/O advantages over text-based formats like CSV
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer also covers other formats (native DuckDB files, CSV) and includes best practices for data loading. All sources are properly cited with their exact local file paths as requested.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 3,930 | Cost: $0.4198
     Cumulative: 3,930 tokens | $0.4198
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent is summarizing the results of its research and has indicated that the task is complete, rather than asking for further input or clarification.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 21 | Duration: 114.6s
â•‘    Tokens: 3,930 | Cost: $0.4198
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  LLM Judge score: 1.00

======================================================================
â“ Question 2/10
...How does MotherDuck integrate with DuckDB?
======================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 2
â”‚ Answer this question by searching through the .kurt/sources/ directory:

...How does MotherDuck integrate with DuckDB?

Required Steps:
1. Search for relevant markdown files in .kurt/sources/ using glob and grep
2. Read the most relevant files (aim for 3-5 most relevant sources)
3. As you read each file, keep track of:
   - The file path
   - The file title/name
   - Key information extracted from that file
4. Synthesize a comprehensive answer based on the information found
5. Write your answer to: /tmp/answer_without_kg_2.md

IMPORTANT - Your answer MUST have this structure:
```markdown
# Answer

[Your comprehensive answer here, drawing from the sources you read]

## Sources

List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
[etc.]

DO NOT use URLs or document titles as links. Show the actual file path you read with the Read tool.
```

The Sources section is MANDATORY - you must list every file you read to answer the question.
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I'll help you search through the .kurt/sources/ directory to find information about how MotherDuck integrates with DuckDB.
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Glob â†’ **/*.md

  [96mğŸ”§ TOOL:[0m Grep â†’ MotherDuck.*DuckDB|DuckDB.*MotherDuck|integration|integrate
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/about-us.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/acceptable-use-policy.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/adithya-krishnan.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/aditya-aomani.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/alex-monahan.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/antony-courtney.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/amrutha-gujjar.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/anna-geller.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/doug-raymond.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/brett-griffin.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/daniel-palma.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/andrew-mcewen.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/christophe-oudar.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/andrew-witten.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/emily-lucek.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/diptanu-gon-choudhury.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/faraz-hameed.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/boaz-leskes.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/manveer-chawla.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/hamilton-ulmer.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/joseph-hwang.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/luciano-galvao-filho.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/leslie-thomas.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/garrett-obrien.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/jordan-tigani.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/graham-wetzler.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/jacob-matson.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/jessica-libman.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/jelte-fennema-nio.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/peter-boncz.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/nick-ursa.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/nouras-haddad.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/mehdi-ouazza.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/marcos-ortiz.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/mike-xu.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/marc-d.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/michael-hunger.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/mike-spaulding.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/oli-dinov.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/simon-spati.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/thomas-tfmv-mcgeehan.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/stephanie-wang.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/ryan-boyd.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/tino-tereshko.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/sung-won-chung.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/sean-m-tracey.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/saqib-rasul.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/sahil-gupta.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/pranav-aurora.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/sheila-sitaram.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/till-dohmen.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/analyze-json-data-using-sql.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/analyze-sqlite-databases-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/MotherDuck-Visualize-Embeddings-Marimo.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/analyze-data-in-azure-with-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/yannick-welsch.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/zhenni-wu.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/yves-le-maout.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/vasilije-markovic.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-duckdb-snippet-sets-with-motherduck-sharing-databases.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/analyze-x-data-nodejs-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-mega-giga-instance-sizes-huge-scale.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/big-data-is-dead.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-series-seed-and-a.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/bigquery-to-duckdb-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/building-motherduck-partner-ecosystem.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-small-data-sf-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/building-data-applications-with-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-engineers-answer-10-top-reddit-questions.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-engineering-toolkit-infrastructure-devops.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-engineer-highlights-PyConDE-2023.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-app-generator.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-engineering-toolkit-essential-tools.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-cognee-sql-analytics-graph-rag.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/differential-storage-building-block-for-data-warehouse.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/dosomething-motherduck-data-warehouse-ROI.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-110-hidden-gems.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-dec-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-oct-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-nov-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2023.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-four.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "durationMs": 10,
  [2m  â”‚[0m   "numFiles": 100,
  [2m  â”‚[0m   "truncated": true
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/scaling-duckdb-panel-ondemand.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/take-flight-with-the-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/data-discoverability-secoda-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-power-of-wasm-for-analytics-duckdb-in-the-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-surprising-birth-of-duckdb-ft-co-creator-hannes-muhleisen.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search more specifically for content about MotherDuck and DuckDB integration:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ how.*work|architecture|hybrid|local.*cloud|connects|connection

  [96mğŸ”§ TOOL:[0m Grep â†’ motherduck.*works|what is motherduck|motherduck overview
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/more-than-a-vibe-ai-driven-sql-that-actually-works.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-use.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/quacking-around-the-web.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/react-components-testing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/ducklake-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/index.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/rill-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/streamlit.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/tableau.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/scaling-duckdb-panel-ondemand.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-power-of-wasm-for-analytics-duckdb-in-the-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-if-sql-queries-returned-results-instantly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/speed-querying-stackoverflow-data-with-duckdb-ft-michael-hunger.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-bad-the-good-and-the-ugly-navigating-the-new-waves-of-machine-learning-ft-adi-polak.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-future-of-bi-exploring-the-impact-of-bi-as-code-tools-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/moving-forward-from-ad-hoc-reports-with-duckdb-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/leveraging-duckdb-and-delta-lake-together.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/machine-learning-and-ai-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/more-than-a-vibe-ai-driven-sql-that-actually-works.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/getting-started-with-motherduck-webinar.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/going-beyond-the-dataframe.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/is-bi-too-big-for-small-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-vs-pandas-vs-polars-for-python-devs.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 30,
  [2m  â”‚[0m   "appliedLimit": 30
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Great! Let me read the most relevant files. I'll start with the hybrid analytics guide, the getting started guide, and some key blog posts:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/getting-started-with-motherduck-webinar.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/index.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     "content": "---\ntitle: hybrid-analytics-guide\ncontent_type: tutorial\nsource_url: https://motherduck.com/learn-more/hybrid-analytics-guide\nindexed_at: '2025-11-25T09:57:23.352209'\ncontent_hash: 91344eaa7ee54322\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO LEARN](https://motherduck.com/learn-more/)\n\n# Hybrid Analytics: Query Local & Cloud Data Instantly\n\n12 min readBY\n\n[Aditya Somani](https://motherduck.com/authors/aditya-aomani/)\n\n![Hybrid Analytics: Query Local & Cloud Data Instantly](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FZero_latency_e8da68f055.png&w=3840&q=75)\n\nAre your analytics queries timing out in Postgres? Does your laptop grind to a halt when you try to analyze a large CSV file? If you feel caught between the memory limits of local tools and the spiraling costs of cloud data warehouses, you're not alone. The traditional divide between local development and cloud-scale analytics creates friction, slows down projects, and drains budgets.\n\nBut there is a modern, more efficient way. Imagine running complex SQL queries on a 20 GB Parquet file directly on your laptop, without a flicker of memory strain. Picture joining a local spreadsheet with a massive table in Amazon S3 using a single, elegant command. This is the reality of the **hybrid analytics workflow**, a powerful approach that unifies your local machine and the cloud into a single, unified data environment.\n\nThis guide is your definitive resource for understanding and implementing this new workflow. We'll break down the core concepts, provide practical examples, and show you how to solve the most common data bottlenecks for good. You will learn why traditional databases fail for analytics, how to analyze datasets larger than your RAM, and how the power of \"Dual Execution\" lets you instantly join local files with cloud data, all without the cost and complexity of managing a server cluster.\n\n## Why Are My Analytics Queries on Postgres & MySQL Timing Out?\n\nIf your analytical queries on a database like PostgreSQL or MySQL are consistently slow or timing out, you've hit a fundamental architectural limit. These databases are masterpieces of engineering for **[Online Transaction Processing (OLTP)](https://estuary.dev/blog/postgres-to-motherduck/)**. These are the small, fast operations that power applications, like creating a user or updating an order. However, this same design becomes a major bottleneck for **[Online Analytical Processing (OLAP)](https://en.wikipedia.org/wiki/Online_analytical_processing)**, which involves complex queries that scan and aggregate vast amounts of data.\n\nThe root of the problem is their **[row-oriented](https://en.wikipedia.org/wiki/Data_orientation)** storage model. A row-store keeps all data for a single record together on disk, which is efficient for retrieving an entire user profile. But for an analytical query that only needs to sum the `sale_amount` column across millions of rows, a row-store is forced to read every single column for every single row. This wastes an enormous amount of I/O and CPU cycles on data it doesn't need, which is the primary reason your dashboards are slow and your queries fail.\n\nThe next logical step is to move analytical workloads to a system built on a **[columnar architecture](https://motherduck.com/learn-more/columnar-storage-guide/)**. A columnar database organizes data by column, storing all values for `sale_amount` together. When your query asks for the sum of `sale_amount`, the database reads _only_ that column, greatly reducing I/O and speeding up performance by orders of magnitude. Modern analytical engines like DuckDB couple this with **[vectorized query execution](https://15721.courses.cs.cmu.edu/spring2024/notes/06-vectorization.pdf)**, a technique that processes data in large batches or \"vectors\" instead of row-by-row. This method fully utilizes modern CPU capabilities to perform a single instruction on multiple data points simultaneously, leading to large gains in computational efficiency.\n\n| Architecture | Row-Oriented (OLTP) | Column-Oriented (OLAP) |\n| --- | --- | --- |\n| **Primary Use Case** | Application backends, frequent small updates. | Business intelligence, data analytics, large scans. |\n| **Data Storage** | All data for a single record is stored together. | All data for a single column is stored together. |\n| **Query Example** | `SELECT * FROM users WHERE user_id = 123;` (Fast) | `SELECT SUM(sale_amount) FROM sales;` (Slow) |\n| **Best For** | Fast writes and single-record lookups. | Fast aggregations and filtering on specific columns. |\n\nBy offloading analytics to a purpose-built columnar engine, you let your transactional database continue to excel at what it does best while your analytical queries run in a [high-speed, optimized environment](https://motherduck.com/blog/postgres-duckdb-options/).\n\n## How Can I Analyze a 20 GB Parquet File on My Laptop Without It Crashing?\n\nThe second major bottleneck for data professionals is local memory. Trying to load a 20 GB file into a traditional in-memory library like Pandas on a 16 GB laptop will almost certainly result in a `MemoryError`. This happens because these tools must load the entire dataset into your computer's RAM before they can begin processing.\n\nThe modern solution is **out-of-core processing**, a strategy where data is processed in manageable chunks directly from disk without ever being fully loaded into RAM. This is the key feature of engines like DuckDB, which uses a [streaming execution model to handle datasets far larger than your available memory](https://duckdb.org/2024/07/09/memory-management.html). If a complex operation requires more memory than is available (by default, 80% of your system's RAM), DuckDB can \"spill\" intermediate results to a temporary file on disk, ensuring your query completes without crashing.\n\nThis capability turns your laptop into a surprisingly powerful analytics workstation. With an out-of-core engine, analyzing that 20 GB file becomes straightforward. You can simply use SQL to query it directly from a Python script or Jupyter notebook.\n\n```python\nCopy code\n\nimport duckdb\n\n# This query runs out-of-core, never loading the full 20GB file into RAM.\n# DuckDB's streaming engine processes the file in chunks.\nresult = duckdb.sql(\"\"\"\n  SELECT\n      product_category,\n      AVG(sale_price) as avg_price\n  FROM read_parquet('large_sales_dataset.parquet')\n  WHERE region = 'North America'\n  GROUP BY product_category\n  ORDER BY avg_price DESC;\n\"\"\").arrow()\n\nprint(result)\n```\n\nThis simple, powerful approach allows you to perform heavy-duty data analysis on your local machine, iterating quickly without waiting for a cloud cluster to spin up or worrying about memory crashes.\n\n## How Can I Join Local CSVs with Cloud Data in a Single Query?\n\nThis is where the hybrid workflow really shows its strength. Joining a local CSV of fresh sales data with a historical customer table in an S3 bucket has traditionally been a major challenge. The problem is \"data gravity,\" since it's [slow and expensive to move large amounts of data](https://motherduck.com/learn-more/no-etl-query-raw-files/). A naive query would have to either upload your entire local file to the cloud or download the entire cloud table to your laptop, both of which are very inefficient.\n\nMotherDuck solves this with an innovative query planner called **Dual Execution**. It treats your laptop and the MotherDuck cloud as two nodes in a single distributed system. When you run a hybrid query, the planner intelligently breaks it down and pushes computation to where the data lives, minimizing data transfer.\n\n> \"The most innovative feature of MotherDuck's architecture is its dual execution model. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\" - [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained/)\n\nImagine you want to join a local CSV of new product pricing with a massive sales table in MotherDuck, but only for a specific product. Instead of moving entire tables, the Dual Execution planner does the following:\n\n1. **Local Scan:** It scans the small pricing CSV on your laptop.\n2. **Filter Locally:** It applies the filter for the specific product on your machine.\n3. **Transfer Minimal Data:** It sends only the single, filtered pricing row (a few bytes of data) to the cloud.\n4. **Join in the Cloud:** It performs the final join against the massive sales table in the MotherDuck cloud.\n\nThis process can reduce network traffic by orders of magnitude compared to traditional methods, turning a difficult query into an interactive one. You can see this in action with a simple `EXPLAIN` statement, which [shows which parts of the query run locally `(L)` and which run remotely `(R)`](https://motherduck.com/docs/key-tasks/running-hybrid-queries/).\n\n## Why Are Data Engineers So Excited About Dual-Execution Engines?\n\nData engineers are excited because dual-execution engines solve one of their biggest challenges: the \"it worked on my machine\" problem. In traditional setups, the tools and data used for local development are often completely different from the production cloud environment, leading to bugs and deployment failures.\n\nMotherDuck eliminates this by using the **exact same DuckDB engine** both locally and in the cloud. A query that works on your laptop is guaranteed to work in the cloud, creating a direct path from development to production. This greatly improves developer experience (DX) and accelerates iteration cycles.\n\nThis architecture provides three key benefits:\n\n1. **Faster Development:** Engineers can build and test pipelines with the zero-latency feedback of local development before scaling to the cloud.\n2. **Lower Costs:** By using the free, powerful compute on users' laptops and minimizing data transfer, this model significantly reduces cloud bills.\n3. **Better Collaboration:** It transforms the traditionally \"single-player\" DuckDB into a [\"multiplayer\" platform](https://motherduck.com/videos/339/bringing-duckdb-to-the-cloud-dual-execution-explained/) where teams can share databases and work from a single source of truth.\n\nThis powerful combination of local speed and cloud scale is backed by real-world success.\n\n> **[Case Study: Finqore's 60x Pipeline Acceleration](https://motherduck.com/case-studies/)**\n> Finqore, a financial technology company, was struggling with an 8-hour data pipeline built on Postgres. By migrating to a hybrid workflow with MotherDuck, they were able to use DuckDB's performance and MotherDuck's serverless scale to transform that pipeline into an **8-minute workflow**, a 60x improvement that unlocked real-time data exploration for their team.\n\n## How Can I Run SQL Analytics Without Managing Clusters?\n\nThe operational overhead of provisioning, scaling, and maintaining server clusters is a major drain on data teams. **[Serverless SQL analytics](https://motherduck.com/docs/concepts/architecture-and-capabilities/)** removes this burden entirely. Instead of managing infrastructure, you simply write and run your queries, and the platform handles the rest.\n\nThis model is not only simpler but also much more cost-effective for the \"spiky\" workloads typical of analytics. You pay only for the compute you actually use, avoiding the massive costs of idle clusters.\n\n| Feature / Scenario | Traditional Cloud Warehouse (e.g., Snowflake/BigQuery) | Hybrid Approach (Local + Serverless) |\n| --- | --- | --- |\n| **Development & Prototyping** | Billed for active compute time, even for small test queries. | **Free.** Uses local machine resources with no cloud costs. |\n| **Pricing Model** | Complex credits or per-TB scanned, which is hard to predict. | Simple, usage-based billing; pay only for queries you run. |\n| **Idle Compute** | Billed for provisioned clusters, even when idle, which can create a costly [idle tax on short-running queries](https://motherduck.com/learn-more/reduce-snowflake-costs-duckdb). | **No cost.** Serverless architecture has no idle compute. |\n| **Hardware Requirement** | Entirely reliant on expensive, provisioned cloud hardware. | Leverages powerful, existing local hardware (laptops, servers). |\n\nBy adopting a serverless, hybrid approach, you can reduce your cloud data warehouse costs while empowering your team with a faster, more flexible workflow.\n\n## What Lightweight SQL Solutions Can Be Integrated with Jupyter Notebooks?\n\nFor data scientists who work primarily in Jupyter, several excellent tools bring the power of SQL directly into the notebook environment. The most powerful option for serious analytics is the **DuckDB engine** itself. It can query Pandas DataFrames, Arrow tables, and large local files directly with full SQL support. Its performance on analytical queries is excellent in the embedded space.\n\nFor comparison, while Python's standard library includes **SQLite**, it's a row-based OLTP engine and is significantly slower than DuckDB for analytical queries. An independent benchmark found DuckDB to be **[12-35 times faster](https://motherduck.com/learn-more/duckdb-vs-sqlite-databases/)** for aggregations on a 100-million-row dataset. For any analytical task in a notebook, DuckDB is the clear choice for performance and scalability.\n\nAnother popular tool is **[JupySQL](https://jupysql.ploomber.io/)**, which provides convenient SQL \"magics\" (`%sql`, `%%sql`) that turn a notebook cell into a SQL editor. It connects to various backends, including DuckDB, making it great for quick, ad-hoc exploration.\n\n## How Can I Keep Queries on My Laptop in Sync With the Cloud?\n\nThe key to keeping local and cloud queries synchronized is **single-engine semantics**. Because MotherDuck uses the same DuckDB engine on your laptop and in the cloud, a query validated locally is guaranteed to behave identically in production. This eliminates the most common source of dev-prod drift.\n\nA modern dev-to-prod workflow combines this with tools like dbt (data build tool). An analyst can develop and test a dbt model on their laptop against local data for rapid iteration. Once validated, they can promote the model to run in the cloud against the full production dataset with a [single configuration change](https://motherduck.com/blog/dual-execution-dbt/), ensuring complete consistency. This workflow combines the speed of local development with the scale and reliability of a production cloud environment.\n\n## When Should I Not Use This Hybrid Approach?\n\nWhile the hybrid, in-process model is very useful for analytics, it's not a solution for every problem. It's crucial to understand its limitations to know when a different architecture is needed.\n\n1. **High-Concurrency Applications:** An in-process engine like DuckDB is not designed to be a client-server database serving hundreds of simultaneous application users. For powering a public-facing web application, a traditional OLTP database like PostgreSQL or a distributed SQL database is the right choice.\n2. **Real-Time, High-Volume Ingestion:** If your use case involves ingesting thousands of events per second in real-time, a specialized streaming platform or real-time OLAP database would be more suitable.\n3. **Truly Massive, Multi-Terabyte Workloads:** While DuckDB can handle surprisingly large datasets on a single node, if your active working set is in the tens or hundreds of terabytes, you've reached the scale where a distributed cloud data warehouse becomes necessary to parallelize work across a large cluster.\n\nThe beauty of the MotherDuck ecosystem is that it provides a smooth way to scale up. You can start with a local-first, hybrid workflow and, as your concurrency or data scale needs grow, easily push more of the workload to the MotherDuck cloud without changing your core tools or SQL logic.\n\n### TABLE OF CONTENTS\n\n[Why Are My Analytics Queries on Postgres & MySQL Timing Out?](https://motherduck.com/learn-more/hybrid-analytics-guide/#why-are-my-analytics-queries-on-postgres-mysql-timing-out)\n\n[How Can I Analyze a 20 GB Parquet File on My Laptop Without It Crashing?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-analyze-a-20-gb-parquet-file-on-my-laptop-without-it-crashing)\n\n[How Can I Join Local CSVs with Cloud Data in a Single Query?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-join-local-csvs-with-cloud-data-in-a-single-query)\n\n[Why Are Data Engineers So Excited About Dual-Execution Engines?](https://motherduck.com/learn-more/hybrid-analytics-guide/#why-are-data-engineers-so-excited-about-dual-execution-engines)\n\n[How Can I Run SQL Analytics Without Managing Clusters?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-run-sql-analytics-without-managing-clusters)\n\n[What Lightweight SQL Solutions Can Be Integrated with Jupyter Notebooks?](https://motherduck.com/learn-more/hybrid-analytics-guide/#what-lightweight-sql-solutions-can-be-integrated-with-jupyter-notebooks)\n\n[How Can I Keep Queries on My Laptop in Sync With the Cloud?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-keep-queries-on-my-laptop-in-sync-with-the-cloud)\n\n[When Should I Not Use This Hybrid Approach?](https://motherduck.com/learn-more/hybrid-analytics-guide/#when-should-i-not-use-this-hybrid-approach)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\n## FAQS\n\n### My analytics queries on Postgres / MySQL are timing out. What\u2019s the next step up?\n\nThe next step is to move your analytical workloads from a row-oriented (OLTP) database like Postgres to a purpose-built columnar (OLAP) engine like DuckDB. Columnar engines are designed for analytics and are orders of magnitude faster for large scans and aggregations because they only read the specific columns needed for a query.\n\n### How can I analyze a 20 GB Parquet file on my laptop without it crashing?\n\nUse a query engine that supports \"out-of-core\" processing, like DuckDB. It processes data in manageable chunks directly from your disk instead of loading the entire file into RAM. This allows you to analyze datasets that are significantly larger than your computer's available memory.\n\n### Why is it so hard to join local CSVs with cloud data?\n\nThe main challenge is \"data gravity\"\u2014it's slow and expensive to move large datasets across a network. Traditional tools force you to either upload your entire local file or download the entire cloud table. A modern hybrid platform with a dual-execution engine solves this by intelligently minimizing data transfer.\n\n### Can I mix local datasets and S3 data in a single query?\n\nYes. With a hybrid analytics platform like MotherDuck, you can write a single SQL query that joins local files (e.g., CSVs) with data in cloud storage (e.g., Parquet files in S3). The system treats them as if they exist in one unified environment, abstracting away their physical location.\n\n### How can I run SQL analytics without managing clusters?\n\nAdopt a serverless SQL analytics platform. These services handle all infrastructure provisioning, scaling, and maintenance for you. You simply run your queries and pay only for the compute you use, which eliminates the high cost and operational overhead of managing idle clusters.\n\n### Why are data engineers excited about dual-execution engines?\n\nDual-execution engines solve the \"it worked on my machine\" problem by using the exact same query engine on the developer's laptop and in the cloud. This guarantees consistency, speeds up development cycles, lowers costs by leveraging local compute, and enables better team collaboration on a single source of truth.\n\n### What lightweight SQL solutions can be integrated with Jupyter notebooks?\n\nDuckDB is the leading lightweight SQL solution for Jupyter. It can be installed via pip and can directly query Pandas DataFrames, Arrow tables, and large local files with full SQL support. It is significantly faster for analytical queries than other embedded options like SQLite.\n\n### How can I keep queries on my laptop in sync with the cloud?\n\nUse a platform that offers single-engine semantics, like MotherDuck. Because it runs the same DuckDB engine locally and in the cloud, a query validated on your laptop is guaranteed to behave identically in production. This eliminates dev-prod drift and ensures consistency.\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 212,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 212
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/getting-started-with-motherduck-webinar.md",
  [2m  â”‚[0m     "content": "---\ntitle: getting-started-with-motherduck-webinar\ncontent_type: event\nsource_url: https://motherduck.com/videos/getting-started-with-motherduck-webinar\nindexed_at: '2025-11-25T20:44:42.911566'\ncontent_hash: 7e2a069d42290080\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\n\\[Webinar\\] Getting Started with MotherDuck - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[\\[Webinar\\] Getting Started with MotherDuck](https://www.youtube.com/watch?v=bcU3voqZnCE)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=bcU3voqZnCE&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 41:41\n\n\u2022Live\n\n\u2022\n\nWebinarMotherDuck Features\n\n# Getting Started with MotherDuck Webinar\n\n2024/06/19\n\nMotherDuck co-founder Ryan Boyd and Produck Expert Nathaniel Thompson provide an overview on MotherDuck, featuring DuckDB analyst ergonomics, Column Explorer, Fixit, Tableau visualization, Python data analysis, Wasm and more.\n\n## Related Videos\n\n[![\"Data-based: Going Beyond the Dataframe\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FData_based_f32745b461.png&w=3840&q=75)](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n[2025-11-20](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n### [Data-based: Going Beyond the Dataframe](https://motherduck.com/videos/going-beyond-the-dataframe)\n\nLearn how to turbocharge your Python data work using DuckDB and MotherDuck with Pandas. We walk through performance comparisons, exploratory data analysis on bigger datasets, and an end-to-end ML feature engineering pipeline.\n\nWebinar\n\nPython\n\nAI, ML and LLMs\n\n[![\"Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FHex_Webinar_778e3959e4.png&w=3840&q=75)](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n[2025-11-14](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n### [Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck)\n\nAI isn't here to replace data work, it's here to make it better. Watch this webinar to see how Hex and MotherDuck build AI workflows that prioritize context, iteration, and real-world impact.\n\nWebinar\n\nAI, ML and LLMs\n\n[![\"Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Ficeberg_video_thumbnail_adfdabe3d6.jpg&w=3840&q=75)\\\\\n\\\\\n46:26](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n[2025-06-12](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n### [Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse)\n\nBuilding a data stack means choosing between easy SaaS and complex open-source. Apache Iceberg is a middle ground, but its catalog is a hurdle. New tools now simplify using Iceberg with DuckDB to create a flexible, local data lakehouse.\n\nMotherDuck Features\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 121,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 121
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     "content": "---\ntitle: announcing-motherduck-duckdb-in-the-cloud\ncontent_type: blog\nsource_url: https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud\nindexed_at: '2025-11-25T19:57:33.138925'\ncontent_hash: a5ee88402c607603\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Announcing MotherDuck: Hybrid Execution Scales DuckDB from your Laptop into the Cloud\n\n2023/06/22 - 4 min read\n\nBY\nMotherDuck team\n\nDuckDB has become widely known as \u201cSQLite for Analytics\u201d \u2013 a powerful SQL analytics engine with broad adoption in development workflows, ad-hoc analytics on the laptop and embedded applications. MotherDuck wants to make it even easier to use, so we\u2019ve worked alongside the creators of DuckDB to build a cloud-based serverless analytics platform. Today is a large milestone in that journey \u2013 MotherDuck is now available by invitation.\n\nMotherDuck in 100 seconds (by a duck \ud83e\udd86) - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[MotherDuck in 100 seconds (by a duck \ud83e\udd86)](https://www.youtube.com/watch?list=PLIYcNkSjh-0zP7fwKzhnbx5ur1Mf80Q_p&v=BINA_ytZXMY)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\n1/1\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nFull screen is unavailable. [Learn More](https://support.google.com/youtube/answer/6276924)\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?list=PLIYcNkSjh-0zP7fwKzhnbx5ur1Mf80Q_p&v=BINA_ytZXMY&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n[Previous (SHIFT+p)](https://www.youtube.com/watch?list=PLIYcNkSjh-0zP7fwKzhnbx5ur1Mf80Q_p&v=BINA_ytZXMY \"Previous (SHIFT+p)\")\n\n0:00 / 1:51\n\n\u2022Live\n\n\u2022\n\n## Hybrid execution: cloud and laptop working together\n\nData scientists, analysts, and engineers love DuckDB because it works great no matter where their data lives. Since many data professionals have powerful laptops sitting 85% idle, they often want to bring the data to their local machine to make it even more efficient to crunch, especially when performing ad hoc analysis and development. MotherDuck lets you analyze this local data locally, while still JOINing with data processed in the cloud, giving you efficient use of all your compute resources.\n\nIn the example below, the table `yellow_cab_nyc` lives in MotherDuck in the cloud, and I have a CSV on my laptop table with currency conversions. We want to see the average cost of NYC taxi trips by passenger count in different currencies by JOINing these two tables. Yes, we\u2019re seamlessly joining data on my laptop with data in the cloud!\n\n![Post asset](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fexample_hybrid_7bb8f08e2d.png&w=3840&q=75)\n\nYou can even do hybrid query execution with data stored in s3, with MotherDuck securely storing and managing your AWS credentials.\n\n![example_s3.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fexample_s3_d97e746203.png&w=3840&q=75)\n\nNote, these examples are part of our [sample datasets and queries](https://motherduck.com/docs/category/sample-datasets--queries/), feel free to run them yourself!\n\nYou might wonder how this works under the covers. By connecting your DuckDB instance to MotherDuck, you establish a radically different type of distributed system - one, in which one node is MotherDuck in the cloud, and another node is wherever your DuckDB lives, be it your laptop or a lambda, Python or CLI, JDBC or MotherDuck\u2019s own web app. Both nodes execute queries in concert in the most optimal way, automatically routing parts of queries to the right location.\n\n## MotherDuck includes a web notebook and Git-style Collaboration\n\nWant to run some quick SQL queries without downloading and installing DuckDB? The MotherDuck web application provides a notebook-like UI. This enables you to analyze local CSVs and parquet files, upload them and manage them alongside your other data stored in MotherDuck.\n\n![app_motherduck_beta.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fapp_motherduck_beta_v2_69fbe04dcf.png%3Fupdated_at%3D2023-06-21T23%3A24%3A18.647Z&w=3840&q=75)\n\nAs a DuckDB-in-the-cloud company, naturally MotherDuck embeds DuckDB in its web application using WASM. Results of your SQL queries are cached in this DuckDB instance, enabling you to instantly sort, pivot, and filter query results!\n\nWant to share your DuckDB data with colleagues? Using SQL, you can create a shareable snapshot of your data, which your colleagues can easily attach in MotherDuck.\n![example_share.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fexample_share_cfe1d14570.png&w=3840&q=75)\n\nThis SQL command will return a shareable URL which can then be used by your colleague to access the shared database.\n![example_attach.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fexample_attach_ff22a57c32.png&w=3840&q=75)\n\n## Anywhere you can Duck, you can MotherDuck\n\nDuckDB has been starred by over 10k developers on GitHub, and it might be due to the simplicity of getting up and running with a downloadable, open source analytics engine. We want to continue (and improve!) that amazing experience as we bring DuckDB to the cloud.\n\nOne way to do this is by ensuring MotherDuck works well with many of the most popular technologies in the modern data stack, including ingestion, orchestration and BI+Visualization tools.\n\n![partner-logos.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fpartner_logos_242d207bb6.png&w=3840&q=75)\n\nWe strive to make MotherDuck as easy to adopt as DuckDB. To that end, any DuckDB instance in the world running in Python or CLI can connect to MotherDuck with a single line of code. Suddenly, by running this command your DuckDB magically becomes supercharged by MotherDuck. Such ease of onboarding could only have been possible via close collaboration with the creators of DuckDB!\n\n![d_to_md.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fd_to_md_1dfe63b09b.png&w=3840&q=75)\n\n## Continuing to Make Analytics Ducking Awesome\n\nOne of the primary reasons we were driven to build a serverless analytics platform on top of DuckDB was their fast-paced innovation. Many features in DuckDB have gone from thoughts in academic papers to committed code in a few weeks.\n\nWe\u2019re launching MotherDuck now and doing weekly releases because we admire and want to emulate this speed of execution. Thanks in advance for all the feedback you can provide to make MotherDuck a better product!\n\n## Get Started\n\n[Request an invite](https://motherduck.com/) now to get started using MotherDuck, and join the flock on [slack.motherduck.com](https://slack.motherduck.com/).\n\nAnd, if you\u2019re in San Francisco next week, don\u2019t forget to [register for the MotherDuck Party](https://motherduck-party.eventbrite.com/), watch DuckDB co-creator Hannes [keynote the Data + AI conference](https://www.databricks.com/dataaisummit/session/data-ai-summit-keynote-thursday), and join MotherDuck co-founder Ryan Boyd in his [technical session](https://www.databricks.com/dataaisummit/session/if-duck-quacks-forest-and-everyone-hears-should-you-care).\n\n### TABLE OF CONTENTS\n\n[Hybrid execution: cloud and laptop working together](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#hybrid-execution-cloud-and-laptop-working-together)\n\n[MotherDuck includes a web notebook and Git-style Collaboration](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#motherduck-includes-a-web-notebook-and-git-style-collaboration)\n\n[Anywhere you can Duck, you can MotherDuck](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#anywhere-you-can-duck-you-can-motherduck)\n\n[Continuing to Make Analytics Ducking Awesome](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#continuing-to-make-analytics-ducking-awesome)\n\n[Get Started](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#get-started)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![This Month in the DuckDB Ecosystem: June 2023](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fthumbnail_duckdb_newsletter_1_34b0dc65cf.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-seven/)\n\n[2023/06/16 - Marcos Ortiz](https://motherduck.com/blog/duckdb-ecosystem-newsletter-seven/)\n\n### [This Month in the DuckDB Ecosystem: June 2023](https://motherduck.com/blog/duckdb-ecosystem-newsletter-seven)\n\nThis Month in the DuckDB Ecosystem: June 2023\n\n[![DuckDB Tutorial For Beginners](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgetting_started_duckdb_thumbnail_70b197b1ab.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n[2024/10/31 - Mehdi Ouazza, Ryan Boyd](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n### [DuckDB Tutorial For Beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners)\n\nGet up to speed quickly with DuckDB, including installation, VSCode workflow integration and your first SQL analytics project.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 188,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 188
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md",
  [2m  â”‚[0m     "content": "---\ntitle: cidr-paper-hybrid-query-processing-motherduck\ncontent_type: event\nsource_url: https://motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck\nindexed_at: '2025-11-25T19:56:16.076582'\ncontent_hash: 23a9f33edc9cbbbb\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Just Released: Hybrid Query Processing Paper at CIDR 2024\n\n2024/01/16 - 2 min read\n\nBY\n\n[Peter Boncz](https://motherduck.com/authors/peter-boncz/)\n\nThe Conference on Innovative Data systems Research (CIDR) is underway in California and we\u2019re proud to be presenting a peer-reviewed [paper on the MotherDuck hybrid query processing architecture](https://www.cidrdb.org/cidr2024/papers/p46-atwal.pdf).\n\n[Hybrid query processing](https://motherduck.com/learn-more/hybrid-analytics-guide/) allows you to execute database queries either on your local machine, in the cloud, or using a combination of both. It adds useful capabilities to DuckDB, for instance the sharing of DuckDB databases between different team members via the cloud. It also allows you to create web applications with DuckDB running inside your browser, that can jointly execute queries with MotherDuck in the cloud.\n\nThe research and implementation of this architecture has been a collaboration between MotherDuck, DuckDB Labs and myself as a visiting database researcher on sabbatical from CWI, the Dutch national computer science research institute from which DuckDB was born.\n\nBecause designing and implementing a cutting-edge database system like MotherDuck is non-trivial, there are in fact quite a bit of research elements in what we do, even when software engineering. For example, we need to understand how to optimally plan hybrid queries when there are asymmetrical network connections (like in consumer internet) or cost differences in storage, compute and energy. This is why the collaboration between academia and industry is so important in databases; it has already provided a lot of inspiration for my research group at CWI while providing benefits to MotherDuck\u2019s users.\n\nI look forward to gaining other inspiration for my research group and MotherDuck from my fellow researchers at CIDR. Although CIDR is a relatively small conference, it attracts a distinguished audience of researchers and practitioners working in data systems attending it. The conference was originally created by two Turing Award winners: Jim Gray and Michael Stonebraker, both founding figures of the database field.\n\nOur CIDR paper is [now available for download](https://www.cidrdb.org/cidr2024/papers/p46-atwal.pdf) and provides an in-depth view of MotherDuck and our hybrid query architecture. I truly hope that you also will find it interesting. If so, please spread the word and pass it along to people who you think also will find this interesting!\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![Why Python Developers Need DuckDB (And Not Just Another DataFrame Library)](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Fwhy_pythondev_1_22167e31bf.png&w=3840&q=75)](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/)\n\n[2025/10/08 - Mehdi Ouazza](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/)\n\n### [Why Python Developers Need DuckDB (And Not Just Another DataFrame Library)](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries)\n\nUnderstand why a database is much more than just a dataframe library\n\n[![DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_4_1_b6209aca06.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n[2025/10/09 - Alex Monahan, Garrett O'Brien](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n### [DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/blog/announcing-duckdb-141-motherduck)\n\nMotherDuck now supports DuckDB 1.4.1 and DuckLake 0.3, with new SQL syntax, faster sorting, Iceberg interoperability, and more. Read on for the highlights from these major releases.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 74,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 74
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/index.md",
  [2m  â”‚[0m     "content": "---\ntitle: motherduck.com\ncontent_type: event\nsource_url: https://motherduck.com/\nindexed_at: '2025-11-25T20:45:26.921185'\ncontent_hash: 121e46675911459e\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/#)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n# Making Big Data Feel Small\n\nDuckDB cloud data warehouse scaling to terabytes\nfor customer-facing analytics and BI\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\n[LEARN MORE](https://motherduck.com/product/)\n\n[BOOK A DEMO](https://motherduck.com/contact-us/product-expert/)\n\n[instant sql blog post](https://motherduck.com/blog/introducing-instant-sql/)\n\n![DuckDB Book in the clouds](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduckdb-book-in-the-clouds.d9791904.png&w=3840&q=75)\n\n## \"DuckDB In Action\" Book for Free\n\nGet the complete book for free in your inbox!\n\nWork Email\n\nSubscribe to MotherDuck news\n\nSubscribe to DuckDB ecosystem newsletter\n\nSUBMIT\n\nFINALLY:\n\nA database you don't hate\n\n## Why It's Better\n\n![Scales Vertically and Horizontally to handle Spikey Workloads](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-why-motherduck-scales.b290b9bc.png&w=3840&q=75)\n\nScales Vertically and Horizontally to handle Spikey Workloads\n\n![Reads YOUR data.  (plaintext, json, parquet, iceberg, xls, csv)](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-why-motherduck-reads.51fdba51.png&w=3840&q=75)\n\nReads YOUR data. (plaintext, json, parquet, iceberg, xls, csv)\n\n![Fast, columnar central data storage optimized for analytics](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-why-motherduck-fast.bf89edac.png&w=3840&q=75)\n\nFast, columnar central data storage optimized for analytics\n\n![Run locally, deploy to the cloud for reliability and collaboration. Fits into your workflow](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-why-motherduck-run-locally.3c9900e8.png&w=3840&q=75)\n\nRun locally, deploy to the cloud for reliability and collaboration. Fits into your workflow\n\n## Who is it for?\n\nAnalytics that works for everyone\n\n[![Software Engineers](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-coding-duck.c6c8efa9.png&w=3840&q=75)\\\\\n**Software Engineers** \\\\\nWho ended up with a big data problem](https://motherduck.com/product/app-developers) [![Data Scientists](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-scientist-duck.5805adc0.png&w=3840&q=75)\\\\\n**Data Scientists** \\\\\nWho ended up having to do data engineering](https://motherduck.com/product/duckdb-users) [![Data Engineers](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-plumber-duck.27784453.png&w=3840&q=75)\\\\\n**Data Engineers** \\\\\nWith slow, brittle pipelines](https://motherduck.com/product/data-teams)\n\n## Who is it for?\n\nAnalytics that works for everyone\n\n[![Software Engineers](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-coding-duck.c6c8efa9.png&w=3840&q=75)\\\\\n**Software Engineers** \\\\\nWho ended up with a big data problem](https://motherduck.com/product/app-developers) [![Data Scientists](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-scientist-duck.5805adc0.png&w=3840&q=75)\\\\\n**Data Scientists** \\\\\nWho ended up having to do data engineering](https://motherduck.com/product/duckdb-users) [![Data Engineers](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-plumber-duck.27784453.png&w=3840&q=75)\\\\\n**Data Engineers** \\\\\nWith slow, brittle pipelines](https://motherduck.com/product/data-teams)\n\nUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse Cases\n\n## Use Cases\n\n!['For Data Teams' duck illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdata-teams-ducks.6fea331d.png&w=3840&q=75)!['For App Devs' duck illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fapp-devs-ducks.51eb4ea9.png&w=3840&q=75)\n\n![For Data Teams ducks](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdata-teams-ducks.6fea331d.png&w=3840&q=75)\n\n### Data Warehousing\n\nIs your data all over the place? Start making sense of your business by bringing it together for internal business intelligence and analytics. Build pure SQL pipelines, share data and quickly collaborate with your team.\n\n[How it Works](https://motherduck.com/docs/getting-started/data-warehouse/)\n\n[Case Studies](https://motherduck.com/case-studies/)\n\n![For App Devs ducks](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fapp-devs-ducks.51eb4ea9.png&w=3840&q=75)\n\n### Customer-facing Analytics\n\nUnlike traditional BI, customer-facing analytics is built directly into your product for end users. It delivers near real-time, low-latency insights at scale \u2014 think milliseconds, not minutes \u2014 and must handle thousands to millions of concurrent queries. MotherDuck's architecture, from per-user tenancy to Wasm support, is designed for the unique requirements of Customer-Facing Analytics to drive increased user engagement directly in your app experience.\n\n[How it Works](https://motherduck.com/docs/getting-started/customer-facing-analytics/)\n\n[Case Studies](https://motherduck.com/case-studies/)\n\n## How We Scale\n\n### Duckling Sizes\n\nMotherDuck\u2019s per-user tenancy model gives each user an isolated\nduckling\n\n![Tooltip Duckling illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftooltip-duckling-illus.32805477.png&w=3840&q=75)\n\nA Duckling is a dedicated DuckDB instance for each user, ensuring optimal performance and scalability in data analytics.\n\n(DuckDB instance) in one of five sizes to enable individual, user-level configuration.\n\n![MotherDuck Assets illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmotherduck-assets-illus.f80662b9.png&w=3840&q=75)\n\n![Pulse Duckling](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduckling-1-placeholder.8fd25135.png&w=3840&q=75)\n\n![Pulse Instance type illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Finstance-type-illus-pulse.8c39b9cc.png&w=3840&q=75)\n\nPulse\n\nOur smallest instance, perfect for ad-hoc analytics tasks\n\n![Standard Duckling](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduckling-2-placeholder.e3942696.png&w=3840&q=75)\n\n![Standard Instance type illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Finstance-type-illus-standard.3d0ae9e9.png&w=3840&q=75)\n\nStandard\n\nBuilt to handle common data warehouse workloads, including loads and transforms\n\n![Jumbo Duckling](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduckling-3-placeholder.9e54ea44.png&w=3840&q=75)\n\n![Jumbo Instance type illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Finstance-type-illus-jumbo.b77c7bca.png&w=3840&q=75)\n\nJumbo\n\nFor larger data warehouse workloads with many transformations or complex aggregations\n\n![Mega Duckling](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduckling-4-placeholder.356f25ee.png&w=3840&q=75)\n\n![Mega Instance type illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Finstance-type-illus-mega.cd4cd44f.png&w=3840&q=75)\n\nMega\n\nAn extremely large instance for when you need complex transformations done quickly\n\n![Giga Duckling](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduckling-5-placeholder.7ca13555.png&w=3840&q=75)\n\n![Giga Instance type illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Finstance-type-illus-giga.3228dda6.png&w=3840&q=75)\n\nGiga\n\nLargest instances enable the toughest transformations to run faster\n\n[Contact us](https://motherduck.com/contact-us/product-expert/)\n\n![Magnifying glass duckling sizes illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmagnifying-glass-duckling-sizes.5db83c77.png&w=3840&q=75)\n\nTake a closer look at\n\n[DUCKLING SIZES](https://motherduck.com/product/pricing/#instance-types)\n\n### Per-user tenancy   and vertical scaling\n\nMotherDuck employs a per-user tenancy and vertical scaling strategy. Users connect to their own MotherDuck Ducklings (DuckDB instances), which are sized (pulse, standard, jumbo, mega, giga) to meet their specific needs. There is also the option for additional Ducklings, through read scaling (explained below), to ensure flexible resource allocation. Ultimately, each Duckling establishes a connection with the central Data Warehouse storage.\n\n![Per-user tenancy      and vertical scaling](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhow-we-scale-per-user-tenancy.3641ecda.png&w=3840&q=75)\n\n### Read Scaling\n\nMotherDuck's read scaling capabilities allow users to connect via a BI Tool to dedicated Ducklings that function as read replicas. These read replicas can be provisioned in various sizes (pulse, standard, jumbo, mega or giga) to accommodate different needs. Ultimately, these read replicas connect to the Data Warehouse storage, enabling efficient handling of read operations.\n\n![Read Scaling](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhow-we-scale-read-scaling.a2333854.png&w=3840&q=75)\n\n## Ecosystem\n\nModern Duck Stack\n\n### CLOUD DATA WAREHOUSE\n\n### Sources\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpostgres-sql.d737f4f5.png&w=750&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Famazon-s3.72386dfc.png&w=640&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsalesforce.35d52b31.png&w=384&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fstripe.2160e881.png&w=384&q=75)\n\n[**Business Intelligence**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftableau.0ec523e2.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpower-bi.f3563826.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fomni.cb7aa381.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmetabase.5f8fe44e.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Business+Intelligence) [**Ingestion**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffivetran.26f3817d.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fairbyte.8371d2f0.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Festuary.babad369.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlthub.e9892b97.png&w=384&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Ingestion) [**Data Science & AI**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhex.275dad7d.png&w=256&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fjupyter.2a6af3de.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fcolab.30ab10af.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmarimo.b699c73d.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Data+Science+%26+AI) [**Reverse ETL**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fcensus.352f1d69.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Reverse+ETL) [**Transformation**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdbt.fd2184d1.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftobiko.d0e3d1e5.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsql-mesh.6fceb012.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Transformation) [**Dev Tools**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdatagrip.f48eba23.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpuppy-graph.848400c6.png&w=750&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdagster.e1970a7c.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Dev+Tools)\n\n[**Orchestration**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fairflow.7f70081a.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdagster.e1970a7c.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fprefect.fd6371b3.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fkestra.aa10acfc.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Orchestration)\n\n[**Data Quality**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmonte-carlo.2143f962.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsecoda.9b7e86fb.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsoda.59e5aa02.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fgreat-expectations.c544f1ef.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Data+Quality)\n\nMotherDuck with DuckDB was by far the fastest of the OLAP platforms we evaluated - both in the cloud and run on our developer's machines - bridging price and performance and greatly increasing productivity.\n\n[READ MORE](https://motherduck.com/case-studies/saasworks/)\n\n![Jim O\u2019Neill's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fjim_oneil_photo_930c637f49.jpg&w=3840&q=75)\n\nJim O\u2019Neill\n\nCTO and Co-founder at FinQore\n\nMotherDuck with DuckDB was by far the fastest of the OLAP platforms we evaluated - both in the cloud and run on our developer's machines - bridging price and performance and greatly increasing productivity.\n\n[READ MORE](https://motherduck.com/case-studies/saasworks/)\n\nWe used to do analytics in a MySQL database with all of our daily device, telemetry, and image processing data. There was no way to scale that further using MySQL. With MotherDuck, we\u2019re finally starting to find patterns in our data to help customers grow produce more effectively.\n\n[READ MORE](https://motherduck.com/case-studies/gardyn/)\n\n![Rob Teeuwen's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2F1689248039646_d0c68f49b7.jpeg&w=3840&q=75)\n\nRob Teeuwen\n\nData Scientist at Gardyn\n\nWe used to do analytics in a MySQL database with all of our daily device, telemetry, and image processing data. There was no way to scale that further using MySQL. With MotherDuck, we\u2019re finally starting to find patterns in our data to help customers grow produce more effectively.\n\n[READ MORE](https://motherduck.com/case-studies/gardyn/)\n\nBringing MotherDuck into our data stack has been a game changer. We're reducing friction and supercharging our dev and ops experience by leveraging the DuckDB interoperability across local and cloud.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nBringing MotherDuck into our data stack has been a game changer. We're reducing friction and supercharging our dev and ops experience by leveraging the DuckDB interoperability across local and cloud.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nMoving to MotherDuck, a billed by-the-second cloud offering is a no-brainer for us, considering the elegance and efficiency of a single node system compared to traditional OLAP solutions.\n\n[ATM.COM](https://atm.com/)\n\n![Nico Ritschel's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fnico_ritschel_photo_41ba132b33.jpg&w=3840&q=75)\n\nNico Ritschel\n\nDirector of Engineering at atm.com\n\nMoving to MotherDuck, a billed by-the-second cloud offering is a no-brainer for us, considering the elegance and efficiency of a single node system compared to traditional OLAP solutions.\n\n[ATM.COM](https://atm.com/)\n\nInstant SQL in MotherDuck will save me the misery of having to try and wrangle SQL in my BI tool where iteration speed can be very slow. This lets me get the data right earlier in the process and with faster feedback than waiting for a chart to render, or having to clear an analytics cache.\n\n[DashFuel](https://www.getdashfuel.com/)\n\n![Mike McClannahan's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmike_e96491a8eb.jpeg&w=3840&q=75)\n\nMike McClannahan\n\nCTO, DashFuel\n\nInstant SQL in MotherDuck will save me the misery of having to try and wrangle SQL in my BI tool where iteration speed can be very slow. This lets me get the data right earlier in the process and with faster feedback than waiting for a chart to render, or having to clear an analytics cache.\n\n[DashFuel](https://www.getdashfuel.com/)\n\nMotherDuck proving once again they are not constrained by what is easy or common. Instant SQL will not only enable complex query construction for those who find the task daunting (everyone?), it'll help past me communicate with future me.\n\n[DashFuel](https://www.getdashfuel.com/)\n\n![SJ Browne's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fsj_contemplating_6411ae5dcf.jpg&w=3840&q=75)\n\nSJ Browne\n\nSoftware Engineer at DashFuel\n\nMotherDuck proving once again they are not constrained by what is easy or common. Instant SQL will not only enable complex query construction for those who find the task daunting (everyone?), it'll help past me communicate with future me.\n\n[DashFuel](https://www.getdashfuel.com/)\n\nMotherDuck is the GOAT\n\n[READ MORE](https://motherduck.com/case-studies/reflex-careers-gig-economy-retail/)\n\n![Nate Hamm's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimage_15820c5f78.png&w=3840&q=75)\n\nNate Hamm\n\nSr Software Developer at Reflex\n\nMotherDuck is the GOAT\n\n[READ MORE](https://motherduck.com/case-studies/reflex-careers-gig-economy-retail/)\n\nWe found that DuckDB and MotherDuck are amazing tools for small data teams like ours.\n\n[READ MORE](https://motherduck.com/case-studies/dosomething-non-profit-tco-cost-savings/)\n\n![Dave Crusoe's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2F1515814825678_e2e8b2e5e7.jpeg&w=3840&q=75)\n\nDave Crusoe\n\nVP of Product & Engineering at DoSomething\n\nWe found that DuckDB and MotherDuck are amazing tools for small data teams like ours.\n\n[READ MORE](https://motherduck.com/case-studies/dosomething-non-profit-tco-cost-savings/)\n\nDuckDB has a lot of folks in the data community excited\u2014it's a new and unique tool in practitioners' toolboxes and has captured the imaginations of many. MotherDuck's efforts bringing DuckDB to the cloud will broaden the reach of this promising technology.\n\n[DBT LABS](https://www.getdbt.com/)\n\n![Tristan Handy's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Ftristan_handy_photo_ef2233a588.jpg&w=3840&q=75)\n\nTristan Handy\n\nCEO & Founder at dbt Labs\n\nDuckDB has a lot of folks in the data community excited\u2014it's a new and unique tool in practitioners' toolboxes and has captured the imaginations of many. MotherDuck's efforts bringing DuckDB to the cloud will broaden the reach of this promising technology.\n\n[DBT LABS](https://www.getdbt.com/)\n\nMotherDuck is insanely performant, and there\u2019s no infrastructure to manage. The cost of experimentation is incredibly low \u2014 which means we can afford to ask more questions and build more features, faster.\n\n[READ MORE](https://motherduck.com/case-studies/kultura-capital/)\n\n![Kristov Paulus's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_05_08_at_10_29_30_AM_ac663c67b9.png&w=3840&q=75)\n\nKristov Paulus\n\nFounder, Chief Investment Officer at Kultura Capital\n\nMotherDuck is insanely performant, and there\u2019s no infrastructure to manage. The cost of experimentation is incredibly low \u2014 which means we can afford to ask more questions and build more features, faster.\n\n[READ MORE](https://motherduck.com/case-studies/kultura-capital/)\n\nI just onboarded some non-technical users to MotherDuck, and I can\u2019t imagine having done so in a different system with this level of ease and lack of intimidation. Between the \u2018Filter\u2019 button, Column Explorer, and the FROM syntax - they feel empowered to answer many of their own questions! \ud83d\ude42\n\n[READ MORE](https://motherduck.com/blog/dosomething-motherduck-data-warehouse-ROI/)\n\n![Sahil Gupta's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fsahil_portrait_600bd2844e.png&w=3840&q=75)\n\nSahil Gupta\n\nSenior Data Engineer at dosomething.org\n\nI just onboarded some non-technical users to MotherDuck, and I can\u2019t imagine having done so in a different system with this level of ease and lack of intimidation. Between the \u2018Filter\u2019 button, Column Explorer, and the FROM syntax - they feel empowered to answer many of their own questions! \ud83d\ude42\n\n[READ MORE](https://motherduck.com/blog/dosomething-motherduck-data-warehouse-ROI/)\n\nMotherDuck with DuckDB was by far the fastest of the OLAP platforms we evaluated - both in the cloud and run on our developer's machines - bridging price and performance and greatly increasing productivity.\n\n[READ MORE](https://motherduck.com/case-studies/saasworks/)\n\n![Jim O\u2019Neill's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fjim_oneil_photo_930c637f49.jpg&w=3840&q=75)\n\nJim O\u2019Neill\n\nCTO and Co-founder at FinQore\n\nMotherDuck with DuckDB was by far the fastest of the OLAP platforms we evaluated - both in the cloud and run on our developer's machines - bridging price and performance and greatly increasing productivity.\n\n[READ MORE](https://motherduck.com/case-studies/saasworks/)\n\nWe used to do analytics in a MySQL database with all of our daily device, telemetry, and image processing data. There was no way to scale that further using MySQL. With MotherDuck, we\u2019re finally starting to find patterns in our data to help customers grow produce more effectively.\n\n[READ MORE](https://motherduck.com/case-studies/gardyn/)\n\n![Rob Teeuwen's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2F1689248039646_d0c68f49b7.jpeg&w=3840&q=75)\n\nRob Teeuwen\n\nData Scientist at Gardyn\n\nWe used to do analytics in a MySQL database with all of our daily device, telemetry, and image processing data. There was no way to scale that further using MySQL. With MotherDuck, we\u2019re finally starting to find patterns in our data to help customers grow produce more effectively.\n\n[READ MORE](https://motherduck.com/case-studies/gardyn/)\n\nBringing MotherDuck into our data stack has been a game changer. We're reducing friction and supercharging our dev and ops experience by leveraging the DuckDB interoperability across local and cloud.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nBringing MotherDuck into our data stack has been a game changer. We're reducing friction and supercharging our dev and ops experience by leveraging the DuckDB interoperability across local and cloud.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nMoving to MotherDuck, a billed by-the-second cloud offering is a no-brainer for us, considering the elegance and efficiency of a single node system compared to traditional OLAP solutions.\n\n[ATM.COM](https://atm.com/)\n\n![Nico Ritschel's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fnico_ritschel_photo_41ba132b33.jpg&w=3840&q=75)\n\nNico Ritschel\n\nDirector of Engineering at atm.com\n\nMoving to MotherDuck, a billed by-the-second cloud offering is a no-brainer for us, considering the elegance and efficiency of a single node system compared to traditional OLAP solutions.\n\n[ATM.COM](https://atm.com/)\n\nInstant SQL in MotherDuck will save me the misery of having to try and wrangle SQL in my BI tool where iteration speed can be very slow. This lets me get the data right earlier in the process and with faster feedback than waiting for a chart to render, or having to clear an analytics cache.\n\n[DashFuel](https://www.getdashfuel.com/)\n\n![Mike McClannahan's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmike_e96491a8eb.jpeg&w=3840&q=75)\n\nMike McClannahan\n\nCTO, DashFuel\n\nInstant SQL in MotherDuck will save me the misery of having to try and wrangle SQL in my BI tool where iteration speed can be very slow. This lets me get the data right earlier in the process and with faster feedback than waiting for a chart to render, or having to clear an analytics cache.\n\n[DashFuel](https://www.getdashfuel.com/)\n\nMotherDuck proving once again they are not constrained by what is easy or common. Instant SQL will not only enable complex query construction for those who find the task daunting (everyone?), it'll help past me communicate with future me.\n\n[DashFuel](https://www.getdashfuel.com/)\n\n![SJ Browne's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fsj_contemplating_6411ae5dcf.jpg&w=3840&q=75)\n\nSJ Browne\n\nSoftware Engineer at DashFuel\n\nMotherDuck proving once again they are not constrained by what is easy or common. Instant SQL will not only enable complex query construction for those who find the task daunting (everyone?), it'll help past me communicate with future me.\n\n[DashFuel](https://www.getdashfuel.com/)\n\n## Our Community\n\n[![MotherDuck profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmedia.licdn.com%2Fdms%2Fimage%2Fv2%2FD560BAQEODuOUOAdoQg%2Fcompany-logo_100_100%2FB56ZfmzIGPG0AQ-%2F0%2F1751923851982%2Fmotherduck_logo%3Fe%3D1762992000%26v%3Dbeta%26t%3DelhKxwqBElgX_PNnhA3vQAu8QdPUILGxxa4doqv60-w&w=96&q=75)\\\\\n\\\\\nMotherDuck\\\\\n\\\\\n@motherduck\\\\\n\\\\\n@duckdb is empowering machine learning use cases! \ud83e\udd16\\\\\nAny good model training starts with some good data exploration.\\\\\n\\\\\n@huggingface now enables SQL queries using DuckDB on any dataset stored on the Hub!](https://twitter.com/motherduck/status/1666804451552935938)\n\n[![DuckDB profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1274363897676521474%2FqgbqYYuV_x96.jpg&w=96&q=75)\\\\\n\\\\\nDuckDB\\\\\n\\\\\n@duckdb\\\\\n\\\\\nDuckDB Co-Creator @hfmuehleisen and MotherDuck Founder @jrdntgn in this new podcast - among other things about the origin story of DuckDB and later MotherDuck:](https://twitter.com/duckdb/status/1649305101570306048)\n\n[![Garrett McClintock profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1438016780060950531%2FCn11Ke8O_400x400.jpg&w=96&q=75)\\\\\n\\\\\nGarrett McClintock\\\\\n\\\\\n@GJMcClintock\\\\\n\\\\\njust built a pipeline using @dltHub and @motherduck and it's very nice. 10/10 experience once you get the hang of it and very scalable.](https://x.com/GJMcClintock/status/1800962608939303002)\n\n[![Andrew Madson profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2F1706413992317_120c2864d9.jpeg&w=96&q=75)\\\\\n\\\\\nAndrew Madson\\\\\n\\\\\nandrew-madsen\\\\\n\\\\\nInstant SQL! This is one of the highlights of Data Council... Instant SQL caches a sample of your table instantly, and synchronously updates the query results, AS YOU TYPE!](https://www.linkedin.com/posts/andrew-madson_duckdb-instant-sql-this-is-one-of-the-activity-7321023818074894336-sbar?utm_source=share&utm_medium=member_desktop&rcm=ACoAAACw0KEBx7lMe9uvdRJxJyQifAb-I4X7yQA)\n\n[![KKovacs [\u30b9] profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F995607718508589058%2FtQPO3yiD_400x400.jpg&w=96&q=75)\\\\\n\\\\\nKKovacs \\[\u30b9\\]\\\\\n\\\\\n@kkovacs\\\\\n\\\\\nI don\u2019t want to say that #DuckDB is a silver bullet because there is no such thing, but it\u2019s surprisingly projectile-shaped, very shiny, and killed all the various werewolves (ok, data problems) I\u2019ve thrown at it](https://twitter.com/kkovacs/status/1801607688889012587)\n\n[![evidence profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1699896475864510464%2F_XFRsLSv_400x400.jpg&w=96&q=75)\\\\\n\\\\\nevidence\\\\\n\\\\\n@evidence\\_dev\\\\\n\\\\\nDuckDB is increasingly being built into every data tool, but DuckDB isn't designed to be used as a cloud data warehouse\\\\\n\\\\\n@motherduck\\\\\nis solving that problem, and they\u2019re doing it in a delightful, uniquely DuckDB way...](https://twitter.com/evidence_dev/status/1801630536252768584)\n\n[![Zach Wilson profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmedia.licdn.com%2Fdms%2Fimage%2Fv2%2FD5603AQGLd4zOUXaJJg%2Fprofile-displayphoto-shrink_200_200%2FB56ZcbuHVJHUAY-%2F0%2F1748516763636%3Fe%3D1762992000%26v%3Dbeta%26t%3DXmqD42PALO0Cur0sGLHL8Wo_3RMwSo9tGHycq0jpFI8&w=96&q=75)\\\\\n\\\\\nZach Wilson\\\\\n\\\\\neczachly\\\\\n\\\\\nMatt Martin and I did a runtime benchmark of Apache Spark vs DuckDB (by MotherDuck)\\\\\nWe found that data sets under 20 gigabytes ran about 100 times faster on DuckDB than they did on Apache Spark!\\\\\n\\\\\nThis benchmark uses plain parquet files and COUNT distinct to truly measure performance without skipping any data so we know the differences are correct.\\\\\n\\\\\nHere's the complete writeup: https://lnkd.in/gXuWGWyy](https://www.linkedin.com/posts/eczachly_matt-martin-and-i-did-a-runtime-benchmark-activity-7376304145978310657-GzkR/)\n\n[![Daniel Beach profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2F22616816.fs1.hubspotusercontent-na1.net%2Fhubfs%2F22616816%2Fdaniel%2520beach.jpg&w=96&q=75)\\\\\n\\\\\nDaniel Beach\\\\\n\\\\\n@dataengineeringcentral\\\\\n\\\\\nThis MotherDuck thingy is such a beauty it brings a tear to me old eye. You have no idea how many new tools (maybe you do) I agonize over while trying to bring you the good, the bad, and the ugly.](https://dataengineeringcentral.substack.com/p/honest-review-of-motherduck)\n\n[![Ryan Dolley profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fryan_dolley_a1178e8a04.jpeg&w=96&q=75)\\\\\n\\\\\nRyan Dolley\\\\\n\\\\\nryandolley\\\\\n\\\\\nMotherDuck ... bringing big innovation to SQL to make it more like a real language and less like a 'groovy ancient programming language from the 70s.' Check it out.](https://www.linkedin.com/posts/ryandolley_duckdb-activity-7320941264818884609-1fij?utm_source=share&utm_medium=member_desktop&rcm=ACoAAACw0KEBx7lMe9uvdRJxJyQifAb-I4X7yQA)\n\n[![Robert Lancer profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Frobert_lancer_e5d68edd81.jpeg&w=96&q=75)\\\\\n\\\\\nRobert Lancer\\\\\n\\\\\nrobertlancer\\\\\n\\\\\nThe progress on the MotherDuck UI is amazing! It went from minimal utility to my preferred place to write SQL in a matter of months](https://www.linkedin.com/feed/update/urn:li:ugcPost:7163558312741982208?commentUrn=ur[%E2%80%A6]89847054557184%2Curn%3Ali%3AugcPost%3A7163558312741982208%29)\n\n[![Caleb Fahlgren profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1716604301563289600%2FYycgFNAn_400x400.jpg&w=96&q=75)\\\\\n\\\\\nCaleb Fahlgren\\\\\n\\\\\n@calebfahlgren\\\\\n\\\\\nThe amount of data I am storing in \\\\\n@motherduck rn for a new project is criminal \ud83e\udd23\\\\\nDecided to try it out since I am a big \\\\\n@duckdb user. Is pretty good so far. Have few hundred million records.](https://x.com/calebfahlgren)\n\n[![Simon Sp\u00e4ti \ud83c\udfd4\ufe0f profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1137678291605676032%2FITc3M_ah_400x400.png&w=96&q=75)\\\\\n\\\\\nSimon Sp\u00e4ti \ud83c\udfd4\ufe0f\\\\\n\\\\\n@sspaeti\\\\\n\\\\\nI'm just checking a little about the hosted DuckDB by @motherduck. What a nice user experience. It automatically detects errors, and \\`accept & run\\` fixes common errors. Love it.](https://x.com/sspaeti/status/1798290369110749458)\n\n[![Jacob Matson profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1278773580122054656%2F3SG9rUwl_x96.jpg&w=96&q=75)\\\\\n\\\\\nJacob Matson\\\\\n\\\\\n@matsonj\\\\\n\\\\\nusing sql for data cleaning vs excel (reason 879 to learn sql)\\\\\n\\\\\nlevenstein distance between 590strings in excel (350k combinations): \\\\\nexcel: 25 mins of spinning. \\\\\nduckdb: 1.2s](https://twitter.com/matsonj/status/1669434599095816192)\n\n[![Bob Currier profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fbob_currier_d620fb0c2d.jpeg&w=96&q=75)\\\\\n\\\\\nBob Currier\\\\\n\\\\\nbobcurrier\\\\\n\\\\\nIf you're not familiar with DuckDB, you definitely want to give it a look. I've recently converted all my existing code from using CSV files and SQLite databases for exploratory analysis to parquet files, DuckDB and MotherDuck. The transformation in my workflow has been nothing short of astounding. 10GB of CSV files turned into 1GB of parquet files that are now stored in S3 buckets. Using DuckDB and MotherDuck, I can run exploratory queries seamlessly thanks to the DuckStack with dual execution with local processing and cloud scale. My query development time has decreased by 90%. And I'm loving the team feature of the MotherDuck platform. OCEANCODA is rocking the small data life! #duckdb #python #motherduck #eda #ai #ml (Graphic courtesy of Manning Publications Co. book 'DuckDB in Action.' ) Check it out.](https://www.linkedin.com/feed/update/urn:li:activity:7221573493295910912/)\n\n[![evidence profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1699896475864510464%2F_XFRsLSv_400x400.jpg&w=96&q=75)\\\\\n\\\\\nevidence\\\\\n\\\\\n@evidence\\_dev\\\\\n\\\\\nDuckDB is increasingly being built into every data tool, but DuckDB isn't designed to be used as a cloud data warehouse\\\\\n\\\\\n@motherduck is solving that problem, and they\u2019re doing it in a delightful, uniquely DuckDB way](https://x.com/evidence_dev/status/1801630536252768584)\n\n[![Ethan profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1530999951609606144%2FPhNCpNOa_400x400.jpg&w=96&q=75)\\\\\n\\\\\nEthan\\\\\n\\\\\n@@ethanf\\_17\\\\\n\\\\\nDuckDB is my new best friend for analytics. It's so fast, makes it easy to manipulate dataframes, ingests anything, and exports it to whatever form I want. \\\\\n\\\\\nParquet, Arrow dataframes, and DuckDB are the future for analytics.](https://x.com/ethanf_17/status/1809835395695407529)\n\n[![uwe geercken profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1297147628%2FMyAvatar_x96.png&w=96&q=75)\\\\\n\\\\\nuwe geercken\\\\\n\\\\\n@uweeegeee\\\\\n\\\\\n#duckdb is really impressive: good terminal client, solid, easy and fast import of csv data and impressive speed on a 20mio data set. With automatic schema creation. Easy export to parquet and more. I think I am gonna spend more time on it...](https://twitter.com/uweeegeee/status/1611663518033498117)\n\n[![Etienne Posthumus profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1564546125352271882%2FxFZq7oWV.jpg&w=96&q=75)\\\\\n\\\\\nEtienne Posthumus\\\\\n\\\\\n@epoz\\\\\n\\\\\nI have been kicking the tyres of @motherduck - and it is \ud83e\udd2f\\\\\n\\\\\nBeing able to do hybrid queries over local and humongous remote datasets are incredible.\\\\\n\\\\\nThe potential applications are making my head spin with awe and excitement. Well done.](https://twitter.com/epoz/status/1688924943806013441)\n\n[![Jamin Ball profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1346111891618189312%2FSQzYMMst_x96.jpg&w=96&q=75)\\\\\n\\\\\nJamin Ball\\\\\n\\\\\n@jaminball\\\\\n\\\\\nAnother awesome & insightful post from the @motherduck team. The last few decades have been about scale-out. The next decade will be about scale-up! What you can accomplish with a single machine these days is incredibly impressive](https://twitter.com/jaminball/status/1659324081848864768)\n\n[![Danny Blumenthal profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FDanny_Blumenthal_7bb40ab793.jpeg&w=96&q=75)\\\\\n\\\\\nDanny Blumenthal\\\\\n\\\\\ndpblumenthal\\\\\n\\\\\nOur friends at MotherDuck stand out in the data space for how consistently they keep shipping awesome new features.](https://www.linkedin.com/posts/dpblumenthal_instant-sql-is-here-activity-7321571222222217218-mAm5/?utm_source=share&utm_medium=member_desktop&rcm=ACoAAACw0KEBx7lMe9uvdRJxJyQifAb-I4X7yQA)\n\n[![Nico Ritschel profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1671183451771527169%2Fg6RdaL5q_x96.jpg&w=96&q=75)\\\\\n\\\\\nNico Ritschel\\\\\n\\\\\n@nicoritschel\\\\\n\\\\\nI just deployed to Rill cloud backed by Motherduck and it worked great\u2014 wonderful experience...\\\\\nIn-browser CLI auth with Rill Cloud, added my Motherduck token, invited some viewers, I'm live. Took 3 minutes.](https://twitter.com/nicoritschel/status/1686595084454961152)\n\n[![Damon Cortesi profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1551732159416635392%2FIFOUWmsD_x96.jpg&w=96&q=75)\\\\\n\\\\\nDamon Cortesi\\\\\n\\\\\n@dacort\\\\\n\\\\\nBecause, I don't have enough time on my hands, I created a DuckDB extension that allows you to import data by scanning an Athena table. \\\\\n\\\\\nhttps://github.com/dacort/duckdb-athena-extension](https://twitter.com/dacort/status/1633952720561516544)\n\n[![George Fraser profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1571011203736174593%2FobtbMbnD_x96.jpg&w=96&q=75)\\\\\n\\\\\nGeorge Fraser\\\\\n\\\\\n@frasergeorgew\\\\\n\\\\\nHow fast is DuckDB compared to the best commercial data warehouses? I decided to benchmark it myself. Short version: very fast! But it's not (yet) great at scaling up to many cores. https://fivetran.com/blog/how-fast-is-duckdb-really](https://twitter.com/frasergeorgew/status/1638314840488923139)\n\n[![MotherDuck profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmedia.licdn.com%2Fdms%2Fimage%2Fv2%2FD560BAQEODuOUOAdoQg%2Fcompany-logo_100_100%2FB56ZfmzIGPG0AQ-%2F0%2F1751923851982%2Fmotherduck_logo%3Fe%3D1762992000%26v%3Dbeta%26t%3DelhKxwqBElgX_PNnhA3vQAu8QdPUILGxxa4doqv60-w&w=96&q=75)\\\\\n\\\\\nMotherDuck\\\\\n\\\\\n@motherduck\\\\\n\\\\\n@duckdb is empowering machine learning use cases! \ud83e\udd16\\\\\nAny good model training starts with some good data exploration.\\\\\n\\\\\n@huggingface now enables SQL queries using DuckDB on any dataset stored on the Hub!](https://twitter.com/motherduck/status/1666804451552935938)\n\n[![DuckDB profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1274363897676521474%2FqgbqYYuV_x96.jpg&w=96&q=75)\\\\\n\\\\\nDuckDB\\\\\n\\\\\n@duckdb\\\\\n\\\\\nDuckDB Co-Creator @hfmuehleisen and MotherDuck Founder @jrdntgn in this new podcast - among other things about the origin story of DuckDB and later MotherDuck:](https://twitter.com/duckdb/status/1649305101570306048)\n\n[![Garrett McClintock profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1438016780060950531%2FCn11Ke8O_400x400.jpg&w=96&q=75)\\\\\n\\\\\nGarrett McClintock\\\\\n\\\\\n@GJMcClintock\\\\\n\\\\\njust built a pipeline using @dltHub and @motherduck and it's very nice. 10/10 experience once you get the hang of it and very scalable.](https://x.com/GJMcClintock/status/1800962608939303002)\n\n[![Andrew Madson profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2F1706413992317_120c2864d9.jpeg&w=96&q=75)\\\\\n\\\\\nAndrew Madson\\\\\n\\\\\nandrew-madsen\\\\\n\\\\\nInstant SQL! This is one of the highlights of Data Council... Instant SQL caches a sample of your table instantly, and synchronously updates the query results, AS YOU TYPE!](https://www.linkedin.com/posts/andrew-madson_duckdb-instant-sql-this-is-one-of-the-activity-7321023818074894336-sbar?utm_source=share&utm_medium=member_desktop&rcm=ACoAAACw0KEBx7lMe9uvdRJxJyQifAb-I4X7yQA)\n\n[![KKovacs [\u30b9] profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F995607718508589058%2FtQPO3yiD_400x400.jpg&w=96&q=75)\\\\\n\\\\\nKKovacs \\[\u30b9\\]\\\\\n\\\\\n@kkovacs\\\\\n\\\\\nI don\u2019t want to say that #DuckDB is a silver bullet because there is no such thing, but it\u2019s surprisingly projectile-shaped, very shiny, and killed all the various werewolves (ok, data problems) I\u2019ve thrown at it](https://twitter.com/kkovacs/status/1801607688889012587)\n\n[![evidence profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1699896475864510464%2F_XFRsLSv_400x400.jpg&w=96&q=75)\\\\\n\\\\\nevidence\\\\\n\\\\\n@evidence\\_dev\\\\\n\\\\\nDuckDB is increasingly being built into every data tool, but DuckDB isn't designed to be used as a cloud data warehouse\\\\\n\\\\\n@motherduck\\\\\nis solving that problem, and they\u2019re doing it in a delightful, uniquely DuckDB way...](https://twitter.com/evidence_dev/status/1801630536252768584)\n\n[![Zach Wilson profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmedia.licdn.com%2Fdms%2Fimage%2Fv2%2FD5603AQGLd4zOUXaJJg%2Fprofile-displayphoto-shrink_200_200%2FB56ZcbuHVJHUAY-%2F0%2F1748516763636%3Fe%3D1762992000%26v%3Dbeta%26t%3DXmqD42PALO0Cur0sGLHL8Wo_3RMwSo9tGHycq0jpFI8&w=96&q=75)\\\\\n\\\\\nZach Wilson\\\\\n\\\\\neczachly\\\\\n\\\\\nMatt Martin and I did a runtime benchmark of Apache Spark vs DuckDB (by MotherDuck)\\\\\nWe found that data sets under 20 gigabytes ran about 100 times faster on DuckDB than they did on Apache Spark!\\\\\n\\\\\nThis benchmark uses plain parquet files and COUNT distinct to truly measure performance without skipping any data so we know the differences are correct.\\\\\n\\\\\nHere's the complete writeup: https://lnkd.in/gXuWGWyy](https://www.linkedin.com/posts/eczachly_matt-martin-and-i-did-a-runtime-benchmark-activity-7376304145978310657-GzkR/)\n\n[![Daniel Beach profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2F22616816.fs1.hubspotusercontent-na1.net%2Fhubfs%2F22616816%2Fdaniel%2520beach.jpg&w=96&q=75)\\\\\n\\\\\nDaniel Beach\\\\\n\\\\\n@dataengineeringcentral\\\\\n\\\\\nThis MotherDuck thingy is such a beauty it brings a tear to me old eye. You have no idea how many new tools (maybe you do) I agonize over while trying to bring you the good, the bad, and the ugly.](https://dataengineeringcentral.substack.com/p/honest-review-of-motherduck)\n\n[![Ryan Dolley profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fryan_dolley_a1178e8a04.jpeg&w=96&q=75)\\\\\n\\\\\nRyan Dolley\\\\\n\\\\\nryandolley\\\\\n\\\\\nMotherDuck ... bringing big innovation to SQL to make it more like a real language and less like a 'groovy ancient programming language from the 70s.' Check it out.](https://www.linkedin.com/posts/ryandolley_duckdb-activity-7320941264818884609-1fij?utm_source=share&utm_medium=member_desktop&rcm=ACoAAACw0KEBx7lMe9uvdRJxJyQifAb-I4X7yQA)\n\n[![Robert Lancer profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Frobert_lancer_e5d68edd81.jpeg&w=96&q=75)\\\\\n\\\\\nRobert Lancer\\\\\n\\\\\nrobertlancer\\\\\n\\\\\nThe progress on the MotherDuck UI is amazing! It went from minimal utility to my preferred place to write SQL in a matter of months](https://www.linkedin.com/feed/update/urn:li:ugcPost:7163558312741982208?commentUrn=ur[%E2%80%A6]89847054557184%2Curn%3Ali%3AugcPost%3A7163558312741982208%29)\n\n[![Caleb Fahlgren profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1716604301563289600%2FYycgFNAn_400x400.jpg&w=96&q=75)\\\\\n\\\\\nCaleb Fahlgren\\\\\n\\\\\n@calebfahlgren\\\\\n\\\\\nThe amount of data I am storing in \\\\\n@motherduck rn for a new project is criminal \ud83e\udd23\\\\\nDecided to try it out since I am a big \\\\\n@duckdb user. Is pretty good so far. Have few hundred million records.](https://x.com/calebfahlgren)\n\n[![Simon Sp\u00e4ti \ud83c\udfd4\ufe0f profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1137678291605676032%2FITc3M_ah_400x400.png&w=96&q=75)\\\\\n\\\\\nSimon Sp\u00e4ti \ud83c\udfd4\ufe0f\\\\\n\\\\\n@sspaeti\\\\\n\\\\\nI'm just checking a little about the hosted DuckDB by @motherduck. What a nice user experience. It automatically detects errors, and \\`accept & run\\` fixes common errors. Love it.](https://x.com/sspaeti/status/1798290369110749458)\n\n[![Jacob Matson profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1278773580122054656%2F3SG9rUwl_x96.jpg&w=96&q=75)\\\\\n\\\\\nJacob Matson\\\\\n\\\\\n@matsonj\\\\\n\\\\\nusing sql for data cleaning vs excel (reason 879 to learn sql)\\\\\n\\\\\nlevenstein distance between 590strings in excel (350k combinations): \\\\\nexcel: 25 mins of spinning. \\\\\nduckdb: 1.2s](https://twitter.com/matsonj/status/1669434599095816192)\n\n[![Bob Currier profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fbob_currier_d620fb0c2d.jpeg&w=96&q=75)\\\\\n\\\\\nBob Currier\\\\\n\\\\\nbobcurrier\\\\\n\\\\\nIf you're not familiar with DuckDB, you definitely want to give it a look. I've recently converted all my existing code from using CSV files and SQLite databases for exploratory analysis to parquet files, DuckDB and MotherDuck. The transformation in my workflow has been nothing short of astounding. 10GB of CSV files turned into 1GB of parquet files that are now stored in S3 buckets. Using DuckDB and MotherDuck, I can run exploratory queries seamlessly thanks to the DuckStack with dual execution with local processing and cloud scale. My query development time has decreased by 90%. And I'm loving the team feature of the MotherDuck platform. OCEANCODA is rocking the small data life! #duckdb #python #motherduck #eda #ai #ml (Graphic courtesy of Manning Publications Co. book 'DuckDB in Action.' ) Check it out.](https://www.linkedin.com/feed/update/urn:li:activity:7221573493295910912/)\n\n[![evidence profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1699896475864510464%2F_XFRsLSv_400x400.jpg&w=96&q=75)\\\\\n\\\\\nevidence\\\\\n\\\\\n@evidence\\_dev\\\\\n\\\\\nDuckDB is increasingly being built into every data tool, but DuckDB isn't designed to be used as a cloud data warehouse\\\\\n\\\\\n@motherduck is solving that problem, and they\u2019re doing it in a delightful, uniquely DuckDB way](https://x.com/evidence_dev/status/1801630536252768584)\n\n[![Ethan profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1530999951609606144%2FPhNCpNOa_400x400.jpg&w=96&q=75)\\\\\n\\\\\nEthan\\\\\n\\\\\n@@ethanf\\_17\\\\\n\\\\\nDuckDB is my new best friend for analytics. It's so fast, makes it easy to manipulate dataframes, ingests anything, and exports it to whatever form I want. \\\\\n\\\\\nParquet, Arrow dataframes, and DuckDB are the future for analytics.](https://x.com/ethanf_17/status/1809835395695407529)\n\nFOLLOW US\n\n[MotherDuck Twitter](https://twitter.com/motherduck) [MotherDuck LinkedIn](https://linkedin.com/company/motherduck/)\n\n## Join the flock\n\n[CHAT WITH US ON SLACK](https://slack.motherduck.com/)\n\n[![Mobile Slack banner](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fslack-banner-mobile.20467522.png&w=2048&q=75)![Desktop Slack banner](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fslack-banner-desktop.7f4b0dc6.png&w=2048&q=75)](https://slack.motherduck.com/)\n\n## SUBSCRIBE\n\n### Subscribe to our newsletter\n\nE-mail\n\nSubscribe to other MotherDuck emails\n\nSubmit\n\n![Newsletter section illus](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fnewsletter-home.c93874dd.png&w=3840&q=75)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 906,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 906
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! Now let me search for and read a couple more technical documents about the architecture and connection process:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ connect|attach|dual execution|hybrid execution

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     "content": "---\ntitle: announcing-motherduck-general-availability-data-warehousing-with-duckdb\ncontent_type: blog\nsource_url: https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb\nindexed_at: '2025-11-25T19:56:19.426175'\ncontent_hash: baf2faf8b3f47cb9\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Announcing MotherDuck General Availability: Data Warehousing with DuckDB at Scale\n\n2024/06/11 - 7 min read\n\nBY\nMotherDuck team\n\nOver the last year, thousands of users have tested, validated and helped improve MotherDuck as a serverless data warehouse and backend for interactive apps. We\u2019ve now solidified the product, pricing, partnerships, support teams and internal business processes needed to reach an important milestone: General Availability (GA).\n\nMotherDuck and DuckDB are making analytics ducking awesome for the 99% of users who do not need a complex data infrastructure and for whom [big data is truly dead](https://motherduck.com/blog/big-data-is-dead/). MotherDuck now has many customers in production experiencing the simplicity and efficiency of DuckDB with the collaboration and scale of a serverless cloud data warehouse.\n\n![MotherDuck goes GA summary](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmotherduck_goes_ga_embed_smaller_woline_56d7780de8.jpg&w=3840&q=75)\n\n## Production-ready DuckDB\n\nJust last week, DuckDB Labs announced that DuckDB has reached 1.0.0 and is now committed to backwards compatibility. In our [post congratulating the DuckDB team](https://motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release), we outlined why database nerds love DuckDB: performance, innovation velocity, versatility, ease of use, rich and user-friendly SQL, and extreme portability. Thanks to DuckDB, analytics can run virtually anywhere, liberated from the shackles of complex and expensive distributed systems. As an embedded database, it\u2019s the perfect \u2018Lego\u2019 building block that can snap into any process just by linking in a library. These same characteristics led us to build a cloud data warehouse on top of DuckDB and in collaboration with the creators.\n\n## Simple, Multiplayer at Scale\n\nMotherDuck makes it simple to start uploading and querying your data, whether it sits on your local machine, in blob storage or even on the web. The data can be in many different formats, including parquet, csv, json, Iceberg and Delta Lake. Your local DuckDB can work seamlessly with MotherDuck using [Dual Execution](https://motherduck.com/product/#:~:text=Hybrid,%20Dual%20Query%20Execution), with parts of your queries running locally and other parts scaling to the cloud.\n\nThe cloud creates unique opportunities for sharing data. MotherDuck allows you to upload your data and share a named snapshot with your colleagues in two lines of SQL. Although snapshots can be very useful to have a consistent view of your data across the team for tasks like building machine learning models, snapshots can also be automatically updated. Now with MotherDuck GA, shares can be [restricted to your organization and easily discoverable](https://motherduck.com/docs/key-tasks/sharing-data/sharing-within-org).\n\nA cloud data warehouse needs to scale for all your users and applications. MotherDuck eliminates fighting over common resources by assigning separate, isolated compute instances to each user and simplifying administration and costs for organizations. These compute instances individually scale up to handle workloads of many terabytes for some of our customers. They also scale down to zero when they\u2019re not being used, so you don\u2019t pay when you\u2019re not actively running queries.\n\n## Unmatched Efficiency of Pricing and Execution\n\nCustomers have frequently referenced [high costs for status-quo cloud data warehouses](https://motherduck.com/learn-more/modern-data-warehouse-playbook/) as a big concern. Because of the efficiency of DuckDB\u2019s query engine and MotherDuck\u2019s scale-up architecture, we\u2019re able to offer [pricing](https://motherduck.com/product/pricing/) that is often an order of magnitude lower than other alternatives\n\nNot only is the pricing competitive, but it\u2019s also fine-grained and efficient. By billing at second-level granularity, you only pay for the cloud CPU time you actually use. And, when we\u2019re able to take advantage of your local compute through Dual Execution, you don\u2019t pay at all.\n\n> \u201cWith MotherDuck working to solve amazing problems through data, our behaviors have changed because we know we don't have to pay enormous costs every time we run a query, so we've got almost limitless performance,\u201d said Ravi Chandra, Chief Technology Officer at Dexibit.\n\n## Backed by a World-Class Team\n\nThe team building MotherDuck hails from some of the top companies in data: Google BigQuery, Snowflake, Databricks, SingleStore and more. We\u2019re united by shared values and a shared mission to make analytics ducking awesome.\n\nOur friends at Looker were known to have the best customer success organization in the data industry: the Department of Customer Love, founded by [Margaret Rosas](https://www.linkedin.com/in/mrosas/). Margaret has joined us at MotherDuck to lead our [customer success team](https://motherduck.com/customer-support/), the Hatchery, where our customers are nurtured and taught to fly.\n\nAs we go GA, we also wanted to consolidate engineering under a single leader who can help us scale the team. \u200b\u200bWe\u2019ve asked [Frances Perry](https://www.linkedin.com/in/frances-perry/) to lead our engineering organization. Frances came to us from Google where she was an engineering director on Google Compute Engine, built Google\u2019s internal data processing infrastructure and also released that infrastructure to the world as Cloud Dataflow.\n\n## Now SOC 2 Certified\n\nWe know that [trust and security](https://motherduck.com/trust-and-security/) are critical as you choose a data warehouse to power your business. We leverage a defense in-depth strategy, maintain operational security processes, and build customer trust through certified auditor attestations.\n\nMotherDuck successfully underwent an audit for SOC 2 Type I, which evaluates our systems relevant to security, availability, and confidentiality. With this attestation completed, we have a Type II planned for later in 2024.\n\nTo continue strengthening internal processes and controls, [Myoung Kang](https://www.linkedin.com/in/myoungkang/) has joined the company full-time as Head of Operations. Myoung is a renowned startup veteran who has worked for many companies, including Notion, Convex, and Preset where she was interim CFO.\n\n## Expanded Modern Duck Stack\n\nMotherDuck partners with more than [50 leading companies and technologies](https://motherduck.com/ecosystem/) to make the [Modern Duck Stack](https://motherduck.com/product/#ecosystem). Alongside MotherDuck GA, we\u2019re excited to announce that some of the most requested BI, data integration and data observability tools have been added to the flock.\n\n- **Tableau**: 60,000 companies globally rely on Tableau (part of Salesforce) for data visualization. Tableau Desktop and Server now support MotherDuck, with Tableau Cloud support coming later this year. The connector can be easily found on the [Tableau Exchange](https://exchange.tableau.com/products/1021)\n- **PowerBI**: 5 million organizations worldwide use Microsoft Power BI for data visualization, including 97% of Fortune 500. The [MotherDuck connector](https://motherduck.com/docs/integrations/bi-tools/powerbi) for Power BI is officially launched, and MotherDuck has been accepted to the Microsoft for Startups Founders Hub program\n- **Fivetran**: Fivetran is the leader in data integration for the modern data stack, powering 5,000 customers. The [MotherDuck destination](https://fivetran.com/docs/destinations/motherduck) connector was developed in close collaboration with the Fivetran engineering team, and is now an official Fivetran destination.\n- **Monte Carlo**: Monte Carlo, the leader in data observability, has built a [MotherDuck integration](https://docs.getmontecarlo.com/docs/motherduck). It allows our customers to monitor their databases and look for anomalies through custom SQL rules, which can be created in either the UI wizard and/or programmatically via monitors as code.\n\n## New Startup Program with $10k in Credits\n\nMotherDuck has partnered with leading VC firms to offer $10k in credits to eligible startups in need of a data warehouse or backend for their data apps. Early stage startups with up to 300 employees and less than $100M in funding can submit a [short application](https://motherduck.com/startups/).\n\n## Take Flight with MotherDuck - Now GA\n\nIf you don\u2019t already have a MotherDuck account, visit [app.motherduck.com](https://app.motherduck.com/) to get started. We have a [fully-featured 30-day free trial of the Standard Plan](https://motherduck.com/product/pricing/) and a forever Free Plan available for ongoing usage.\n\n> \u201cOur data pipelines used to take eight hours. Now they're taking eight minutes, and I see a world where they take eight seconds. This is why we made the big bet on DuckDB and MotherDuck. It's only possible with DuckDB and MotherDuck,\u201d said Jim O'Neill, Co-founder and CTO at FinQore.\n\nIf you\u2019re not quite ready to get started, you can [learn more](https://motherduck.com/product/) about the product, [browse our docs](https://motherduck.com/docs/), and read about how [FinQore](https://motherduck.com/case-studies/saasworks/), [Dexibit](https://motherduck.com/case-studies/dexibit/) and [Mosaic](https://motherduck.com/case-studies/dominik-moritz/) use MotherDuck.\n\nWe also have an upcoming [live demo and discussion](https://motherduck.com/getting-started-with-motherduck/) on **Tuesday, June 18th at 10am Pacific**.\n\nLastly, if you\u2019re in San Francisco, [join us to celebrate](https://www.eventbrite.com/e/motherducking-party-after-dataai-summit-san-francisco-tickets-901904038257) tonight at our MotherDuck\u2019ing Party happening alongside the Data + AI Summit.\n\n### TABLE OF CONTENTS\n\n[Production-ready DuckDB](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#production-ready-duckdb)\n\n[Simple, Multiplayer at Scale](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#simple-multiplayer-at-scale)\n\n[Unmatched Efficiency of Pricing and Execution](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#unmatched-efficiency-of-pricing-and-execution)\n\n[Backed by a World-Class Team](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#backed-by-a-world-class-team)\n\n[Now SOC 2 Certified](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#now-soc-2-certified)\n\n[Expanded Modern Duck Stack](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#expanded-modern-duck-stack)\n\n[New Startup Program with $10k in Credits](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#new-startup-program-with-10k-in-credits)\n\n[Take Flight with MotherDuck - Now GA](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#take-flight-with-motherduck-now-ga)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![MotherDuck is Landing in Europe! Announcing our EU Region](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Feu_launch_blog_b165ff2751.png&w=3840&q=75)](https://motherduck.com/blog/motherduck-in-europe/)\n\n[2025/09/24 - Garrett O'Brien, Sheila Sitaram](https://motherduck.com/blog/motherduck-in-europe/)\n\n### [MotherDuck is Landing in Europe! Announcing our EU Region](https://motherduck.com/blog/motherduck-in-europe)\n\nServerless analytics built on DuckDB, running entirely in the EU.\n\n[![DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_4_1_b6209aca06.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n[2025/10/09 - Alex Monahan, Garrett O'Brien](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n### [DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/blog/announcing-duckdb-141-motherduck)\n\nMotherDuck now supports DuckDB 1.4.1 and DuckLake 0.3, with new SQL syntax, faster sorting, Iceberg interoperability, and more. Read on for the highlights from these major releases.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 146,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 146
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/is-bi-too-big-for-small-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/faster-data-pipelines-development-with-mcp-and-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/build-bigger-with-small-ai-running-small-models-locally.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 15,
  [2m  â”‚[0m   "appliedLimit": 15
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Great! Now let me read the dual execution video transcript for more technical details:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     "content": "---\ntitle: bringing-duckdb-to-the-cloud-dual-execution-explained\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained\nindexed_at: '2025-11-25T20:45:20.295187'\ncontent_hash: ab4030806c409492\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nBringing DuckDB to the Cloud: Dual Execution Explained - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Bringing DuckDB to the Cloud: Dual Execution Explained](https://www.youtube.com/watch?v=2eVNm4A8sLw)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=2eVNm4A8sLw&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 1:08:11\n\n\u2022Live\n\n\u2022\n\nYouTubeQuack & CodeMotherDuck Features\n\n# Bringing DuckDB to the Cloud: Dual Execution Explained\n\n2024/06/28\n\nBringing DuckDB's analytical power to the cloud requires more than just running it on a server. While DuckDB excels at processing data on a local machine, the dynamics of data analysis change when dealing with cloud-scale data, collaboration, and shared resources. This shift introduces challenges around [security](https://motherduck.com/learn-more/secure-startup-data-warehouse/), concurrent access, and performance, which is where a purpose-built cloud architecture becomes essential.\n\nMotherDuck, a modern cloud data warehouse, is built on DuckDB to solve these challenges. It extends DuckDB\u2019s capabilities without forking the open source project, creating a hybrid system that intelligently balances local and cloud computing. In a conversation with MotherDuck founding engineer Stephanie, she explained the architecture and the innovative query model, known as **dual execution**, that makes this possible.\n\n* * *\n\n## The MotherDuck Architecture: Beyond Hosted DuckDB\n\nA common misconception is that MotherDuck is simply DuckDB hosted in the cloud. The reality is a more sophisticated system designed to overcome the limitations of using a locally-optimized engine in a distributed environment. MotherDuck\u2019s architecture is built on three key components and a core philosophy of not forking DuckDB.\n\nInstead of maintaining a separate version, MotherDuck leverages DuckDB\u2019s powerful **extension system**. This allows MotherDuck to add new capabilities at multiple layers, including the SQL parser, optimizer, and storage interface, while staying current with the latest open source DuckDB releases. This tight integration means users benefit from the rapid innovation of the DuckDB community almost immediately.\n\nThe architecture can be broken down into three main layers:\n\n1. **The Client Extension**: This is how MotherDuck integrates with the DuckDB ecosystem. Whether a user is working in the CLI, a Python script, or a JDBC connection, a simple `ATTACH` command connects their local DuckDB instance to the MotherDuck cloud. This extension is even used to run DuckDB in the browser via WASM for the MotherDuck UI, enabling client-side processing to reduce latency and cloud compute costs.\n2. **The Compute Layer**: In the cloud, queries are processed by containerized DuckDB instances, fondly called \"ducklings.\" These compute resources are scaled based on user needs, providing the necessary CPU and memory to handle complex analytical queries on large datasets.\n3. **The Storage Layer**: DuckDB's native file format is optimized for a single writer on a local file system. This model is not well-suited for the cloud, where multiple users need to read and write to the same database concurrently. To solve this, MotherDuck implemented a differential storage system that maps a logical database file to append-only snapshot layers in cloud object storage. This design is cloud-friendly, enabling efficient in-place updates and forming the foundation for features like database sharing and time travel.\n\n* * *\n\n## Enabling Collaboration with Database Sharing and Secure Credentials\n\nThis unique architecture transforms DuckDB from a \"single-player\" tool into a collaborative \"multiplayer\" platform. One of the most significant advantages is **database sharing**. Instead of emailing SQL scripts or passing around large database files, team members within the same organization can grant query access to their databases. This streamlines collaboration, ensuring everyone works from a consistent and up-to-date version of the data.\n\nSecurity is another critical aspect of any cloud data platform. MotherDuck provides a **centralized Secret Manager**, co-designed with the DuckDB team. Users can create persistent, encrypted secrets for accessing external data sources like AWS S3, Google Cloud Storage, or Delta Lake. Once a secret is created in MotherDuck, it can be reused across different clients and sessions without needing to expose or reconfigure credentials on each local machine.\n\n* * *\n\n## Understanding Dual Execution\n\nThe most innovative feature of MotherDuck's architecture is its **dual execution model**. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\n\nThe process was demonstrated with a query joining two tables. When both tables reside in a MotherDuck database, the query is executed entirely in the cloud. An `EXPLAIN` plan for this query reveals that all operators, from the table scans to the final join, are marked as **remote**. The only local operation is downloading the final result set to the client.\n\nThe real power of dual execution becomes apparent when a query involves both local and cloud data. In the demonstration, one of the remote tables was copied to a local Parquet file. The query was then modified to join this local file with the table that remained in the cloud.\n\nThe `EXPLAIN` plan for this new query showed a mix of local and remote operations. The scan of the Parquet file and its associated processing happened **locally**, leveraging the user's machine. The scan of the cloud table happened **remotely** on MotherDuck's compute. The system then efficiently transferred only the necessary intermediate data to complete the join. This hybrid approach avoids needlessly uploading the local file to the cloud or downloading the entire remote table, resulting in faster and more efficient queries.\n\n* * *\n\n## Taking Control with Manual Execution Overrides\n\nWhile MotherDuck's optimizer is designed to make the most efficient choice automatically, some situations require manual control. For this, users can use the `md_run` parameter within scan functions like `read_parquet()` or `read_csv()`.\n\nBy setting `md_run = 'local'`, a user can force the scan of a remote S3 file to be executed on their local client. This involves downloading the data, which might be desirable if the data needs to be processed near the user's location or if they prefer using local credentials. Conversely, setting `md_run = 'remote'` forces the operation to run in the MotherDuck cloud.\n\nThis control is especially useful for performance tuning. A direct comparison showed that querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds). By pushing the computation to the data's location, MotherDuck minimizes network I/O and delivers results more quickly.\n\n* * *\n\n## A Symbiotic Future for Local and Cloud Analytics\n\nMotherDuck's architecture is a thoughtful extension of DuckDB's core principles, adapting its local-first power for the demands of the cloud. The dual execution model provides a flexible and efficient bridge between a user's laptop and cloud data, optimizing workloads in a way that pure-cloud or pure-local systems cannot.\n\nThis relationship is symbiotic. As the largest production user of DuckDB, MotherDuck continuously pushes the engine to its limits, uncovering opportunities for improvement and contributing enhancements back to the open source project. This collaboration ensures that both DuckDB and MotherDuck will continue to evolve, offering a powerful and seamless analytical experience that spans from local exploration to cloud-scale production.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Ficeberg_video_thumbnail_adfdabe3d6.jpg&w=3840&q=75)\\\\\n\\\\\n46:26](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n[2025-06-12](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n### [Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse)\n\nBuilding a data stack means choosing between easy SaaS and complex open-source. Apache Iceberg is a middle ground, but its catalog is a hurdle. New tools now simplify using Iceberg with DuckDB to create a flexible, local data lakehouse.\n\nMotherDuck Features\n\n[![\"Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FG_Se_B_Sox_AW_Fg_HD_f4fddaa9ab.jpg&w=3840&q=75)](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n[2025-04-23](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n### [Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly)\n\nHamilton Ulmer shares insights from MotherDuck's Instant SQL Mode, exploring how real-time query result previews eliminate the traditional write-run-debug cycle through client-side parsing and DuckDB-WASM caching.\n\nSQL\n\nTalk\n\nMotherDuck Features\n\n[![\"DuckDB: Run dbt build with sub-second execution times\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_03_26_at_3_40_16_PM_c6e3a8096f.png&w=3840&q=75)\\\\\n\\\\\n24:06](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n[2025-03-13](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n### [DuckDB: Run dbt build with sub-second execution times](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times)\n\nWhether you're new to DuckDB or looking to optimize your workflows, this session will provide practical insights to help you leverage its full potential.\n\nYouTube\n\nData Pipelines\n\ndbt\n\nMotherDuck Features\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 184,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 184
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! Now I have comprehensive information. Let me write the answer to the file:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_2.md

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me check if the file exists first:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ ls -la /tmp/answer_without_kg_2.md 2>&1 || echo "File does not exist"
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m -rw-------  1 julien  wheel  5196 Nov 27 14:00 /tmp/answer_without_kg_2.md
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m The file exists, let me read it first then write to it:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /tmp/answer_without_kg_2.md

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_2.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/tmp/answer_without_kg_2.md",
  [2m  â”‚[0m     "content": "# How MotherDuck Integrates with DuckDB\n\nMotherDuck extends DuckDB to the cloud through a sophisticated integration architecture that maintains full compatibility with the open-source database while adding cloud-scale capabilities.\n\n## Integration Architecture\n\n### Three-Layer System\n\nMotherDuck's integration is built on three core components:\n\n1. **Client Extension**: MotherDuck integrates directly with the DuckDB ecosystem through an extension system. Users can connect their local DuckDB instance to MotherDuck with a simple command, regardless of whether they're working in the CLI, Python, JDBC, or even the browser via WebAssembly.\n\n2. **Compute Layer**: Cloud-based queries are processed by containerized DuckDB instances called \"ducklings.\" These compute resources scale based on user needs, providing the necessary CPU and memory for analytical queries.\n\n3. **Storage Layer**: Instead of DuckDB's native single-writer file format, MotherDuck implements a differential storage system that maps logical database files to append-only snapshot layers in cloud object storage. This enables concurrent access, database sharing, and time travel capabilities.\n\n### Non-Fork Philosophy\n\nCritically, MotherDuck does not fork DuckDB. Instead, it leverages DuckDB's powerful extension system to add capabilities at multiple layers including the SQL parser, optimizer, and storage interface. This allows MotherDuck to stay current with the latest open-source DuckDB releases and benefit from rapid community innovation.\n\n## Simple Connection\n\nConnecting to MotherDuck from DuckDB requires only a single command:\n\n```sql\nATTACH 'md:';\n```\n\nUsers simply need a MotherDuck token (which can be set as an environment variable) to authenticate. Once connected, the same DuckDB interface works seamlessly across local and cloud environments.\n\n## Dual Execution Model\n\nThe most innovative feature of MotherDuck's integration is **dual execution** - a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the cloud.\n\n### How It Works\n\n- When all data resides in MotherDuck databases, queries execute entirely in the cloud\n- When queries involve both local and cloud data, the system runs a mix of local and remote operations\n- The optimizer transfers only the necessary intermediate data to complete operations, avoiding needless data movement\n\n### Performance Benefits\n\nBy pushing computation to where the data lives, MotherDuck minimizes network I/O. In demonstrations, querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds).\n\n### Manual Control\n\nUsers can override automatic optimization using the `md_run` parameter:\n- `md_run = 'local'` forces execution on the local client\n- `md_run = 'remote'` forces execution in the MotherDuck cloud\n\n## Local Development, Cloud Production\n\nMotherDuck enables a seamless workflow where developers can:\n\n1. **Develop locally** with lightning-fast feedback cycles at zero cost\n2. **Deploy to cloud** by simply adding \"md:\" to their connection string\n3. **Run identical workloads** in both environments with zero code changes\n\nThis \"local dev, cloud prod\" model ensures what works locally will perform at scale in production.\n\n## Collaboration Features\n\nThe integration transforms DuckDB from a \"single-player\" tool into a collaborative \"multiplayer\" platform:\n\n- **Database Sharing**: Team members can grant query access to their databases without emailing files or managing S3 buckets\n- **Concurrent Access**: Multiple users can query the same datasets simultaneously, each getting their own dedicated \"duckling\" compute instance\n- **Centralized Secret Manager**: Create persistent, encrypted credentials for cloud storage that work across different clients and sessions\n\n## Extension Support\n\nMotherDuck's client extension can run DuckDB in various environments:\n- CLI interfaces\n- Python scripts\n- JDBC connections\n- Web browsers via WebAssembly (used in the MotherDuck UI for client-side processing)\n\nThis flexibility ensures users can work with their preferred tools while seamlessly accessing both local and cloud data.\n\n## Symbiotic Relationship\n\nAs the largest production user of DuckDB, MotherDuck continuously pushes the engine to its limits and contributes enhancements back to the open-source project. This ensures both DuckDB and MotherDuck evolve together, offering a powerful analytical experience from local exploration to cloud-scale production.\n\n---\n\n## Sources\n\n- **Getting Started with MotherDuck**: `.kurt/sources/motherduck.com/getting-started-with-motherduck.md`\n- **DuckDB & MotherDuck for Beginners Guide**: `.kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md`\n- **Bringing DuckDB to the Cloud: Dual Execution Explained**: `.kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md`\n- **DuckDB Users Product Page**: `.kurt/sources/motherduck.com/product/duckdb-users.md`\n- **MotherDuck Homepage**: `.kurt/sources/motherduck.com/index.md`\n",
  [2m  â”‚[0m     "numLines": 92,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 92
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "update",
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_2.md",
  [2m  â”‚[0m   "content": "<6033 chars>",
  [2m  â”‚[0m   "structuredPatch": [
  [2m  â”‚[0m     {
  [2m  â”‚[0m       "oldStart": 1,
  [2m  â”‚[0m       "oldLines": 91,
  [2m  â”‚[0m       "newStart": 1,
  [2m  â”‚[0m       "newLines": 54,
  [2m  â”‚[0m       "lines": [
  [2m  â”‚[0m         "-# How MotherDuck Integrates with DuckDB",
  [2m  â”‚[0m         "+# Answer",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-MotherDuck extends DuckDB to the cloud through a sophisticated integration architecture that maintains full compatibility with the open-source database while adding cloud-scale capabilities.",
  [2m  â”‚[0m         "+MotherDuck integrates with DuckDB through a sophisticated hybrid architecture built on three core principles: extending DuckDB without forking it, enabling seamless local-to-cloud connectivity, and implementing an intelligent dual execution model.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Integration Architecture",
  [2m  â”‚[0m         "+## Connection and Extension Architecture",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### Three-Layer System",
  [2m  â”‚[0m         "+MotherDuck integrates with DuckDB using DuckDB's powerful **extension system** rather than maintaining a separate forked version. Users can connect their local DuckDB instance to MotherDuck with a simple `ATTACH` command, which works across multiple environments including the CLI, Python scripts, JDBC connections, and even in the browser via WebAssembly (WASM). This extension-based approach allows MotherDuck to add capabilities at multiple layers\u2014including the SQL parser, optimizer, and storage interface\u2014while staying current with the latest open source DuckDB releases.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-MotherDuck's integration is built on three core components:",
  [2m  â”‚[0m         "+The architecture consists of three main layers:",
  [2m  â”‚[0m         "+1. **Client Extension**: Enables integration across the DuckDB ecosystem and runs DuckDB in the browser via WASM for the MotherDuck UI",
  [2m  â”‚[0m         "+2. **Compute Layer**: Processes queries using containerized DuckDB instances (\"ducklings\") in the cloud that scale based on user needs",
  [2m  â”‚[0m         "+3. **Storage Layer**: Implements a differential storage system that maps logical database files to append-only snapshot layers in cloud object storage, solving DuckDB's single-writer limitation for multi-user cloud environments",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-1. **Client Extension**: MotherDuck integrates directly with the DuckDB ecosystem through an extension system. Users can connect their local DuckDB instance to MotherDuck with a simple command, regardless of whether they're working in the CLI, Python, JDBC, or even the browser via WebAssembly.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-2. **Compute Layer**: Cloud-based queries are processed by containerized DuckDB instances called \"ducklings.\" These compute resources scale based on user needs, providing the necessary CPU and memory for analytical queries.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-3. **Storage Layer**: Instead of DuckDB's native single-writer file format, MotherDuck implements a differential storage system that maps logical database files to append-only snapshot layers in cloud object storage. This enables concurrent access, database sharing, and time travel capabilities.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-### Non-Fork Philosophy",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-Critically, MotherDuck does not fork DuckDB. Instead, it leverages DuckDB's powerful extension system to add capabilities at multiple layers including the SQL parser, optimizer, and storage interface. This allows MotherDuck to stay current with the latest open-source DuckDB releases and benefit from rapid community innovation.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-## Simple Connection",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-Connecting to MotherDuck from DuckDB requires only a single command:",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-```sql",
  [2m  â”‚[0m         "-ATTACH 'md:';",
  [2m  â”‚[0m         "-```",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-Users simply need a MotherDuck token (which can be set as an environment variable) to authenticate. Once connected, the same DuckDB interface works seamlessly across local and cloud environments.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         " ## Dual Execution Model",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-The most innovative feature of MotherDuck's integration is **dual execution** - a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the cloud.",
  [2m  â”‚[0m         "+The most innovative aspect of MotherDuck's integration is its **dual execution model**, which treats a user's laptop and the MotherDuck cloud as two nodes in a single distributed system. When a query is executed, MotherDuck's optimizer intelligently decides whether to run parts of the query locally on the client or remotely in the cloud, with the primary goal of minimizing data movement.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### How It Works",
  [2m  â”‚[0m         "+For example, when joining a local CSV file with a cloud-based table:",
  [2m  â”‚[0m         "+- The system scans the local CSV on the user's laptop",
  [2m  â”‚[0m         "+- Applies filters locally to reduce data size",
  [2m  â”‚[0m         "+- Transfers only the minimal filtered data (potentially just a few bytes) to the cloud",
  [2m  â”‚[0m         "+- Performs the final join against the large cloud table in MotherDuck",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-- When all data resides in MotherDuck databases, queries execute entirely in the cloud",
  [2m  â”‚[0m         "-- When queries involve both local and cloud data, the system runs a mix of local and remote operations",
  [2m  â”‚[0m         "-- The optimizer transfers only the necessary intermediate data to complete operations, avoiding needless data movement",
  [2m  â”‚[0m         "+This approach can reduce network traffic by orders of magnitude compared to traditional methods that would require uploading entire local files or downloading complete cloud tables. Users can verify which parts of a query run where using the `EXPLAIN` statement, which shows local operations marked as `(L)` and remote operations as `(R)`.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### Performance Benefits",
  [2m  â”‚[0m         "+## Single-Engine Semantics",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-By pushing computation to where the data lives, MotherDuck minimizes network I/O. In demonstrations, querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds).",
  [2m  â”‚[0m         "+Because MotherDuck uses the **exact same DuckDB engine** both locally and in the cloud, queries validated on a laptop are guaranteed to behave identically in production. This eliminates the common \"it worked on my machine\" problem and provides true dev-to-prod consistency. This architecture offers three key benefits:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### Manual Control",
  [2m  â”‚[0m         "+1. **Faster Development**: Engineers can build and test pipelines with zero-latency local feedback before scaling to cloud",
  [2m  â”‚[0m         "+2. **Lower Costs**: By leveraging free local compute and minimizing data transfer, cloud bills are significantly reduced",
  [2m  â”‚[0m         "+3. **Better Collaboration**: Transforms the traditionally single-player DuckDB into a multiplayer platform where teams can share databases",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-Users can override automatic optimization using the `md_run` parameter:",
  [2m  â”‚[0m         "-- `md_run = 'local'` forces execution on the local client",
  [2m  â”‚[0m         "-- `md_run = 'remote'` forces execution in the MotherDuck cloud",
  [2m  â”‚[0m         "+## Additional Cloud Features",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Local Development, Cloud Production",
  [2m  â”‚[0m         "+MotherDuck extends DuckDB's capabilities with cloud-native features:",
  [2m  â”‚[0m         "+- **Database Sharing**: Team members can grant query access to databases without emailing files or scripts",
  [2m  â”‚[0m         "+- **Secret Manager**: Centralized, encrypted storage for credentials to access external data sources (S3, GCS, Delta Lake)",
  [2m  â”‚[0m         "+- **Per-User Tenancy**: Each user gets isolated DuckDB instances (ducklings) in various sizes (pulse, standard, jumbo, mega, giga)",
  [2m  â”‚[0m         "+- **Serverless Scaling**: Compute scales to zero when not in use, with billing at second-level granularity",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-MotherDuck enables a seamless workflow where developers can:",
  [2m  â”‚[0m         "+## Manual Control Options",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-1. **Develop locally** with lightning-fast feedback cycles at zero cost",
  [2m  â”‚[0m         "-2. **Deploy to cloud** by simply adding \"md:\" to their connection string",
  [2m  â”‚[0m         "-3. **Run identical workloads** in both environments with zero code changes",
  [2m  â”‚[0m         "+While the optimizer typically makes efficient choices automatically, users can override execution location using the `md_run` parameter in scan functions like `read_parquet()` or `read_csv()`. Setting `md_run = 'remote'` forces operations to run in MotherDuck's cloud, while `md_run = 'local'` forces local execution. This control is useful for performance tuning\u2014for example, querying a 2GB Parquet file on S3 remotely in MotherDuck was shown to be significantly faster (25 seconds) than downloading it locally first (36 seconds).",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-This \"local dev, cloud prod\" model ensures what works locally will perform at scale in production.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-## Collaboration Features",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-The integration transforms DuckDB from a \"single-player\" tool into a collaborative \"multiplayer\" platform:",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-- **Database Sharing**: Team members can grant query access to their databases without emailing files or managing S3 buckets",
  [2m  â”‚[0m         "-- **Concurrent Access**: Multiple users can query the same datasets simultaneously, each getting their own dedicated \"duckling\" compute instance",
  [2m  â”‚[0m         "-- **Centralized Secret Manager**: Create persistent, encrypted credentials for cloud storage that work across different clients and sessions",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-## Extension Support",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-MotherDuck's client extension can run DuckDB in various environments:",
  [2m  â”‚[0m         "-- CLI interfaces",
  [2m  â”‚[0m         "-- Python scripts",
  [2m  â”‚[0m         "-- JDBC connections",
  [2m  â”‚[0m         "-- Web browsers via WebAssembly (used in the MotherDuck UI for client-side processing)",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-This flexibility ensures users can work with their preferred tools while seamlessly accessing both local and cloud data.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-## Symbiotic Relationship",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-As the largest production user of DuckDB, MotherDuck continuously pushes the engine to its limits and contributes enhancements back to the open-source project. This ensures both DuckDB and MotherDuck evolve together, offering a powerful analytical experience from local exploration to cloud-scale production.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "----",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         " ## Sources",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-- **Getting Started with MotherDuck**: `.kurt/sources/motherduck.com/getting-started-with-motherduck.md`",
  [2m  â”‚[0m         "-- **DuckDB & MotherDuck for Beginners Guide**: `.kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md`",
  [2m  â”‚[0m         "-- **Bringing DuckDB to the Cloud: Dual Execution Explained**: `.kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md`",
  [2m  â”‚[0m         "-- **DuckDB Users Product Page**: `.kurt/sources/motherduck.com/product/duckdb-users.md`",
  [2m  â”‚[0m         "-- **MotherDuck Homepage**: `.kurt/sources/motherduck.com/index.md`",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md - Comprehensive explanation of dual execution, hybrid query processing, how MotherDuck minimizes data transfer, and the benefits of single-engine semantics across local and cloud environments",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/videos/getting-started-with-motherduck-webinar.md - Overview webinar covering MotherDuck features and DuckDB analyst ergonomics",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md - Original announcement explaining hybrid execution architecture, how local and cloud nodes work together, and the distributed system model where DuckDB instances connect to MotherDuck",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/index.md - Homepage content describing MotherDuck's architecture, per-user tenancy model, duckling sizes, and how it scales DuckDB",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md - Academic paper reference on MotherDuck's hybrid query processing architecture developed in collaboration between MotherDuck, DuckDB Labs, and CWI research institute",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md - General availability announcement detailing dual execution, per-user compute instances, scale-to-zero architecture, and serverless capabilities",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md - Detailed technical explanation of MotherDuck's architecture including the extension system, three-layer architecture, dual execution model with query examples, and manual execution control options"
  [2m  â”‚[0m       ]
  [2m  â”‚[0m     }
  [2m  â”‚[0m   ]
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! I have successfully completed the task. Here's a summary of what I did:
  [94mâ”‚[0m 
  [94mâ”‚[0m ## Task Completion Summary
  [94mâ”‚[0m 
  [94mâ”‚[0m I searched through the `.kurt/sources/` directory to answer the question "How does MotherDuck integrate with DuckDB?" and wrote a comprehensive answer to `/tmp/answer_without_kg_2.md`.
  [94mâ”‚[0m 
  [94mâ”‚[0m ### Research Process:
  [94mâ”‚[0m 1. **Searched for relevant files** using glob and grep to find content about MotherDuck, DuckDB integration, architecture, and hybrid/dual execution
  [94mâ”‚[0m 2. **Read 7 key source files** to gather comprehensive information:
  [94mâ”‚[0m    - Hybrid analytics guide (technical deep-dive)
  [94mâ”‚[0m    - Getting started webinar
  [94mâ”‚[0m    - Original MotherDuck announcement blog post
  [94mâ”‚[0m    - Homepage content
  [94mâ”‚[0m    - CIDR academic paper reference
  [94mâ”‚[0m    - General availability announcement
  [94mâ”‚[0m    - Dual execution explained video transcript
  [94mâ”‚[0m 
  [94mâ”‚[0m ### Answer Structure:
  [94mâ”‚[0m The answer covers:
  [94mâ”‚[0m - **Connection & Extension Architecture**: How MotherDuck uses DuckDB's extension system with a 3-layer architecture
  [94mâ”‚[0m - **Dual Execution Model**: The innovative hybrid query processing that intelligently splits work between local and cloud
  [94mâ”‚[0m - **Single-Engine Semantics**: How using the same engine everywhere eliminates dev-prod drift
  [94mâ”‚[0m - **Additional Cloud Features**: Database sharing, secret manager, per-user tenancy, serverless scaling
  [94mâ”‚[0m - **Manual Control Options**: The `md_run` parameter for performance tuning
  [94mâ”‚[0m 
  [94mâ”‚[0m ### Sources Section:
  [94mâ”‚[0m All 7 files I actually read are listed with their exact local file paths and descriptions of what information was used from each source, as required.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 11,591 | Cost: $0.4750
     Cumulative: 11,591 tokens | $0.4750
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âš ï¸  Agent asked question but no UserAgent available
     Detection: heuristic: Strong question indicator detected

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 16 | Duration: 104.5s
â•‘    Tokens: 11,591 | Cost: $0.4750
â•‘    Stop reason: Agent asked question but no UserAgent available
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  LLM Judge score: 1.00

======================================================================
â“ Question 3/10
...What SQL features from DuckDB are not yet supported in MotherDuck?
======================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 3
â”‚ Answer this question by searching through the .kurt/sources/ directory:

...What SQL features from DuckDB are not yet supported in MotherDuck?

Required Steps:
1. Search for relevant markdown files in .kurt/sources/ using glob and grep
2. Read the most relevant files (aim for 3-5 most relevant sources)
3. As you read each file, keep track of:
   - The file path
   - The file title/name
   - Key information extracted from that file
4. Synthesize a comprehensive answer based on the information found
5. Write your answer to: /tmp/answer_without_kg_3.md

IMPORTANT - Your answer MUST have this structure:
```markdown
# Answer

[Your comprehensive answer here, drawing from the sources you read]

## Sources

List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
[etc.]

DO NOT use URLs or document titles as links. Show the actual file path you read with the Read tool.
```

The Sources section is MANDATORY - you must list every file you read to answer the question.
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I'll help you find information about SQL features from DuckDB that are not yet supported in MotherDuck. Let me search through the .kurt/sources/ directory.
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/**/*.md

  [96mğŸ”§ TOOL:[0m Grep â†’ not supported|unsupported|limitation|doesn't support|does not support|unavailable

  [96mğŸ”§ TOOL:[0m Grep â†’ DuckDB.*not|feature.*not available|missing feature
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/acceptable-use-policy.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/about-us.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/adithya-krishnan.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/aditya-aomani.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/alex-monahan.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/antony-courtney.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/doug-raymond.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/amrutha-gujjar.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/andrew-mcewen.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/brett-griffin.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/anna-geller.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/emily-lucek.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/daniel-palma.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/andrew-witten.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/diptanu-gon-choudhury.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/christophe-oudar.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/boaz-leskes.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/faraz-hameed.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/manveer-chawla.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/leslie-thomas.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/luciano-galvao-filho.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/hamilton-ulmer.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/joseph-hwang.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/garrett-obrien.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/graham-wetzler.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/jacob-matson.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/jordan-tigani.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/jelte-fennema-nio.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/jessica-libman.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/nick-ursa.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/peter-boncz.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/mehdi-ouazza.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/nouras-haddad.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/marcos-ortiz.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/marc-d.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/oli-dinov.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/mike-xu.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/michael-hunger.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/mike-spaulding.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/simon-spati.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/thomas-tfmv-mcgeehan.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/ryan-boyd.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/stephanie-wang.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/sahil-gupta.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/tino-tereshko.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/sean-m-tracey.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/saqib-rasul.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/sheila-sitaram.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/sung-won-chung.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/pranav-aurora.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/till-dohmen.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/yannick-welsch.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/zhenni-wu.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/yves-le-maout.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/authors/vasilije-markovic.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/analyze-json-data-using-sql.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/analyze-sqlite-databases-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/MotherDuck-Visualize-Embeddings-Marimo.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/analyze-data-in-azure-with-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-duckdb-snippet-sets-with-motherduck-sharing-databases.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/analyze-x-data-nodejs-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-mega-giga-instance-sizes-huge-scale.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/bigquery-to-duckdb-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-series-seed-and-a.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-small-data-sf-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/building-motherduck-partner-ecosystem.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/big-data-is-dead.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/building-data-applications-with-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-engineers-answer-10-top-reddit-questions.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-engineering-toolkit-infrastructure-devops.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-engineer-highlights-PyConDE-2023.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-app-generator.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-engineering-toolkit-essential-tools.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/differential-storage-building-block-for-data-warehouse.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-cognee-sql-analytics-graph-rag.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/dosomething-motherduck-data-warehouse-ROI.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-dec-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-110-hidden-gems.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-oct-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-nov-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2023.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2025.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-four.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "durationMs": 8,
  [2m  â”‚[0m   "numFiles": 100,
  [2m  â”‚[0m   "truncated": true
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/is-bi-too-big-for-small-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-duck-in-the-hand-is-worth-two-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-new-paradigm-for-data-visualization-with-just-sql-markdown.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/quacking-around-the-web.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-OLAP.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/cultivating-growth-how-gardyn-scaled-its-data-operations-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/an-evolving-dag-for-the-llm-world-julia-schottenstein-of-langchain-at-small-data-sf-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-use.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-OLAP.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/privacy-policy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me search more specifically for terms related to DuckDB features and limitations in MotherDuck:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ limitation|constraint|difference|caveat

  [96mğŸ”§ TOOL:[0m Grep â†’ SQL.*feature|extension.*not|function.*not

  [96mğŸ”§ TOOL:[0m Grep â†’ motherduck.*does not|motherduck.*doesn't|cannot.*motherduck
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/cultivating-growth-how-gardyn-scaled-its-data-operations-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/an-evolving-dag-for-the-llm-world-julia-schottenstein-of-langchain-at-small-data-sf-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/build-bigger-with-small-ai-running-small-models-locally.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-use.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/quacking-around-the-web.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-OLAP.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/privacy-policy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-if-sql-queries-returned-results-instantly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/lies-damn-lies-and-benchmarks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/machine-learning-and-ai-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-use.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-use.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/candidate-privacy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/effortless-etl-unstructured-data-unstructuredio-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-excel-extension.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 15,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me look for documentation files that might have information about features or FAQs:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/motherduck.com/docs/**/*.md

  [96mğŸ”§ TOOL:[0m Grep â†’ FAQ|frequently asked|known issue|limitation
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "durationMs": 2,
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/cultivating-growth-how-gardyn-scaled-its-data-operations-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/an-evolving-dag-for-the-llm-world-julia-schottenstein-of-langchain-at-small-data-sf-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-use.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-OLAP.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/privacy-policy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/pandas-dataframes-guide.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search for specific technical content about features and differences:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ difference.*DuckDB|DuckDB.*MotherDuck|feature.*support

  [96mğŸ”§ TOOL:[0m Grep â†’ attach.*database|local.*cloud|hybrid

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/motherduck.com/blog/*motherduck*.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/MotherDuck-Visualize-Embeddings-Marimo.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-duckdb-snippet-sets-with-motherduck-sharing-databases.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/bigquery-to-duckdb-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/building-data-applications-with-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/building-motherduck-partner-ecosystem.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/dosomething-motherduck-data-warehouse-ROI.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/duckdb-puppygraph-graph-model-on-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/ducklake-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-1.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/effortless-etl-unstructured-data-unstructuredio-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/geospatial-for-beginner-duckdb-spatial-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/google-sheets-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/llm-data-pipelines-prompt-motherduck-dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/json-log-analysis-duckdb-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/introducing-motherduck-for-business-analytics.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/motherduck-in-europe.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/motherduck-data-warehouse.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/motherduck-headquarters-seattle-opening.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/motherduck-ai-sql-fixit-inline-editing-features.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/log-processing-savings-bacalhau-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/motherduck-open-for-all-with-series-b.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/motherduck-union-orchestration.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/motherduck-kestra-etl-pipelines.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/new-collaboration-sharing-motherduck-data-warehouse-organization-auto-join.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/motherduck-reflections-sigmod-pods-2024.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/motherduck-window-functions-in-sql.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/motherduck-vercel-marketplace-native-integration.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/pg_duckdb-postgresql-extension-for-duckdb-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/pg to motherduck at motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/pushing-geo-boundaries-with-motherduck-geobase.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/secoda-motherduck-integration-modern-duck-stack.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/simplifying-iot-analytics-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/streaming-data-to-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/streamkap-mysql-to-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/streaming-oracle-to-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/unstructured-analysis-tensorlake-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_e1tng3gc/.kurt/sources/motherduck.com/blog/tableau-cloud-motherduck.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "durationMs": 2,
  [2m  â”‚[0m   "numFiles": 48,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/scaling-duckdb-panel-ondemand.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/take-flight-with-the-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-power-of-wasm-for-analytics-duckdb-in-the-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-surprising-birth-of-duckdb-ft-co-creator-hannes-muhleisen.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/faster-data-pipelines-development-with-mcp-and-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/an-evolving-dag-for-the-llm-world-julia-schottenstein-of-langchain-at-small-data-sf-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/build-bigger-with-small-ai-running-small-models-locally.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-duck-in-the-hand-is-worth-two-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/postgres-integration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/quacking-around-the-web.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/react-components-testing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/sql-duckdb-book-form.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Good! Now let me read some of the most relevant files. Let me start with the announcing MotherDuck posts and the hybrid query processing paper:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     "content": "---\ntitle: announcing-motherduck-duckdb-in-the-cloud\ncontent_type: blog\nsource_url: https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud\nindexed_at: '2025-11-25T19:57:33.138925'\ncontent_hash: a5ee88402c607603\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Announcing MotherDuck: Hybrid Execution Scales DuckDB from your Laptop into the Cloud\n\n2023/06/22 - 4 min read\n\nBY\nMotherDuck team\n\nDuckDB has become widely known as \u201cSQLite for Analytics\u201d \u2013 a powerful SQL analytics engine with broad adoption in development workflows, ad-hoc analytics on the laptop and embedded applications. MotherDuck wants to make it even easier to use, so we\u2019ve worked alongside the creators of DuckDB to build a cloud-based serverless analytics platform. Today is a large milestone in that journey \u2013 MotherDuck is now available by invitation.\n\nMotherDuck in 100 seconds (by a duck \ud83e\udd86) - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[MotherDuck in 100 seconds (by a duck \ud83e\udd86)](https://www.youtube.com/watch?list=PLIYcNkSjh-0zP7fwKzhnbx5ur1Mf80Q_p&v=BINA_ytZXMY)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\n1/1\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nFull screen is unavailable. [Learn More](https://support.google.com/youtube/answer/6276924)\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?list=PLIYcNkSjh-0zP7fwKzhnbx5ur1Mf80Q_p&v=BINA_ytZXMY&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n[Previous (SHIFT+p)](https://www.youtube.com/watch?list=PLIYcNkSjh-0zP7fwKzhnbx5ur1Mf80Q_p&v=BINA_ytZXMY \"Previous (SHIFT+p)\")\n\n0:00 / 1:51\n\n\u2022Live\n\n\u2022\n\n## Hybrid execution: cloud and laptop working together\n\nData scientists, analysts, and engineers love DuckDB because it works great no matter where their data lives. Since many data professionals have powerful laptops sitting 85% idle, they often want to bring the data to their local machine to make it even more efficient to crunch, especially when performing ad hoc analysis and development. MotherDuck lets you analyze this local data locally, while still JOINing with data processed in the cloud, giving you efficient use of all your compute resources.\n\nIn the example below, the table `yellow_cab_nyc` lives in MotherDuck in the cloud, and I have a CSV on my laptop table with currency conversions. We want to see the average cost of NYC taxi trips by passenger count in different currencies by JOINing these two tables. Yes, we\u2019re seamlessly joining data on my laptop with data in the cloud!\n\n![Post asset](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fexample_hybrid_7bb8f08e2d.png&w=3840&q=75)\n\nYou can even do hybrid query execution with data stored in s3, with MotherDuck securely storing and managing your AWS credentials.\n\n![example_s3.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fexample_s3_d97e746203.png&w=3840&q=75)\n\nNote, these examples are part of our [sample datasets and queries](https://motherduck.com/docs/category/sample-datasets--queries/), feel free to run them yourself!\n\nYou might wonder how this works under the covers. By connecting your DuckDB instance to MotherDuck, you establish a radically different type of distributed system - one, in which one node is MotherDuck in the cloud, and another node is wherever your DuckDB lives, be it your laptop or a lambda, Python or CLI, JDBC or MotherDuck\u2019s own web app. Both nodes execute queries in concert in the most optimal way, automatically routing parts of queries to the right location.\n\n## MotherDuck includes a web notebook and Git-style Collaboration\n\nWant to run some quick SQL queries without downloading and installing DuckDB? The MotherDuck web application provides a notebook-like UI. This enables you to analyze local CSVs and parquet files, upload them and manage them alongside your other data stored in MotherDuck.\n\n![app_motherduck_beta.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fapp_motherduck_beta_v2_69fbe04dcf.png%3Fupdated_at%3D2023-06-21T23%3A24%3A18.647Z&w=3840&q=75)\n\nAs a DuckDB-in-the-cloud company, naturally MotherDuck embeds DuckDB in its web application using WASM. Results of your SQL queries are cached in this DuckDB instance, enabling you to instantly sort, pivot, and filter query results!\n\nWant to share your DuckDB data with colleagues? Using SQL, you can create a shareable snapshot of your data, which your colleagues can easily attach in MotherDuck.\n![example_share.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fexample_share_cfe1d14570.png&w=3840&q=75)\n\nThis SQL command will return a shareable URL which can then be used by your colleague to access the shared database.\n![example_attach.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fexample_attach_ff22a57c32.png&w=3840&q=75)\n\n## Anywhere you can Duck, you can MotherDuck\n\nDuckDB has been starred by over 10k developers on GitHub, and it might be due to the simplicity of getting up and running with a downloadable, open source analytics engine. We want to continue (and improve!) that amazing experience as we bring DuckDB to the cloud.\n\nOne way to do this is by ensuring MotherDuck works well with many of the most popular technologies in the modern data stack, including ingestion, orchestration and BI+Visualization tools.\n\n![partner-logos.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fpartner_logos_242d207bb6.png&w=3840&q=75)\n\nWe strive to make MotherDuck as easy to adopt as DuckDB. To that end, any DuckDB instance in the world running in Python or CLI can connect to MotherDuck with a single line of code. Suddenly, by running this command your DuckDB magically becomes supercharged by MotherDuck. Such ease of onboarding could only have been possible via close collaboration with the creators of DuckDB!\n\n![d_to_md.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fd_to_md_1dfe63b09b.png&w=3840&q=75)\n\n## Continuing to Make Analytics Ducking Awesome\n\nOne of the primary reasons we were driven to build a serverless analytics platform on top of DuckDB was their fast-paced innovation. Many features in DuckDB have gone from thoughts in academic papers to committed code in a few weeks.\n\nWe\u2019re launching MotherDuck now and doing weekly releases because we admire and want to emulate this speed of execution. Thanks in advance for all the feedback you can provide to make MotherDuck a better product!\n\n## Get Started\n\n[Request an invite](https://motherduck.com/) now to get started using MotherDuck, and join the flock on [slack.motherduck.com](https://slack.motherduck.com/).\n\nAnd, if you\u2019re in San Francisco next week, don\u2019t forget to [register for the MotherDuck Party](https://motherduck-party.eventbrite.com/), watch DuckDB co-creator Hannes [keynote the Data + AI conference](https://www.databricks.com/dataaisummit/session/data-ai-summit-keynote-thursday), and join MotherDuck co-founder Ryan Boyd in his [technical session](https://www.databricks.com/dataaisummit/session/if-duck-quacks-forest-and-everyone-hears-should-you-care).\n\n### TABLE OF CONTENTS\n\n[Hybrid execution: cloud and laptop working together](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#hybrid-execution-cloud-and-laptop-working-together)\n\n[MotherDuck includes a web notebook and Git-style Collaboration](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#motherduck-includes-a-web-notebook-and-git-style-collaboration)\n\n[Anywhere you can Duck, you can MotherDuck](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#anywhere-you-can-duck-you-can-motherduck)\n\n[Continuing to Make Analytics Ducking Awesome](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#continuing-to-make-analytics-ducking-awesome)\n\n[Get Started](https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/#get-started)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![This Month in the DuckDB Ecosystem: June 2023](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fthumbnail_duckdb_newsletter_1_34b0dc65cf.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-seven/)\n\n[2023/06/16 - Marcos Ortiz](https://motherduck.com/blog/duckdb-ecosystem-newsletter-seven/)\n\n### [This Month in the DuckDB Ecosystem: June 2023](https://motherduck.com/blog/duckdb-ecosystem-newsletter-seven)\n\nThis Month in the DuckDB Ecosystem: June 2023\n\n[![DuckDB Tutorial For Beginners](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgetting_started_duckdb_thumbnail_70b197b1ab.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n[2024/10/31 - Mehdi Ouazza, Ryan Boyd](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n### [DuckDB Tutorial For Beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners)\n\nGet up to speed quickly with DuckDB, including installation, VSCode workflow integration and your first SQL analytics project.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 188,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 188
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md",
  [2m  â”‚[0m     "content": "---\ntitle: cidr-paper-hybrid-query-processing-motherduck\ncontent_type: event\nsource_url: https://motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck\nindexed_at: '2025-11-25T19:56:16.076582'\ncontent_hash: 23a9f33edc9cbbbb\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Just Released: Hybrid Query Processing Paper at CIDR 2024\n\n2024/01/16 - 2 min read\n\nBY\n\n[Peter Boncz](https://motherduck.com/authors/peter-boncz/)\n\nThe Conference on Innovative Data systems Research (CIDR) is underway in California and we\u2019re proud to be presenting a peer-reviewed [paper on the MotherDuck hybrid query processing architecture](https://www.cidrdb.org/cidr2024/papers/p46-atwal.pdf).\n\n[Hybrid query processing](https://motherduck.com/learn-more/hybrid-analytics-guide/) allows you to execute database queries either on your local machine, in the cloud, or using a combination of both. It adds useful capabilities to DuckDB, for instance the sharing of DuckDB databases between different team members via the cloud. It also allows you to create web applications with DuckDB running inside your browser, that can jointly execute queries with MotherDuck in the cloud.\n\nThe research and implementation of this architecture has been a collaboration between MotherDuck, DuckDB Labs and myself as a visiting database researcher on sabbatical from CWI, the Dutch national computer science research institute from which DuckDB was born.\n\nBecause designing and implementing a cutting-edge database system like MotherDuck is non-trivial, there are in fact quite a bit of research elements in what we do, even when software engineering. For example, we need to understand how to optimally plan hybrid queries when there are asymmetrical network connections (like in consumer internet) or cost differences in storage, compute and energy. This is why the collaboration between academia and industry is so important in databases; it has already provided a lot of inspiration for my research group at CWI while providing benefits to MotherDuck\u2019s users.\n\nI look forward to gaining other inspiration for my research group and MotherDuck from my fellow researchers at CIDR. Although CIDR is a relatively small conference, it attracts a distinguished audience of researchers and practitioners working in data systems attending it. The conference was originally created by two Turing Award winners: Jim Gray and Michael Stonebraker, both founding figures of the database field.\n\nOur CIDR paper is [now available for download](https://www.cidrdb.org/cidr2024/papers/p46-atwal.pdf) and provides an in-depth view of MotherDuck and our hybrid query architecture. I truly hope that you also will find it interesting. If so, please spread the word and pass it along to people who you think also will find this interesting!\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![Why Python Developers Need DuckDB (And Not Just Another DataFrame Library)](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Fwhy_pythondev_1_22167e31bf.png&w=3840&q=75)](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/)\n\n[2025/10/08 - Mehdi Ouazza](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/)\n\n### [Why Python Developers Need DuckDB (And Not Just Another DataFrame Library)](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries)\n\nUnderstand why a database is much more than just a dataframe library\n\n[![DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_4_1_b6209aca06.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n[2025/10/09 - Alex Monahan, Garrett O'Brien](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n### [DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/blog/announcing-duckdb-141-motherduck)\n\nMotherDuck now supports DuckDB 1.4.1 and DuckLake 0.3, with new SQL syntax, faster sorting, Iceberg interoperability, and more. Read on for the highlights from these major releases.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 74,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 74
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     "content": "---\ntitle: announcing-motherduck-general-availability-data-warehousing-with-duckdb\ncontent_type: blog\nsource_url: https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb\nindexed_at: '2025-11-25T19:56:19.426175'\ncontent_hash: baf2faf8b3f47cb9\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Announcing MotherDuck General Availability: Data Warehousing with DuckDB at Scale\n\n2024/06/11 - 7 min read\n\nBY\nMotherDuck team\n\nOver the last year, thousands of users have tested, validated and helped improve MotherDuck as a serverless data warehouse and backend for interactive apps. We\u2019ve now solidified the product, pricing, partnerships, support teams and internal business processes needed to reach an important milestone: General Availability (GA).\n\nMotherDuck and DuckDB are making analytics ducking awesome for the 99% of users who do not need a complex data infrastructure and for whom [big data is truly dead](https://motherduck.com/blog/big-data-is-dead/). MotherDuck now has many customers in production experiencing the simplicity and efficiency of DuckDB with the collaboration and scale of a serverless cloud data warehouse.\n\n![MotherDuck goes GA summary](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmotherduck_goes_ga_embed_smaller_woline_56d7780de8.jpg&w=3840&q=75)\n\n## Production-ready DuckDB\n\nJust last week, DuckDB Labs announced that DuckDB has reached 1.0.0 and is now committed to backwards compatibility. In our [post congratulating the DuckDB team](https://motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release), we outlined why database nerds love DuckDB: performance, innovation velocity, versatility, ease of use, rich and user-friendly SQL, and extreme portability. Thanks to DuckDB, analytics can run virtually anywhere, liberated from the shackles of complex and expensive distributed systems. As an embedded database, it\u2019s the perfect \u2018Lego\u2019 building block that can snap into any process just by linking in a library. These same characteristics led us to build a cloud data warehouse on top of DuckDB and in collaboration with the creators.\n\n## Simple, Multiplayer at Scale\n\nMotherDuck makes it simple to start uploading and querying your data, whether it sits on your local machine, in blob storage or even on the web. The data can be in many different formats, including parquet, csv, json, Iceberg and Delta Lake. Your local DuckDB can work seamlessly with MotherDuck using [Dual Execution](https://motherduck.com/product/#:~:text=Hybrid,%20Dual%20Query%20Execution), with parts of your queries running locally and other parts scaling to the cloud.\n\nThe cloud creates unique opportunities for sharing data. MotherDuck allows you to upload your data and share a named snapshot with your colleagues in two lines of SQL. Although snapshots can be very useful to have a consistent view of your data across the team for tasks like building machine learning models, snapshots can also be automatically updated. Now with MotherDuck GA, shares can be [restricted to your organization and easily discoverable](https://motherduck.com/docs/key-tasks/sharing-data/sharing-within-org).\n\nA cloud data warehouse needs to scale for all your users and applications. MotherDuck eliminates fighting over common resources by assigning separate, isolated compute instances to each user and simplifying administration and costs for organizations. These compute instances individually scale up to handle workloads of many terabytes for some of our customers. They also scale down to zero when they\u2019re not being used, so you don\u2019t pay when you\u2019re not actively running queries.\n\n## Unmatched Efficiency of Pricing and Execution\n\nCustomers have frequently referenced [high costs for status-quo cloud data warehouses](https://motherduck.com/learn-more/modern-data-warehouse-playbook/) as a big concern. Because of the efficiency of DuckDB\u2019s query engine and MotherDuck\u2019s scale-up architecture, we\u2019re able to offer [pricing](https://motherduck.com/product/pricing/) that is often an order of magnitude lower than other alternatives\n\nNot only is the pricing competitive, but it\u2019s also fine-grained and efficient. By billing at second-level granularity, you only pay for the cloud CPU time you actually use. And, when we\u2019re able to take advantage of your local compute through Dual Execution, you don\u2019t pay at all.\n\n> \u201cWith MotherDuck working to solve amazing problems through data, our behaviors have changed because we know we don't have to pay enormous costs every time we run a query, so we've got almost limitless performance,\u201d said Ravi Chandra, Chief Technology Officer at Dexibit.\n\n## Backed by a World-Class Team\n\nThe team building MotherDuck hails from some of the top companies in data: Google BigQuery, Snowflake, Databricks, SingleStore and more. We\u2019re united by shared values and a shared mission to make analytics ducking awesome.\n\nOur friends at Looker were known to have the best customer success organization in the data industry: the Department of Customer Love, founded by [Margaret Rosas](https://www.linkedin.com/in/mrosas/). Margaret has joined us at MotherDuck to lead our [customer success team](https://motherduck.com/customer-support/), the Hatchery, where our customers are nurtured and taught to fly.\n\nAs we go GA, we also wanted to consolidate engineering under a single leader who can help us scale the team. \u200b\u200bWe\u2019ve asked [Frances Perry](https://www.linkedin.com/in/frances-perry/) to lead our engineering organization. Frances came to us from Google where she was an engineering director on Google Compute Engine, built Google\u2019s internal data processing infrastructure and also released that infrastructure to the world as Cloud Dataflow.\n\n## Now SOC 2 Certified\n\nWe know that [trust and security](https://motherduck.com/trust-and-security/) are critical as you choose a data warehouse to power your business. We leverage a defense in-depth strategy, maintain operational security processes, and build customer trust through certified auditor attestations.\n\nMotherDuck successfully underwent an audit for SOC 2 Type I, which evaluates our systems relevant to security, availability, and confidentiality. With this attestation completed, we have a Type II planned for later in 2024.\n\nTo continue strengthening internal processes and controls, [Myoung Kang](https://www.linkedin.com/in/myoungkang/) has joined the company full-time as Head of Operations. Myoung is a renowned startup veteran who has worked for many companies, including Notion, Convex, and Preset where she was interim CFO.\n\n## Expanded Modern Duck Stack\n\nMotherDuck partners with more than [50 leading companies and technologies](https://motherduck.com/ecosystem/) to make the [Modern Duck Stack](https://motherduck.com/product/#ecosystem). Alongside MotherDuck GA, we\u2019re excited to announce that some of the most requested BI, data integration and data observability tools have been added to the flock.\n\n- **Tableau**: 60,000 companies globally rely on Tableau (part of Salesforce) for data visualization. Tableau Desktop and Server now support MotherDuck, with Tableau Cloud support coming later this year. The connector can be easily found on the [Tableau Exchange](https://exchange.tableau.com/products/1021)\n- **PowerBI**: 5 million organizations worldwide use Microsoft Power BI for data visualization, including 97% of Fortune 500. The [MotherDuck connector](https://motherduck.com/docs/integrations/bi-tools/powerbi) for Power BI is officially launched, and MotherDuck has been accepted to the Microsoft for Startups Founders Hub program\n- **Fivetran**: Fivetran is the leader in data integration for the modern data stack, powering 5,000 customers. The [MotherDuck destination](https://fivetran.com/docs/destinations/motherduck) connector was developed in close collaboration with the Fivetran engineering team, and is now an official Fivetran destination.\n- **Monte Carlo**: Monte Carlo, the leader in data observability, has built a [MotherDuck integration](https://docs.getmontecarlo.com/docs/motherduck). It allows our customers to monitor their databases and look for anomalies through custom SQL rules, which can be created in either the UI wizard and/or programmatically via monitors as code.\n\n## New Startup Program with $10k in Credits\n\nMotherDuck has partnered with leading VC firms to offer $10k in credits to eligible startups in need of a data warehouse or backend for their data apps. Early stage startups with up to 300 employees and less than $100M in funding can submit a [short application](https://motherduck.com/startups/).\n\n## Take Flight with MotherDuck - Now GA\n\nIf you don\u2019t already have a MotherDuck account, visit [app.motherduck.com](https://app.motherduck.com/) to get started. We have a [fully-featured 30-day free trial of the Standard Plan](https://motherduck.com/product/pricing/) and a forever Free Plan available for ongoing usage.\n\n> \u201cOur data pipelines used to take eight hours. Now they're taking eight minutes, and I see a world where they take eight seconds. This is why we made the big bet on DuckDB and MotherDuck. It's only possible with DuckDB and MotherDuck,\u201d said Jim O'Neill, Co-founder and CTO at FinQore.\n\nIf you\u2019re not quite ready to get started, you can [learn more](https://motherduck.com/product/) about the product, [browse our docs](https://motherduck.com/docs/), and read about how [FinQore](https://motherduck.com/case-studies/saasworks/), [Dexibit](https://motherduck.com/case-studies/dexibit/) and [Mosaic](https://motherduck.com/case-studies/dominik-moritz/) use MotherDuck.\n\nWe also have an upcoming [live demo and discussion](https://motherduck.com/getting-started-with-motherduck/) on **Tuesday, June 18th at 10am Pacific**.\n\nLastly, if you\u2019re in San Francisco, [join us to celebrate](https://www.eventbrite.com/e/motherducking-party-after-dataai-summit-san-francisco-tickets-901904038257) tonight at our MotherDuck\u2019ing Party happening alongside the Data + AI Summit.\n\n### TABLE OF CONTENTS\n\n[Production-ready DuckDB](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#production-ready-duckdb)\n\n[Simple, Multiplayer at Scale](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#simple-multiplayer-at-scale)\n\n[Unmatched Efficiency of Pricing and Execution](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#unmatched-efficiency-of-pricing-and-execution)\n\n[Backed by a World-Class Team](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#backed-by-a-world-class-team)\n\n[Now SOC 2 Certified](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#now-soc-2-certified)\n\n[Expanded Modern Duck Stack](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#expanded-modern-duck-stack)\n\n[New Startup Program with $10k in Credits](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#new-startup-program-with-10k-in-credits)\n\n[Take Flight with MotherDuck - Now GA](https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb/#take-flight-with-motherduck-now-ga)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![MotherDuck is Landing in Europe! Announcing our EU Region](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Feu_launch_blog_b165ff2751.png&w=3840&q=75)](https://motherduck.com/blog/motherduck-in-europe/)\n\n[2025/09/24 - Garrett O'Brien, Sheila Sitaram](https://motherduck.com/blog/motherduck-in-europe/)\n\n### [MotherDuck is Landing in Europe! Announcing our EU Region](https://motherduck.com/blog/motherduck-in-europe)\n\nServerless analytics built on DuckDB, running entirely in the EU.\n\n[![DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_4_1_b6209aca06.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n[2025/10/09 - Alex Monahan, Garrett O'Brien](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n### [DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/blog/announcing-duckdb-141-motherduck)\n\nMotherDuck now supports DuckDB 1.4.1 and DuckLake 0.3, with new SQL syntax, faster sorting, Iceberg interoperability, and more. Read on for the highlights from these major releases.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 146,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 146
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     "content": "---\ntitle: bringing-duckdb-to-the-cloud-dual-execution-explained\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained\nindexed_at: '2025-11-25T20:45:20.295187'\ncontent_hash: ab4030806c409492\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nBringing DuckDB to the Cloud: Dual Execution Explained - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Bringing DuckDB to the Cloud: Dual Execution Explained](https://www.youtube.com/watch?v=2eVNm4A8sLw)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=2eVNm4A8sLw&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 1:08:11\n\n\u2022Live\n\n\u2022\n\nYouTubeQuack & CodeMotherDuck Features\n\n# Bringing DuckDB to the Cloud: Dual Execution Explained\n\n2024/06/28\n\nBringing DuckDB's analytical power to the cloud requires more than just running it on a server. While DuckDB excels at processing data on a local machine, the dynamics of data analysis change when dealing with cloud-scale data, collaboration, and shared resources. This shift introduces challenges around [security](https://motherduck.com/learn-more/secure-startup-data-warehouse/), concurrent access, and performance, which is where a purpose-built cloud architecture becomes essential.\n\nMotherDuck, a modern cloud data warehouse, is built on DuckDB to solve these challenges. It extends DuckDB\u2019s capabilities without forking the open source project, creating a hybrid system that intelligently balances local and cloud computing. In a conversation with MotherDuck founding engineer Stephanie, she explained the architecture and the innovative query model, known as **dual execution**, that makes this possible.\n\n* * *\n\n## The MotherDuck Architecture: Beyond Hosted DuckDB\n\nA common misconception is that MotherDuck is simply DuckDB hosted in the cloud. The reality is a more sophisticated system designed to overcome the limitations of using a locally-optimized engine in a distributed environment. MotherDuck\u2019s architecture is built on three key components and a core philosophy of not forking DuckDB.\n\nInstead of maintaining a separate version, MotherDuck leverages DuckDB\u2019s powerful **extension system**. This allows MotherDuck to add new capabilities at multiple layers, including the SQL parser, optimizer, and storage interface, while staying current with the latest open source DuckDB releases. This tight integration means users benefit from the rapid innovation of the DuckDB community almost immediately.\n\nThe architecture can be broken down into three main layers:\n\n1. **The Client Extension**: This is how MotherDuck integrates with the DuckDB ecosystem. Whether a user is working in the CLI, a Python script, or a JDBC connection, a simple `ATTACH` command connects their local DuckDB instance to the MotherDuck cloud. This extension is even used to run DuckDB in the browser via WASM for the MotherDuck UI, enabling client-side processing to reduce latency and cloud compute costs.\n2. **The Compute Layer**: In the cloud, queries are processed by containerized DuckDB instances, fondly called \"ducklings.\" These compute resources are scaled based on user needs, providing the necessary CPU and memory to handle complex analytical queries on large datasets.\n3. **The Storage Layer**: DuckDB's native file format is optimized for a single writer on a local file system. This model is not well-suited for the cloud, where multiple users need to read and write to the same database concurrently. To solve this, MotherDuck implemented a differential storage system that maps a logical database file to append-only snapshot layers in cloud object storage. This design is cloud-friendly, enabling efficient in-place updates and forming the foundation for features like database sharing and time travel.\n\n* * *\n\n## Enabling Collaboration with Database Sharing and Secure Credentials\n\nThis unique architecture transforms DuckDB from a \"single-player\" tool into a collaborative \"multiplayer\" platform. One of the most significant advantages is **database sharing**. Instead of emailing SQL scripts or passing around large database files, team members within the same organization can grant query access to their databases. This streamlines collaboration, ensuring everyone works from a consistent and up-to-date version of the data.\n\nSecurity is another critical aspect of any cloud data platform. MotherDuck provides a **centralized Secret Manager**, co-designed with the DuckDB team. Users can create persistent, encrypted secrets for accessing external data sources like AWS S3, Google Cloud Storage, or Delta Lake. Once a secret is created in MotherDuck, it can be reused across different clients and sessions without needing to expose or reconfigure credentials on each local machine.\n\n* * *\n\n## Understanding Dual Execution\n\nThe most innovative feature of MotherDuck's architecture is its **dual execution model**. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\n\nThe process was demonstrated with a query joining two tables. When both tables reside in a MotherDuck database, the query is executed entirely in the cloud. An `EXPLAIN` plan for this query reveals that all operators, from the table scans to the final join, are marked as **remote**. The only local operation is downloading the final result set to the client.\n\nThe real power of dual execution becomes apparent when a query involves both local and cloud data. In the demonstration, one of the remote tables was copied to a local Parquet file. The query was then modified to join this local file with the table that remained in the cloud.\n\nThe `EXPLAIN` plan for this new query showed a mix of local and remote operations. The scan of the Parquet file and its associated processing happened **locally**, leveraging the user's machine. The scan of the cloud table happened **remotely** on MotherDuck's compute. The system then efficiently transferred only the necessary intermediate data to complete the join. This hybrid approach avoids needlessly uploading the local file to the cloud or downloading the entire remote table, resulting in faster and more efficient queries.\n\n* * *\n\n## Taking Control with Manual Execution Overrides\n\nWhile MotherDuck's optimizer is designed to make the most efficient choice automatically, some situations require manual control. For this, users can use the `md_run` parameter within scan functions like `read_parquet()` or `read_csv()`.\n\nBy setting `md_run = 'local'`, a user can force the scan of a remote S3 file to be executed on their local client. This involves downloading the data, which might be desirable if the data needs to be processed near the user's location or if they prefer using local credentials. Conversely, setting `md_run = 'remote'` forces the operation to run in the MotherDuck cloud.\n\nThis control is especially useful for performance tuning. A direct comparison showed that querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds). By pushing the computation to the data's location, MotherDuck minimizes network I/O and delivers results more quickly.\n\n* * *\n\n## A Symbiotic Future for Local and Cloud Analytics\n\nMotherDuck's architecture is a thoughtful extension of DuckDB's core principles, adapting its local-first power for the demands of the cloud. The dual execution model provides a flexible and efficient bridge between a user's laptop and cloud data, optimizing workloads in a way that pure-cloud or pure-local systems cannot.\n\nThis relationship is symbiotic. As the largest production user of DuckDB, MotherDuck continuously pushes the engine to its limits, uncovering opportunities for improvement and contributing enhancements back to the open source project. This collaboration ensures that both DuckDB and MotherDuck will continue to evolve, offering a powerful and seamless analytical experience that spans from local exploration to cloud-scale production.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Ficeberg_video_thumbnail_adfdabe3d6.jpg&w=3840&q=75)\\\\\n\\\\\n46:26](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n[2025-06-12](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n### [Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse)\n\nBuilding a data stack means choosing between easy SaaS and complex open-source. Apache Iceberg is a middle ground, but its catalog is a hurdle. New tools now simplify using Iceberg with DuckDB to create a flexible, local data lakehouse.\n\nMotherDuck Features\n\n[![\"Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FG_Se_B_Sox_AW_Fg_HD_f4fddaa9ab.jpg&w=3840&q=75)](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n[2025-04-23](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n### [Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly)\n\nHamilton Ulmer shares insights from MotherDuck's Instant SQL Mode, exploring how real-time query result previews eliminate the traditional write-run-debug cycle through client-side parsing and DuckDB-WASM caching.\n\nSQL\n\nTalk\n\nMotherDuck Features\n\n[![\"DuckDB: Run dbt build with sub-second execution times\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_03_26_at_3_40_16_PM_c6e3a8096f.png&w=3840&q=75)\\\\\n\\\\\n24:06](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n[2025-03-13](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n### [DuckDB: Run dbt build with sub-second execution times](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times)\n\nWhether you're new to DuckDB or looking to optimize your workflows, this session will provide practical insights to help you leverage its full potential.\n\nYouTube\n\nData Pipelines\n\ndbt\n\nMotherDuck Features\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 184,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 184
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me search for more specific information about limitations or unsupported features:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ extension|ATTACH|COPY|EXPORT|IMPORT

  [96mğŸ”§ TOOL:[0m Grep â†’ transaction|ACID|concurrency

  [96mğŸ”§ TOOL:[0m Grep â†’ storage.*layer|file.*format|persistent
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/faster-data-pipelines-development-with-mcp-and-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 10,
  [2m  â”‚[0m   "appliedLimit": 10
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-power-of-wasm-for-analytics-duckdb-in-the-browser.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 10,
  [2m  â”‚[0m   "appliedLimit": 10
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 10,
  [2m  â”‚[0m   "appliedLimit": 10
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me check the DuckLake documentation to understand the storage layer better:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/ducklake-motherduck.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md

  [96mğŸ”§ TOOL:[0m Grep â†’ write.*mode|single.*writer|multi.*writer|concurrent.*write
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/ducklake-motherduck.md",
  [2m  â”‚[0m     "content": "---\ntitle: ducklake-motherduck\ncontent_type: blog\nsource_url: https://motherduck.com/blog/ducklake-motherduck\nindexed_at: '2025-11-25T19:58:22.353389'\ncontent_hash: bdfe83f99f0b2283\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# A Duck Walks into a Lake\n\n2025/05/28 - 9 min read\n\nBY\n\n[Jordan Tigani](https://motherduck.com/authors/jordan-tigani/)\n\nIn the early 2010s, I helped build the storage and metadata system for Google BigQuery. At the time, I was not a database person, and because of this, what we ended up building was different. BigQuery had separation of storage and compute, but we were missing important database features like transactions, atomic updates, and the ability to do a lot of small changes to the data. Painstakingly, over the next several years, we realized what was missing and added those features to the system. The database people were right\u2026 all that \u201cstuff\u201d is really important.\n\nSeeing the rise of data lakehouse formats feels like d\u00e9j\u00e0 vu all over again. It feels like we\u2019re having a moment where we\u2019re slowly and painfully re-learning some of the lessons of the past - when it comes to data, you\u2019re going to want database semantics like ACID operations and multi-statement transactions.\n\nWhen you\u2019re building a proprietary system, you can make big architectural changes and improvements; but when you\u2019re building an open-source standard that multiple people are going to implement, it is almost impossible. This is why, for example, small but important changes in Parquet are still not widely adopted, after 10 years. Once you start to get widespread adoption, things become very hard to change. Despite the huge amount of enthusiasm behind formats like Iceberg and Delta Lake, they have some pretty gnarly holes in their semantics.\n\n## Separation of Data and Metadata\n\nThe main data lakehouse formats, Iceberg, Delta Lake, and Hudi, were all created with a unifying constraint: everything has to be stored in S3 (or any other S3-compatible object store). The rationale was that this made them simple to set up and prevented dependencies on third-party tools like a database or other services. You could have a \u201ctable-like\u201d interface, and all you needed to know was the path to a manifest file on S3. This comingling of metadata and data in the same storage location was convenient, if unorthodox.\n\nCloud Data Warehouses, on the other hand, were built with different constraints. They store Metadata and Data in separate storage systems. Data gets stored in an object store, and Metadata gets stored in a transactional database. BigQuery uses Colossus, an internal Google object store, for data and Spanner for metadata. Snowflake uses S3 for the data and Foundation DB for metadata. The advantage of using a transactional database for metadata is that you can use it to make concurrent, atomic transactions.\n\n## S3: Just because it stores data doesn\u2019t make it a database\n\nYou can, of course, use S3 as a database. You can also use a tennis racket as a fire extinguisher. You might have to work a little bit harder, and you might also set fire to your pants. If you need to put out a fire and you\u2019re only allowed to use sporting equipment, a tennis racket can do the job. But if someone handed you an actual fire extinguisher, wouldn\u2019t you switch?\n\nS3 can be used as a database, if you cross your eyes and relax your definition of database. You can torture it to do some database-like things, but you have to work very hard, and it still doesn\u2019t work super well. Have you ever wondered why there are so few implementations that can do Iceberg writes? It isn\u2019t because no one cares; it is because it is really hard. You can think of S3 as a kind of wonky key-value store. You can\u2019t update multiple objects at the same time. You can\u2019t really modify an object, just overwrite it. Latency can be very high, and variance can be higher. Some operations don\u2019t really guarantee consistency, so you have to be very careful about how you use it. If you try to read a lot of data at once, S3 may throttle your connections, and either way, AWS will bill you for each request.\n\n## Begun, the Catalog Wars Have\n\nWouldn\u2019t it be funny if after bending over backwards to avoid putting metadata in a database, the LakeHouse community decided to go ahead and add a metadata database to store table names? Well, that\u2019s what happened when folks realized that they didn\u2019t want to type in giant S3 paths to manifest files all the time. To do this, you needed a catalog.\n\nWhat is a catalog? It is a transactional database. Catalogs store lists of tables, their schemas, their names, etc. You want to be able to treat them like tables you have in a data warehouse, and having to know the manifest file paths for all of your tables is awkward, at best. Now that everyone seems to have stopped squabbling about whether to use Iceberg or Hudi, a new front has opened up: Which catalog should you use? Unity? Polaris? Glue? Iceberg Rest Catalog? AWS Iceberg Tables?\n\nSo to revisit: We went through a ton of contortions to store metadata in S3 instead of a database, and then added a database anyway. This begs the question, why not move the rest of the metadata into the database?\n\nWhat is the metadata that is still in S3? First, the version history. This lets query engines have snapshot isolation and also enables time travel. Second, the location of all of the data files that are active in any version. When updates are happening, the list of active files is changing continually. Third, statistics about what data is in which file. This is very helpful to allow query engines to only read files that have data in ranges that they\u2019re looking for. This kind of data is ideal to move into the catalog, and having it in the catalog would save a ton of effort trying to manage it on S3.\n\n## Welcome to the DuckLake\n\n[DuckLake](https://ducklake.select/) is an integrated data lake and catalog format created by the founders of DuckDB. It stores table and physical metadata in a database of your choosing, and data in an S3-compatible object store as Parquet files. Despite the \u201cduck\u201d in the name, [it doesn\u2019t even require that you use DuckDB](https://duckdb.org/2025/05/27/ducklake.html). Because the metadata operations are defined in terms of database schemas and transactions, they are highly portable. DuckLake is actually more portable than Iceberg because it is easier to implement.\n\nLet\u2019s compare DuckLake to Iceberg. Most tables contain data that is written over time. In Iceberg, you end up accumulating metadata and manifest files because every change to a table\u2014appends, updates, or deletes\u2014adds new metadata. Just to find out which files you need to read can involve many separate S3 reads. If you have to read this information without a cache, it could take hundreds of milliseconds. In DuckLake, finding out which files to read is just a SQL query away. If you back DuckLake with Postgres, you should be able to get an answer in a couple of milliseconds. That\u2019s the difference between a cold S3 scan and a lightning-fast index lookup.\n\nNow, let\u2019s say you\u2019re trickling data into a table, with a handful of updates every few seconds. It is pretty easy to do 1,000 updates per hour, or around 25k updates per day. In Iceberg, you\u2019re going to generate a forest of tiny files; not just the Parquet files, but also the metadata and snapshot files. That metadata adds up over time. So you need to do not only data file compaction but also metadata file compaction. DuckLake provides more flexibility. There is no small metadata file problem. DuckLake requires fewer compactions and can apply optimizations like pointing multiple snapshots to different portions of a single Parquet file.\n\n## A MotherDucking great Lakehouse\n\nAt MotherDuck, we\u2019re really excited about DuckLake. While it\u2019s still evolving, it\u2019s already a powerful, open format\u2014and we\u2019re rolling out full hosted support over the coming weeks.\n\nWhat does that mean?\n\n- **Fast, cloud-proximate queries:** Sure, you can query DuckLake data from your laptop. But even if you have a high-bandwidth internet connection, MotherDuck\u2019s servers, which sit close to your data, will be a lot faster. And no cloud egress fees.\n- **Scalable data transformation:** Running ETL jobs on your laptop is a vibe\u2026 but not a good one. MotherDuck gives you cloud muscle when you need it, with a click or an API call.\n- **Hands-free optimization:** Keeping lakehouse data in good shape means background compaction and smart file layouts. Let us do that for you. Your queries will thank you.\n- **Bring your own bucket\u2026 or not:** Use your own S3/R2/GCS bucket, or let MotherDuck host one for you. Either way, you stay in control, and we\u2019ll make sure it just works.\n- **Integrated Auth:** MotherDuck can broker credentials, so even if one of your users wants to run another query engine, they\u2019ll be granted access to the correct data paths.\n\nDuckLake is open by design. It\u2019s not just for DuckDB. The catalog interface supports integration with other engines, tools, and ingestion systems. No lock-in. No walled garden. Just ducks, data, and freedom.\n\n## The Iceberg Hedge\n\nThe momentum towards open data formats has been astonishing over the last year or so, and only seems to be accelerating. The last time the data world saw something of this magnitude, where people went all in on a technology before it was even ready for prime time, was with Hadoop in 2010. DuckLake offers a hedge in case the technical difficulties in Iceberg prove too difficult.\n\nBut Iceberg support is still important in DuckDB and MotherDuck. There are lots of people using Iceberg, and there are tons of ecosystem tools being built around Iceberg; it is a super important format to support. Moreover, DuckLake will ultimately be able to import from Iceberg, which can help with migration. Iceberg export is also planned for the not too distant future, enabling interoperability with other tools that only speak Iceberg.\n\nDuckLake is a clean, open solution that brings together the best parts of modern data lakes and warehouses. Give it a try and let us know your thoughts in our [Community Slack](https://slack.motherduck.com/). We\u2019d love to hear more about what you\u2019re building and what you\u2019d like to see as we roll out hosted support.\n\nIf you ever feel the urge to put out a fire with a tennis racket, we\u2019re here with a better way.\n\n## DuckLake and the Future of Open Table Formats\n\nOn **Tuesday, June 17th**, I hope you\u2019ll join DuckDB\u2019s Hannes M\u00fchleisen and me for a conversation on [DuckLake & The Future of Open Table Formats](https://lu.ma/mt9f8xh1?utm_source=blog) to discuss what sparked DuckLake\u2019s creation, how it differs from existing open table formats, and what it means for the future of data architecture.\n\n### TABLE OF CONTENTS\n\n[Separation of Data and Metadata](https://motherduck.com/blog/ducklake-motherduck/#separation-of-data-and-metadata)\n\n[S3: Just because it stores data doesn\u2019t make it a database](https://motherduck.com/blog/ducklake-motherduck/#s3-just-because-it-stores-data-doesnt-make-it-a-database)\n\n[Begun, the Catalog Wars Have](https://motherduck.com/blog/ducklake-motherduck/#begun-the-catalog-wars-have)\n\n[Welcome to the DuckLake](https://motherduck.com/blog/ducklake-motherduck/#welcome-to-the-ducklake)\n\n[A MotherDucking great Lakehouse](https://motherduck.com/blog/ducklake-motherduck/#a-motherducking-great-lakehouse)\n\n[The Iceberg Hedge](https://motherduck.com/blog/ducklake-motherduck/#the-iceberg-hedge)\n\n[DuckLake and the Future of Open Table Formats](https://motherduck.com/blog/ducklake-motherduck/#ducklake-and-the-future-of-open-table-formats)\n\n!['DuckDB In Action' book cover](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduckdb-book-full-cover.68e4f598.png&w=3840&q=75)\n\nGet your free book!\n\nE-mail\n\nSubscribe to other MotherDuck news\n\nSubmit\n\nFree Book!\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![The Open Lakehouse Stack: DuckDB and the Rise of Table Formats](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fopendata_stack_7f9a4498ee.png&w=3840&q=75)](https://motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats/)\n\n[2025/05/23 - Simon Sp\u00e4ti](https://motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats/)\n\n### [The Open Lakehouse Stack: DuckDB and the Rise of Table Formats](https://motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats)\n\nLearn how DuckDB and open table formats like Iceberg power a fast, composable analytics stack on affordable cloud storage\n\n[![Breaking the Excel-SQL Barrier: Leveraging DuckDB's Excel Extension](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FBreaking_Excel_SQL_barrier_d4e2cf549e.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-excel-extension/)\n\n[2025/05/27 - Jacob Matson](https://motherduck.com/blog/duckdb-excel-extension/)\n\n### [Breaking the Excel-SQL Barrier: Leveraging DuckDB's Excel Extension](https://motherduck.com/blog/duckdb-excel-extension)\n\nNow in MotherDuck & DuckDB, its never been easier to join in your data from spreadsheet sources.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 148,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 148
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     "content": "---\ntitle: announcing-ducklake-support-motherduck-preview\ncontent_type: event\nsource_url: https://motherduck.com/blog/announcing-ducklake-support-motherduck-preview\nindexed_at: '2025-11-25T19:56:40.004419'\ncontent_hash: 597676a0c332e92e\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# MotherDuck Managed DuckLakes Now in Preview: Scale to Petabytes\n\n2025/07/01 - 8 min read\n\nBY\n\n[Ryan Boyd](https://motherduck.com/authors/ryan-boyd/)\n\nAt MotherDuck, we believe in using the right tool for the job. For [95% of companies](https://motherduck.com/blog/big-data-is-dead), our low-latency data warehouse with fast storage delivers sub-second queries perfectly.\n\nBut what about organizations with truly massive data requirements\u2014petabytes of historical data, billions of daily events, or global-scale analytics?\n\nEnter [DuckLake](https://ducklake.select/): an open table format designed from the ground up for extreme scale, offering the same massive data capabilities as Apache Iceberg and Delta Lake, but with radically faster performance through database-backed metadata and intelligent partitioning.\n\n> Get the same scale as Iceberg/Delta Lake, but with the snappy performance of a modern data warehouse.\n\nMotherDuck is proud to preview our support for this emerging format, enabling you to back MotherDuck databases with a DuckLake catalog and storage.\n\n## How is DuckLake different from Iceberg or Delta Lake?\n\nWhile Iceberg and Delta Lake pioneered open table formats for massive scale, they suffer from a fundamental performance bottleneck: metadata operations. Every read and write must traverse complex file-based metadata structures, creating latency that compounds at scale.\n\nDuckLake solves this by storing metadata in a transactional database (PostgreSQL, MySQL), delivering:\n\n- **10-100x faster metadata lookups** \\- Database indexes beat file scanning every time\n- **Instant partition pruning** \\- `SQL WHERE` clauses on metadata, not file traversal\n- **Rapid writes at scale** \\- No complex manifest file merging, just database transactions\n- **Simplified data stack** \\- No additional catalog server, just a standard transactional database that you likely already have organizational expertise in running\n\n**Result:** Get the same petabyte scale as Iceberg/Delta Lake, but with the snappy performance of a modern data warehouse.\n\n**Bonus**: DuckLake recognizes that many organizations think of \u2018databases\u2019 of inter-related tables, instead of isolated tables, so multi-table ACID transactions are available and you can easily accomplish multi-table schema evolution.\n\n## MotherDuck and DuckLake: Warehouse Speed at Lake Scale\n\nToday we're launching a [preview of DuckLake](https://motherduck.com/docs/integrations/file-formats/ducklake/)\u2014bringing MotherDuck's sub-second query performance to petabyte-scale data lakes.\n\nBy using MotherDuck as your DuckLake catalog database, you get:\n\n- **Lightning-fast metadata operations** powered by MotherDuck's infrastructure\n- **Seamless scale transitions**\u2014start with MotherDuck storage, graduate to DuckLake as you grow\n- **Unified SQL interface** whether querying megabytes or petabytes\n\n### MotherDuck Databases backed by DuckLake Storage + Catalog\n\nYou have the choice of what S3-compatible blobstore to use for your DuckLake. Simply [configure](https://motherduck.com/docs/sql-reference/motherduck-sql-reference/create-secret/) a `SECRET` in MotherDuck to specify permissions for that blobstore, and then you can create new databases, specifying the blobstore to use to store the database.\n\n```sql\nCopy code\n\nCREATE SECRET IN MOTHERDUCK (TYPE S3, \u2026);\nCREATE DATABASE my_db (TYPE ducklake, DATA_PATH 's3://my-bucket/my-prefix/');\n```\n\nIn this mode, MotherDuck automatically creates the DuckLake catalog database and manages it inside MotherDuck - providing access to the catalog database either in MotherDuck, or for use by local DuckDB clients.\n\nDon\u2019t want to manage your own storage and deal with secrets? MotherDuck can fully manage your DuckLake for you \u2013 just don\u2019t provide a `DATA_PATH`.\n\n```sql\nCopy code\n\nCREATE DATABASE my_db (TYPE ducklake);\n```\n\nTIP: Bucket Access\nIf you choose a fully-managed DuckLake (by not specifying `DATA_PATH`), you won't have access to the underlying storage bucket outside of MotherDuck at this time. See [our roadmap](https://motherduck.com/blog/announcing-ducklake-support-motherduck-preview/#future-support-our-roadmap) below.\n\n## Access Managed DuckLakes from your Own Cloud (or Laptop)\n\nIf you supply your own cloud storage bucket, you can bring your own compute (BYOC) to your DuckLake. Today, this allows you to configure DuckDB to use the DuckLake metadata catalog on MotherDuck, but read and write directly to your cloud storage (let\u2019s say from your AWS Lambda jobs!).\n\nIn the DuckDB CLI (as an example), create a secret that provides access to your `DATA_PATH`:\n\n```arduino\nCopy code\n\nCREATE PRESISTENT SECRET my_secret (\n    TYPE S3,\n    KEY_ID 'my_s3_access_key',\n    SECRET 'my_s3_secret_key',\n    REGION 'my-bucket-region'\n);\n```\n\nNext, attach the DuckLake to your DuckDB session:\n\n```bash\nCopy code\n\nATTACH 'ducklake:md:__ducklake_metadata_<database_name>' AS <alias>;\n```\n\nNow, you can say `USE <alias>;` to default your DuckDB session to your DuckLake, or just reference the `<alias>` in your queries. The following will copy a file from a MotherDuck-owned S3 bucket into your DuckLake as a new table.\n\n```sql\nCopy code\n\nCREATE TABLE <alias>.air_quality AS\nSELECT * FROM 's3://us-prd-motherduck-open-datasets/who_ambient_air_quality/parquet/who_ambient_air_quality_database_version_2024.parquet';\n```\n\nThis capability of DuckLakes gets much more interesting when additional data processing frameworks implement support for [the DuckLake specification](https://ducklake.select/docs/stable/specification/introduction.html). Support for using DuckLake with Apache Spark is in development.\n\n### How do I use my own compute with a fully-managed DuckLake?\n\nRight now, if you want to be able to bring your own compute, you also need to bring your own cloud storage bucket.\n\nSupport for using your own compute with a fully-managed DuckLake will be available soon. Although the storage buckets in this scenario will continue to be owned and managed by MotherDuck, we\u2019ll provide signed URLs which clients can use to access these buckets.\n\n## Time Travel\n\nDuckLake takes consistent snapshots of your data and enables you to query the state of the data as of any snapshot.\n\nHere's an example looking at the state of your customer table 1 week ago:\n\n```sql\nCopy code\n\nSELECT * FROM customer AT (TIMESTAMP => now() - INTERVAL '1 week');\n```\n\nIn order to see the available snapshots, you can use the `snapshots()` table function:\n\n```sql\nCopy code\n\nSELECT * FROM snapshots();\n```\n\nYou can then run queries against the data at the time of a specific known snapshot:\n\n```sql\nCopy code\n\nSELECT * FROM customer AT (VERSION => 3);\n```\n\nMore information on the time travel semantics is available in the DuckLake [time travel](https://ducklake.select/docs/stable/duckdb/usage/time_travel.html) and [snapshots](https://ducklake.select/docs/stable/duckdb/usage/snapshots) documentation.\n\n## Preview features at a glance\n\nThis is an early release of MotherDuck's support for DuckLake. We will continue to [expand our capabilities](https://motherduck.com/blog/announcing-ducklake-support-motherduck-preview/#future-support-our-roadmap), making your DuckLake faster, easier to use and easier to manage.\n\n| Fully-managed DuckLake | Provides an easy way to get started with DuckLake. MotherDuck hosts the DuckLake metadata catalog and storage. The storage is not yet accessible outside of MotherDuck. ( [docs](https://motherduck.com/docs/integrations/file-formats/ducklake/#creating-a-fully-managed-ducklake-database)) |\n| **Bring your own bucket** | Keep your storage within your own cloud. Simply configure the secrets for MotherDuck to access your storage and then create your DuckLake referencing this storage as your `DATA_PATH`. MotherDuck will maintain the metadata catalog, while all data will be stored in your bucket. ( [docs](https://motherduck.com/docs/integrations/file-formats/ducklake/#bring-your-own-bucket)) |\n| **Use your own compute with your \u201cbring your own bucket\u201d DuckLake** | MotherDuck makes it possible to use your own compute and read/write directly to your DuckLake in your storage bucket, while MotherDuck holds the metadata catalog. ( [docs](https://motherduck.com/docs/integrations/file-formats/ducklake/#using-local-compute)) |\n| **Sharing your DuckLake with other MotherDuck users** | With MotherDuck, you\u2019d typically use a service account to create and manage your DuckLake, while providing access to other MotherDuck users using [shares](https://motherduck.com/docs/key-tasks/sharing-data/sharing-overview/). With the preview launch, you can provide read-only access to DuckLakes using shares. Access can be shared with an entire organization, with specific users or to all MotherDuck users. ( [docs](https://motherduck.com/docs/key-tasks/sharing-data/sharing-overview/)) |\n| **Fast writes and parquet imports** | Due to the database-hosted metadata catalog, DuckLake provides fast writes using simple transactions, without having to update multiple layers of metadata. This improves performance of many operations compared to other open table formats. ( [DuckLake manifesto](https://ducklake.select/manifesto/)) |\n| **JOIN/UNION all your data in SQL** | Analytics SQL queries should just work. Even for complex analytical workloads, working with DuckLake tables is just like working with any other MotherDuck tables. You can `JOIN` and `UNION` them, include them in CTEs, and take advantage of DuckDB\u2019s excellent support for file formats like CSV and JSON, as well as remote protocols like HTTP. |\n| **Time travel** | Ducks like to go back to the future too! Time travel enables querying the state of the database as of any recorded snapshot. ( [DuckLake docs](https://ducklake.select/docs/stable/duckdb/usage/time_travel.html)) |\n| **Metadata-only multi-table schema evolution** | MotherDuck supports this DuckLake feature to add/rename columns and promote types without re-writing any data files. ( [DuckLake docs](https://ducklake.select/docs/stable/duckdb/usage/schema_evolution)) |\n\n## Future Support: Our Roadmap\n\nAs we work towards GA and beyond, we\u2019ll continue to expand our support for DuckLake at MotherDuck. Since we\u2019re building in the open, we want to share the roadmap with you.\n\n| Expanded storage access control | For fully-managed DuckLakes, MotherDuck will handle authentication and authorization through catalog metadata and issue presigned URLs for storage bucket access.\u000b\u000bMotherDuck will also handle delegated access in \u2018bring your own bucket\u2019 scenarios so that users don\u2019t need to have direct access to the bucket secrets in order to work with a DuckLake as long as they\u2019ve been granted appropriate permissions. |\n| **Automatic data management** | MotherDuck will implement automatic data compaction and garbage collection, which can only be manually triggered in the preview release. |\n| **Streaming write support** | MotherDuck will implement support to efficiently handle streaming writes without exploding the number of individual Parquet files in the DuckLake. The initial implementation (already live) is using the experimental DuckLake feature called [Data Inlining](https://ducklake.select/docs/stable/duckdb/advanced_features/data_inlining.html), but the final implementation is TBD. |\n| **Row-level and column-level security** | We understand this is important for some use cases and plan on supporting it in our DuckLake implementation. |\n| **Postgres endpoint for catalog access** | We want to make DuckLake as easy to use as intended by the DuckDB creators, and it\u2019s important to quack the right protocols. |\n| **Iceberg Import/Export** | DuckLake will support the ability to import and export data in the Iceberg table format. |\n| **Access your DuckLake from other compute engines** | While you can access your MotherDuck DuckLake from local DuckDB instances today, we think an open table format should provide access from additional data processing engines. Support for Apache Spark is coming, but please let us know if there are other engines/frameworks you'd like to see supported. |\n| **Access to external DuckLakes** | MotherDuck will make it possible for users to attach DuckLakes where the metadata and storage are hosted externally. |\n| **Improved semantics** | We want your DuckLake to work as you\u2019d expect (and better!) Although the preview release has some restrictions around multiple MotherDuck users writing to the same DuckLake and eventual consistency on data shares, we\u2019re working hard to enable an improved experience. |\n\n[Find us on Slack](http://slack.motherduck.com/) and reach out to let us know what you think of this preview release and which of the planned features are most important to you. Of course, if there are additional DuckLake capabilities you wish to see, please share those as well.\n\nINFO: Iceberg Support in MotherDuck\nWe will continue to expand MotherDuck's [support for Iceberg](https://motherduck.com/docs/integrations/file-formats/apache-iceberg/), incorporating improvements [planned](https://github.com/duckdb/duckdb-iceberg/issues/37#issuecomment-2976002390) by the core DuckDB team.\n\n### TABLE OF CONTENTS\n\n[How is DuckLake different from Iceberg or Delta Lake?](https://motherduck.com/blog/announcing-ducklake-support-motherduck-preview/#how-is-ducklake-different-from-iceberg-or-delta-lake)\n\n[MotherDuck and DuckLake: Warehouse Speed at Lake Scale](https://motherduck.com/blog/announcing-ducklake-support-motherduck-preview/#motherduck-and-ducklake-warehouse-speed-at-lake-scale)\n\n[Access Managed DuckLakes from your Own Cloud](https://motherduck.com/blog/announcing-ducklake-support-motherduck-preview/#access-managed-ducklakes-from-your-own-cloud)\n\n[Time Travel](https://motherduck.com/blog/announcing-ducklake-support-motherduck-preview/#time-travel)\n\n[Preview features at a glance](https://motherduck.com/blog/announcing-ducklake-support-motherduck-preview/#preview-features-at-a-glance)\n\n[Future Support: Our Roadmap](https://motherduck.com/blog/announcing-ducklake-support-motherduck-preview/#future-support-our-roadmap)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![Why REST and JDBC Are Killing Your Data Stack \u2014 Flight SQL to the Rescue](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fflight_thumbnail_31453866be.png&w=3840&q=75)](https://motherduck.com/blog/flight-sql-vs-rest-vs-jdbc/)\n\n[2025/06/13 - Thomas (TFMV) McGeehan](https://motherduck.com/blog/flight-sql-vs-rest-vs-jdbc/)\n\n### [Why REST and JDBC Are Killing Your Data Stack \u2014 Flight SQL to the Rescue](https://motherduck.com/blog/flight-sql-vs-rest-vs-jdbc)\n\nUnderstand how Flight SQL can speed up how your serve data with DuckDB\n\n[![I Made Cursor + AI Write Perfect SQL. Here's the Exact Setup](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fvibe_coding_0d05fa9c9f.png&w=3840&q=75)](https://motherduck.com/blog/vibe-coding-sql-cursor/)\n\n[2025/06/27 - Jacob Matson](https://motherduck.com/blog/vibe-coding-sql-cursor/)\n\n### [I Made Cursor + AI Write Perfect SQL. Here's the Exact Setup](https://motherduck.com/blog/vibe-coding-sql-cursor)\n\nStop debugging AI-generated SQL queries. Learn the exact Cursor + MotherDuck setup that makes AI write working SQL on the first try, with step-by-step instructions.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 240,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 240
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": "<3758 chars>",
  [2m  â”‚[0m   "numLines": 15,
  [2m  â”‚[0m   "appliedLimit": 15
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search more specifically for documentation or technical details about what features are not supported:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ local.*file|persistent.*database|\.db file|database.*file

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/product/duckdb-users.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/learn-more/what-is-duckdb.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     "content": "---\ntitle: duckdb-users\ncontent_type: product_page\nsource_url: https://motherduck.com/product/duckdb-users/\nindexed_at: '2025-11-25T20:15:52.565114'\ncontent_hash: dfb4ed9cd3383b9d\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n# local + Cloud analytics,  backed by the same database\n\nA duckdb-powered data warehouse for production grade analytics\n\n[Get Started](https://motherduck.com/get-started/)\n\nLEARN MORE\n\nWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuck\n\n## Transform Solo DuckDB Into a Collaborative Powerhouse\n\nDuckDB's in-process nature means your data stays trapped on your machine. MotherDuck eliminates this limitation with a simple \"md:\" prefix.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fshare-data-not-files.db8baefd.png&w=3840&q=75)\n\n### Share Data, Not Files\n\nNo more exporting files or managing S3 buckets to share your work. Create database shares with specific team members while maintaining the familiar DuckDB experience.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmultiple-users-one-database.0b9d7dc7.png&w=3840&q=75)\n\n### Multiple Users, One Database\n\nEnable simultaneous access for Data Engineers, Analytics teams, and Scientists to the same datasets. Each user gets their own dedicated compute instance \u201cduckling\u201d for concurrent queries without conflicts.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-bring-the-flock-together.be57c6e9.png&w=2048&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-bring-the-flock-together.be57c6e9.png&w=2048&q=75)\n\n## Local Speed with Cloud-Scale Production Power\n\nEnjoy the best of both worlds with a seamless dev/prod workflow for your data. Develop locally for immediate feedback, then deploy identical workloads to the cloud with zero code changes.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Frapid-iteration-zero-cost.ff94f539.png&w=3840&q=75)\n\n### Rapid Iteration at Zero Cost\n\nExperiment locally with lightning-fast feedback cycles without spending a dime. When your solution is ready, your exact same queries work in the cloud by simply adding \"md:\" to your connection.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftest-safely-deploy-confidently.ed329fbf.png&w=3840&q=75)\n\n### Test Safely, Deploy Confidently\n\nPrototype complex data transformations without risking expensive cloud resources. This local dev, cloud prod model ensures what works locally will perform at scale in production.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fship-data-solutions-faster.bdfdd521.png&w=3840&q=75)\n\n### Ship Data Solutions Faster\n\nEliminate the friction between development and deployment environments. Your team can innovate with quick local tests and push proven solutions to cloud production seamlessly.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-scalable-node-architecture.91c200dd.png&w=1920&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-scalable-node-architecture.91c200dd.png&w=1920&q=75)\n\n## Familiar Interface with Intelligent Superpowers\n\nMotherDuck maintains the DuckDB interface you love while adding thoughtful, AI-backed intelligent assistance. Experience enhanced productivity with features designed for SQL experts and business users.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fvisual-insights-at-a-glance.60ec6358.png&w=3840&q=75)\n\n### Visual Insights at a Glance\n\nBeautiful visualizations help you understand complex data patterns immediately. MotherDuck's design affordances make exploration intuitive while preserving the power of DuckDB's analytical capabilities.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fautomatic-error-correction.fe59afe0.png&w=3840&q=75)\n\n### Automatic Error Correction\n\n\"Fix It\" repairs common syntax errors with a single click, eliminating frustration from missing commas or quotes. This intelligent assistance ensures everyone on your team can work productively with SQL, regardless of expertise level.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fnatural-language-to-sql.72331bc3.png&w=3840&q=75)\n\n### Natural Language to SQL\n\nNever written SQL before? Non-technical team members can contribute with \"PROMPT QUERY\" turning plain English into correct SQL. This feature democratizes data access by removing the barrier of SQL syntax knowledge.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-ergonomic-and-effortless.b3f51a46.png&w=3840&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-ergonomic-and-effortless.b3f51a46.png&w=3840&q=75)\n\n## Features\n\n[![Cloud database storage feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-cloud.3e9c0a0a.png&w=640&q=75)\\\\\n**Cloud database storage** \\\\\nManage your DuckDB database catalog in the cloud for easy scaling and collaboration.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/cloud-database-storage/)\n\n[![SQL analytics engine feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-analytics.f04308e0.png&w=640&q=75)\\\\\n**SQL analytics engine** \\\\\nEfficiently run the same SQL queries on data stored locally, in MotherDuck or in your data lake.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/sql-analytics-engine/)\n\n[![Simplified database sharing feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-db.11e966ef.png&w=640&q=75)\\\\\n**Simplified database sharing** \\\\\nCreate a shareable snapshot of your data, which your colleagues can easily attach and query in MotherDuck.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/simplified-database-sharing/)\n\n[![Dual query execution feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-query.28419698.png&w=640&q=75)\\\\\n**Dual query execution** \\\\\nUse idle compute on your laptop in concert with the cloud to increase speed and lower cost.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/dual-query-execution/)\n\n[![Notebook-like UI feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-ui.c08b01e4.png&w=1080&q=75)\\\\\n**Notebook-like UI** \\\\\nUse the web to browse the data catalog, write SQL, filter and sort results and share data.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/notebook-like-ui/)\n\n[![Strong DuckDB ecosystem feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-ecosystem.0ab95f7c.png&w=640&q=75)\\\\\n**Strong DuckDB ecosystem** \\\\\nUse with 25+ tools in the modern data stack for import, orchestration and business intelligence.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/strong-duckdb-ecosystem/)\n\n## Architecture\n\nManaged DuckDB-in-the-cloud\n\n## Ecosystem\n\nModern Duck Stack\n\n### CLOUD DATA WAREHOUSE\n\n### Sources\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpostgres-sql.d737f4f5.png&w=750&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Famazon-s3.72386dfc.png&w=640&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsalesforce.35d52b31.png&w=384&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fstripe.2160e881.png&w=384&q=75)\n\n[**Business Intelligence**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftableau.0ec523e2.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpower-bi.f3563826.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fomni.cb7aa381.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmetabase.5f8fe44e.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Business+Intelligence) [**Ingestion**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffivetran.26f3817d.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fairbyte.8371d2f0.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Festuary.babad369.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlthub.e9892b97.png&w=384&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Ingestion) [**Data Science & AI**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhex.275dad7d.png&w=256&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fjupyter.2a6af3de.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fcolab.30ab10af.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmarimo.b699c73d.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Data+Science+%26+AI) [**Reverse ETL**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fcensus.352f1d69.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Reverse+ETL) [**Transformation**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdbt.fd2184d1.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftobiko.d0e3d1e5.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsql-mesh.6fceb012.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Transformation) [**Dev Tools**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdatagrip.f48eba23.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpuppy-graph.848400c6.png&w=750&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdagster.e1970a7c.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Dev+Tools)\n\n[**Orchestration**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fairflow.7f70081a.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdagster.e1970a7c.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fprefect.fd6371b3.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fkestra.aa10acfc.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Orchestration)\n\n[**Data Quality**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmonte-carlo.2143f962.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsecoda.9b7e86fb.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsoda.59e5aa02.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fgreat-expectations.c544f1ef.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Data+Quality)\n\n## Case Studies\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nOur data pipelines used to take eight hours. Now they're taking eight minutes, and I see a world where they take eight seconds. This is why we made the big bet on DuckDB and MotherDuck. It's only possible with DuckDB and MotherDuck.\n\n[READ MORE](https://motherduck.com/case-studies/finqore/)\n\n![Jim O\u2019Neill's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fjim_oneil_photo_930c637f49.jpg&w=3840&q=75)\n\nJim O\u2019Neill\n\nCo-founder and CTO at FinQore\n\nOur data pipelines used to take eight hours. Now they're taking eight minutes, and I see a world where they take eight seconds. This is why we made the big bet on DuckDB and MotherDuck. It's only possible with DuckDB and MotherDuck.\n\n[READ MORE](https://motherduck.com/case-studies/finqore/)\n\nWith MotherDuck, it only took an hour to get an app up and running, which was pretty cool. It uses 18 million rows of data, but once it\u2019s loaded, we have all this interactivity right on the website. And since it's all coming from MotherDuck, we can create a shareable snapshot of our data that others can attach to and query.\n\n[READ MORE](https://motherduck.com/case-studies/dominik-moritz/)\n\n![Dominik Moritz's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdominik_berlin_f8a31170af.jpg&w=3840&q=75)\n\nDominik Moritz\n\nProfessor at Carnegie Mellon University\n\nWith MotherDuck, it only took an hour to get an app up and running, which was pretty cool. It uses 18 million rows of data, but once it\u2019s loaded, we have all this interactivity right on the website. And since it's all coming from MotherDuck, we can create a shareable snapshot of our data that others can attach to and query.\n\n[READ MORE](https://motherduck.com/case-studies/dominik-moritz/)\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nOur data pipelines used to take eight hours. Now they're taking eight minutes, and I see a world where they take eight seconds. This is why we made the big bet on DuckDB and MotherDuck. It's only possible with DuckDB and MotherDuck.\n\n[READ MORE](https://motherduck.com/case-studies/finqore/)\n\n![Jim O\u2019Neill's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fjim_oneil_photo_930c637f49.jpg&w=3840&q=75)\n\nJim O\u2019Neill\n\nCo-founder and CTO at FinQore\n\nOur data pipelines used to take eight hours. Now they're taking eight minutes, and I see a world where they take eight seconds. This is why we made the big bet on DuckDB and MotherDuck. It's only possible with DuckDB and MotherDuck.\n\n[READ MORE](https://motherduck.com/case-studies/finqore/)\n\nWith MotherDuck, it only took an hour to get an app up and running, which was pretty cool. It uses 18 million rows of data, but once it\u2019s loaded, we have all this interactivity right on the website. And since it's all coming from MotherDuck, we can create a shareable snapshot of our data that others can attach to and query.\n\n[READ MORE](https://motherduck.com/case-studies/dominik-moritz/)\n\n![Dominik Moritz's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdominik_berlin_f8a31170af.jpg&w=3840&q=75)\n\nDominik Moritz\n\nProfessor at Carnegie Mellon University\n\nWith MotherDuck, it only took an hour to get an app up and running, which was pretty cool. It uses 18 million rows of data, but once it\u2019s loaded, we have all this interactivity right on the website. And since it's all coming from MotherDuck, we can create a shareable snapshot of our data that others can attach to and query.\n\n[READ MORE](https://motherduck.com/case-studies/dominik-moritz/)\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n## Get in Touch\n\nWe\u2019d love to learn more about what you\u2019re building. A Product Expert will fly right back to you with resources to help you take flight.\n\nFirst Name\n\nLast Name\n\nE-mail\n\nCompany\n\nLeave us a message.\n\nThis site is protected by reCAPTCHA and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply.\n\nSubmit\n\n![Duck looking up](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduck-looking-up.1c763865.png&w=3840&q=75)\n\nAuthorization Response\n\nreCAPTCHA\n\nRecaptcha requires verification.\n\n[Privacy](https://www.google.com/intl/en/policies/privacy/) \\- [Terms](https://www.google.com/intl/en/policies/terms/)\n\nprotected by **reCAPTCHA**\n\n[Privacy](https://www.google.com/intl/en/policies/privacy/) \\- [Terms](https://www.google.com/intl/en/policies/terms/)",
  [2m  â”‚[0m     "numLines": 403,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 403
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     "content": "---\ntitle: what-is-duckdb\ncontent_type: tutorial\nsource_url: https://motherduck.com/learn-more/what-is-duckdb\nindexed_at: '2025-11-25T09:56:48.634315'\ncontent_hash: 301f22bee0efb32c\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO LEARN](https://motherduck.com/learn-more/)\n\n# What is DuckDB?\n\n7 min read\n\n![What is DuckDB?](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Flearn_more_duck_db_df1112cb47.png&w=3840&q=75)\n\nDuckDB offers high-performance analytical database capabilities with simplicity, speed, and portability. Data scientists, application developers, data engineers, and analysts use DuckDB to process and analyze large datasets efficiently. As an embedded database system, DuckDB brings powerful analytical capabilities directly into your applications and workflows.\n\n## Understanding DuckDB: The Basics\n\nDuckDB functions as an embeddable SQL OLAP (Online Analytical Processing) database management system. Let's break down what this means:\n\n**Embeddable**: DuckDB runs directly within your application, eliminating the need for a separate server process. This design simplifies deployment and reduces overhead, making it ideal for applications that require built-in analytical capabilities.\n\n**SQL**: It supports SQL fully, making it familiar to those with database experience. DuckDB implements a wide range of SQL features, including complex joins, subqueries, window functions, and more. This comprehensive SQL support allows users to write sophisticated queries to analyze their data effectively.\n\n**OLAP**: DuckDB optimizes [analytical processing](https://motherduck.com/learn-more/what-is-OLAP/), excelling at complex queries on large datasets. Its architecture focuses on handling the types of queries common in data analysis and business intelligence, such as aggregations, complex joins, and scans of large portions of data.\n\n![Post Image](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fduckdb_ecosystem_svg_bcb500ebd1.svg%3Fv%3D3&w=3840&q=75)\n\n## When to Use DuckDB\n\nDuckDB excels in several scenarios:\n**Data Science and Analysis**: Query large datasets efficiently in Python or R. DuckDB integrates seamlessly with data science workflows, allowing you to perform SQL queries directly on [pandas DataFrames](https://motherduck.com/learn-more/pandas-dataframes-guide/) or R data frames without data transfer overhead.\n\n**Application Development**: Embed analytical capabilities in various application types. Whether you're building desktop software, mobile apps, or web applications, DuckDB provides a lightweight yet powerful solution for integrating data analysis features.\n\n**Data Engineering**: Handle different file formats and complex queries in data pipelines. DuckDB's ability to work with formats like CSV, Parquet, and JSON makes it a versatile tool for data transformation and preparation tasks.\n\n**Local Data Processing**: Perform heavy computations on local datasets without a database server. DuckDB shines when you need to analyze data that's too large for memory but not so large that it requires a distributed system, helping you avoid the ['big data tax' of over-provisioned cloud warehouses](https://motherduck.com/learn-more/modern-data-warehouse-playbook/).\n\n**Prototyping and Testing**: Set up database schemas and test query performance quickly. DuckDB's ease of use makes it an excellent tool for rapid prototyping and testing of data models before deploying to larger systems.\n\n## Key Features of DuckDB\n\nDuckDB stands out with several key features that make it a powerful tool for data analysis:\n\n**1\\. Simplicity**\n\n- Operates without external dependencies, simplifying installation and deployment\n- Integrates easily into various environments, from local development to production systems\n- Eliminates complex setup processes, allowing you to start analyzing data immediately\n- Reduces infrastructure needs, a key factor in [how DuckDB slashes cloud warehouse costs](https://motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck/)\n\n**2\\. Portability**\n\n- Runs on all major operating systems (Windows, macOS, Linux) and CPU architectures (x86, ARM)\n- Supports various programming languages, including Python, R, Java, C++, and more\n- Enables consistent performance across different platforms, from small edge devices to large servers\n\n**3\\. Rich Feature Set**\n\nDespite its simplicity, DuckDB offers a comprehensive set of features:\n\n- Complex SQL queries and window functions for sophisticated data analysis\n- ACID guarantees through a custom-built storage manager, ensuring data integrity\n- Support for common file formats (CSV, Parquet, JSON), allowing easy integration with existing data sources\n- Automatic indexing to optimize query performance\n- User-defined functions and aggregates, enabling customization for specific use cases\n\n**4\\. Speed**\n\nDuckDB uses a columnar-vectorized query execution engine, which provides several performance benefits:\n\n- Processes data in batches for better performance, reducing per-row overhead\n- Leverages modern CPU architectures, including SIMD instructions, for efficient data processing\n- Enables efficient I/O operations and compression, minimizing data movement and storage requirements\n- Implements advanced query optimization techniques, such as predicate pushdown and adaptive query execution\n\n**5\\. Extensibility**\n\nDuckDB's architecture allows for significant extensibility:\n\n- Add new data types to support domain-specific data\n- Create custom functions to implement specialized analytical operations\n- Integrate additional file formats to work with various data sources\n- Extend SQL syntax to support specific analytical needs\n\n## DuckDB vs Other Databases\n\nTo understand DuckDB's place in the database ecosystem, let's compare it to other database types:\n\n- **vs OLTP Databases (PostgreSQL, MySQL)**: DuckDB focuses on analytical queries, optimizing for reading and analyzing large volumes of data. In contrast, OLTP databases excel at transaction processing, handling many small, frequent updates efficiently.\n\n- **vs Other OLAP Databases**: DuckDB stands out with its embedded nature and simple deployment. While systems like Clickhouse or Apache Druid offer powerful distributed computing capabilities, DuckDB provides high performance in a single-node, embedded context, making it ideal for local data processing and embedded analytics.\n\n- **vs SQLite**: Both DuckDB and SQLite embed easily into applications, but they serve different purposes. DuckDB optimizes for analytical queries and large dataset processing, while SQLite targets transactional workloads and serves as a local data storage solution for applications.\n\n\n## DuckDB and MotherDuck\n\nWhen discussing DuckDB, it's important to distinguish between:\n\n**DuckDB**: The open-source database system we've been describing\n\n**MotherDuck**: A separate company that provides a cloud data warehouse built on DuckDB\n\nMotherDuck aims to offer additional features and scalability options while leveraging DuckDB's core strengths. Think of it as the difference between using Postgres locally and using a managed Postgres service in the cloud.\n\n## Getting Started with DuckDB\n\nInstall DuckDB easily via package managers or direct download. Here's a quick Python example to illustrate its simplicity:\n\nCreate a connection (this creates a new database if it doesn't exist)\n\n```ini\nCopy code\n\ncon = duckdb.connect('my_database.db')\n```\n\nCreate a table and insert data\n\n```lua\nCopy code\n\ncon.execute(\"CREATE TABLE users (id INTEGER, name VARCHAR)\")\ncon.execute(\"INSERT INTO users VALUES (1, 'Alice'), (2, 'Bob')\")\n```\n\nQuery the data\n\n```scss\nCopy code\n\nresult = con.execute(\"SELECT * FROM users\").fetchall()\nprint(result)\n```\n\nPerform a more complex query\n\n```ini\nCopy code\n\nresult = con.execute(\"\"\"\n    SELECT name, COUNT(*) as count\n    FROM users\n    GROUP BY name\n    HAVING COUNT(*) > 0\n\"\"\").fetchall()\nprint(result)\ncon.close()\n```\n\nThis example demonstrates how to create a database, insert data, and perform both simple and more complex queries using DuckDB.\n\n## Performance Tips\n\nTo maximize DuckDB's performance:\n\n- Choose appropriate data types for your columns to optimize storage and query speed\n- Use parallel query execution on multi-core systems to take advantage of DuckDB's ability to parallelize operations\n- Optimize queries with the EXPLAIN command to understand and improve query execution plans\n- Use the Parquet file format for large datasets to benefit from its columnar storage and compression capabilities\n- Understand the fundamental [physics of data warehouse performance](https://motherduck.com/learn-more/diagnose-fix-slow-queries/) to address bottlenecks in the right order, starting with I/O.\n\n## Conclusion\n\nDuckDB combines simplicity, speed, and powerful analytical capabilities in a unique package. Whether you analyze data, develop applications, or build data pipelines, DuckDB provides an efficient solution for processing large datasets locally. Its embedded nature and focus on analytical workloads make it a versatile tool in the modern data ecosystem.\nAs you explore DuckDB, you'll discover how it enhances your data workflows, from rapid prototyping to production-ready analytics. DuckDB's combination of SQL familiarity and high-performance analytics brings advanced data processing capabilities to a wide range of applications and use cases.\nHappy querying, and may DuckDB empower your data analysis journey!\n\n### TABLE OF CONTENTS\n\n[Understanding DuckDB: The Basics](https://motherduck.com/learn-more/what-is-duckdb/#understanding-duckdb-the-basics)\n\n[When to Use DuckDB](https://motherduck.com/learn-more/what-is-duckdb/#when-to-use-duckdb)\n\n[Key Features of DuckDB](https://motherduck.com/learn-more/what-is-duckdb/#key-features-of-duckdb)\n\n[DuckDB vs Other Databases](https://motherduck.com/learn-more/what-is-duckdb/#duckdb-vs-other-databases)\n\n[DuckDB and MotherDuck](https://motherduck.com/learn-more/what-is-duckdb/#duckdb-and-motherduck)\n\n[Getting Started with DuckDB](https://motherduck.com/learn-more/what-is-duckdb/#getting-started-with-duckdb)\n\n[Performance Tips](https://motherduck.com/learn-more/what-is-duckdb/#performance-tips)\n\n[Conclusion](https://motherduck.com/learn-more/what-is-duckdb/#conclusion)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\n## Additional Resources\n\n[Blog\\\\\n\\\\\nWhy use DuckDB for Analytics](https://motherduck.com/blog/six-reasons-duckdb-slaps/) [Docs\\\\\n\\\\\nUsing the DuckDB CLI](https://motherduck.com/docs/getting-started/connect-query-from-duckdb-cli/) [Video\\\\\n\\\\\nWhat's new in DuckDB & MotherDuck](https://www.youtube.com/watch?v=t_rLbKmld7g) [Docs\\\\\n\\\\\nDuckDB vs SQLite](https://motherduck.com/learn-more/duckdb-vs-sqlite-databases/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 218,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 218
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/going-beyond-the-dataframe.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-vs-pandas-vs-polars-for-python-devs.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 10,
  [2m  â”‚[0m   "appliedLimit": 10
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Based on my research through the MotherDuck documentation, I now have enough information to provide a comprehensive answer. Let me write the answer to the file:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_3.md

  [96mğŸ”§ TOOL:[0m Bash â†’ touch /tmp/answer_without_kg_3.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m 
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /tmp/answer_without_kg_3.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/tmp/answer_without_kg_3.md",
  [2m  â”‚[0m     "content": "# SQL Features from DuckDB Not Yet Supported in MotherDuck\n\nBased on the MotherDuck documentation, there are several SQL features and capabilities from DuckDB that have limitations or are not yet fully supported in MotherDuck due to its cloud architecture. While MotherDuck maintains strong compatibility with DuckDB and uses the same engine both locally and in the cloud, some advanced features remain unavailable on the server-side.\n\n## Key Unsupported Features\n\n### 1. Multiple Simultaneous Writers\n\n**Limitation:** MotherDuck currently **only supports one writer per database** at a time. While multiple users can read from the same shared database simultaneously, only one user can have write access.\n\n**Why:** This is a fundamental architectural constraint of MotherDuck's storage system. Even though MotherDuck has solved the \"reading while writing\" problem through its differential storage engine and snapshot isolation, the multiple writer problem remains unsolved.\n\n**Impact:** Workloads requiring simultaneous writes from different users need to be structured carefully. In practice, the typical pattern is:\n- Data ingestion and transformation done by a shared service account\n- Reading can be done by many different users\n- Data writes to separate data universes can be split between service accounts\n\n**Future:** MotherDuck is working on multi-writer support by routing writes to a single backend, simulating multiple writers even though DuckDB doesn't natively allow it.\n\n### 2. Real-Time Write Visibility for Shared Databases\n\n**Limitation:** When databases are shared with other users, there can be a **delay before readers see the latest changes** made by the database owner.\n\n**Two Modes:**\n- **AUTOUPDATE shares:** Changes are published to readers as soon as possible, but there can still be a small delay\n- **Manual shares:** Require explicit `UPDATE SHARE` commands to publish changes to readers\n\n**Why:** To give readers a \"clean snapshot\" of the data, MotherDuck ensures consistency at the storage layer. Readers might not be able to see up-to-the-moment changes being written by the owner.\n\n**Use Case:** Sometimes this delay is actually useful - for example, when making a handful of changes that should appear together, or running unit tests before publishing results.\n\n**Future:** MotherDuck is working to reduce this gap and will introduce upper bounds guarantees on how long it takes to publish data.\n\n### 3. Custom User-Defined Functions (UDFs)\n\n**Python UDFs and Native UDFs** are not supported on the MotherDuck server-side. This includes:\n- Custom Python functions that extend DuckDB's capabilities\n- Native/compiled UDFs written in C/C++\n\n**Workaround**: Perform these transformations upstream (on the client-side or in your data pipeline) before loading data into MotherDuck.\n\n### 4. Server-Side `ATTACH` Operations\n\nThe **`ATTACH` statement** for connecting to other databases (such as PostgreSQL, MySQL, or SQLite) is not available on the MotherDuck server. DuckDB locally supports attaching to various databases to query them directly, but this capability is not yet enabled in MotherDuck's cloud environment.\n\n**Workaround**: Export data from external databases and load it into MotherDuck, or perform cross-database queries locally using DuckDB's dual execution model.\n\n### 5. Custom Extensions\n\n**Custom DuckDB extensions** cannot be loaded on the MotherDuck server. While DuckDB has a rich ecosystem of extensions for various data formats, protocols, and functionality (spatial data, HTTP, JSON, etc.), custom or third-party extensions beyond what MotherDuck provides are not supported in the cloud environment.\n\n**Why:** MotherDuck uses DuckDB's extension system to add its own capabilities, but for security and stability reasons in a multi-tenant cloud environment, not all custom or third-party extensions may be supported.\n\n**Workaround**: Use client-side DuckDB with the required extensions for preprocessing, or leverage MotherDuck's supported built-in extensions.\n\n### 6. Certain DuckDB Configuration Settings\n\n**Limitation:** Some DuckDB PRAGMA settings or configuration options that control local behavior (like memory limits, thread counts for local execution, or file system paths) may work differently or not be available in MotherDuck's cloud environment.\n\n**Why:** MotherDuck manages compute resources through its containerized \"ducklings\" and handles resource allocation at the platform level.\n\n## Architectural Context and Differences\n\nIt's important to understand that these limitations stem from MotherDuck's serverless, managed architecture. MotherDuck prioritizes:\n- **Security**: Preventing arbitrary code execution in the cloud environment\n- **Performance**: Maintaining predictable, optimized query execution\n- **Stability**: Ensuring consistent behavior across all users\n\nThe dual-execution model allows many of these operations to be performed locally on the client side, where full DuckDB functionality is available, while still benefiting from MotherDuck's cloud-scale storage and compute for large datasets.\n\n### Storage Model Differences\n\nWhile not \"unsupported features,\" it's important to understand these architectural differences:\n\n1. **In-Place File Mutations:** DuckDB's native file format is optimized for a single writer on a local file system. MotherDuck doesn't support this model directly because cloud object stores like S3 have immutable files. MotherDuck uses a **differential storage system** that sits at the filesystem layer, making it appear like data has been updated in-place while actually using append-only files underneath.\n\n2. **Write-Ahead Log (WAL):** DuckDB often skips the WAL for batch updates as a performance optimization. MotherDuck's architecture handles this differently by implementing separation at a lower filesystem layer using differential storage rather than relying on WAL-based replication.\n\n3. **Database File Model:** MotherDuck uses a differential storage engine with append-only files and metadata tracking, rather than DuckDB's single-file model. This enables features like zero-copy data sharing and database snapshots.\n\n## Other Considerations\n\n### Regional Availability\nWhile not a SQL feature limitation, MotherDuck currently operates in:\n- AWS `us-east-1` (US East)\n- AWS `eu-central-1` (Frankfurt, Europe)\n\nTeams with strict data residency requirements outside these regions will need to consider this constraint.\n\n### Workload Limitations\nMotherDuck is optimized for **OLAP (analytical)** workloads, not **OLTP (transactional)** workloads. This means:\n- Not designed for thousands of concurrent, low-latency writes and updates per second\n- Database-level permissions rather than fine-grained row-level security\n- Not currently designed to serve thousands of simultaneous public-facing analytical queries\n\n## Compatibility Strengths\n\nDespite these limitations, MotherDuck maintains excellent compatibility with DuckDB for the vast majority of analytical SQL operations, including:\n- Full SQL query syntax support\n- Columnar storage and vectorized execution\n- ACID transactions (with serializable isolation)\n- Complex joins and aggregations\n- Direct querying of Parquet, CSV, and JSON files\n- Integration with cloud object storage (S3, GCS, Azure Blob Storage)\n- Hybrid execution (joining local and cloud data)\n\n## Future Roadmap\n\nMotherDuck is actively working on:\n- Multi-writer support by routing writes to a single backend\n- Reduced latency for share updates with guaranteed upper bounds\n- Time travel and branching features (made easy by immutable storage)\n- Improved caching for faster 'warm start' access\n\n## Summary\n\nMost SQL features from DuckDB **are supported** in MotherDuck. The main limitations are:\n1. **Single writer per database** (no concurrent writes)\n2. **Eventual consistency for shared databases** (small delay in seeing changes)\n3. **Custom Python/Native User-Defined Functions (UDFs)**\n4. **Server-side `ATTACH` to external databases**\n5. **Custom DuckDB extensions**\n6. **Different underlying storage model** (append-only vs. in-place updates)\n\nThese limitations are trade-offs made to enable MotherDuck's key benefits: cloud scale, collaboration, data sharing, and separation of storage and compute while maintaining ACID compliance. For most analytical workloads, the combination of MotherDuck's cloud capabilities and local DuckDB's full feature set (via dual execution) provides a powerful and flexible solution.\n\n---\n\n## Sources\n\n1. **Separating Storage and Compute in DuckDB** - [motherduck.com/blog/separating-storage-compute-duckdb](https://motherduck.com/blog/separating-storage-compute-duckdb/)\n   - Comprehensive blog post explaining MotherDuck's architecture, storage model, and sharing limitations\n   - Details the single-writer constraint: \"There are still some restrictions that remain; in MotherDuck, only one user can have write access to a database.\"\n   - Explains the differential storage system and how it differs from DuckDB's native file format\n   - Discusses eventual consistency for shared databases and future work on multi-writer support\n\n2. **Bringing DuckDB to the Cloud: Dual Execution Explained** - [motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained/)\n   - Video and article explaining MotherDuck's architecture, extension system, and storage layer\n   - Covers the dual execution model and how queries are routed between local and cloud compute\n   - Explains database sharing and secure credentials management\n\n3. **ACID Transactions in Databases: A Data Engineer's Guide** - [motherduck.com/learn-more/acid-transactions-sql](https://motherduck.com/learn-more/acid-transactions-sql)\n   - Guide explaining DuckDB's ACID compliance and how MotherDuck maintains these guarantees\n   - Discusses serializable transactions and snapshot isolation in DuckDB\n\n4. **Fix Slow BI Dashboards Guide** - [motherduck.com/learn-more/fix-slow-bi-dashboards](https://motherduck.com/learn-more/fix-slow-bi-dashboards)\n   - States: \"MotherDuck does not yet support the full range of DuckDB's SQL features. Notably, custom Python/native User-Defined Functions (UDFs), server-side `ATTACH` to other databases (like Postgres), and custom extensions are not yet available.\"\n\n5. **MotherDuck: A BigQuery Alternative** - [motherduck.com/learn-more/bigquery-alternative-motherduck](https://motherduck.com/learn-more/bigquery-alternative-motherduck)\n   - Discusses MotherDuck's hybrid architecture and the distinction between client-side and server-side capabilities\n   - Highlights known trade-offs including limitations for high-throughput transactional workloads\n\n6. **MotherDuck Product Page** - [motherduck.com/product](https://motherduck.com/product/)\n   - Overview of MotherDuck's architecture and features, including the dual execution query engine\n",
  [2m  â”‚[0m     "numLines": 155,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 155
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

âš ï¸  GUARDRAIL: Max duration (300s) exceeded!
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "update",
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_3.md",
  [2m  â”‚[0m   "content": "<5186 chars>",
  [2m  â”‚[0m   "structuredPatch": [
  [2m  â”‚[0m     {
  [2m  â”‚[0m       "oldStart": 1,
  [2m  â”‚[0m       "oldLines": 154,
  [2m  â”‚[0m       "newStart": 1,
  [2m  â”‚[0m       "newLines": 56,
  [2m  â”‚[0m       "lines": [
  [2m  â”‚[0m         "-# SQL Features from DuckDB Not Yet Supported in MotherDuck",
  [2m  â”‚[0m         "+# Answer",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-Based on the MotherDuck documentation, there are several SQL features and capabilities from DuckDB that have limitations or are not yet fully supported in MotherDuck due to its cloud architecture. While MotherDuck maintains strong compatibility with DuckDB and uses the same engine both locally and in the cloud, some advanced features remain unavailable on the server-side.",
  [2m  â”‚[0m         "+Based on the available documentation, MotherDuck is built on DuckDB and maintains compatibility with DuckDB's core SQL features. However, there are some architectural differences and limitations inherent to MotherDuck's cloud-based nature compared to local DuckDB:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Key Unsupported Features",
  [2m  â”‚[0m         "+## Storage Model Differences",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### 1. Multiple Simultaneous Writers",
  [2m  â”‚[0m         "+**Single-Writer File System**: DuckDB's native file format is optimized for a single writer on a local file system. This model is not well-suited for the cloud, where multiple users need to read and write to the same database concurrently. MotherDuck addresses this by implementing a differential storage system that maps a logical database file to append-only snapshot layers in cloud object storage, which is fundamentally different from DuckDB's local file-based approach.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-**Limitation:** MotherDuck currently **only supports one writer per database** at a time. While multiple users can read from the same shared database simultaneously, only one user can have write access.",
  [2m  â”‚[0m         "+## Hybrid Execution Model",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-**Why:** This is a fundamental architectural constraint of MotherDuck's storage system. Even though MotherDuck has solved the \"reading while writing\" problem through its differential storage engine and snapshot isolation, the multiple writer problem remains unsolved.",
  [2m  â”‚[0m         "+MotherDuck uses a **dual execution** architecture where queries can run either locally on the client, remotely in the MotherDuck cloud, or as a hybrid of both. While this is powerful, it means that certain operations may behave differently than in pure local DuckDB. The optimizer automatically decides query execution location, though users can override this with the `md_run` parameter in functions like `read_parquet()` or `read_csv()`.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-**Impact:** Workloads requiring simultaneous writes from different users need to be structured carefully. In practice, the typical pattern is:",
  [2m  â”‚[0m         "-- Data ingestion and transformation done by a shared service account",
  [2m  â”‚[0m         "-- Reading can be done by many different users",
  [2m  â”‚[0m         "-- Data writes to separate data universes can be split between service accounts",
  [2m  â”‚[0m         "+## Collaboration and Concurrency",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-**Future:** MotherDuck is working on multi-writer support by routing writes to a single backend, simulating multiple writers even though DuckDB doesn't natively allow it.",
  [2m  â”‚[0m         "+While DuckDB is fundamentally a \"single-player\" embedded database, MotherDuck transforms it into a collaborative \"multiplayer\" platform. This requires architectural differences in how concurrent writes are handled. MotherDuck provides isolated compute instances (called \"ducklings\") per user to handle concurrent access, whereas local DuckDB has limited support for concurrent writes to ensure data integrity.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### 2. Real-Time Write Visibility for Shared Databases",
  [2m  â”‚[0m         "+## Storage Backend",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-**Limitation:** When databases are shared with other users, there can be a **delay before readers see the latest changes** made by the database owner.",
  [2m  â”‚[0m         "+MotherDuck uses its own storage layer rather than DuckDB's native file format. The system offers:",
  [2m  â”‚[0m         "+- **MotherDuck native storage**: Fast, low-latency storage for the data warehouse",
  [2m  â”‚[0m         "+- **DuckLake storage**: For petabyte-scale data, using a database-backed metadata catalog and S3-compatible object storage",
  [2m  â”‚[0m         "+- **Hybrid approach**: Can query data across local files, MotherDuck cloud storage, and external sources like S3",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-**Two Modes:**",
  [2m  â”‚[0m         "-- **AUTOUPDATE shares:** Changes are published to readers as soon as possible, but there can still be a small delay",
  [2m  â”‚[0m         "-- **Manual shares:** Require explicit `UPDATE SHARE` commands to publish changes to readers",
  [2m  â”‚[0m         "+This is different from DuckDB's approach of working directly with local database files (.db files) or reading from file formats like Parquet and CSV.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-**Why:** To give readers a \"clean snapshot\" of the data, MotherDuck ensures consistency at the storage layer. Readers might not be able to see up-to-the-moment changes being written by the owner.",
  [2m  â”‚[0m         "+## Extension System Integration",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-**Use Case:** Sometimes this delay is actually useful - for example, when making a handful of changes that should appear together, or running unit tests before publishing results.",
  [2m  â”‚[0m         "+MotherDuck leverages DuckDB's extension system to add capabilities rather than forking the codebase. This means MotherDuck extends DuckDB at multiple layers including the SQL parser, optimizer, and storage interface. While this maintains compatibility, it means some features are implemented through extensions rather than being native.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-**Future:** MotherDuck is working to reduce this gap and will introduce upper bounds guarantees on how long it takes to publish data.",
  [2m  â”‚[0m         "+## Key Architectural Components",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### 3. Custom User-Defined Functions (UDFs)",
  [2m  â”‚[0m         "+MotherDuck's architecture consists of three main layers not present in standalone DuckDB:",
  [2m  â”‚[0m         "+1. **Client Extension**: Enables connection to MotherDuck via the `ATTACH` command",
  [2m  â”‚[0m         "+2. **Cloud Compute Layer**: Containerized DuckDB instances (\"ducklings\") that scale based on user needs",
  [2m  â”‚[0m         "+3. **Differential Storage Layer**: Cloud-optimized storage that enables features like database sharing and time travel",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-**Python UDFs and Native UDFs** are not supported on the MotherDuck server-side. This includes:",
  [2m  â”‚[0m         "-- Custom Python functions that extend DuckDB's capabilities",
  [2m  â”‚[0m         "-- Native/compiled UDFs written in C/C++",
  [2m  â”‚[0m         "+## Important Notes",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-**Workaround**: Perform these transformations upstream (on the client-side or in your data pipeline) before loading data into MotherDuck.",
  [2m  â”‚[0m         "+The documentation does not provide an explicit list of \"SQL features not supported in MotherDuck.\" Instead, MotherDuck is positioned as an enhancement to DuckDB that maintains SQL compatibility while adding cloud-scale capabilities like:",
  [2m  â”‚[0m         "+- Centralized Secret Manager for managing credentials",
  [2m  â”‚[0m         "+- Database sharing for team collaboration",
  [2m  â”‚[0m         "+- Time travel (with DuckLake)",
  [2m  â”‚[0m         "+- Automatic scaling and resource management",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### 4. Server-Side `ATTACH` Operations",
  [2m  â”‚[0m         "+The fundamental SQL query capabilities appear to be preserved, with differences primarily in the underlying storage, execution model, and multi-user collaboration features rather than in unsupported SQL syntax or functions.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-The **`ATTACH` statement** for connecting to other databases (such as PostgreSQL, MySQL, or SQLite) is not available on the MotherDuck server. DuckDB locally supports attaching to various databases to query them directly, but this capability is not yet enabled in MotherDuck's cloud environment.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-**Workaround**: Export data from external databases and load it into MotherDuck, or perform cross-database queries locally using DuckDB's dual execution model.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-### 5. Custom Extensions",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-**Custom DuckDB extensions** cannot be loaded on the MotherDuck server. While DuckDB has a rich ecosystem of extensions for various data formats, protocols, and functionality (spatial data, HTTP, JSON, etc.), custom or third-party extensions beyond what MotherDuck provides are not supported in the cloud environment.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-**Why:** MotherDuck uses DuckDB's extension system to add its own capabilities, but for security and stability reasons in a multi-tenant cloud environment, not all custom or third-party extensions may be supported.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-**Workaround**: Use client-side DuckDB with the required extensions for preprocessing, or leverage MotherDuck's supported built-in extensions.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-### 6. Certain DuckDB Configuration Settings",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-**Limitation:** Some DuckDB PRAGMA settings or configuration options that control local behavior (like memory limits, thread counts for local execution, or file system paths) may work differently or not be available in MotherDuck's cloud environment.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-**Why:** MotherDuck manages compute resources through its containerized \"ducklings\" and handles resource allocation at the platform level.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-## Architectural Context and Differences",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-It's important to understand that these limitations stem from MotherDuck's serverless, managed architecture. MotherDuck prioritizes:",
  [2m  â”‚[0m         "-- **Security**: Preventing arbitrary code execution in the cloud environment",
  [2m  â”‚[0m         "-- **Performance**: Maintaining predictable, optimized query execution",
  [2m  â”‚[0m         "-- **Stability**: Ensuring consistent behavior across all users",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-The dual-execution model allows many of these operations to be performed locally on the client side, where full DuckDB functionality is available, while still benefiting from MotherDuck's cloud-scale storage and compute for large datasets.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-### Storage Model Differences",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-While not \"unsupported features,\" it's important to understand these architectural differences:",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-1. **In-Place File Mutations:** DuckDB's native file format is optimized for a single writer on a local file system. MotherDuck doesn't support this model directly because cloud object stores like S3 have immutable files. MotherDuck uses a **differential storage system** that sits at the filesystem layer, making it appear like data has been updated in-place while actually using append-only files underneath.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-2. **Write-Ahead Log (WAL):** DuckDB often skips the WAL for batch updates as a performance optimization. MotherDuck's architecture handles this differently by implementing separation at a lower filesystem layer using differential storage rather than relying on WAL-based replication.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-3. **Database File Model:** MotherDuck uses a differential storage engine with append-only files and metadata tracking, rather than DuckDB's single-file model. This enables features like zero-copy data sharing and database snapshots.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-## Other Considerations",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-### Regional Availability",
  [2m  â”‚[0m         "-While not a SQL feature limitation, MotherDuck currently operates in:",
  [2m  â”‚[0m         "-- AWS `us-east-1` (US East)",
  [2m  â”‚[0m         "-- AWS `eu-central-1` (Frankfurt, Europe)",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-Teams with strict data residency requirements outside these regions will need to consider this constraint.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-### Workload Limitations",
  [2m  â”‚[0m         "-MotherDuck is optimized for **OLAP (analytical)** workloads, not **OLTP (transactional)** workloads. This means:",
  [2m  â”‚[0m         "-- Not designed for thousands of concurrent, low-latency writes and updates per second",
  [2m  â”‚[0m         "-- Database-level permissions rather than fine-grained row-level security",
  [2m  â”‚[0m         "-- Not currently designed to serve thousands of simultaneous public-facing analytical queries",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-## Compatibility Strengths",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-Despite these limitations, MotherDuck maintains excellent compatibility with DuckDB for the vast majority of analytical SQL operations, including:",
  [2m  â”‚[0m         "-- Full SQL query syntax support",
  [2m  â”‚[0m         "-- Columnar storage and vectorized execution",
  [2m  â”‚[0m         "-- ACID transactions (with serializable isolation)",
  [2m  â”‚[0m         "-- Complex joins and aggregations",
  [2m  â”‚[0m         "-- Direct querying of Parquet, CSV, and JSON files",
  [2m  â”‚[0m         "-- Integration with cloud object storage (S3, GCS, Azure Blob Storage)",
  [2m  â”‚[0m         "-- Hybrid execution (joining local and cloud data)",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-## Future Roadmap",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-MotherDuck is actively working on:",
  [2m  â”‚[0m         "-- Multi-writer support by routing writes to a single backend",
  [2m  â”‚[0m         "-- Reduced latency for share updates with guaranteed upper bounds",
  [2m  â”‚[0m         "-- Time travel and branching features (made easy by immutable storage)",
  [2m  â”‚[0m         "-- Improved caching for faster 'warm start' access",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-## Summary",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-Most SQL features from DuckDB **are supported** in MotherDuck. The main limitations are:",
  [2m  â”‚[0m         "-1. **Single writer per database** (no concurrent writes)",
  [2m  â”‚[0m         "-2. **Eventual consistency for shared databases** (small delay in seeing changes)",
  [2m  â”‚[0m         "-3. **Custom Python/Native User-Defined Functions (UDFs)**",
  [2m  â”‚[0m         "-4. **Server-side `ATTACH` to external databases**",
  [2m  â”‚[0m         "-5. **Custom DuckDB extensions**",
  [2m  â”‚[0m         "-6. **Different underlying storage model** (append-only vs. in-place updates)",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-These limitations are trade-offs made to enable MotherDuck's key benefits: cloud scale, collaboration, data sharing, and separation of storage and compute while maintaining ACID compliance. For most analytical workloads, the combination of MotherDuck's cloud capabilities and local DuckDB's full feature set (via dual execution) provides a powerful and flexible solution.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "----",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         " ## Sources",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-1. **Separating Storage and Compute in DuckDB** - [motherduck.com/blog/separating-storage-compute-duckdb](https://motherduck.com/blog/separating-storage-compute-duckdb/)",
  [2m  â”‚[0m         "-   - Comprehensive blog post explaining MotherDuck's architecture, storage model, and sharing limitations",
  [2m  â”‚[0m         "-   - Details the single-writer constraint: \"There are still some restrictions that remain; in MotherDuck, only one user can have write access to a database.\"",
  [2m  â”‚[0m         "-   - Explains the differential storage system and how it differs from DuckDB's native file format",
  [2m  â”‚[0m         "-   - Discusses eventual consistency for shared databases and future work on multi-writer support",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-2. **Bringing DuckDB to the Cloud: Dual Execution Explained** - [motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained/)",
  [2m  â”‚[0m         "-   - Video and article explaining MotherDuck's architecture, extension system, and storage layer",
  [2m  â”‚[0m         "-   - Covers the dual execution model and how queries are routed between local and cloud compute",
  [2m  â”‚[0m         "-   - Explains database sharing and secure credentials management",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-3. **ACID Transactions in Databases: A Data Engineer's Guide** - [motherduck.com/learn-more/acid-transactions-sql](https://motherduck.com/learn-more/acid-transactions-sql)",
  [2m  â”‚[0m         "-   - Guide explaining DuckDB's ACID compliance and how MotherDuck maintains these guarantees",
  [2m  â”‚[0m         "-   - Discusses serializable transactions and snapshot isolation in DuckDB",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-4. **Fix Slow BI Dashboards Guide** - [motherduck.com/learn-more/fix-slow-bi-dashboards](https://motherduck.com/learn-more/fix-slow-bi-dashboards)",
  [2m  â”‚[0m         "-   - States: \"MotherDuck does not yet support the full range of DuckDB's SQL features. Notably, custom Python/native User-Defined Functions (UDFs), server-side `ATTACH` to other databases (like Postgres), and custom extensions are not yet available.\"",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-5. **MotherDuck: A BigQuery Alternative** - [motherduck.com/learn-more/bigquery-alternative-motherduck](https://motherduck.com/learn-more/bigquery-alternative-motherduck)",
  [2m  â”‚[0m         "-   - Discusses MotherDuck's hybrid architecture and the distinction between client-side and server-side capabilities",
  [2m  â”‚[0m         "-   - Highlights known trade-offs including limitations for high-throughput transactional workloads",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-6. **MotherDuck Product Page** - [motherduck.com/product](https://motherduck.com/product/)",
  [2m  â”‚[0m         "-   - Overview of MotherDuck's architecture and features, including the dual execution query engine",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md - Information about hybrid execution and how MotherDuck extends DuckDB to the cloud",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md - Reference to the CIDR 2024 paper on hybrid query processing architecture",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md - Details on dual execution, collaboration features, and production capabilities",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md - Detailed explanation of MotherDuck's architecture including storage layer differences, dual execution model, and the three-layer architecture",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/blog/ducklake-motherduck.md - Information about DuckLake storage format and the challenges of using S3 vs traditional databases for metadata",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md - Details on DuckLake's database-backed metadata approach and differences from DuckDB's file-based storage",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/product/duckdb-users.md - Overview of how MotherDuck transforms single-player DuckDB into multiplayer collaborative platform",
  [2m  â”‚[0m         "+- .kurt/sources/motherduck.com/learn-more/what-is-duckdb.md - Foundation information about DuckDB's core features and capabilities as baseline for comparison"
  [2m  â”‚[0m       ]
  [2m  â”‚[0m     }
  [2m  â”‚[0m   ]
  [2m  â”‚[0m }
  [2m  â””â”€[0m
âš ï¸  SDK error: Exceeded max duration of 300s
Traceback (most recent call last):
  File "/Users/julien/Documents/wik/wikumeo/projects/kurt-core/eval/framework/runner.py", line 1249, in _execute_with_sdk
    raise RuntimeError(
RuntimeError: Exceeded max duration of 300s
âŒ Scenario execution failed: Exceeded max duration of 300s
Traceback (most recent call last):
  File "/Users/julien/Documents/wik/wikumeo/projects/kurt-core/eval/framework/runner.py", line 307, in _run_async
    await self._execute_question_set(scenario, workspace, metrics_collector)
  File "/Users/julien/Documents/wik/wikumeo/projects/kurt-core/eval/framework/runner.py", line 482, in _execute_question_set
    await self._run_question_conversation(
  File "/Users/julien/Documents/wik/wikumeo/projects/kurt-core/eval/framework/runner.py", line 809, in _run_question_conversation
    await self._execute_with_sdk(
  File "/Users/julien/Documents/wik/wikumeo/projects/kurt-core/eval/framework/runner.py", line 1249, in _execute_with_sdk
    raise RuntimeError(
RuntimeError: Exceeded max duration of 300s
```
