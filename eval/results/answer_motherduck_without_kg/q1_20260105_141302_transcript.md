# Scenario: answer_motherduck_without_kg

**Status**: âœ… PASSED
**Timestamp**: 20260105_141302

**Duration**: 88.46 seconds
**Tokens Used**: 0

---

## Conversation


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ SCENARIO: answer_motherduck_without_kg
   Answer MotherDuck questions by searching sources directly (no KG)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


ğŸ” Verifying .claude installation...
   âœ“ .claude folder exists at /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.claude
   âœ“ Skills: 0 found
   âœ“ Commands: 0 found

ğŸ§ª Running 10 of 10 question(s) defined in questions_motherduck.yaml


======================================================================
â“ Question 1/10
...... What file formats are most efficient for loading data into MotherDuck?
======================================================================


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 1
â”‚ Answer this question by searching through the .kurt/sources/ directory:

...... What file formats are most efficient for loading data into MotherDuck?

Required Steps:
1. Search for relevant markdown files in .kurt/sources/ using glob and grep
2. Read the most relevant files (aim for 3-5 most relevant sources)
3. As you read each file, keep track of:
   - The file path
   - The file title/name
   - Key information extracted from that file
4. Synthesize a comprehensive answer based on the information found
5. Write your answer to: /tmp/answer_without_kg_1.md

IMPORTANT - Your answer MUST have this structure:
```markdown
# Answer

[Your comprehensive answer here, drawing from the sources you read]

## Sources

List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
[etc.]

DO NOT use URLs or document titles as links. Show the actual file path you read with the Read tool.
```

The Sources section is MANDATORY - you must list every file you read to answer the question.

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [96mğŸ”§ TOOL:[0m Grep â†’ file format.*load|loading.*format|efficient.*format|format.*efficient|parquet|csv|json.*load|load.*data

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/**/*load*.md

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/**/*format*.md

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/**/*import*.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "durationMs": 481,
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/glossary/data load tool (dlt).md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "durationMs": 622,
  [2m  â”‚[0m   "numFiles": 2,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/entities.txt",
  [2m  â”‚[0m     ".kurt/sources/claims.txt",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/claims.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/.metadata/entities.yaml",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/spatial-data-management-with-duckdb-ft-mattforrest.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/leveraging-duckdb-and-delta-lake-together.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/faster-data-pipelines-development-with-mcp-and-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/going-beyond-the-dataframe.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ibis-one-library-to-query-any-backend.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-vs-pandas-vs-polars-for-python-devs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ducklake-big-data-small-coalesce-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-dbt-accelerating-the-developer-experience-with-local-power.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-python-end-to-end-data-engineering-project-13.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/4-lightning-talks-on-practical-ai-workflows-from-notion-1password-motherduck-evidence.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-duck-in-the-hand-is-worth-two-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-new-paradigm-for-data-visualization-with-just-sql-markdown.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/sql-duckdb-book-form.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/quacking-around-the-web.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-OLAP.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/product-analytics-motherduck-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/web-assembly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-use-cases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/pandas-dataframes-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-struct-nested-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/ducklake-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-lake-vs-data-warehouse-vs-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-warehouse-tco.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/hack-night.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/index.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/acid-transactions-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/big-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/relational database.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/storage layer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/storage.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/time-series.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/auto inference.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data build tool (dbt).md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data engineering.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data load tool (dlt).md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data pipeline.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/database.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/dataset.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/SQL analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Pandas DataFrames.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/S3 bucket.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/DuckDB CLI.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/DuckDB.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/ELT.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/ETL.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/EXPLAIN clause.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Amazon S3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Apache Arrow.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/CLI.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Cloud Storage.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/CSV.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/sqlmesh.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/paradime.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/sling-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/fivetran.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/hevo.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/mage.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/meltano.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/dlt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/airbyte.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/artie.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/cloudquery.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/contact-us/product-expert.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-brief.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-summary-chapter2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-news.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/gardyn.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/layers-multi-tenant-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/reflex-careers-gig-economy-retail.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/community-and-open-source.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/atm-com-analytics-costs-sql-expressibility.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dexibit.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dominik-moritz.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-simple-joys-of-scaling-up.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/unstructured-analysis-tensorlake-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vector-technologies-ai-data-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-everybody-hates-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-keyboard-shortcuts-for-joyful-querying.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streaming-data-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streaming-oracle-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streamlining-ai-agents-duckdb-rag-solutions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/summer-data-engineering-roadmap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/tableau-cloud-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/simplifying-iot-analytics-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/six-reasons-duckdb-slaps.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-recap-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-recap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/solving-advent-code-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/spark-ducklake-getting-started.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/secoda-motherduck-integration-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/semantic-layer-duckdb-tutorial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-faker-duckdb-exploration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/quacktosql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/read-scaling-preview.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/redshift-files-hunt-for-big-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/scaling-duckdb-with-ducklings.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg to motherduck at motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg_duckdb-postgresql-extension-for-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-duckdb-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-mooncake-columnstore.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/postgres-duckdb-options.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/preswald-health-data-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pushing-geo-boundaries-with-motherduck-geobase.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/perf-is-not-enough.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-in-europe.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-kestra-etl-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-open-for-all-with-series-b.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-reflections-sigmod-pods-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-union-orchestration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/just-enough-sql-for-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/local-duckdb-ui-visual-data-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/log-processing-savings-bacalhau-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-column-explorer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-fixit-ai-sql-error-fixer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-motherduck-for-business-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/json-log-analysis-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/google-sheets-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/how-to-extract-analytics-from-bluesky.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/galileo-world-geospatial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/geospatial-for-beginner-duckdb-spatial-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/getting-started-ducklake-table-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/getting-started-gis-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/git-for-data-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-ducks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/from-data-lake-to-lakehouse-duckdb-portable-catalog.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-wasm-in-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/ducklake-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/effortless-etl-unstructured-data-unstructuredio-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/estuary-streaming-cdc-replication.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/fabi-ai-llm-prompt-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-python-e2e-data-engineering-project-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-text2sql-llm.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-the-great-federator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-versus-pandas-versus-polars.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-three.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-two.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-excel-extension.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-puppygraph-graph-model-on-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-one.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-seven.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-ten.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-nine.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eleven.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-four.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-110-hidden-gems.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-cognee-sql-analytics-graph-rag.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineer-highlights-PyConDE-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-essential-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-infrastructure-devops.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineers-answer-10-top-reddit-questions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-nov-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/differential-storage-building-block-for-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/building-data-applications-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/building-motherduck-partner-ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-app-generator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-small-data-sf-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/big-data-is-dead.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/bigquery-to-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-snippet-sets-with-motherduck-sharing-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-mega-giga-instance-sizes-huge-scale.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-series-seed-and-a.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-sqlite-databases-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-x-data-nodejs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-data-in-azure-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-json-data-using-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/MotherDuck-Visualize-Embeddings-Marimo.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/sheila-sitaram.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/ryan-boyd.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/jacob-matson.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/luciano-galvao-filho.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/mehdi-ouazza.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/adithya-krishnan.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/andrew-witten.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 287
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/getting-started-ducklake-table-format.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "durationMs": 911,
  [2m  â”‚[0m   "numFiles": 5,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search for more specific content about data loading and file formats:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ parquet|csv|json|iceberg|delta|efficient.*load|performance.*load

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     "content": "---\ntitle: why-choose-parquet-table-file-format\ncontent_type: tutorial\nsource_url: https://motherduck.com/learn-more/why-choose-parquet-table-file-format\nindexed_at: '2025-11-25T09:56:57.618325'\ncontent_hash: 2fb6399ae13aa7e1\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO LEARN](https://motherduck.com/learn-more/)\n\n# Parquet File Format: What It Is, Benefits, and Alternatives\n\n11 min readBY\n\n[Aditya Somani](https://motherduck.com/authors/aditya-aomani/)\n\n![Parquet File Format: What It Is, Benefits, and Alternatives](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdataframe_3_3e66a00165.png&w=3840&q=75)\n\nIn data engineering, efficient storage and processing of massive datasets is often very important. As data volumes continue to grow, organizations are turning to innovative file formats and table structures to optimize their data warehousing and analytics workflows. Among these, the Apache Parquet file format has emerged as a popular choice for its columnar storage layout and impressive compression capabilities.\n\nParquet, designed for use with large-scale data processing frameworks like Apache Spark and Hadoop, has gained significant traction in both the \"big data\" and \"small data\" communities. Its ability to enable efficient data storage and retrieval, coupled with its support for complex data types and schema evolution, makes it a compelling option for a wide range of data-intensive applications.\n\nIn this article, we will dive deep into the world of Parquet, exploring its key features, benefits, and use cases. We will also compare Parquet to other popular file formats and table structures, such as Apache Iceberg and Delta Lake, to help you make informed decisions when designing your data architecture.\n\n## Key Takeaways\n\n- **What is Parquet**: An open-source columnar storage file format for efficient analytics.\n- **Core Benefits**: Superior compression, faster query performance (via column pruning and predicate pushdown), and schema evolution support.\n- **Common Alternatives**: Compared to row-based (CSV, Avro), columnar (ORC), and table formats (Iceberg, Delta Lake).\n- **DuckDB & MotherDuck**: Parquet integrates seamlessly with DuckDB for high-performance SQL queries directly on Parquet files.\n\n## What is Parquet?\n\nParquet is a [columnar storage file format](https://motherduck.com/learn-more/columnar-storage-guide/). When data engineers ask 'what is a Parquet file?', the simple answer is that it's a file that stores data in columns, not rows. This Parquet data format is designed for efficient data processing, particularly in the context of big data applications. Developed as part of the Apache Hadoop ecosystem, Parquet has gained widespread adoption due to its ability to optimize storage and query performance.\n\n### Columnar Storage Format\n\nOne of the key characteristics of Parquet is its columnar storage layout. Unlike traditional row-based formats, Parquet organizes data by columns rather than rows. This means that all values for a particular column are stored contiguously on disk, enabling faster retrieval and better compression ratios.\n\nBy storing data in a columnar fashion, Parquet excels at analytical queries that involve reading a subset of columns from a large dataset. This is particularly beneficial for data warehousing and business intelligence scenarios, where queries often focus on specific columns rather than entire rows.\n\nThe columnar storage format also allows for more efficient compression techniques. Since values within a column tend to be of the same data type and exhibit similar patterns, Parquet can apply appropriate compression algorithms to achieve higher compression ratios compared to row-based formats. This reduction in storage footprint translates to lower storage costs and faster query execution, as less data needs to be read from disk.\n\n![Post Image](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fstorage_comparison_1_5c87b9f5c1.svg&w=3840&q=75)\n\n### Open Source and Widely Supported\n\nParquet is an open-source project governed by the Apache Software Foundation, ensuring its transparency, community-driven development, and long-term sustainability. The open-source nature of Parquet has fostered a vibrant ecosystem, with contributions from industry leaders and a growing user base.\n\nOne of the key advantages of Parquet's open-source status is its wide support across various big data processing frameworks. Major platforms like Apache Spark, Apache Hadoop, and Presto have native support for reading and writing Parquet files, making it easy to integrate Parquet into existing data pipelines.\n\nThis broad compatibility ensures interoperability between different tools and systems, allowing organizations to leverage Parquet across their data ecosystem. It also mitigates the risk of vendor lock-in, as Parquet can be used with a variety of open-source and commercial solutions.\n\n## Why Use Parquet?\n\nParquet stands out in data processing environments for its ability to efficiently manage large datasets while ensuring high performance. Its unique approach to data storage enhances retrieval speeds and optimizes storage efficiency. This is achieved through intelligent data organization and compression strategies that minimize unnecessary data handling.\n\n### Efficient Compression and Encoding\n\nParquet's architecture benefits from exceptional data compression capabilities, essential for managing extensive datasets. Instead of treating data uniformly, Parquet leverages specialized encoding techniques such as dictionary, run-length, and delta encoding to optimize storage. These methods reduce the data footprint, translating into cost savings and improved access speeds. By minimizing I/O operations, Parquet enhances query performance, making it ideal for data-intensive tasks.\n\n![Post Image](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fencoding_compression_e1bd66680f.svg&w=3840&q=75)\n\n### Schema Evolution and Nested Data Types\n\nParquet is designed to handle evolving data structures with ease, supporting seamless schema modifications. This flexibility allows for the addition or alteration of columns without disrupting existing workflows, ensuring continuous data integrity. Parquet's proficiency with complex data structures\u2014like nested fields\u2014facilitates versatile data modeling. Its adaptability is vital for accommodating dynamic business requirements and integrating diverse datasets.\n\n### Predicate Pushdown and Column Pruning\n\nParquet effectively supports techniques like predicate pushdown and column pruning, which are crucial for optimizing data queries. By bringing filtering and aggregation operations closer to the storage layer, Parquet reduces the amount of data that needs processing. This approach not only speeds up queries but also lowers computational demands, enabling swift responses. Consequently, Parquet allows data professionals to execute complex queries on extensive datasets efficiently, providing timely and actionable insights.\n\n## Parquet Alternatives and Comparisons\n\nNavigating the landscape of data storage solutions reveals a variety of formats, each offering unique strengths tailored to specific needs. While Parquet remains a prominent choice, exploring its alternatives can provide valuable insights into selecting the best fit for particular data environments.\n\n### CSV Files\n\nCSV files are a straightforward, text-based format that organizes data in a tabular form with rows and columns separated by delimiters. This simplicity makes CSV highly portable and easy to use for data exchange across different systems. However, it lacks advanced functionalities like compression and schema management, which limits its efficiency in handling large-scale datasets.\n\n### Apache ORC\n\nApache ORC stands out with its columnar storage capabilities, optimized for high-performance data processing tasks. It excels in compressing and managing large datasets efficiently, offering features like ACID transactions that ensure data integrity during updates and queries. ORC's tight integration with Hive-specific functionalities makes it an appealing choice for Hadoop ecosystems, enabling seamless operations within Hive data warehouses.\n\n### Apache Avro\n\nApache Avro offers a flexible, row-based format that emphasizes efficient data serialization and schema evolution. Avro provides robust support for evolving schemas, allowing changes without requiring data rewrites\u2014ideal for applications needing frequent schema updates and cross-system data exchange. Its compact binary serialization format enhances data transmission efficiency across distributed systems.\n\n### Delta Lake and Apache Iceberg\n\nDelta Lake and Apache Iceberg build on the strengths of Parquet, introducing advanced table management features. Delta Lake, with its seamless Spark integration, offers capabilities like ACID transactions and data versioning, supporting both batch and streaming data processing. This makes it suitable for environments requiring consistent data updates and real-time analytics.\n\nApache Iceberg is crafted to optimize large-scale data lake operations, providing comprehensive support for multiple file formats, including Parquet. It facilitates complex data management with features like schema evolution and time travel, ensuring data consistency and adaptability across diverse processing engines and storage solutions.\n\n## Choosing the Right Format\n\nSelecting the optimal data format necessitates a keen understanding of your data access patterns. For workloads requiring extensive data scans with a focus on specific data attributes, leveraging formats designed for efficient data retrieval can significantly enhance performance. In contrast, for scenarios where frequent updates or point-specific data access is essential, a format that facilitates rapid row-level operations may be more advantageous.\n\n### Ecosystem Compatibility\n\nThe integration of a data format with existing systems is paramount for operational efficiency. Formats that align well with current data processing tools and frameworks simplify the implementation process and reduce potential disruptions. Evaluating the collective expertise of your team with a given format can inform the decision-making process, ensuring a smooth transition and effective utilization of the chosen technology.\n\n### Data Volume and Scalability\n\nAnticipating the trajectory of data growth and volume is critical in format selection. For environments managing substantial datasets, selecting a format that balances storage efficiency with retrieval speed is crucial. Formats equipped with advanced scalability features, such as those designed for extensive data management, offer robust solutions to handle burgeoning data lakes. These options ensure data integrity while maintaining high performance as data scales.\n\n## How you read and write parquet files in DuckDB\n\nDuckDB provides extensive support for Parquet files, seamlessly integrating them into workflows that demand high-performance data analysis. Users can take advantage of DuckDB's capabilities while preserving the structural and performance benefits of Parquet.\n\n### Reading Parquet Files\n\nWith DuckDB, accessing Parquet files becomes a streamlined process, eliminating the need for data importation. This approach offers:\n\n- **Direct File Access**: DuckDB enables SQL queries directly on Parquet files, allowing immediate data exploration without additional data loading steps.\n- **Optimized Column Retrieval**: By leveraging Parquet's columnar nature, DuckDB efficiently processes column-specific queries, ensuring rapid data retrieval and minimizing unnecessary data scans.\n\nThese features facilitate efficient and effective data analysis, maximizing the performance advantages inherent in Parquet's design.\n\n### Writing Parquet Files\n\nExporting data to Parquet format in DuckDB is straightforward, ensuring that the advantages of Parquet's compression and structuring are retained:\n\n- **SQL-Driven Export**: Users can export results to Parquet with simple SQL commands, preserving the data's integrity and organization.\n- **Advanced Compression Options**: DuckDB supports the use of Parquet's compression methods during export, optimizing file size for storage efficiency.\n\nThese functionalities enable seamless data management from analysis to storage, ensuring that DuckDB users can fully leverage the benefits of both technologies in their data workflows.\n\n## How the support for Parquet files in DuckDB differs from the support for other open file formats like Iceberg and Delta Lake\n\nDuckDB excels in its support for Parquet files, offering direct integration that maximizes the potential of Parquet\u2019s columnar storage benefits. This integration underscores DuckDB\u2019s focus on performance and efficiency. In comparison, its handling of other formats like Iceberg and Delta Lake involves additional considerations due to their distinct capabilities in data management and transactional features.\n\n### Parquet Integration in DuckDB\n\nDuckDB\u2019s architecture is purpose-built to leverage the column-oriented design of Parquet. This allows for seamless data processing and minimizes overhead, enhancing query performance. The optimized handling of Parquet files means that DuckDB can efficiently execute analytical tasks by taking full advantage of Parquet\u2019s compression and encoded structures, making it an ideal choice for applications requiring swift data interrogation.\n\n### Handling Iceberg and Delta Lake\n\nWhen it comes to supporting Iceberg and Delta Lake, DuckDB must navigate the complexities introduced by these formats\u2019 advanced features. Both formats provide robust table management functionalities that extend beyond simple storage solutions and may not be fully supported.\n\n### Performance Considerations\n\nThe performance dynamics between DuckDB and these formats are influenced by the specific use cases they address. Parquet\u2019s design aligns well with DuckDB\u2019s strengths, facilitating efficient data retrieval processes. In contrast, utilizing Iceberg or Delta Lake may necessitate additional performance considerations, particularly when dealing with their advanced metadata and transaction management capabilities, which may impact processing efficiency based on workload demands.\n\nDuckDB\u2019s approach to these formats showcases its flexibility and adaptability, offering robust support for Parquet while accommodating the advanced features of Iceberg and Delta Lake for more complex data management needs.\n\nThe ongoing transformation in data storage and management highlights Parquet's role as a cornerstone of [modern data strategies](https://motherduck.com/learn-more/modern-data-warehouse-playbook/). Its architecture aligns with the demands of large-scale data operations, offering efficient solutions that meet the needs of complex analytics. This adaptability ensures Parquet remains integral across diverse computing environments, valued for its performance and reliability.\n\nEmerging table formats, including Delta Lake and Apache Iceberg, introduce enhancements that build on Parquet's capabilities. These formats offer advanced features like transactional support and time travel, catering to sophisticated data management requirements. Their ability to efficiently handle massive datasets makes them indispensable for organizations seeking comprehensive data governance solutions.\n\nDuckDB's integration of Parquet reflects its commitment to high-performance data processing, optimizing Parquet's strengths for analytical workflows. The platform navigates the complexities of newer formats like Iceberg and Delta Lake, ensuring robust support while maintaining efficient data operations. This synergy exemplifies how DuckDB leverages Parquet's strengths alongside the advanced capabilities of modern table formats to deliver effective data solutions.\n\nAs the data landscape continues to evolve, understanding the intricacies of Parquet and its alternatives is crucial for making informed decisions about your data architecture. By leveraging the strengths of Parquet and integrating it with powerful tools like DuckDB, you can unlock the full potential of your data and drive meaningful insights. If you're ready to experience the benefits of Parquet and explore a collaborative data warehousing solution, [join us at MotherDuck and get started today](https://app.motherduck.com/?auth_flow=signup).\n\n### TABLE OF CONTENTS\n\n[Key Takeaways](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#key-takeaways)\n\n[What is Parquet?](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#what-is-parquet)\n\n[Why Use Parquet?](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#why-use-parquet)\n\n[Parquet Alternatives and Comparisons](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#parquet-alternatives-and-comparisons)\n\n[Choosing the Right Format](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#choosing-the-right-format)\n\n[How you read and write parquet files in DuckDB](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#how-you-read-and-write-parquet-files-in-duckdb)\n\n[How the support for Parquet files in DuckDB differs from the support for other open file formats like Iceberg and Delta Lake](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#how-the-support-for-parquet-files-in-duckdb-differs-from-the-support-for-other-open-file-formats-like-iceberg-and-delta-lake)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\n## FAQS\n\n### What is the Parquet file format?\n\nApache Parquet is an open-source columnar storage file format designed for efficient data processing. Unlike row-based formats like CSV, it stores data by columns, enabling faster query performance and superior compression for large-scale analytical workloads.\n\n### What are the main benefits of using Parquet files?\n\nParquet's main benefits include efficient data compression and encoding, schema evolution support, and performance optimization. It uses techniques like predicate pushdown and column pruning to speed up analytical queries by only reading the data that is needed.\n\n### What are the alternatives to Parquet?\n\nCommon alternatives to Parquet include row-based formats like CSV and Avro, and other columnar formats like Apache ORC. Newer table formats like Delta Lake and Apache Iceberg often use Parquet as their underlying file storage format while adding transactional guarantees.\n\n## Additional Resources\n\n[Docs\\\\\n\\\\\nWhat is a Data Warehouse?](https://motherduck.com/learn-more/what-is-a-data-warehouse/) [Docs\\\\\n\\\\\nWhat is OLAP?](https://motherduck.com/learn-more/what-is-OLAP/) [Docs\\\\\n\\\\\nLoading Data into MotherDuck with Parquet](https://motherduck.com/docs/key-tasks/loading-data-into-motherduck/loading-data-md-python/) [Docs\\\\\n\\\\\nUse DuckDB to Convert CSV to Parquet](https://duckdbsnippets.com/snippets/6/quickly-convert-a-csv-to-parquet-bash-function) [Video\\\\\n\\\\\nQuerying Parquet Files on S3 with DuckDB](https://www.youtube.com/watch?v=fZj6kTwXN1U)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 212,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 212
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     "content": "---\ntitle: csv-files-persist-duckdb-solution\ncontent_type: blog\nsource_url: https://motherduck.com/blog/csv-files-persist-duckdb-solution\nindexed_at: '2025-11-25T19:57:13.232430'\ncontent_hash: 28f2997ebdb99804\nhas_code_examples: true\nhas_step_by_step: true\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Why CSV Files Won\u2019t Die and How DuckDB Conquers Them\n\n2025/02/04 - 9 min read\n\nBY\n\n[Mehdi Ouazza](https://motherduck.com/authors/mehdi-ouazza/)\n\nI've been working in the data field for a decade, across various companies, and one constant challenge that\u2019s almost unavoidable is dealing with CSV files.\n\nYes, there are far more efficient formats, such as [Parquet](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/), which avoid schema nightmares thanks to their typing, but CSV files persist for many reasons:\n\n- They\u2019re easy to edit and read, requiring no dependencies\u2014just open the file.\n- They\u2019re universal: many services still exchange data in CSV format.\n- Want to download data from social media or your CRM? CSV.\n- Need transaction history from your bank? CSV.\n\nHowever, this simplicity comes with its own set of challenges, especially if you want to process CSVs without breaking pipelines or pulling your hair out.\n\nFortunately, DuckDB has an exceptional CSV parser. The team behind it invested heavily in building their own, and in this post, I\u2019ll show you a real-world example where I had to parse multiple CSV files. I\u2019ll also share some SQL tricks and demonstrate how smoothly everything worked using DuckDB and MotherDuck, resulting in a ready-to-query database.\n\nThe cherry on top? The final output is a database containing all Stack Overflow survey responses from the past seven years. Stick around if you\u2019re curious about extracting insights or querying the data yourself!\n\n## The biggest challenges when reading CSVs\n\nIn my opinion, there are four significant challenges when working with CSV files:\n\n1. **Schema Management**\n2. **Row-Level Errors**\n3. **Encoding Issues**\n\nThese challenges become even more complex when handling multiple CSVs that need to be read or joined to each other.\n\nLet\u2019s see how we address these issues with Stack Overflow survey data.\n\n## About the Dataset\n\nEach year, Stack Overflow publishes the results of their developer survey, including raw data in\u2014you guessed it\u2014CSV format. These files are available on their website: [https://survey.stackoverflow.co/](https://survey.stackoverflow.co/).\n\nHere\u2019s an example of how the dataset is organized:\n\n```css\nCopy code\n\n\u251c\u2500\u2500 raw\n\u2502   \u251c\u2500\u2500 2011 Stack Overflow Survey Results.csv\n\u2502   \u251c\u2500\u2500 2012 Stack Overflow Survey Results.csv\n\u2502   \u251c\u2500\u2500 2013 Stack Overflow Survey Responses.csv\n\u2502   \u251c\u2500\u2500 2014 Stack Overflow Survey Responses.csv\n\u2502   \u251c\u2500\u2500 2015 Stack Overflow Developer Survey Responses.csv\n\u2502   \u251c\u2500\u2500 2016 Stack Overflow Survey Results\n\u2502   \u2502   \u251c\u2500\u2500 2016 Stack Overflow Survey Responses.csv\n\u2502   \u2502   \u2514\u2500\u2500 READ_ME_-_The_Public_2016_Stack_Overflow_Developer_Survey_Results.txt\n\u2502   \u251c\u2500\u2500 stack-overflow-developer-survey-2017\n\u2502   \u2502   \u251c\u2500\u2500 DeveloperSurvey2017QuestionaireCleaned.pdf\n\u2502   \u2502   \u251c\u2500\u2500 README_2017.txt\n\u2502   \u2502   \u251c\u2500\u2500 survey_results_public.csv\n\u2502   \u2502   \u2514\u2500\u2500 survey_results_schema.csv\n\u2502   \u251c\u2500\u2500 stack-overflow-developer-survey-2018\n\u2502   \u2502   \u251c\u2500\u2500 Developer_Survey_Instrument_2018.pdf\n\u2502   \u2502   \u251c\u2500\u2500 README_2018.txt\n\u2502   \u2502   \u251c\u2500\u2500 survey_results_public.csv\n\u2502   \u2502   \u2514\u2500\u2500 survey_results_schema.csv\n\u2502   \u251c\u2500\u2500 stack-overflow-developer-survey-2019\n\u2502   \u2502   \u251c\u2500\u2500 README_2019.txt\n\u2502   \u2502   \u251c\u2500\u2500 so_survey_2019.pdf\n\u2502   \u2502   \u251c\u2500\u2500 survey_results_public.csv\n\u2502   \u2502   \u2514\u2500\u2500 survey_results_schema.csv\n[..]\n```\n\nKey observations:\n\n1. **Schema Changes Over the Years**\n\n\nSome questions and their formats evolve annually, making it difficult to standardize across years.\n2. **Pre-2016 Format**\n\n\nEach column represents a question, with names like:\n\n`What Country or Region do you live in?, How old are you?, How many years of IT/Programming experience do you have?, ...`\n\nAdditional challenges include:\n\n\u2022 Column names with unusual characters.\n\n\u2022 Querying such column names can be tedious.\n\nFrom 2017 onward, Stack Overflow improved the exports by separating:\n\n\u2022 A file containing the answers (columns with clean names for each question).\n\n\u2022 A schema file (.csv) that maps question codes to full question text.\n\nTo keep things manageable, I focused on datasets from 2017 onward.\n\n## Manual cleaning over automation\n\nWe\u2019ve all wasted hours trying to automate tasks that could have been done manually in minutes. This is a common trap for data engineers. Sometimes, quick manual cleanup is the most efficient approach.\n\nHere\u2019s what I did:\n\n\u2022 Placed all CSVs in a single folder.\n\n\u2022 Renamed files by adding the corresponding year as a prefix (e.g., `<year>_<file_name>`).\n\n\u2022 Ensured column names in schema files were consistent (e.g., renamed name to qname where needed).\n\nThese steps took less than five minutes and saved me headaches later. Not everything needs to be automated!\n\n## Loading the CSVs\n\nNow for the exciting part: loading the data. DuckDB supports glob patterns for loading multiple files. For complex structures like [Hive partitions](https://duckdb.org/docs/data/partitioning/hive_partitioning.html), it works seamlessly too.\n\nHere\u2019s the core query for loading survey results:\n\n```sql\nCopy code\n\nCREATE OR REPLACE TABLE stackoverflow_survey.survey_results AS\n    SELECT\n        * EXCLUDE (filename),\n        substring(parse_filename(filename), 1, 4) as year,\n    FROM read_csv_auto(\n        'data_2017_2024/*survey_results*.csv',\n        union_by_name=true,\n        filename=true)\n```\n\n**Breakdown:**\n\n1. We `CREATE` a table based on a `SELECT` statement.\n2. We select all columns but `EXCLUDE` the filename. This is a path of the containing file; we get this one by enabling `filename=true`.\n3. We parse the `filename` to get only the year. As we have a convention on the file name to prefix by `<year>`, we take the first four chars and create a `year` column\n4. We use the glob pattern to only load `*survey_results*` as a single table (we'll do another query for the `survey_schemas`)\n\nAlright, let's run this one... \ud83d\ude4f\n\n```sql\nCopy code\n\nduckdb.duckdb.ConversionException: Conversion Error: CSV Error on Line: 35365\nOriginal Line: 35499,I am a developer by profession,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA\nError when converting column \"Hobbyist\". Could not convert string \"NA\" to 'BOOLEAN'\n\nColumn Hobbyist is being converted as type BOOLEAN\nThis type was auto-detected from the CSV file.\nPossible solutions:\n* Override the type for this column manually by setting the type explicitly, e.g. types={'Hobbyist': 'VARCHAR'}\n* Set the sample size to a larger value to enable the auto-detection to scan more values, e.g. sample_size=-1\n* Use a COPY statement to automatically derive types from an existing table.\n\n  file = ./2017_2024_schema/2020_survey_results_public.csv\n  delimiter = , (Auto-Detected)\n  quote = \" (Auto-Detected)\n  escape = \" (Auto-Detected)\n  new_line = \\n (Auto-Detected)\n  header = true (Auto-Detected)\n  skip_rows = 0 (Auto-Detected)\n  comment = \\0 (Auto-Detected)\n  date_format =  (Auto-Detected)\n  timestamp_format =  (Auto-Detected)\n  null_padding = 0\n  sample_size = 20480\n  ignore_errors = false\n  all_varchar = 0\n```\n\nBad news, it didn't successfully parse the CSVs. But the GREAT news is that we have an excellent log error!\n\nWe know :\n\n- On which line we have an issue\n- A proper error message `Could not convert string \"NA\" to 'BOOLEAN'`\n- Possibles solutions\n\nThis saves so much time! Sometimes, just one row can mess up the whole process, and if the error message isn\u2019t clear, you\u2019re stuck guessing what went wrong. You might even end up throwing out your CSV or trying random fixes over and over.\n\nFor us, increasing the sample\\_size fixed the problem right away. \ud83d\udc4d\n\n## Wrapping up and automate the rest\n\nWith the initial query successful, the next steps were to:\n\n1. Repeat the process for schema files.\n2. Add row count checks to ensure no data was lost during merging of the CSVs\n\nHere's a generic function to wrap the query we saw and run them depending on the pattern name of the files (either for `results` or `schemas`).\n\n```python\nCopy code\n\nCSV_DIR = './data_2017_2024'\n\n# Global configuration\nFILE_CONFIGS = [\\\n    {'pattern': 'schema', 'table': 'survey_schemas'},\\\n    {'pattern': 'public', 'table': 'survey_results'}\\\n]\n\ndef process_survey_files(csv_dir: str) -> None:\n    \"\"\"\n    Process Stack Overflow survey CSV files and load them into DuckDB tables\n    \"\"\"\n    con = duckdb.connect('stackoverflow_survey.db')\n\n    for config in FILE_CONFIGS:\n        logging.info(f\"Processing {config['pattern']} files...\")\n        con.execute(f\"\"\"\n            CREATE OR REPLACE TABLE stackoverflow_survey.{config['table']} AS\n            SELECT\n                * EXCLUDE (filename),\n                substring(parse_filename(filename), 1, 4) as year,\n            FROM read_csv_auto(\n                '{csv_dir}/*{config['pattern']}*.csv',\n                union_by_name=true,\n                filename=true,\n                sample_size=-1\n            )\n        \"\"\")\n\n        # Log row count\n        count = con.execute(f\"SELECT COUNT(*) FROM stackoverflow_survey.{config['table']}\").fetchone()[0]\n        logging.info(f\"Loaded {count} rows into {config['table']}\")\n\n        # Log unique years\n        years = con.execute(f\"SELECT DISTINCT year FROM stackoverflow_survey.{config['table']} ORDER BY year\").fetchall()\n        logging.info(f\"{config['table']} years: {[year[0] for year in years]}\")\n\n    con.close()\n```\n\nFinally, we added another function to check row count and make sure we didn't lose any rows during the process :\n\n```python\nCopy code\n\ndef verify_row_counts(csv_dir: str) -> None:\n    \"\"\"\n    Verify that the sum of individual file counts matches the merged table counts\n    \"\"\"\n    con = duckdb.connect('stackoverflow_survey.db')\n\n    for config in FILE_CONFIGS:\n        pattern = config['pattern']\n        table = config['table']\n\n        logging.info(f\"\\nVerifying {pattern} files counts...\")\n        individual_counts = 0\n\n        for filename in os.listdir(csv_dir):\n            if pattern in filename and filename.endswith('.csv'):\n                file_path = os.path.join(csv_dir, filename)\n                count = con.execute(f\"SELECT COUNT(*) FROM read_csv_auto('{file_path}')\").fetchone()[0]\n                logging.info(f\"{filename}: {count} rows\")\n                individual_counts += count\n\n        merged_count = con.execute(f\"SELECT COUNT(*) FROM stackoverflow_survey.{table}\").fetchone()[0]\n        logging.info(f\"Individual {pattern} files total: {individual_counts}\")\n        logging.info(f\"Merged {table} total: {merged_count}\")\n\n        assert individual_counts  merged_count, f\"{pattern} row count mismatch: {individual_counts} != {merged_count}\"\n\n    con.close()\n    logging.info(\"\u2705 All row counts verified successfully!\")\n```\n\n## Sharing the dataset\n\nNow that I have a DuckDB database containing both tables (results and schemas), the only thing left is to share it! Let's see how that works with MotherDuck.\n\nI\u2019m using the DuckDB CLI, but this could also be part of a Python script. It\u2019s just four simple commands:\n\n```sql\nCopy code\n\nduckdb\nD ATTACH 'stackoverflow_survey.db'\nD ATTACH 'md:'\nD CREATE DATABASE cloud_stackoverflow_survey FROM stackoverflow_survey;\nD CREATE SHARE FROM cloud_stackoverflow_survey;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                            share_url                            \u2502\n\u2502                             varchar                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 md:_share/sample_data/23b0d623-1361-421d-ae77-125701d471e6      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n1. We attach the local DuckDB database with `ATTACH` command.\n2. We connect to MotherDuck using `ATTACH 'md';`. Note that I have my [`motherduck_token`](https://motherduck.com/docs/key-tasks/authenticating-and-connecting-to-motherduck/authenticating-to-motherduck/#authentication-using-an-access-token) stored in an `ENV`.\n3. We upload the database to MotherDuck using the `CREATE DATABASE x FROM x`\n4. We create a public share so that anyone can start querying!\n\nTo make it even easier for MotherDuck users, I put this one in the existing demo database [`sample_data`](https://motherduck.com/docs/getting-started/sample-data-queries/datasets/), which is attached by default for any users.\n\n## Querying the dataset\n\nThis dataset offers plenty of opportunities to uncover insights, but I\u2019ll wrap up this blog with a simple query that wasn\u2019t included in the original StackOverflow study.\n\nI wanted to explore the average happiness score of people based on their work location (remote, in-person, or hybrid).\n\n```sql\nCopy code\n\nSELECT RemoteWork,\n       AVG(CAST(JobSat AS DOUBLE)) AS AvgJobSatisfaction,\n       COUNT(*) AS RespondentCount\nFROM sample_data.stackoverflow_survey.survey_results\nWHERE JobSat NOT IN ('NA')\n  AND RemoteWork NOT IN ('NA')\n  AND YEAR='2024'\nGROUP BY ALL;\n```\n\nand the results :\n\n```sql\nCopy code\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              RemoteWork              \u2502 AvgJobSatisfaction \u2502 RespondentCount \u2502\n\u2502               varchar                \u2502       double       \u2502      int64      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 In-person                            \u2502  6.628152818991098 \u2502            5392 \u2502\n\u2502 Remote                               \u2502  7.072592992884806 \u2502           11103 \u2502\n\u2502 Hybrid (some remote, some in-person) \u2502  6.944303596894311 \u2502           12622 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nTwo interesting takeaways: remote and ybrid workers make up the majority of survey responses, and on average, they seem to be happier too!\n\nCheck out [our documentation](https://motherduck.com/docs/getting-started/sample-data-queries/stackoverflow-survey/) if you want to explore this dataset further.\n\nIn the meantime, get ready to tackle future CSV challenges with ease\u2014DuckDB and MotherDuck (start for [free!](https://motherduck.com/get-started/)) have got you covered!\n\n* * *\n\n### Why DuckDB\u2019s CSV Parser is Special\n\n- [https://duckdb.org/2023/10/27/csv-sniffer.html](https://duckdb.org/2023/10/27/csv-sniffer.html)\n- [https://duckdb.org/2024/12/05/csv-files-dethroning-parquet-or-not.html](https://duckdb.org/2024/12/05/csv-files-dethroning-parquet-or-not.html)\n- [Why CSVs Still Matter: The Indispensable File Format](https://youtu.be/I07qV2hij4E?si=DjCapBT3eg5UWLdn)\n\n### TABLE OF CONTENTS\n\n[The biggest challenges when reading CSVs](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#the-biggest-challenges-when-reading-csvs)\n\n[About the Dataset](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#about-the-dataset)\n\n[Manual cleaning over automation](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#manual-cleaning-over-automation)\n\n[Loading the CSVs](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#loading-the-csvs)\n\n[Wrapping up and automate the rest](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#wrapping-up-and-automate-the-rest)\n\n[Sharing the dataset](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#sharing-the-dataset)\n\n[Querying the dataset](https://motherduck.com/blog/csv-files-persist-duckdb-solution/#querying-the-dataset)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![DuckDB Ecosystem: January 2025](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fnewsletter_a65cff5430.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025/)\n\n[2025/01/10 - Simon Sp\u00e4ti](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025/)\n\n### [DuckDB Ecosystem: January 2025](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025)\n\nDuckDB Monthly #25: PyIceberg, 0$ data distribution and more!\n\n[![Local dev and cloud prod for faster dbt development](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FLocal_Dev_Cloud_Prod_083b07b92e.png&w=3840&q=75)](https://motherduck.com/blog/dual-execution-dbt/)\n\n[2025/01/16 - Jacob Matson](https://motherduck.com/blog/dual-execution-dbt/)\n\n### [Local dev and cloud prod for faster dbt development](https://motherduck.com/blog/dual-execution-dbt)\n\nSpark the Joy of beautiful local development workflows with MotherDuck & dbt\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 418,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 418
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-OLAP.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/product-analytics-motherduck-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/web-assembly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-use-cases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/pandas-dataframes-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-struct-nested-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/ducklake-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-application.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-lake-vs-data-warehouse-vs-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-warehouse-tco.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 29
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me read a few more relevant files about data loading efficiency:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     "content": "---\ntitle: 'No-ETL: Query Raw CSV & JSON Files Directly with SQL'\ncontent_type: guide\ndescription: Skip complex ETL. Learn the No-ETL method for startups to query multiple\n  raw CSV, JSON, & Parquet files directly with SQL. Get insights in minutes, not months.\npublished_date: '2025-10-17T00:00:00'\nsource_url: https://motherduck.com/learn-more/no-etl-query-raw-files\nindexed_at: '2025-11-25T20:37:09.260086'\ncontent_hash: 317312307e45ea1c\nhas_step_by_step: true\nhas_narrative: true\n---\n\n# The No-ETL Playbook: How to Query Raw CSV & JSON Files Directly with SQL\n\nFor startups and small companies, your most valuable data, like customer sign-ups, sales transactions, and product usage logs, is likely scattered across hundreds of local files like CSVs, Excel spreadsheets, and JSON logs. The conventional wisdom says you need to build a complex and expensive **Extract, Transform, Load (ETL)** pipeline before you can even begin to analyze it. But for a lean team that needs to move fast, this approach is a trap. It\u2019s slow, rigid, and drains precious engineering resources before you\u2019ve asked your first question.\n\nWhat if you could skip the pipeline and go straight to the insights?\n\nA modern, **\"No-ETL\"** approach allows you to do just that. Instead of spending months building a complex data infrastructure, you can use simple SQL to query your raw data files directly where they live, on your laptop or in cloud storage. This guide will show you how this lean, serverless method turns fragmented files into a powerful, queryable database in minutes, not months. You will learn why traditional ETL is a bottleneck, how to get immediate answers from your raw files, and how to consolidate thousands of fragmented files into a single, high-performance database, all while unifying data across your local machine and the cloud.\n\n## Why Are Traditional Data Pipelines a Trap for Startups?\n\nFor decades, the path from data to decision was a one-way street paved with ETL. This process involves extracting data from various sources, transforming it into a rigid, predefined schema, and loading it into a central data warehouse. While this model can work for large enterprises with stable processes, it creates a \"pipeline city\" that demands constant maintenance and becomes a major bottleneck for agile startups that need to pivot quickly.\n\nThe \"No-ETL\" philosophy flips this script entirely. Instead of moving all your data to a central processing location, you bring the processing power directly to your data. By pointing SQL at the files you already have, you can eliminate 80-90% of the traditional pipeline steps. This approach, powered by MotherDuck's serverless engine, allows teams to [consolidate data from dozens of sources in days, not months](https://motherduck.com/learn-more/modern-data-warehouse-playbook/).\n\nThis shift dramatically accelerates your time-to-insight and slashes operational costs. Processing a 100MB CSV file on your laptop is instantaneous and costs nothing, whereas using a traditional cloud warehouse incurs charges for compute time and network data transfer. Companies that adopt this lean model report [ 70-90% cost reductions](https://motherduck.com/learn-more/modern-data-warehouse-playbook/) compared to traditional cloud data warehouses.\n\n### How Did Five Startups Slash Data Prep Time from Hours to Minutes?\n\nThe benefits of abandoning premature ETL aren't just theoretical. Startups across various industries have replaced slow, brittle pipelines and overloaded databases with MotherDuck\u2019s query-in-place model, leading to dramatic improvements in speed, cost, and agility. These changes often lead to huge improvements, not just small percentage gains.\n\n| Company & Industry | Previous State & Pain Point | Transformation with MotherDuck | Quantified Outcome |\n|---|---|---|---|\nFinqore (FinTech) | 8-hour data pipelines using Postgres for complex financial data and a heavy reliance on manual Excel processes. | Replaced Postgres to process and unify data directly, enabling a real-time metrics explorer and AI agents. |\n8 hours to 8 minutes |\n\n**Gardyn**(IoT / AgTech)[Pipeline time cut from](https://motherduck.com/case-studies/gardyn/)at 10x lower cost than other data warehouses.**over 24 hours to under 1 hour****UDisc**(Sports Tech)[dbt job time reduced from](https://motherduck.com/case-studies/udisc-motherduck-sports-management/), and typical queries dropped from minutes to**6 hours to 30 minutes****5 seconds**.**Dexibit**(Analytics)[Analytical query times reduced from](https://motherduck.com/case-studies/dexibit/), eliminating traditional data warehouse costs.**minutes to a few seconds****Layers**(SaaS)[dashboards loading in](https://motherduck.com/case-studies/layers-multi-tenant-data-warehouse/).**110 ms**## How Can You Get Instant Answers from Excel & CSV Files?\n\nFor many teams, valuable data is trapped in local spreadsheets. The traditional path to analyzing this data, which includes manual imports, database setup, and schema definition, is slow and frustrating. MotherDuck eliminates this friction by allowing you to query CSV and Excel files directly with SQL, just as if they were database tables.\n\nThis is powered by DuckDB's [ read_csv_auto](https://duckdb.org/docs/stable/data/csv/auto_detection.html) and\n\n[functions, which automatically infer column names, data types, and file dialects (like delimiters) by sampling the file. This](https://duckdb.org/docs/stable/guides/file_formats/excel_import.html)\n\n`read_xlsx`\n\n**schema-on-read**capability means a non-technical user can go from a local file to a powerful SQL query in seconds, without writing any\n\n`CREATE TABLE`\n\nstatements or managing a database server.For a local CSV file, a query is as simple as this:\n\nCopy code\n\n```\nSELECT\nproduct_category,\nSUM(sale_amount) AS total_sales\nFROM 'transactions.csv'\nGROUP BY ALL\nORDER BY total_sales DESC;\n```\n\n\nMotherDuck [automatically detects the columns and their types](https://duckdb.org/docs/stable/data/csv/overview.html), allowing you to filter, aggregate, and sort on the fly. This direct-query capability extends to files stored in cloud object storage and [even Google Sheets](https://motherduck.com/docs/key-tasks/data-warehousing/Replication/spreadsheets/), providing a unified way to access spreadsheet data wherever it lives.\n\n### What About Messy Spreadsheets? How to Handle Common Pitfalls in SQL\n\n\"Wild\" CSV and Excel files often have inconsistencies. Instead of spending time on manual cleanup, you can handle these common issues directly in your SQL query using optional parameters.\n\n| Pitfall | Problem Description | SQL Solution with MotherDuck |\n|---|---|---|\nIncorrect Delimiter | A CSV file uses a pipe (`|` ) or semicolon (`;` ) instead of a comma, causing columns to be misread. | Use the\n`delim` parameter |\n\n`read_csv('data.csv', delim = '|')`\n\n.**Inconsistent Date Formats**`MM/DD/YYYY`\n\nor `DD-Mon-YY`\n\n.[to provide the exact format string:](https://duckdb.org/docs/stable/data/csv/overview.html)`dateformat`\n\nparameter`read_csv('data.csv', dateformat = '%m/%d/%Y')`\n\n.**Header and Footer Rows**[to select only the data cells:](https://duckdb.org/docs/stable/guides/file_formats/excel_import.html)`range`\n\nparameter`range = 'A5:Z100'`\n\n.**Mixed Data Types**[, then use](https://duckdb.org/docs/stable/guides/file_formats/excel_import.html)`all_varchar = true`\n\n`TRY_CAST()`\n\nin your `SELECT`\n\nstatement to safely convert types.## How Do I Turn Thousands of Fragmented Files into a Single Database?\n\nA common challenge for growing companies is data fragmentation, where analytics data is spread across thousands of individual files in cloud storage like Amazon S3. MotherDuck sidesteps complex ingestion jobs by [treating an entire folder of files as a single, queryable database table](https://duckdb.org/docs/stable/data/multiple_files/overview.html).\n\nBy using SQL with glob patterns, you can instantly query a whole collection of Parquet, CSV, or JSON files directly in S3. For example, the following query will scan all Parquet files for the year 2025, no matter how many subdirectories they are in, and treat them as one large table:\n\nCopy code\n\n```\nSELECT\nevent_type,\nCOUNT(*) AS event_count\nFROM read_parquet('s3://my-bucket/logs/2025/**/*.parquet')\nGROUP BY event_type;\n```\n\n\nThis is a high-performance feature, not just a convenience. MotherDuck\u2019s query engine [pushes down filters and projections to the file level](https://duckdb.org/docs/stable/data/parquet/overview.html), minimizing the amount of data read from cloud storage and reducing costs. For even greater efficiency, organizing files using [ Hive-style partitioning](https://duckdb.org/docs/stable/guides/performance/how_to_tune_workloads.html) (e.g.,\n\n`/year=2025/month=10/`\n\n) allows the engine to skip entire folders that don\u2019t match a query\u2019s `WHERE`\n\nclause, dramatically reducing scan time and cost.## How Can I Join Local Files with Data in the Cloud?\n\nStartups rarely have their data in one neat location. You might have recent sales data in a CSV on your laptop, historical logs in an S3 bucket, and user profiles in a managed database. MotherDuck\u2019s [ Dual Execution feature](https://motherduck.com/docs/concepts/architecture-and-capabilities/) unifies these fragmented datasets into a single analytical layer without requiring you to move the data first.\n\nWhen your local DuckDB client is connected to MotherDuck, they form a distributed system where the query optimizer [intelligently routes parts of your query to where the data lives](https://motherduck.com/videos/339/bringing-duckdb-to-the-cloud-dual-execution-explained/). A query on a local file runs entirely on your machine's resources. A query on a large S3 file runs in the MotherDuck cloud to use its scale. Most powerfully, a join between the two is automatically optimized to minimize data transfer.\n\nAn analyst can prototype a query by joining a local spreadsheet with a massive cloud table. MotherDuck is smart enough to automatically push filters down to the local file, send only the small, filtered result to the cloud, and perform the final join there. This [avoids the slow and costly process of uploading entire local files](https://motherduck.com/learn-more/fix-slow-bi-dashboards/) just to perform a join. The entire process is smooth, and you simply write standard SQL.\n\n## How Can You Explore Data Before Building a Formal Pipeline?\n\nThe schema-on-read approach is powerful because it lets you [explore and understand your data before committing](https://motherduck.com/duckdb-book-summary-chapter5/) to a rigid transformation pipeline. This \"explore-then-model\" workflow de-risks data projects by allowing you to assess data quality, discover hidden patterns, and validate business assumptions upfront. With MotherDuck, you can use a suite of simple SQL commands to profile your raw CSV, JSON, and Parquet files directly.\n\n### How Can You See the Structure of Your JSON and Parquet Files Automatically?\n\nMotherDuck provides powerful functions to look inside the structure of your semi-structured and columnar files without manually parsing them.\n\nFor **Parquet files**, you can [query the file\u2019s internal metadata directly](https://duckdb.org/docs/stable/data/parquet/metadata.html) to see column names, types, and nullability. This is very helpful for understanding the data you've received from a partner or another system.\n\nCopy code\n\n```\nSELECT * FROM parquet_schema('your_file.parquet');\n```\n\n\nFor **JSON files**, the [ read_json_auto function automatically infers a schema](https://duckdb.org/docs/stable/data/json/loading_json.html), representing nested objects as\n\n`STRUCT`\n\ns and arrays as `LIST`\n\ns. You can see this inferred schema by creating a temporary table and describing it:Copy code\n\n```\nCREATE TABLE temp_json AS SELECT * FROM read_json_auto('api_response.json');\nDESCRIBE temp_json;\n```\n\n\n### How Can You Use SQL to Explore JSON Data Without Knowing Its Structure?\n\nJSON's nested, schema-less nature makes it notoriously difficult to analyze with traditional SQL. MotherDuck lets you [explore and query deeply nested JSON files immediately](https://motherduck.com/blog/analyze-json-data-using-sql/), even with zero prior knowledge of their structure.\n\nYou can instantly navigate the nested structure using simple **dot notation** (e.g., `SELECT user.name.first FROM 'users.json'`\n\n) and flatten complex arrays into rows using the ** UNNEST** function. This turns what was once a painful data preparation task into a simple, interactive exploration process.\n\nCopy code\n\n```\n-- Explore nested JSON and flatten an array of items into separate rows\nSELECT\norder_id,\ncustomer.id AS customer_id,\nitem.product_id::INTEGER,\nitem.quantity::INTEGER\nFROM read_json_auto('orders.json'), UNNEST(line_items) AS t(item);\n```\n\n\n## Is the \"No-ETL\" Approach a Permanent Solution?\n\nFor a lean team, building a full-blown ETL pipeline too early is a strategic error. It locks you into a rigid structure before you fully understand your data's value or how your business questions will evolve. The \"No-ETL\" approach is a better starting point for most startups.\n\nHowever, \"No-ETL\" does not mean \"Never-ETL.\" As your company matures and your data processes become more standardized, certain triggers justify introducing a more formalized, lightweight EL(T) process, where raw data is loaded into cloud storage and then transformed within the warehouse. You should consider this change when you need things like **repeatability for audits**, such as for financial reporting or compliance. It also becomes valuable when you need to improve **performance on complex joins** for frequently-run dashboards, or when business operations depend on [ strict data freshness SLAs](https://motherduck.com/learn-more/modern-data-warehouse-use-cases/) of minutes, not hours. Finally, as your team grows, a formalized model is essential for\n\n[, allowing you to systematically clean data or mask PII before exposing it to a wider audience.](https://motherduck.com/blog/motherduck-kestra-etl-pipelines/)\n\n**data governance at scale**This **progressive modeling pattern**, which involves starting with raw files, creating semantic views, and only materializing tables when necessary, allows your data architecture to evolve with your business, not against it.\n\n## Your Path Forward: From Files to Insights\n\nThe message for startups is clear: stop building pipelines and start asking questions. The modern, No-ETL approach used by MotherDuck removes the friction between your data and your decisions. By letting your entire team query raw files directly with the SQL they already know, you unlock a level of speed and agility that traditional data stacks simply cannot match. Start by exploring your local files, scale to the cloud, and let your data architecture grow with your business needs. The power to be data-driven is no longer locked behind complex engineering projects. It's right there in your files, waiting for a query.\n\nStart using MotherDuck now!\n\n## FAQS\n\n### Do I really need an ETL pipeline if I\u2019m just getting started?\n\nFor most startups, no. A \"No-ETL\" approach lets you query raw CSV, JSON, and Excel files directly with SQL. This is faster and cheaper, allowing you to get insights immediately without building complex infrastructure.\n\n### How can I analyze data in Excel and CSV files without hassle?\n\nYou can use SQL to query local Excel and CSV files as if they were database tables. Modern tools like DuckDB automatically detect columns and data types, so you can go from a spreadsheet to analysis in seconds without any setup.\n\n### How do I query thousands of files in a folder as a single database?\n\nUse SQL with glob patterns (e.g., `FROM 's3://bucket/logs/**/*.parquet'`\n\n). This treats an entire folder of files in cloud storage as one large, queryable table, eliminating the need for complex data ingestion jobs.\n\n### Can I explore my data before building an ETL pipeline?\n\nYes, this is a key benefit of the No-ETL approach. By querying raw files directly, you can profile data quality, discover patterns, and validate assumptions with SQL before committing to a rigid transformation model.\n\n### How can I see the structure of my JSON and Parquet files automatically?\n\nUse simple SQL commands. For Parquet, `parquet_schema('file.parquet')`\n\nreveals the schema. For JSON, `read_json_auto()`\n\ninfers the structure, which you can view with a `DESCRIBE`\n\ncommand on the query result.\n\n### Why is schema design so difficult for startups?\n\nStartups evolve rapidly, causing data sources and business needs to change constantly. A rigid, upfront schema (schema-on-write) becomes a bottleneck. A flexible, schema-on-read approach is better as it applies structure at query time.",
  [2m  â”‚[0m     "numLines": 229,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 229
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md",
  [2m  â”‚[0m     "content": "---\ntitle: taming-wild-csvs-with-duckdb-data-engineering\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering\nindexed_at: '2025-11-25T19:57:41.542448'\ncontent_hash: 6594cfafde3c18de\nhas_code_examples: true\nhas_step_by_step: true\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Taming Wild CSVs: Advanced DuckDB Techniques for Data Engineers\n\n2025/05/17 - 12 min read\n\nBY\n\n[Ryan Boyd](https://motherduck.com/authors/ryan-boyd/)\n\nIt's 2:30 AM. The office is empty. Your coffee's gone cold, and you're staring blankly at your screen as it mockingly displays: `Error: Could not convert string 'N/A' to INTEGER` on line 56,789. All you wanted was to import a \"simple\" CSV export from that upstream system. Sound familiar?\n\nWe've all been in CSV purgatory. That moment when what should be a five-minute task turns into a multi-hour ordeal because somewhere, buried deep in that innocent-looking file, lurks an inconsistent delimiter, a rogue quote, or my personal favorite\u2014columns that mysteriously appear and disappear like fish fry diving underwater to avoid being eaten by our duck friends.\n\nI've spent countless hours wrestling with problematic CSVs, but after discovering some of DuckDB's lesser-known features, those late-night CSV battles have become far less common. While DuckDB's automatic CSV reader is already impressively smart, knowing a few advanced techniques can save you from writing custom preprocessing scripts when things get messy.\n\nIn this guide, I'll share the DuckDB techniques that have repeatedly saved me from CSV hell:\n\n- How to diagnose what DuckDB actually thinks your [**CSV looks like**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#peeking-under-the-hood-sniffcsv)\n- Deep dive into the [**CSV sniffer**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#how-the-sniffer-works) and how it works under the hood\n- Ways to [**handle problematic rows**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#wrangling-the-errors) without aborting your entire import\n- Strategies for [**dealing with inconsistent schemas**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#handling-shifting-schemas-unionbyname) across files\n- [**Fine-tuning options**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#fine-tuning-detection-and-overrides) when auto-detection needs a helping hand\n- Real-world robustness: how DuckDB performs on a [**benchmark of messy CSVs**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#duckdbs-csv-parser-champion-of-the-pollock-benchmark)\n\nLet's dive in and see if we can make your next CSV import session a little less...quackers.\n\n## **Peeking Under the Hood: sniff\\_csv**\n\nBefore attempting to load the data using DuckDB\u2019s auto-detection capabilities, it's incredibly useful to understand what DuckDB _thinks_ it's dealing with. Is it guessing the delimiter correctly? Did it detect the header? What types is it inferring? The [sniff\\_csv()](https://duckdb.org/docs/stable/data/csv/auto_detection.html) function is your reconnaissance tool here.\n\nInstead of blindly running `read_csv` and potentially hitting errors, run `sniff_csv` first. It analyzes a sample of the file and reports back the detected dialect, types, header presence, and more.\n\n**Let\u2019s imagine a file [events.csv](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/events.csv):**\n\n```txt\nCopy code\n\nEventDate|Organizer|City|Venue\n2022-03-01|OpenTech|San Francisco, CA|Moscone Center, Hall A\n2022-03-02|OpenTech|San Francisco, CA|Moscone Center, Hall B\n2022-03-03|OpenTech|San Francisco, CA|Moscone Center, Hall C\n```\n\n**Let's see what DuckDB makes of this:**\n\n```sql\nCopy code\n\nFROM sniff_csv('events.csv');\n```\n\nINFO: Using full http URLs\nFor brevity, we'll just include the filename in each of the example commands below. You can download the example files using the links (eg, how events.csv is linked above). However, you can also specify the full http URL instead because DuckDB supports the http protocol for retrieving CSV files.\n\n```sql\nCopy code\n\nFROM sniff_csv('http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/events.csv');\n```\n\n**You can also control how much of the file it samples:**\n\n```sql\nCopy code\n\nFROM sniff_csv('events.csv', sample_size=10000); -- Sample 10k rows\n```\n\n**Or sample the whole file (careful with huge files!):**\n\n```sql\nCopy code\n\nFROM sniff_csv('events.csv', sample_size=-1);\n```\n\n**The output provides a wealth of information in a single row table:**\n\n- `Delimiter`, `Quote`, `Escape`, `NewLineDelimiter`: The detected structural characters.\n- `SkipRows`: Number of rows it thinks should be skipped at the start.\n- `HasHeader`: Boolean flag if a header is detected.\n- `Columns`: A list of structs showing detected column names and types (e.g., `{'name': 'VARCHAR', 'age': 'BIGINT'}`).\n- `DateFormat`, `TimestampFormat`: Any special date/time formats it detected.\n- `Prompt`: This is extremely useful! It gives you a `read_csv` command _with_ all the detected options explicitly set. You can copy, paste, and modify this as needed.\n\nRunning `sniff_csv` first can save you significant guesswork when an import fails. If the detected `Delimiter` is wrong, or it thinks `HasHeader` is true when it isn't, you know exactly which options to override in your `read_csv` call.\n\n### **How the Sniffer Works**\n\nDuckDB's CSV sniffer actually operates through multiple phases to determine the best way to read your file:\n\n1. **Dialect Detection**: At the time of writing, sniffer tests 24 different combinations of dialect configurations (delimiters, quotes, escapes, newlines) to see which one creates the most consistent number of columns across rows.\n\n2. **Type Detection**: After determining the dialect, the sniffer analyzes the first chunk of data (2048 rows by default) to detect column types, trying to cast values from most to least specific types (SQLNULL \u2192 BOOLEAN \u2192 BIGINT \u2192 DOUBLE \u2192 TIME \u2192 DATE \u2192 TIMESTAMP \u2192 VARCHAR).\n\n3. **Header Detection**: The sniffer checks if the first valid line can be cast to the detected column types. If it can't, that line is considered a header.\n\n4. **Type Replacement**: If you specified column types, they override the sniffer's detected types.\n\n5. **Type Refinement**: The sniffer validates the detected types on more data using vectorized casting.\n\n\n**Here's a code example showing how to inspect what the sniffer sees in a more complex scenario:**\n\n```sql\nCopy code\n\n-- Examine what the sniffer detects with a larger sample size\nSELECT\n    Delimiter, Quote, Escape, SkipRows, HasHeader, DateFormat, TimestampFormat\nFROM sniff_csv('events.csv',  sample_size=50000);\n\n-- To see the detected column types\nSELECT unnest(Columns)\nFROM sniff_csv('events.csv');\n```\n\nWhen I was working with a dataset containing 20+ columns of mixed types, the `unnest(Columns)` trick was particularly helpful to see exactly which columns were being detected as which types, saving a ton of back-and-forth troubleshooting.\n\n## **Wrangling the Errors: ignore\\_errors, store\\_rejects, and More**\n\nSo `sniff_csv` looks good, but your file _still_ has issues\u2014maybe just a few problematic rows scattered throughout millions of good ones. By default, DuckDB will halt the import on the first error. But often, you just want the valid data and to deal with the bad rows separately.\n\n### **Option 1: Just Skip 'Em (`ignore_errors`)**\n\nThe simplest approach is to tell DuckDB to skip rows that cause parsing or casting errors using `ignore_errors = true`.\n\nLet's imagine a file [`faulty_folks.csv`](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/faulty_folks.csv):\n\n```txt\nCopy code\n\nName,Age\nAlice,30\nBob,forty-two\nCharlie,35\n```\n\nTrying to read this normally with explicit types will fail on Bob's age:\n\n```sql\nCopy code\n\n-- This will error out!\nSELECT * FROM read_csv('faulty_folks.csv', header=true, columns={'Name':'VARCHAR', 'Age':'INTEGER'});\n```\n\nBut if we just want Alice and Charlie:\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('faulty_folks.csv',\n    header = true,\n    -- Specify expected types\n    columns = {'Name': 'VARCHAR', 'Age': 'INTEGER'},\n    ignore_errors = true  -- The key part!\n    );\n```\n\n**Explanation:**\n\n- We define the `columns` we expect, including the `INTEGER` type for `Age`.\n- `ignore_errors = true` tells the reader: if you hit a row where 'Age' can't become an `INTEGER` (like \"forty-two\"), just drop that row and keep going.\n\n**Output:**\n\n| Name | Age |\n| --- | --- |\n| Alice | 30 |\n| Charlie | 35 |\n\nBob gets left behind, but the import succeeds with the valid rows. This approach skips rows with various issues: casting errors, wrong number of columns, unescaped quotes, etc.\n\n### **Option 2: Skip and Store (store\\_rejects)**\n\nIgnoring errors is okay, but generally, you need to know _what_ went wrong and _which_ rows were rejected. Maybe you need to fix the source data or report the issues. This is where `store_rejects = true` becomes invaluable.\n\nWhen you use [`store_rejects`](https://duckdb.org/docs/stable/data/csv/overview.html), DuckDB still skips the bad rows (like `ignore_errors`), but it also logs detailed information about each rejected row and the error encountered into two temporary tables: `reject_scans` and `reject_errors`.\n\n```sql\nCopy code\n\n-- Read the file, storing rejected rows\nSELECT * FROM read_csv(\n    'faulty_folks.csv',\n    header = true,\n    columns = {'Name': 'VARCHAR', 'Age': 'INTEGER'},\n    store_rejects = true -- Store info about errors\n    -- Optional: Customize table names and limit\n    -- rejects_scan = 'my_scan_info',\n    -- rejects_table = 'my_rejected_rows',\n    -- rejects_limit = 100 -- Store max 100 errors per file\n);\n\n-- Now, let's see what was rejected\nFROM reject_errors;\n-- And details about the scan itself\nFROM reject_scans;\n```\n\n**Explanation:**\n\n1. The `read_csv` call runs, skips Bob's row, and returns Alice and Charlie just like before.\n2. The key difference: `store_rejects = true` populates the temporary tables.\n3. `FROM reject_errors;` shows details about the failed rows:\n\n   - `scan_id`, `file_id`: Link back to the specific scan/file.\n   - `line`: The original line number in the CSV.\n   - `column_idx`, `column_name`: Which column had the issue (if applicable).\n   - `error_type`: The category of error (e.g., `CAST`, `TOO_MANY_COLUMNS`).\n   - `csv_line`: The actual content of the rejected line.\n   - `error_message`: The specific error message DuckDB generated.\n4. `FROM reject_scans;` gives metadata about the `read_csv` operation itself (delimiter, quote rule, schema used, file path, etc.).\n\nI've found this incredibly useful for debugging dirty data. You get the clean data loaded _and_ a detailed report on the rejects, all within DuckDB. No more grep-ing through massive files trying to find that one problematic line!\n\n### **Option 3: Relaxing the Rules (strict\\_mode=false and null\\_padding=true)**\n\nSometimes, you just want to _get the data in_, even if it\u2019s a little messy. That\u2019s where DuckDB's more forgiving CSV parsing options can help you out. **`strict_mode = false`** option tells DuckDB to loosen up its parsing expectations. It will _try_ to read rows even if they contain typical formatting problems like:\n\n- Unescaped quote characters in fields (e.g., `\"15\" Laptop\"`).\n- Rows with _more_ columns than defined (DuckDB just drops the extras).\n- Mixed newline formats (like mixing `\\n` and `\\r\\n`).\n\nWARNING: fast, but not precise\nWhen you set `strict_mode=false`, you\u2019re trusting DuckDB to make its best guess. That works great when you want results fast\u2014but double-check the output if data precision matters!\n\nAnother commonly used option is **`null_padding = true,`** which handles rows that come up _short_, meaning they have fewer columns than expected. Instead of throwing an error, DuckDB just fills in the blanks with `NULL`.\n\nLet\u2019s look at an example. Here's a messy CSV file named [`inventory.csv`](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/inventory.csv):\n\n```txt\nCopy code\n\nItemID,Description,Price\n101,\"15\" Laptop\",999.99\n102,\"Wireless Mouse\"\n103,\"Mechanical Keyboard\",129.99,ExtraField\n```\n\nThis file includes:\n\n- An unescaped quote in the first row\u2019s description\n- A missing price in the second row\n- An extra column in the third row\n\nTry reading it normally:\n\n```sql\nCopy code\n\nFROM read_csv('inventory.csv');\n```\n\nDuckDB will skip all lines except the last.\n\n![DuckDB CLI image](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimage1_7ef2565c8e.png&w=3840&q=75)\n\nBut with relaxed settings:\n\n```sql\nCopy code\n\n-- Parsing a messy CSV while gracefully handling missing and extra fields\nFROM read_csv('inventory.csv',\n    strict_mode = false,   -- Forgive formatting quirks\n    null_padding = true   -- Fill missing columns with NULLs\n);\n```\n\n**Resulting Table:**\n\n| ItemID | Description | Price | column3 |\n| --- | --- | --- | --- |\n| 101 | 15 Laptop | 999.99 | NULL |\n| 102 | Wireless Mouse | NULL | NULL |\n| 103 | Mechanical Keyboard | 129.99 | ExtraField |\n\n## **Handling Shifting Schemas: union\\_by\\_name**\n\nHere's another common headache: you have multiple CSV files to load (e.g., monthly reports), but the columns aren't _quite_ the same across files. Maybe a column was added in March, or the order changed in June. Trying to read them together with a simple `read_csv('monthly_report_*.csv')` might fail or produce misaligned data.\n\nDuckDB's `union_by_name = true` option handles this elegantly. When reading multiple files (using globs or lists), it aligns columns based on their _header names_ rather than their position. If a file is missing a column found in other files, it fills it with `NULL`.\n\nImagine [`report_jan.csv`](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/report_jan.csv):\n\n```txt\nCopy code\n\nUserID,MetricA,MetricB\n1,10,100\n2,15,110\n```\n\nAnd [`report_feb.csv`](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/report_feb.csv):\n\n```txt\nCopy code\n\nUserID,MetricB,MetricC,MetricA\n3,120,xyz,20\n4,125,abc,25\n```\n\nNotice the different order and the new `MetricC` in February.\n\n```sql\nCopy code\n\nSELECT *\nFROM read_csv(\n    ['report_jan.csv', 'report_feb.csv'], -- List of files (or glob)\n    union_by_name = true -- The magic!\n);\n```\n\n**Explanation:**\n\n- DuckDB reads the headers from all files involved.\n- It creates a combined schema containing _all_ unique column names (`UserID`, `MetricA`, `MetricB`, `MetricC`).\n- For each file, it matches the data to the combined schema based on the header names found _in that specific file_.\n- Where a column doesn't exist in a file (like `MetricC` in `report_jan.csv`), it inserts `NULL`.\n\n**Output:**\n\n| UserID | MetricA | MetricB | MetricC |\n| --- | --- | --- | --- |\n| 1 | 10 | 100 | NULL |\n| 2 | 15 | 110 | NULL |\n| 3 | 20 | 120 | xyz |\n| 4 | 25 | 125 | Abc |\n\n## **Fine-Tuning Detection and Overrides**\n\nWhile auto-detection is great, sometimes you know better, or the sample DuckDB takes isn't quite representative. Here are some ways to fine-tune the process:\n\n**Bigger Sample:** If type detection seems off (e.g., a column that's mostly integers but has a few floats later gets detected as `BIGINT`), try increasing the sample size:\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('file.csv', sample_size = 50000);\n\n-- Or scan the whole file (can be slow for huge files)\nSELECT * FROM read_csv('file.csv', sample_size = -1);\n```\n\n**Manual Types:** Override specific column types if detection gets it wrong or if you want a different type:\n\n```sql\nCopy code\n\n-- Override by name\nSELECT * FROM read_csv('file.csv',\n  types = {'UserID': 'VARCHAR', 'TransactionAmount': 'DOUBLE'});\n\n-- Or by position if no headers\nSELECT * FROM read_csv('file.csv', header = false,\n  types = ['VARCHAR', 'DOUBLE', 'DATE']);\n```\n\n**Force Header/No Header:** If header detection fails (common if all columns look like strings):\n\n```sql\nCopy code\n\n-- Force header presence\nSELECT * FROM read_csv('file.csv', header = true);\n\n-- Or no header with custom names\nSELECT * FROM read_csv('file.csv',\n header = false,\n names = ['colA', 'colB', 'colC']);\n```\n\n**Date/Timestamp Formats:** If dates aren't ISO 8601 (`YYYY-MM-DD`) or times aren't standard:\n\n```sql\nCopy code\n\n SELECT * FROM read_csv('file.csv',\n  dateformat = '%m/%d/%Y',\n  timestampformat = '%Y-%m-%dT%H:%M:%S.%f');\n```\n\n**Everything is a String:** If you want to load _everything_ as `VARCHAR` and deal with types later:\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('file.csv', all_varchar = true);\n```\n\n**Which Columns Can Be NULL?:** By default, an empty field is treated as `NULL`. If empty strings should be valid values:\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('file.csv',\n  force_not_null = ['column_name1', 'column_name2']);\n```\n\n**Clean Up Names:** Got headers with spaces or weird characters?\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('file.csv', normalize_names = true);\n```\n\nThis will [automatically clean them up](https://duckdb.org/docs/stable/data/csv/overview.html#parameters) (replacing non-alphanumeric with `_`, etc.) during import.\n\n## DuckDB's CSV Parser: Champion of the Pollock Benchmark\n\nFor those really interested in CSV robustness, there's an intriguing benchmark called [Pollock](https://hpi.de/naumann/projects/data-preparation/pollock.html) that evaluates how well different systems handle non-standard CSV files. The creators studied over 245,000 public CSV datasets to identify common violations of the RFC-4180 standard, then created test files with these issues.\n\nIn recent testing, DuckDB [ranked #1](https://github.com/HPI-Information-Systems/Pollock) in the benchmark when configured to handle problematic files, correctly reading 99.61% of the data across all test files. Even in auto-detect mode with minimal configuration, DuckDB still managed to read about 90.75% of the data correctly.\n\nThis is practical validation that the approaches we've covered in this article can handle the vast majority of real-world CSV issues you'll encounter.\n\n| System under test | Pollock score<br>(simple) | Pollock score<br>(weighted) |\n| --- | --- | --- |\n| DuckDB 1.2 | 9.961 | 9.599 |\n| SQLite 3.39.0 | 9.955 | 9.375 |\n| UniVocity 2.9.1 | 9.939 | 7.936 |\n| LibreOffice Calc 7.3.6 | 9.925 | 7.833 |\n| SpreadDesktop | 9.929 | 9.597 |\n| SpreadWeb | 9.721 | 9.431 |\n| Python native csv 3.10.5 | 9.721 | 9.436 |\n| Pandas 1.4.3 | 9.895 | 9.431 |\n| MySQL 8.0.31 | 9.587 | 7.484 |\n| Mariadb 10.9.3 | 9.585 | 7.483 |\n\n## **Taking Flight Beyond the Basics**\n\nWe've covered quite a bit in our journey through DuckDB's CSV capabilities\u2014from diagnosing issues with `sniff_csv` to handling errors with `ignore_errors` and `store_rejects`, merging inconsistent schemas with `union_by_name`, and fine-tuning the whole process with various overrides.\n\nWhat I've come to appreciate about DuckDB is that its CSV reader isn't just a basic loader\u2014it's a sophisticated tool designed to handle real-world data messiness directly within SQL. Most data tools can handle the perfect CSV file, but it's how they deal with the imperfect ones that really matters in day-to-day work.\n\nBy understanding these slightly more advanced options, you can often avoid external preprocessing steps, keeping your data loading logic right within your SQL workflow. The result is cleaner pipelines that are less likely to waddle when faced with unexpected CSV quirks.\n\nThe next time a tricky CSV lands on your desk, remember these techniques. They might just save you some time and frustration, letting you get back to the more interesting parts of data analysis sooner. Happy querying!\n\n### TABLE OF CONTENTS\n\n[Peeking Under the Hood: sniff\\_csv](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#peeking-under-the-hood-sniffcsv)\n\n[Wrangling the Errors: ignore\\_errors, store\\_rejects, and More](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#wrangling-the-errors-ignoreerrors-storerejects-and-more)\n\n[Handling Shifting Schemas: union\\_by\\_name](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#handling-shifting-schemas-unionbyname)\n\n[Fine-Tuning Detection and Overrides](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#fine-tuning-detection-and-overrides)\n\n[DuckDB's CSV Parser: Champion of the Pollock Benchmark](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#duckdbs-csv-parser-champion-of-the-pollock-benchmark)\n\n[Taking Flight Beyond the Basics](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#taking-flight-beyond-the-basics)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![MotherDuck lands on Tableau Cloud: Live, Fast Analytics Unleashed](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FTableau_Cloud_52bd53b821.png&w=3840&q=75)](https://motherduck.com/blog/tableau-cloud-motherduck/)\n\n[2025/05/06 - Jacob Matson](https://motherduck.com/blog/tableau-cloud-motherduck/)\n\n### [MotherDuck lands on Tableau Cloud: Live, Fast Analytics Unleashed](https://motherduck.com/blog/tableau-cloud-motherduck)\n\nUse MotherDuck to power your Tableau Cloud, Server, and Desktop dashboards.\n\n[![DuckDB Ecosystem: May 2025](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fduckdbeco_may_ca294a4d7f.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025/)\n\n[2025/05/08 - Simon Sp\u00e4ti](https://motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025/)\n\n### [DuckDB Ecosystem: May 2025](https://motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025)\n\nDuckDB Monthly #29: From Metabase to Doom, DuckDB powers everything.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 513,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 513
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md",
  [2m  â”‚[0m     "content": "---\ntitle: A Data Engineer's Guide to Columnar Storage\ncontent_type: guide\ndescription: A comprehensive guide to columnar databases for data engineers. Learn\n  about columnar vs. row-oriented architecture, vectorized execution, Parquet, and\n  implementation best practices.\npublished_date: '2025-10-17T00:00:00'\nsource_url: https://motherduck.com/learn-more/columnar-storage-guide\nindexed_at: '2025-11-25T10:52:22.420365'\ncontent_hash: 5dc1777d24bcb793\n---\n\nData keeps growing, and so does the challenge of analyzing it efficiently. While traditional row-based databases have served us well for transactional workloads, they often struggle when you need to crunch through massive datasets for analytics. This is where columnar storage comes in\u2014a different approach to organizing data that can dramatically improve query performance for analytical workloads.\n\nThis guide walks through everything you need to know about columnar storage: how it works, why it's faster for analytics, and how modern engines like DuckDB implement these concepts. Whether you're considering adopting a columnar database or just curious about the technology, you'll get a practical understanding of this foundational piece of the modern data stack.\n\n## What You'll Learn\n\nBy the end of this guide, you'll understand the fundamental differences between row-oriented and column-oriented data storage, the core principles that make columnar databases fast, and how modern engines implement techniques like vectorized execution and advanced compression. We'll also explore the role of standard columnar file formats like [Apache Parquet](https://parquet.apache.org/), examine how architectural patterns are evolving to handle collaboration and scale, and review common use cases and limitations of columnar systems.\n\n## Core Architectural Differences: Columnar vs. Row-Oriented Layouts\n\nThe fundamental difference between these two storage models comes down to data layout on disk. A row-oriented database stores all the values belonging to a single record together sequentially. In contrast, a column-oriented database groups all the values from a single column together.\n\nThis distinction might seem trivial, but it has profound implications for query performance, especially in analytical workloads. Consider a typical analytical query that calculates the average sale amount over time. In this scenario, you only need two columns from what might be a table with dozens of fields. A columnar system can read just those two columns, while a row-based system must read every single column for each row in the query range, even though most of that data is irrelevant.\n\n| Feature | Row-Oriented Storage | Column-Oriented Storage |\n|---|---|---|\nData Layout | Values for a single row are stored contiguously. | Values for a single column are stored contiguously. |\nPrimary Use Case | OLTP (Online Transaction Processing): frequent single-row reads, writes, and updates. |\n|\n\n**I/O Pattern****Compression****Query Performance**`SELECT *`\n\n).**Examples**## What Gives Columnar Databases Their Performance Edge?\n\nThe columnar layout enables several key advantages that make these systems particularly well-suited for analytical workloads.\n\n### Superior Compression\n\nWhen values of the same data type are stored together, they exhibit lower information entropy and can be compressed much more effectively than the mixed data types found in a row. It's common for columnar formats to achieve high compression ratios, which not only reduces storage costs but also speeds up queries by minimizing the amount of data that needs to be read from disk.\n\n### Reduced I/O for Analytical Queries\n\nThis is the primary performance benefit. By reading only the columns necessary for a query, columnar engines minimize disk I/O, which is often the main bottleneck in data processing. If your query needs 2 columns out of 100, a columnar system might only need to read 2% of the total data. Understanding this is the first step in solving what is effectively [a physics problem with a predictable hierarchy of performance bottlenecks](https://motherduck.com/learn-more/diagnose-fix-slow-queries/).\n\n### Efficient Aggregations\n\nWhen data is grouped by column, operations like `SUM()`\n\n, `AVG()`\n\n, or `COUNT()`\n\ncan be performed by tightly looping over a contiguous block of memory. This is far more efficient than fetching values scattered across different parts of rows.\n\n### Natural Fit for Modern CPUs\n\nThe columnar layout is highly compatible with modern CPU architecture. Data can be loaded into CPU registers in vectors\u2014batches of values\u2014and processed in parallel using [SIMD (Single Instruction, Multiple Data)](https://www.cs.columbia.edu/~kar/pubsk/simd.pdf) instructions. This leads to significant computational speedups by reducing instruction dispatch overhead and improving cache locality.\n\n## How Do High-Performance Engines Maximize Speed?\n\nModern columnar databases achieve their speed through a combination of sophisticated techniques. [DuckDB](https://motherduck.com/learn-more/what-is-duckdb/), an open-source embedded analytical database, serves as an excellent reference implementation of these principles.\n\n### Vectorized Query Execution\n\nInstead of processing data one row at a time (the \"Volcano\" or \"iterator\" model), [vectorized engines](https://hive.apache.org/docs/latest/vectorized-query-execution_34838326/) process data in batches (vectors) of thousands of values. This approach amortizes the overhead of function calls for query interpretation and fully leverages the parallel processing capabilities of modern CPUs by reducing cache misses. The entire query plan\u2014from scanning to filtering and aggregation\u2014operates on these vectors, keeping CPU caches full and pipelines efficient.\n\n### Smart Query Optimization\n\nHigh-performance engines intelligently minimize the amount of data they need to process. **Predicate pushdown** allows the engine to apply `WHERE`\n\nclause filters at the storage level, before data is even read into memory. This often works alongside **zone maps** (also called min/max indexing), where metadata for each data block is stored. If a query filters for `age > 40`\n\n, the engine can check the zone maps and skip reading any block where the maximum age is less than 40. DuckDB creates and uses these automatically to accelerate queries.\n\n### Out-of-Core Processing\n\nA key feature of robust columnar engines is their ability to handle datasets larger than available RAM. When a memory-intensive operation like a large sort or join exceeds the memory limit, the engine can offload intermediate data to temporary files on disk and process it in chunks. This allows engines like [DuckDB](https://duckdb.org/docs/stable/guides/performance/how_to_tune_workloads.html) to analyze datasets of 100GB or more on a laptop with only 16GB of RAM.\n\n## What Are the Key Columnar File Formats?\n\nIn the modern data stack, open columnar file formats have become essential for interoperability, allowing different tools and engines to work with the same data.\n\n### Apache Parquet: The Standard\n\nParquet has become the de facto standard columnar file format for data lakes. It's an open-source format highly optimized for analytical workloads, influenced by Google's Dremel paper. A Parquet file is structured into row groups, which contain column chunks. Each column chunk stores the data for a specific column within that row group and is further divided into pages. Crucially, Parquet files also store rich metadata and statistics\u2014min/max values, null counts\u2014for each column chunk, enabling the predicate pushdown and data skipping optimizations we discussed earlier.\n\n### Internal Storage Formats\n\nWhile engines like DuckDB have excellent readers for Parquet, they often use their own highly optimized internal columnar format for data stored within the database itself. Having control over their own format allows databases to implement specific compression schemes or indexing structures that are tightly integrated with their execution engine. DuckDB's internal format, for example, is designed to enable fast random access and efficient, ACID-compliant updates, which can be more challenging with file-based formats like Parquet that are typically immutable and require rewriting the entire file to change.\n\n| Feature | Apache Parquet | DuckDB Internal Format |\n|---|---|---|\nPrimary Design Goal | Write-once, read-many for data lake analytics. | High-performance, transactional (ACID) analytics with efficient updates. |\nStructure | File -> Row Groups -> Column Chunks -> Pages. | Row Groups -> Fixed-size blocks per column for random access. |\nMutability | Immutable; changes require rewriting the file. | Mutable; supports efficient INSERT, UPDATE, DELETE operations. |\nUse Case | Standard for data interchange and long-term storage in data lakes. | Optimized for active, high-performance analytical query processing within DuckDB. |\n\n## Which Compression Methods Provide Maximum Efficiency?\n\nColumnar databases use various specialized encoding techniques, often chosen automatically based on the data's characteristics, to maximize compression without sacrificing performance.\n\n| Compression Technique | How It Works | Best Suited For |\n|---|---|---|\nDictionary Encoding | Replaces frequently repeated values with smaller integer codes and stores a dictionary mapping. | Low to medium cardinality columns (e.g., country codes, product categories). |\nRun-Length Encoding (RLE) | Compresses sequences of identical values by storing the value once and a count of its repetitions. | Sorted columns or columns with long runs of identical values. |\nDelta Encoding | Stores the difference between consecutive values instead of the actual values. | Columns with slowly changing series data, like timestamps or sequential IDs. |\nFrame of Reference (FOR) | Subtracts a minimum value from a block of numbers and stores the smaller offsets. | Columns of integers with a limited range within blocks. |\nFSST (Fast Static Symbol Table) | Tokenizes strings and builds a static dictionary of common substrings to be represented by shorter codes. | High-cardinality string data like URLs or names where dictionary encoding is ineffective. |\n\n### Specialized String Compression\n\nFor string data that doesn't fit well with dictionary encoding\u2014like high-cardinality data such as URLs or names\u2014engines like DuckDB use specialized algorithms. FSST (Fast Static Symbol Table) tokenizes strings and builds a dictionary of common substrings, providing [good compression](https://www.vldb.org/pvldb/vol13/p2649-boncz.pdf) with very fast decompression speeds, often comparable to or better than LZ4.\n\n## How Have Data Platform Architectures Adapted to Columnar Storage?\n\nColumnar systems have evolved beyond monolithic data warehouses into flexible architectures that serve different needs. This evolution reflects a [broader industry shift](https://motherduck.com/blog/big-data-is-dead/), challenging the notion that all data problems are 'big data' problems.\n\n### The In-Process Model\n\nAn in-process database runs directly within an application, eliminating the need for a separate server and network communication. This model provides significant power and simplicity for local data processing.\n\nDuckDB exemplifies this approach as an embedded columnar database. It provides the power of a full analytical SQL engine in a lightweight library that can be integrated directly into Python, R, Java, or C++ applications. Its strengths include incredible speed for interactive analysis in notebooks, local ETL pipelines, and powering data-intensive applications. However, the purely embedded nature presents challenges for team-based work, as it lacks built-in mechanisms for shared state, concurrent access, and centralized governance.\n\n### The Hybrid Model\n\nThis emerging architecture combines the \"local-first\" performance of an embedded engine with the collaboration and scalability of a managed cloud backend. It aims to provide the best of both worlds.\n\n[MotherDuck](https://motherduck.com/product/) implements this hybrid model by extending DuckDB to the cloud. Users can continue working with the familiar DuckDB engine on their local machines but seamlessly query and manage shared data stored centrally in MotherDuck's serverless platform. The query optimizer in this model can decide whether to execute parts of a query locally or remotely, transferring data as needed. This architecture addresses the collaboration challenges of a purely embedded approach by providing a shared source of truth, access controls, and reliable storage, while still leveraging local compute for performance and responsiveness.\n\n## What Are the Most Suitable Applications for Columnar Systems?\n\nThe right columnar architecture depends on the specific problem you're trying to solve.\n\n### Embedded Columnar Databases\n\nDuckDB and similar embedded engines excel in several scenarios. They're perfect for interactive data analysis in notebooks, where data scientists need to quickly explore and analyze gigabytes of data on their laptop. They also work well for local ETL and data transformation pipelines, providing an efficient engine for cleaning, transforming, and enriching data before loading it into another system. Additionally, they can be embedded to power analytical features like dashboards and reports directly within applications.\n\n### Hybrid Columnar Platforms\n\nSolutions like MotherDuck address different needs. They're ideal for collaborative analytics among small-to-medium-sized teams when groups need to work on the same datasets without the complexity of a full-scale data warehouse. They enable centralized data management with decentralized, local-first computation, allowing organizations to govern key datasets centrally while empowering analysts to work with them locally. They're also well-suited for building internal data tools that require shared state and access control.\n\n### A Case Study in Hybrid Efficiency: Trunkrs\n\nA compelling example of the hybrid model's success is the logistics company [Trunkrs](https://motherduck.com/case-studies/trunkrs-same-day-delivery-motherduck-from-redshift/). Previously hampered by a slow and costly Redshift setup, their daily operational meetings struggled with sluggish query performance, preventing deep analysis. After migrating to MotherDuck, they experienced immediately faster, \"snappier\" responses. This allowed their teams to drill down into performance issues in real-time, solving problems more effectively and reducing repeated mistakes. The move not only lowered their data platform's complexity and cost but also made their data feel more like a responsive application, matching the efficiency of their logistics operations.\n\n### Cloud Data Warehouses\n\nTraditional cloud data warehouses like Snowflake and BigQuery remain the best choice for enterprise-scale analytics on massive, petabyte-scale datasets that require distributed, massively parallel processing architecture. They're also essential for serving concurrent BI and reporting needs for hundreds or thousands of users.\n\n| Tool | Architecture | Ideal Use Case | Key Differentiator |\n|---|---|---|---|\nSQLite | Row-Oriented, In-Process | General-purpose embedded database for applications. | Optimized for transactional integrity (OLTP) and low-latency writes. |\nDuckDB | Columnar, In-Process | High-performance interactive analytics and local ETL on a single machine. | \"\n|\n\n**MotherDuck****Snowflake / BigQuery**## What Are Some Effective Strategies for Columnar Database Implementation?\n\nTo maximize the benefits of a columnar system, it's crucial to move beyond a simple lift-and-shift approach and adopt practices that leverage its unique architecture.\n\n**Optimize Physical Data Layout**\n\nThe physical organization of data on disk is paramount.\n\n**Sorting and Clustering:**The single most effective optimization is sorting or clustering your data by a commonly filtered column, like a timestamp. This groups related data together, which dramatically improves the efficiency of compression (e.g., Run-Length Encoding) and enables highly effective data skipping via zone maps.**Partitioning:**Apply partitioning (e.g., by date range) to break large tables into smaller, more manageable pieces. Modern systems like Snowflake use micro-partitions, which are small, immutable chunks of data (e.g., 50-500MB) that contain per-column metadata. This allows the query optimizer to prune (ignore) the vast majority of partitions that are not relevant to a query, drastically reducing scan times.\n\n**Manage Data Ingestion and Updates Efficiently**\n\nColumnar stores are optimized for bulk operations, not single-row writes.\n\n**Batch Data Loads:**Always load data in large batches (thousands or millions of rows at a time). This allows the system to write directly into optimized, compressed columnar segments. Trickle-feeding data one row at a time leads to poor compression and fragmented storage, which harms query performance.**Use Delta Stores for Updates:**To handle updates without sacrificing read performance, many systems use a \"delta store\" or \"write-optimized store.\" New writes and updates go into this separate, row-oriented or memory-optimized store. Periodically, an efficient, often multi-core-aware, merge process combines the delta store into the main read-optimized column store.\n\n**Tune Query Execution and Schema Design**\n\n**Leverage Late Materialization:**Advanced columnar engines use a technique called[late materialization](https://15721.courses.cs.cmu.edu/spring2024/papers/04-execution1/shrinivas-icde2013.pdf). Instead of reconstructing full rows early in the query plan, the engine operates on column vectors for as long as possible, only materializing (stitching together) the final rows needed for the result set. This minimizes data movement and memory overhead.**Denormalize Strategically:**While normalization is essential for OLTP, analytical queries often perform better on wider, denormalized tables that eliminate the need for costly joins at query time. This is a trade-off that increases storage but can dramatically improve the performance of read-heavy workloads.**Use Materialized Views:**For complex, repetitive queries that power dashboards, pre-compute the results into a materialized view. This allows the database to serve results instantly by reading from the pre-calculated table rather than re-running the entire query.\n\n**Embrace Adaptive and Learned Optimizations**\n\n**Analyze Query Patterns:**Understanding which columns are used in filters, joins, and aggregations is key. This knowledge informs decisions about sort keys, partitioning, and denormalization.**Utilize Adaptive Techniques:**Modern systems are moving towards adaptive and AI-driven optimizations. This includes adaptive indexing, where indexes are refined on-the-fly based on query patterns, and learned encoding advisors that can predict the optimal compression scheme for each column, further reducing latency and storage.\n\n## What Are the Inherent Constraints and Compromises?\n\nDespite their analytical prowess, columnar databases aren't a universal solution and come with important trade-offs.\n\n### Transactional Workload Challenges\n\nColumnar systems are poorly suited for workloads with frequent, single-row inserts, updates, or deletes. Modifying a single logical row requires writing to multiple, separate column files, which is highly inefficient compared to row-based systems.\n\n### Write Amplification\n\nTo maintain performance, columnar systems often write data in large, immutable blocks. Updating data typically involves rewriting entire blocks, a phenomenon known as write amplification. This can also create challenges for handling high-concurrency writes.\n\n### SELECT * Query Performance\n\nQueries that retrieve all columns of a table can be slower on a columnar database than on a row-oriented one. This is because the engine has to perform the costly operation of reconstructing the row from various column files.\n\n### Small Dataset Overhead\n\nFor tables with only a few thousand rows, the overhead of columnar processing and metadata can sometimes make them slower than a simple row-oriented database like SQLite. Columnar storage really shines at scale.\n\nWARNING: Not a Silver Bullet While columnar databases offer significant advantages for analytical workloads, they're specialized tools. Don't expect them to replace your transactional databases or perform well on small datasets with heavy write workloads.## Is SQL the Standard for Columnar Databases?\n\nOne of the biggest advantages of modern columnar databases is that they almost universally use standard SQL as their query language. From an analyst's perspective, there's no difference between writing a query for a columnar database like DuckDB and a row-based database like PostgreSQL. They support familiar `SELECT`\n\n, `JOIN`\n\n, `GROUP BY`\n\n, and window functions.\n\nThis standardization means data teams can adopt powerful columnar technology without retraining analysts or abandoning existing SQL-based tools. While some systems may offer specialized functions as extensions, the core language remains the same.\n\n## The Path Forward\n\nColumnar storage has become a critical foundational technology for modern data analytics. As [noted](https://iaeme.com/MasterAdmin/Journal_uploads/IJITMIS/VOLUME_16_ISSUE_1/IJITMIS_16_01_038.pdf) in the *International Journal of Information Technology & Management Information System*:\n\n\"The adoption of columnar storage formats has revolutionized data processing capabilities in modern big data ecosystems, fundamentally transforming how organizations analyze and derive value from their data assets... This transformation extends beyond mere performance improvements, encompassing enhanced data compression, improved query optimization, and better resource utilization in distributed computing environments\" (Tatikonda, Pruthvi. International Journal of Information Technology & Management Information System, 2025).\n\n\nThe concept, first comprehensively introduced in a 1985 paper by GP Copeland and SN Khoshafian, has been refined over decades by systems like MonetDB and C-Store. By optimizing for how analytical queries actually access data, it provides substantial performance improvements over traditional row-based systems. The evolution of columnar architecture\u2014from massive cloud data warehouses to powerful embedded engines like DuckDB and innovative hybrid platforms like MotherDuck\u2014shows a clear trend toward making high-performance analytics more accessible, flexible, and scalable.\n\nUnderstanding the principles, advantages, and trade-offs of this architecture is essential for any data engineer looking to build efficient data platforms. Whether you're processing data locally, collaborating with a team, or scaling to enterprise volumes, there's likely a columnar solution that fits your needs. The key is matching the right architectural pattern to your specific use case and requirements.\n\n## Start Using MotherDuck to See Columnar Storage in Action\n\nMotherDuck offers a free trial for 21 days based on the open source DuckDB database engine, optimized with columnar storage for analytics. [Get Started Now](https://app.motherduck.com/)\n\n## Frequently Asked Questions\n\n### What is driving the widespread adoption of columnar databases for analytical tasks?\n\nThey are significantly faster and more cost-effective for analytical workloads. By only reading the columns needed for a query and using superior compression, they minimize I/O and reduce storage costs, allowing for interactive analysis on very large datasets.\n\n### In what ways does columnar storage boost performance and lower expenses?\n\nIt improves performance by dramatically reducing the amount of data read from disk for typical analytical queries. It reduces costs through high compression ratios that shrink the storage footprint and by lowering data scan costs in cloud environments.\n\n### When are columnar databases not the right choice?\n\nTheir primary limitation is poor performance on transactional (OLTP) workloads that involve frequent single-row inserts, updates, or deletes. They're specialized tools for analytics and aren't meant to replace general-purpose transactional databases.\n\n### What are some guidelines for implementing a columnar database effectively?\n\nThe most critical practices are sorting your data on ingestion based on common query filters, loading data in batches rather than row-by-row, and being explicit about selecting only the columns you need in your queries.\n\n### In which scenarios is an embedded database like DuckDB a strong option?\n\nDuckDB excels for interactive data analysis on a local machine (like in a Python notebook), for building efficient local ETL pipelines, and for embedding analytical capabilities directly into applications without needing an external server.\n\n### What kind of challenges does a hybrid architecture like MotherDuck overcome?\n\nA hybrid architecture addresses the collaboration and scaling challenges of purely embedded tools. It allows teams to work with centralized, managed datasets while still benefiting from the speed and simplicity of local processing, effectively bridging the gap between individual laptop analysis and shared cloud environments.\n\nStart using MotherDuck now!",
  [2m  â”‚[0m     "numLines": 255,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 255
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me search for any additional content about JSON files and loading performance:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/analyze-json-data-using-sql.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_o_o7fbto/.kurt/sources/motherduck.com/blog/analyze-json-data-using-sql.md",
  [2m  â”‚[0m     "content": "---\ntitle: analyze-json-data-using-sql\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/analyze-json-data-using-sql\nindexed_at: '2025-11-25T19:56:20.007742'\ncontent_hash: f6b3e80e1a28155a\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Analyze JSON Data Using SQL and DuckDB\n\n2024/01/10 - 12 min read\n\nBY\nDavid Neal\n\nYou have a deadline flying in fast and a boatload of data to dive through. You extract the data archive to discover all the files have a \u201c.json\u201d extension. Oh, no. JSON is for programmers, right? What do you do now?\n\nNo need to get your feathers in a fluff! DuckDB to the rescue!\n\nDuckDB is a featherlight yet powerful database that supports querying lots of data formats directly using SQL. It can query data locally on disk, in memory, in the cloud, or combine data from multiple sources in a single query!\n\nIn this post, we'll guide you through querying JSON data using DuckDB. You'll learn how to target the data you need with the precision of a duck snatching its favorite bread. Let\u2019s get quacking on how to query JSON data with DuckDB!\n\n## Prerequisites\n\n- Install DuckDB (instructions below).\n- Optional: View or download the [sample JSON data](https://github.com/reverentgeek/duckdb-json-tutorial) used in this tutorial.\n\n## What is JSON?\n\nJSON, which stands for JavaScript Object Notation, is a lightweight data format. It is designed to be fairly easy for humans to read and write, and easy for machines to parse and generate, making it a great way to share data. It was originally created for Web applications to share data between the browser and server and has become a standard for storing and sharing data in many other types of applications. Outside of the browser, JSON is typically stored in a text file with a `.json` extension.\n\nLet\u2019s waddle through some of the basics of JSON! JSON is built on two basic structures:\n\n- A collection of one or more name/value pairs surrounded by curly braces {}, each pair separated by commas.\n- A list of one or more values surrounded by brackets \\[\\], each value separated by commas.\n\nHere\u2019s an example:\n\n```json\nCopy code\n\n{\n  \"ducks\": [\\\n    {\\\n      \"name\": \"Quackmire\",\\\n      \"color\": \"green\",\\\n      \"actions\": [\\\n        \"swimming\",\\\n        \"waddling\",\\\n        \"quacking\"\\\n      ]\\\n    },\\\n    {\\\n      \"name\": \"Feather Locklear\",\\\n      \"color\": \"yellow\",\\\n      \"actions\": [\\\n        \"sunbathing\"\\\n      ]\\\n    },\\\n    {\\\n      \"name\": \"Duck Norris\",\\\n      \"color\": \"brown\",\\\n      \"actions\": [\\\n        \"karate chopping bread\"\\\n      ]\\\n    }\\\n  ],\n  \"totalDucks\": 3\n}\n```\n\n- All of the data is wrapped in curly braces {}, like a cozy nest.\n- Each duck is part of a \"ducks\" array (like a flock of ducks in a row), wrapped by square brackets \\[\\].\n- Each duck in the array is a set of \"name/value\" pairs. For example, \"name\": \"Duck Norris\" tells us one duck's name is Duck Norris.\n\nCurly braces {} are used to represent an object. You might also think of an object as a record, thing, or entity. The name/value pairs are sometimes called properties. The value associated with the name can represent text (a string), a number, true/false (a boolean), a collection of values (an array), or a nested object. An array is represented by square brackets \\[\\] and can be an ordered list of strings, numbers, booleans, or objects.\n\nThe JSON format can represent data structures ranging from simple to complex with nested objects and arrays! This makes it a great way to express and exchange data.\n\n## Install and execute DuckDB\n\nIf you don\u2019t already have DuckDB installed, flap on over to [duckdb.org](https://duckdb.org/#quickinstall) and follow the instructions for your operating system. In this tutorial, you\u2019ll be using DuckDB from the command line.\n\n- _Mac:_ Follow the Homebrew (`brew`) install instructions.\n- _Windows:_ Follow the `winget` install instructions.\n- _Linux:_ Download the appropriate archive for your OS and processor. Extract the `duckdb` executable binary from the archive to a folder where you easily execute it from your terminal.\n\n### Launch DuckDB from the command line\n\nAfter installing DuckDB, open (or reopen) your terminal or command prompt and enter the following to start an in-memory session of DuckDB.\n\n```sh\nCopy code\n\nduckdb\n```\n\n_Note: If you are running Linux, you\u2019ll want to change the current directory to where you extracted the binary and use `./duckdb` to execute the binary_\n\nIf all goes to plan, you should see a new `D` prompt ready for a command or SQL query, similar to the following.\n\n```sh\nCopy code\n\nEnter \".help\" for usage hints.\nConnected to a transient in-memory database.\nUse \".open FILENAME\" to reopen on a persistent database.\nD\n```\n\n### Run your first DuckDB SQL query\n\nFrom the `D` prompt on the command line, type in the following SQL query and press enter. Don\u2019t forget to include the semicolon at the end! SQL queries can span multiple lines, and the semicolon lets DuckDB know you are finished writing the query and it\u2019s ready to execute.\n\n```sql\nCopy code\n\nSELECT current_date - 7;\n```\n\nThe result returned should be the date from seven days ago. `current_date` is one of many SQL functions available, and can be useful for including in query results or filtering data.\n\n## Query JSON files with DuckDB\n\nThis ability to query raw files directly is the foundation of a modern [No-ETL approach](https://motherduck.com/learn-more/no-etl-query-raw-files/), which helps startups and lean teams avoid costly data engineering. In many cases, you can query data directly from a JSON file by specifying a path to the file.\n\n- Create a new text file named `ducks.json` and open it in a text editor.\n- Paste the following JSON data into the file and save it.\n\n```js\nCopy code\n\n[\\\n  {\\\n    \"id\": \"kA0KgL\",\\\n    \"color\": \"red\",\\\n    \"firstName\": \"Marty\",\\\n    \"lastName\": \"McFly\",\\\n    \"gender\": \"male\"\\\n  },\\\n  {\\\n    \"id\": \"dx3ngL\",\\\n    \"color\": \"teal\",\\\n    \"firstName\": \"Duckota\",\\\n    \"lastName\": \"Fanning\",\\\n    \"gender\": \"female\"\\\n  },\\\n  {\\\n    \"id\": \"FQ4dU1\",\\\n    \"color\": \"yellow\",\\\n    \"firstName\": \"Duck\",\\\n    \"lastName\": \"Norris\",\\\n    \"gender\": \"male\"\\\n  },\\\n  {\\\n    \"id\": \"JqS7ZZ\",\\\n    \"color\": \"red\",\\\n    \"firstName\": \"James\",\\\n    \"lastName\": \"Pond\",\\\n    \"gender\": \"male\"\\\n  },\\\n  {\\\n    \"id\": \"ZM5uJL\",\\\n    \"color\": \"black\",\\\n    \"firstName\": \"Darth\",\\\n    \"lastName\": \"Wader\",\\\n    \"gender\": \"male\"\\\n  }\\\n]\n```\n\nWith DuckDB running at the command line, paste the following query and press ENTER.\n\n```sql\nCopy code\n\nSELECT * FROM './ducks.json';\n```\n\nThe results should look similar to the following.\n\n```sh\nCopy code\n\nD SELECT * FROM './ducks.json';\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   id    \u2502  color  \u2502 firstName \u2502 lastName \u2502 gender  \u2502\n\u2502 varchar \u2502 varchar \u2502  varchar  \u2502 varchar  \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 kA0KgL  \u2502 red     \u2502 Marty     \u2502 McFly    \u2502 male    \u2502\n\u2502 dx3ngL  \u2502 teal    \u2502 Duckota   \u2502 Fanning  \u2502 female  \u2502\n\u2502 FQ4dU1  \u2502 yellow  \u2502 Duck      \u2502 Norris   \u2502 male    \u2502\n\u2502 JqS7ZZ  \u2502 red     \u2502 James     \u2502 Pond     \u2502 male    \u2502\n\u2502 ZM5uJL  \u2502 yellow  \u2502 Darth     \u2502 Wader    \u2502 male    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 5 rows                                   5 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Change DuckDB\u2019s output display\n\nIf you\u2019re not 100% satisfied with DuckDB\u2019s output to the console, there are lots of choices to customize the output. Type the following command to list the available output modes.\n\n```sh\nCopy code\n\n.help .mode\n```\n\nTry switching the output mode to column display and rerun the last query to see the difference.\n\n```sh\nCopy code\n\n.mode column\n```\n\n```sql\nCopy code\n\nD SELECT * FROM './ducks.json';\nid      color   firstName  lastName  gender\n------  ------  ---------  --------  ------\nkA0KgL  red     Marty      McFly     male\ndx3ngL  teal    Duckota    Fanning   female\nFQ4dU1  yellow  Duck       Norris    male\nJqS7ZZ  red     James      Pond      male\nZM5uJL  black   Darth      Wader     male\n```\n\nExperiment with other output modes until you find the one you like the most! If you want to switch back to the default DuckDB output mode, use the following command.\n\n```sh\nCopy code\n\n.mode duckbox\n```\n\n### Query multiple JSON files at once\n\nYou can query across multiple files at once using path wildcards. For example, to query all files that end with `.json`:\n\n```sh\nCopy code\n\nSELECT * FROM './*.json';\n```\n\nYou can query from specific files, too, such as:\n\n```sh\nCopy code\n\nSELECT * FROM './monthly-sales-2023*.json';\n```\n\n### Join JSON files together\n\nJust like joining tables together, if there is a common key in one or more different data files, you can join on that key.\n\nIn this example, we have one JSON file that contains a list of ducks in a sanctuary, including ID, name, and color. In another JSON file there is a log of all the the things the ducks were observed doing, surveyed every 10 minutes for a month. This second file has the date and time of the log, the action, and only the ID of the duck. To create a report that summarizes the ducks' activities, you would want to join them together.\n\n```sql\nCopy code\n\nSELECT ducks.firstName || ' ' || ducks.lastName AS duck_name,\n    samples.action,\n    COUNT(*) AS observations\nFROM    './samples.json' AS samples\nJOIN    './ducks.json' AS ducks ON ducks.id = samples.id\nGROUP BY ALL\nORDER BY 1, 3 DESC;\n```\n\n```sh\nCopy code\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   duck_name    \u2502         action          \u2502 observations \u2502\n\u2502    varchar     \u2502         varchar         \u2502    int64     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Captain Quack  \u2502 sleeping                \u2502          890 \u2502\n\u2502 Captain Quack  \u2502 quacking                \u2502          632 \u2502\n\u2502 Captain Quack  \u2502 eating                  \u2502          623 \u2502\n\u2502 Captain Quack  \u2502 annoying                \u2502          594 \u2502\n\u2502 Captain Quack  \u2502 swimming                \u2502          356 \u2502\n\u2502 Captain Quack  \u2502 waddling                \u2502          351 \u2502\n\u2502 Captain Quack  \u2502 sunbathing              \u2502          348 \u2502\n\u2502 Captain Quack  \u2502 twitching               \u2502          125 \u2502\n\u2502 Captain Quack  \u2502 flying                  \u2502          121 \u2502\n\u2502 Captain Quack  \u2502 dancing                 \u2502          117 \u2502\n\u2502 Captain Quack  \u2502 diving                  \u2502          106 \u2502\n\u2502 Captain Quack  \u2502 posting on social media \u2502           57 \u2502\n...\n```\n\n### Import JSON data into DuckDB for further analysis\n\nIf you have a lot of different JSON files, it might make sense to import the data into tables in your local DuckDB database. In the following example, you'll import the `ducks.json` file and `samples.json` together into one table.\n\n```sql\nCopy code\n\nCREATE OR REPLACE TABLE duck_samples AS\nSELECT CAST(samples.sampleTime AS date) AS sample_date,\n    ducks.firstName || ' ' || ducks.lastName AS duck_name,\n    samples.action,\n    COUNT(*) AS observations\nFROM    read_json('./samples.json', columns = { id: 'varchar', sampleTime: 'datetime', action: 'varchar' }) AS samples\nJOIN    './ducks.json' AS ducks ON ducks.id = samples.id\nGROUP BY ALL;\n```\n\nThis example uses the `read_json` function to customize the schema of the imported data, which can be useful for converting dates and times as the data is read and parsed from the JSON data.\n\nWith the `duck_samples` table populated, we can now use it to analyze the data in new ways, such as number of actions performed by all ducks on a given day.\n\n```sql\nCopy code\n\nSELECT ds.sample_date,\n    ds.action,\n    ds.observations,\n    round(( ds.observations / totals.total_obs ) * 100, 1) AS percent_total\nFROM ( SELECT sample_date, action, SUM(observations) AS observations FROM duck_samples GROUP BY ALL ) AS ds\n    JOIN ( SELECT sample_date, SUM(observations) AS total_obs FROM duck_samples GROUP BY ALL ) AS totals\n    ON ds.sample_date = totals.sample_date\nWHERE ds.sample_date = '2024-01-01'\nGROUP BY ALL\nORDER BY 3 DESC;\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sample_date \u2502         action          \u2502 observations \u2502 percent_total \u2502\n\u2502    date     \u2502         varchar         \u2502    int128    \u2502    double     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2024-01-01  \u2502 sleeping                \u2502         1551 \u2502          21.5 \u2502\n\u2502 2024-01-01  \u2502 quacking                \u2502          978 \u2502          13.6 \u2502\n\u2502 2024-01-01  \u2502 eating                  \u2502          977 \u2502          13.6 \u2502\n\u2502 2024-01-01  \u2502 annoying                \u2502          947 \u2502          13.2 \u2502\n\u2502 2024-01-01  \u2502 swimming                \u2502          612 \u2502           8.5 \u2502\n\u2502 2024-01-01  \u2502 waddling                \u2502          600 \u2502           8.3 \u2502\n\u2502 2024-01-01  \u2502 sunbathing              \u2502          598 \u2502           8.3 \u2502\n\u2502 2024-01-01  \u2502 flying                  \u2502          231 \u2502           3.2 \u2502\n\u2502 2024-01-01  \u2502 diving                  \u2502          220 \u2502           3.1 \u2502\n\u2502 2024-01-01  \u2502 twitching               \u2502          208 \u2502           2.9 \u2502\n\u2502 2024-01-01  \u2502 dancing                 \u2502          193 \u2502           2.7 \u2502\n\u2502 2024-01-01  \u2502 posting on social media \u2502           85 \u2502           1.2 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 12 rows                                                    4 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Query complex JSON data\n\nDepending on the structure of the JSON data you are working with it may be necessary to extract values from nested objects or arrays. Nested objects are referred to in DuckDB as a `struct` data type. In some cases, it's possible to access data directly in a struct using syntax that resembles schema or table namespaces. For example, imagine you have JSON file named `ducks-nested-name.json` with the following data.\n\n```json\nCopy code\n\n[\\\n  {\\\n    \"color\": \"red\",\\\n    \"name\": {\\\n      \"firstName\": \"Marty\",\\\n      \"lastName\": \"McFly\"\\\n    },\\\n    \"gender\": \"male\"\\\n  },\\\n  {\\\n    \"color\": \"teal\",\\\n    \"name\": {\\\n      \"firstName\": \"Duckota\",\\\n      \"lastName\": \"Fanning\"\\\n    },\\\n    \"gender\": \"female\"\\\n  },\\\n  {\\\n    \"color\": \"yellow\",\\\n    \"name\": {\\\n      \"firstName\": \"Duck\",\\\n      \"lastName\": \"Norris\"\\\n    },\\\n    \"gender\": \"male\"\\\n  }\\\n]\n```\n\nIf you query the file directly, the results would like the following.\n\n```sql\nCopy code\n\nD SELECT * FROM './ducks-nested-name.json';\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  color  \u2502                    name                     \u2502 gender  \u2502\n\u2502 varchar \u2502 struct(firstname varchar, lastname varchar) \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 red     \u2502 {'firstName': Marty, 'lastName': McFly}     \u2502 male    \u2502\n\u2502 teal    \u2502 {'firstName': Duckota, 'lastName': Fanning} \u2502 female  \u2502\n\u2502 yellow  \u2502 {'firstName': Duck, 'lastName': Norris}     \u2502 male    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nYou can access the nested values under `name` using the following syntax.\n\n```sql\nCopy code\n\nD SELECT color, name.firstName FROM './ducks-nested-name.json';\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  color  \u2502 firstName \u2502\n\u2502 varchar \u2502  varchar  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 red     \u2502 Marty     \u2502\n\u2502 teal    \u2502 Duckota   \u2502\n\u2502 yellow  \u2502 Duck      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nDuckDB provides the `unnest` function to help when dealing with nested data. Taking the first example with Quackmire, Feather Locklear, and Duck Norris, if you query this JSON data without using `unnest`, you'll see the following results.\n\n```sh\nCopy code\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                              ducks                              \u2502 totalDucks \u2502\n\u2502   struct(\"name\" varchar, color varchar, actions varchar[])[]    \u2502   int64    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [{'name': Quackmire, 'color': green, 'actions': [swimming, wa\u2026  \u2502          3 \u2502\\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\\n```\\\n\\\nTo make better use of the data in the `ducks` column, use the `unnest` function to destructure and flatten the data into their own columns.\\\n\\\n```sql\\\nCopy code\\\n\\\nD SELECT unnest(ducks, recursive:= true) AS ducks\\\nFROM './ducks-example.json';\\\n\\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\\n\u2502       name       \u2502  color  \u2502            actions             \u2502\\\n\u2502     varchar      \u2502 varchar \u2502           varchar[]            \u2502\\\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\\\n\u2502 Quackmire        \u2502 green   \u2502 [swimming, waddling, quacking] \u2502\\\n\u2502 Feather Locklear \u2502 yellow  \u2502 [sunbathing]                   \u2502\\\n\u2502 Duck Norris      \u2502 brown   \u2502 [karate chopping bread]        \u2502\\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\\n```\\\n\\\n## Query JSON data from an API\\\n\\\nDuckDB can also parse data directly from APIs that return JSON. The following example uses the [TVmaze API](https://www.tvmaze.com/api), a public API for TV shows.\\\n\\\n```sql\\\nCopy code\\\n\\\nD SELECT show.name, show.type, show.summary\\\nFROM read_json('https://api.tvmaze.com/search/shows?q=duck',\\\n       auto_detect=true);\\\n\\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\\n\u2502      show_name       \u2502  show_type   \u2502                            summary                             \u2502\\\n\u2502         json         \u2502     json     \u2502                              json                              \u2502\\\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\\\n\u2502 \"Duck Dynasty\"       \u2502 \"Reality\"    \u2502 \"<p>In <b>Duck Dynasty</b>, A&amp;E Network introduces the R\u2026  \u2502\\\n\u2502 \"Darkwing Duck\"      \u2502 \"Animation\"  \u2502 \"<p>In the city of St. Canard, the people are plagued by the\u2026  \u2502\\\n\u2502 \"Duck Dodgers\"       \u2502 \"Animation\"  \u2502 \"<p>Animated sci-fi series based on the alter ego of Looney \u2026  \u2502\\\n\u2502 \"Duck Patrol\"        \u2502 \"Scripted\"   \u2502 \"<p><b>Duck Patrol</b> deals with the activities of the offi\u2026  \u2502\\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\\n```\\\n\\\n## Learn more about DuckDB\\\n\\\nTo learn more about what you can do with DuckDB, check out the [DuckDB Snippets Library](https://duckdbsnippets.com/) or download a free copy of [DuckDB in Action](https://motherduck.com/duckdb-book-brief).\\\n\\\n### TABLE OF CONTENTS\\\n\\\n[Prerequisites](https://motherduck.com/blog/analyze-json-data-using-sql/#prerequisites)\\\n\\\n[What is JSON?](https://motherduck.com/blog/analyze-json-data-using-sql/#what-is-json)\\\n\\\n[Install and execute DuckDB](https://motherduck.com/blog/analyze-json-data-using-sql/#install-and-execute-duckdb)\\\n\\\n[Query JSON files with DuckDB](https://motherduck.com/blog/analyze-json-data-using-sql/#query-json-files-with-duckdb)\\\n\\\n[Query complex JSON data](https://motherduck.com/blog/analyze-json-data-using-sql/#query-complex-json-data)\\\n\\\n[Query JSON data from an API](https://motherduck.com/blog/analyze-json-data-using-sql/#query-json-data-from-an-api)\\\n\\\n[Learn more about DuckDB](https://motherduck.com/blog/analyze-json-data-using-sql/#learn-more-about-duckdb)\\\n\\\nStart using MotherDuck now!\\\n\\\n[Try 21 Days Free](https://motherduck.com/get-started/)\\\n\\\nGet Started\\\n\\\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\\\n\\\n### Subscribe to motherduck blog\\\n\\\nE-mail\\\n\\\nSubscribe to other MotherDuck Updates\\\n\\\nSubmit\\\n\\\n## PREVIOUS POSTS\\\n\\\n[![Why Python Developers Need DuckDB (And Not Just Another DataFrame Library)](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Fwhy_pythondev_1_22167e31bf.png&w=3840&q=75)](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/)\\\n\\\n[2025/10/08 - Mehdi Ouazza](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/)\\\n\\\n### [Why Python Developers Need DuckDB (And Not Just Another DataFrame Library)](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries)\\\n\\\nUnderstand why a database is much more than just a dataframe library\\\n\\\n[![DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_4_1_b6209aca06.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\\\n\\\n[2025/10/09 - Alex Monahan, Garrett O'Brien](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\\\n\\\n### [DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/blog/announcing-duckdb-141-motherduck)\\\n\\\nMotherDuck now supports DuckDB 1.4.1 and DuckLake 0.3, with new SQL syntax, faster sorting, Iceberg interoperability, and more. Read on for the highlights from these major releases.\\\n\\\n[View all](https://motherduck.com/blog/)\\\n\\\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 542,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 542
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now I have enough information to write a comprehensive answer. Let me write it to the specified file:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_1.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "create",
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_1.md",
  [2m  â”‚[0m   "content": "<5184 chars>",
  [2m  â”‚[0m   "structuredPatch": [],
  [2m  â”‚[0m   "originalFile": null
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I've completed the task. Here's a summary of what I found:
  [94mâ”‚[0m 
  [94mâ”‚[0m ## Summary
  [94mâ”‚[0m 
  [94mâ”‚[0m Based on searching through the `.kurt/sources/` directory, the **most efficient file formats for loading data into MotherDuck** are:
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. **Apache Parquet** (Most Efficient) - The recommended format due to:
  [94mâ”‚[0m    - Columnar storage that aligns with DuckDB's analytical engine
  [94mâ”‚[0m    - Superior compression (dictionary, run-length, delta encoding)
  [94mâ”‚[0m    - Reduced I/O through column pruning and predicate pushdown
  [94mâ”‚[0m    - Native integration with DuckDB
  [94mâ”‚[0m 
  [94mâ”‚[0m 2. **CSV Files** - Well-supported with DuckDB's exceptional parser:
  [94mâ”‚[0m    - Auto-detection of schema and dialects
  [94mâ”‚[0m    - #1 ranking on Pollock benchmark (99.61% accuracy)
  [94mâ”‚[0m    - Good error handling options for messy data
  [94mâ”‚[0m 
  [94mâ”‚[0m 3. **JSON Files** - Supported but less efficient:
  [94mâ”‚[0m    - Auto-detection with `read_json_auto`
  [94mâ”‚[0m    - Support for nested data via `unnest`
  [94mâ”‚[0m    - Best for semi-structured data and APIs
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer has been written to `/tmp/answer_without_kg_1.md` with full details and source citations from 6 relevant files in the Kurt sources directory.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 23,636 | Cost: $0.7129
     Cumulative: 23,636 tokens | $0.7129
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent clearly states that it has completed the task and provides a summary, indicating that it is not soliciting further user input.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 12 | Duration: 73.4s
â•‘    Tokens: 23,636 | Cost: $0.7129
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  Running LLM judge evaluation...
   ğŸ§  LLM Judge score: 1.00
