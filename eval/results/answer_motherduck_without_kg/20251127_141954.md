# Raw Transcript: answer_motherduck_without_kg

**Status**: âœ… PASSED

**Duration**: 1321.02 seconds
**Tokens Used**: 18708

```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ SCENARIO: answer_motherduck_without_kg
   Answer MotherDuck questions by searching sources directly (no KG)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ” Verifying .claude installation...
   âœ“ .claude folder exists at /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.claude
   âœ“ Skills: 0 found
   âœ“ Commands: 1 found

ğŸ§ª Running 10 question(s) defined in questions_motherduck.yaml

======================================================================
â“ Question 1/10
What file formats are most efficient for loading data into MotherDuck?
======================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 1
â”‚ Search through the .kurt/sources/ directory to answer this question:

What file formats are most efficient for loading data into MotherDuck?

Steps:
1. Search for the most relevant markdown files in .kurt/sources/
2. Read those files and synthesize the answer
3. Write a markdown answer (with a ## Sources section) to: /tmp/answer_without_kg_1.md
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I'll help you find information about the most efficient file formats for loading data into MotherDuck. Let me search through the sources directory.
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ file format|load|loading data|efficient|performance|import

  [96mğŸ”§ TOOL:[0m Grep â†’ parquet|csv|json|avro|arrow
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/take-flight-with-the-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-surprising-birth-of-duckdb-ft-co-creator-hannes-muhleisen.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-if-sql-queries-returned-results-instantly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/speed-querying-stackoverflow-data-with-duckdb-ft-michael-hunger.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-bad-the-good-and-the-ugly-navigating-the-new-waves-of-machine-learning-ft-adi-polak.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/spatial-data-management-with-duckdb-ft-mattforrest.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/leveraging-duckdb-and-delta-lake-together.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/going-beyond-the-dataframe.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ibis-one-library-to-query-any-backend.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Great! I found some relevant files. Let me read the most promising ones, especially those about file formats, loading data, and Parquet/CSV conversion:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     "content": "---\ntitle: simple-way-to-convert-csv-and-parquet-files\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files\nindexed_at: '2025-11-25T20:44:27.252956'\ncontent_hash: 0215f394eabfe4ec\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nSimple way to convert CSV - Parquet files - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Simple way to convert CSV - Parquet files](https://www.youtube.com/watch?v=Y_GXdbet9Gk)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=Y_GXdbet9Gk&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 1:00\n\n\u2022Live\n\n\u2022\n\nYouTubeShortTutorial\n\n# Simple way to convert CSV and Parquet files\n\n2024/01/17\n\nAs developers, we live in the terminal. It\u2019s our command center for everything from Git commits to running applications. But we all have those small, annoying tasks that break our flow\u2014the little papercuts of the development cycle. One of the most common? File format conversion.\n\nYou have a Parquet file, but a legacy tool or a colleague needs it in CSV format. Or maybe you have a massive CSV that you want to compress into the more efficient Parquet format. What's your go-to move? Do you spin up a Jupyter notebook and `import pandas`? Do you write a quick, one-off Python script? Or do you resort to a sketchy online file converter?\n\nThese interruptions, while small, add up. But what if the perfect tool for the job was already on your machine, ready to go?\n\nEnter DuckDB. While you might know it as a powerful embedded analytical database, it's also a versatile Swiss army knife for your data. Because it's a lightweight, serverless tool that speaks fluent SQL and natively understands formats like Parquet, CSV, and JSON, it's the perfect utility for lightning-fast file conversions directly from your command line.\n\nLet's build a powerful, reusable conversion utility in just a few minutes.\n\n### Instant File Conversion with a DuckDB One-Liner\n\nFirst things first, you need the `duckdb` CLI. If you're on macOS, installation is a single command with Homebrew.\n\n```bash\nCopy code\n\nbrew install duckdb\n```\n\nFor other operating systems, check out the [official installation documentation](https://duckdb.org/docs/installation/index).\n\nOnce installed, you have everything you need. Let's say you have a file named `data.parquet` and you want to convert it to `data.csv`. The magic is a single command that leverages DuckDB's powerful `COPY` statement.\n\n```bash\nCopy code\n\nduckdb -c \"COPY (SELECT * FROM 'data.parquet') TO 'data.csv' (HEADER, DELIMITER ',');\"\n```\n\nLet's break down this command to see what's happening:\n\n- `duckdb -c \"...\"`: This is the key to using DuckDB as a scripting tool. The `-c` flag tells DuckDB to execute the SQL command that follows and then immediately exit. No interactive shell, no fuss\u2014just pure, scriptable execution.\n- `COPY (...) TO 'data.csv'`: This is the workhorse. The `COPY` command is incredibly efficient at moving data into and out of DuckDB.\n- `(SELECT * FROM 'data.parquet')`: Instead of copying from a table, we're telling DuckDB to copy the result of a query. The magic here is that DuckDB can query files like Parquet or CSV directly, _as if they were database tables_. It automatically infers the file type and schema from the file extension.\n- `(HEADER, DELIMITER ',')`: These are options specific to the output format. Here, we're telling DuckDB to include a header row in our final CSV file.\n\nAnd that's it. In the time it would have taken you to open a new editor tab, you've converted your file.\n\n### Building a Reusable Conversion Script\n\nThat one-liner is great, but we can make it even better. The real power of the command line comes from creating reusable, generic tools.\n\nLet's wrap this logic into a simple Bash script. Create a file named `file-converter.sh` somewhere convenient, like `~/scripts/`.\n\n```bash\nCopy code\n\n#!/bin/bash\n# file-converter.sh\n\n# Check if two arguments are provided\nif [ \"$#\" -ne 2 ]; then\n    echo \"Usage: $0 <input_file> <output_file>\"\n    exit 1\nfi\n\nINPUT_FILE=$1\nOUTPUT_FILE=$2\n\nduckdb -c \"COPY (SELECT * FROM '${INPUT_FILE}') TO '${OUTPUT_FILE}';\"\n\necho \"Successfully converted ${INPUT_FILE} to ${OUTPUT_FILE}\"\n```\n\nMake the script executable:\n\n```bash\nCopy code\n\nchmod +x ~/scripts/file-converter.sh\n```\n\nNow, you have a generic script that takes an input file and an output file as arguments. The final step is to create a shell alias for ultimate convenience. Open your `.zshrc`, `.bashrc`, or equivalent shell configuration file and add this line:\n\n```bash\nCopy code\n\n# Add to your .zshrc or .bashrc\nalias dconvert='~/scripts/file-converter.sh'\n```\n\nRestart your terminal or run `source ~/.zshrc` to apply the changes. Now, witness your new superpower. You can convert files back and forth with a simple, memorable command.\n\n**Convert Parquet to CSV:**\n\n```bash\nCopy code\n\ndconvert data.parquet data.csv\n# Successfully converted data.parquet to data.csv\n```\n\n**Convert CSV to Parquet:**\n\n```bash\nCopy code\n\ndconvert data.csv data.parquet\n# Successfully converted data.csv to data.parquet\n```\n\nIt doesn't get simpler than that. You've just built a universal file conversion utility that is faster and more reliable than a custom script and safer than any online tool.\n\n### Conclusion: More Than Just Conversion\n\nWe started with a simple problem and ended with an elegant, reusable solution. With one small script, you've added a powerful tool to your developer toolkit, powered by DuckDB.\n\nBut don't forget what's happening inside that command. You aren't just copying bytes; you're running a full-fledged SQL query. This opens up a world of possibilities that go far beyond the simple 1-to-1 conversion our `dconvert` alias handles.\n\nWhat if you only wanted a subset of the data? For more complex tasks, you can bypass the alias and use the `duckdb -c` command directly to run a more powerful query.\n\n```bash\nCopy code\n\n# Filter for specific rows before converting\nduckdb -c \"COPY (SELECT * FROM 'data.parquet' WHERE category = 'A') TO 'filtered_data.csv' (HEADER, DELIMITER ',');\"\n```\n\nWhat if you only needed a few columns?\n\n```bash\nCopy code\n\n# Select specific columns\nduckdb -c \"COPY (SELECT user_id, event_timestamp FROM 'logs.parquet') TO 'events.csv' (HEADER, DELIMITER ',');\"\n```\n\nThe `SELECT` statement is your playground. You can perform filtering, transformations, and even simple aggregations as part of your conversion pipeline, all within that single command.\n\n* * *\n\n### Get Started Today\n\n- Try this out and share your favorite DuckDB one-liners with us on [Twitter](https://twitter.com/motherduckcorp)!\n- For more details on the options available, check out the official DuckDB documentation for the [`COPY` command](https://duckdb.org/docs/sql/data_loading/copy).\n- When your challenges go beyond local files, see how [MotherDuck](https://motherduck.com/) brings the power of DuckDB to the cloud for serverless, collaborative analytics.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Is DuckDB the Secret to Unlocking Your GIS Potential?\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmaxresdefault_1_c988e40ed0.jpg&w=3840&q=75)\\\\\n\\\\\n14:49](https://motherduck.com/videos/is-duckdb-the-secret-to-unlocking-your-gis-potential/)\n\n[2024-08-29](https://motherduck.com/videos/is-duckdb-the-secret-to-unlocking-your-gis-potential/)\n\n### [Is DuckDB the Secret to Unlocking Your GIS Potential?](https://motherduck.com/videos/is-duckdb-the-secret-to-unlocking-your-gis-potential)\n\nIn this video, \u202aMehdi walks you through the basics of working with geospatial data and introduces the DuckDB spatial extension. By the end, you will create your own heatmap using DuckDB, Python, and MotherDuck for sharing and scalability.\n\nYouTube\n\nTutorial\n\n[![\"DuckDB & dataviz | End-To-End Data Engineering Project (3/3)\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fvideo_ta_Pzc2_EE_Eo_23e0b0a9d0.jpg&w=3840&q=75)\\\\\n\\\\\n0:21:46](https://motherduck.com/videos/duckdb-dataviz-end-to-end-data-engineering-project-33/)\n\n[2024-06-27](https://motherduck.com/videos/duckdb-dataviz-end-to-end-data-engineering-project-33/)\n\n### [DuckDB & dataviz \\| End-To-End Data Engineering Project (3/3)](https://motherduck.com/videos/duckdb-dataviz-end-to-end-data-engineering-project-33)\n\nIn this part 3 of the project, @mehdio explores how to build a Dashboard with Evidence using MotherDuck/DuckDb as a data source.\n\nYouTube\n\nBI & Visualization\n\nTutorial\n\n[![\"One data tool with all its dependencies: DuckDB and extensions\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fvideo_we_RMT_Aj_Udic_9e9f5c6d41.jpg&w=3840&q=75)\\\\\n\\\\\n0:00:55](https://motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions/)\n\n[2024-06-21](https://motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions/)\n\n### [One data tool with all its dependencies: DuckDB and extensions](https://motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions)\n\nLearn about DuckDB extensions, including the ability to query data in your AWS S3-powered data lake.\n\nYouTube\n\nShort\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 267,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 267
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     "content": "---\ntitle: why-csvs-still-matter-the-indispensable-file-format\ncontent_type: event\nsource_url: https://motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format\nindexed_at: '2025-11-25T20:44:48.426444'\ncontent_hash: 3788e1cc125b23f9\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nWhy CSVs Still Matter: The Indispensable File Format - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Why CSVs Still Matter: The Indispensable File Format](https://www.youtube.com/watch?v=I07qV2hij4E)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=I07qV2hij4E&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 58:06\n\n\u2022Live\n\n\u2022\n\nYouTubeSources\n\n# Why CSVs Still Matter: The Indispensable File Format\n\n2024/05/07\n\nIn this discussion with Mehdi Ouazza and Pedro Holanda from DuckDB, they talk about all things CSVs and dive into a pragmatic example of how the CSV parser works in DuckDB. Enjoy!\n\n## Related Videos\n\n[![\" pg_duckdb: Ducking awesome analytics in Postgres\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2F0kc_W5o0tcg_HD_03880f61fb.jpg&w=3840&q=75)](https://motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres/)\n\n[2025-06-12](https://motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres/)\n\n### [pg\\_duckdb: Ducking awesome analytics in Postgres](https://motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres)\n\nSupercharge your Postgres analytics! This talk shows how the pg\\_duckdb extension accelerates your slowest queries instantly, often with zero code changes. Learn practical tips and how to use remote columnar storage for even more speed.\n\nTalk\n\nSources\n\n[![\"Can DuckDB revolutionize the data lake experience?\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmaxresdefault_2074c1c09e.jpg&w=3840&q=75)\\\\\n\\\\\n16:37](https://motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience/)\n\n[2024-11-22](https://motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience/)\n\n### [Can DuckDB revolutionize the data lake experience?](https://motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience)\n\nMehdi explores DuckDB as a catalog for Data Lake and Lakehouse pattern. He'll define what we mean by \"data catalog\", gives clear examples on how they work and dive into a pragmatic use case with DuckDB & MotherDuck.\n\nYouTube\n\nData Pipelines\n\nSources\n\n[![\"Leveraging DuckDB and Delta Lake Together\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmaxresdefault_2_426f95538d.jpg&w=3840&q=75)\\\\\n\\\\\n1:02:10](https://motherduck.com/videos/leveraging-duckdb-and-delta-lake-together/)\n\n[2024-07-24](https://motherduck.com/videos/leveraging-duckdb-and-delta-lake-together/)\n\n### [Leveraging DuckDB and Delta Lake Together](https://motherduck.com/videos/leveraging-duckdb-and-delta-lake-together)\n\nQuack & Code on table formats and Delta Lake with Holly Smith from Databricks. Can you use DuckDB and MotherDuck to query your data lakehouse based on Delta Lake? Yes! And Delta Lake provides a better format than Parquet.\n\nYouTube\n\nSources\n\nQuack & Code\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 127,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 127
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     "content": "---\ntitle: how-to-efficiently-load-data-into-ducklake-with-estuary\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary\nindexed_at: '2025-11-25T20:44:50.287804'\ncontent_hash: 54211f956cd60bed\nhas_step_by_step: true\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nHow to Efficiently Load Data into DuckLake with Estuary - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[How to Efficiently Load Data into DuckLake with Estuary](https://www.youtube.com/watch?v=8uce9V9VnjY)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Why am I seeing this?](https://support.google.com/youtube/answer/9004474?hl=en)\n\n[Watch on](https://www.youtube.com/watch?v=8uce9V9VnjY&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 50:08\n\n\u2022Live\n\n\u2022\n\nYouTube\n\n# How to Efficiently Load Data into DuckLake with Estuary\n\n2025/07/26\n\n## Introduction to DuckLake and Real-Time Data Integration\n\nDuckLake represents a new open table format created by the developers of DuckDB. Unlike traditional data lake formats that store metadata in JSON or Avro files within blob storage, DuckLake takes a different approach by storing metadata in a relational database. This architectural decision significantly speeds up metadata transactions, making operations faster and more efficient.\n\n## Understanding MotherDuck's Cloud Data Warehouse\n\nMotherDuck is a cloud data warehouse designed to make big data feel small. Built on top of DuckDB, it focuses on three key principles:\n\n- **Eliminating complexity** from traditional distributed data warehouses\n- **Enabling fast insights** while keeping developers in their workflow\n- **Leveraging DuckDB's performance** for cost-efficient operations\n\nThe platform integrates seamlessly with existing data stack tools and supports standard SQL queries with enhanced features.\n\n## Estuary's Real-Time Data Integration Platform\n\nEstuary provides a real-time data integration platform that supports both streaming and batch data movement. The platform features:\n\n### Architecture Components\n\n- **Capture connectors** that extract data from source systems using change data capture (CDC)\n- **Collections** stored in object storage (S3, GCS, or compatible systems)\n- **Materialization connectors** that load data into destinations\n\n### Key Capabilities\n\n- Support for hundreds of source and destination systems\n- Native CDC for databases like PostgreSQL, MySQL, and Oracle\n- Real-time extraction from SaaS applications like Salesforce and HubSpot\n- No-code setup with managed service\n\n## Setting Up a Real-Time Pipeline to DuckLake\n\nThe process of loading data into DuckLake involves several straightforward steps:\n\n### Source Configuration\n\n1. Connect to your source database (PostgreSQL, MySQL, etc.)\n2. Enable change data capture to track real-time changes\n3. Configure schema evolution settings for automatic handling of schema changes\n\n### DuckLake Setup\n\n1. Create a DuckLake database in MotherDuck\n2. Configure access to your S3 bucket where data will be stored\n3. Set up appropriate access tokens for read/write permissions\n\n### Pipeline Configuration\n\n- Choose sync frequency (from real-time to scheduled batches)\n- Select specific fields to materialize\n- Configure merge queries for maintaining latest state\n\n## Performance Optimization with MotherDuck\n\n### Instant SQL Feature\n\nMotherDuck introduces Instant SQL, which provides query results at the speed of typing by:\n\n- Pre-caching data for immediate feedback\n- Validating SQL syntax in real-time\n- Enabling rapid iteration on complex queries\n\n### Storage Trade-offs\n\nWhen deciding between DuckLake and MotherDuck native storage:\n\n**DuckLake advantages:**\n\n- Open format with broader ecosystem compatibility\n- Support for Spark and other compute engines\n- Better suited for petabyte-scale workloads\n\n**MotherDuck storage advantages:**\n\n- 2-10x faster query performance\n- Optimized for read/write throughput\n- Better caching and regional performance\n\n## Scaling Considerations\n\nMotherDuck now offers larger instance sizes (Mega and Giga) to support intensive data lake operations. These instances are comparable to Snowflake 3XL configurations and enable:\n\n- Terabyte to petabyte-scale operations\n- Complex aggregations and sorting\n- Efficient medallion architecture implementations\n\n## Best Practices for Implementation\n\n### Data Architecture\n\n- Keep raw data in DuckLake for openness and flexibility\n- Move silver/gold layer data to MotherDuck storage for performance\n- Use Estuary's intermediate storage for reliability and replay capabilities\n\n### Partitioning Strategy\n\nWhile Estuary doesn't natively configure partitions, you can:\n\n1. Allow Estuary to create initial tables\n2. Use `ALTER TABLE` commands to add partitions\n3. Subsequent writes will respect partition configuration\n\n### Error Handling and Reliability\n\nThe architecture prevents common streaming issues:\n\n- Intermediate storage prevents message loss\n- Automatic handling of destination unavailability\n- Support for backfills without re-querying sources\n\n## Integration with Modern Data Stack\n\nThe combination of Estuary and MotherDuck integrates with existing tools:\n\n- dbt support through DuckDB adapter\n- AI-powered SQL writing assistance\n- Automatic error detection and fixing\n- Support for multiple materialization targets from single source\n\nThis architecture enables organizations to implement real-time data pipelines without the traditional complexity of streaming systems, making the difference between batch and streaming simply a configuration toggle rather than an architectural decision.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Data-based: Going Beyond the Dataframe\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FData_based_f32745b461.png&w=3840&q=75)](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n[2025-11-20](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n### [Data-based: Going Beyond the Dataframe](https://motherduck.com/videos/going-beyond-the-dataframe)\n\nLearn how to turbocharge your Python data work using DuckDB and MotherDuck with Pandas. We walk through performance comparisons, exploratory data analysis on bigger datasets, and an end-to-end ML feature engineering pipeline.\n\nWebinar\n\nPython\n\nAI, ML and LLMs\n\n[![\"Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FHex_Webinar_778e3959e4.png&w=3840&q=75)](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n[2025-11-14](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n### [Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck)\n\nAI isn't here to replace data work, it's here to make it better. Watch this webinar to see how Hex and MotherDuck build AI workflows that prioritize context, iteration, and real-world impact.\n\nWebinar\n\nAI, ML and LLMs\n\n[![\"Lies, Damn Lies, and Benchmarks\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FLies_Damn_Lies_and_Benchmarks_Thumbnail_404db1bf46.png&w=3840&q=75)](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n[2025-10-31](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n### [Lies, Damn Lies, and Benchmarks](https://motherduck.com/videos/lies-damn-lies-and-benchmarks)\n\nWhy do database benchmarks so often mislead? MotherDuck CEO Jordan Tigani discusses the pitfalls of performance benchmarking, lessons from BigQuery, and why your own workload is the only benchmark that truly matters.\n\nStream\n\nInterview\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 247,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 247
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     "content": "---\ntitle: taming-file-zoos-data-science-with-duckdb-database-files\ncontent_type: event\nsource_url: https://motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files\nindexed_at: '2025-11-25T20:44:13.578877'\ncontent_hash: 8f5fa045a79738b7\nhas_step_by_step: true\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nTaming file zoos: Data science with DuckDB database files - Alex Monahan - YouTube\n\n[Photo image of PyCon US](https://www.youtube.com/channel/UCMjMBMGt0WJQLeluw6qNJuA?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nPyCon US\n\n33.6K subscribers\n\n[Taming file zoos: Data science with DuckDB database files - Alex Monahan](https://www.youtube.com/watch?v=Yq-hxgysWLQ)\n\nPyCon US\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Why am I seeing this?](https://support.google.com/youtube/answer/9004474?hl=en)\n\n[Watch on](https://www.youtube.com/watch?v=Yq-hxgysWLQ&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 28:32\n\n\u2022Live\n\n\u2022\n\nPythonTalk\n\n# Taming file zoos: Data science with DuckDB database files\n\n2025/06/02\n\n### Problem statement\n\nData scientists working in Python often spend the majority of their time cleaning input data, frequently from files. These files have many formats, can be located anywhere, and sometimes have names like \u2018data\\_final\\_final\\_v3.csv\u2019. Data scientists often produce similar files! We call these \u201cfile zoos\u201d.\n\n## Taming file zoos with DuckDB\n\n### DuckDB fits perfectly with Python\n\nThe MIT-licensed DuckDB database management system was designed to fit perfectly into data scientists\u2019 workflows. Install DuckDB\u2019s pre-compiled, dependency-free binary from pip. It can read and write dataframes (Pandas, Polars, and Apache Arrow) for interoperability. It also has an advanced persistent file format.\n\n### Read and write files with confidence\n\nDuckDB can read and write to and from csv, parquet, json - even xlsx and Google Sheets. The csv reader in DuckDB is world-class, quickly querying even messy csvs. DuckDB interoperates with object stores across clouds and reads lakehouse formats like Delta and Iceberg.\n\n### Organize using the DuckDB format\n\nUse DuckDB\u2019s highly compressed columnar file format to persist many large tables all in the same file. Store processing logic in views and functions and even update just portions of the file. DuckDB serves as a catalog when files should remain in place.\n\nBeyond the format itself, DuckDB provides ACID transactional safety and parallel processing, it can be read in 15+ languages, and is guaranteed to be readable for years to come. It unlocks larger-than-memory analyses to solve 2TB problems, not 16GB ones!\n\n### Extensions\n\nCommunity extensions enable DuckDB to read additional formats and are provided through a pip-like package repository.\n\n### Takeaways\n\nAttendees will learn how to install and use DuckDB locally, how to integrate it seamlessly in their existing Python scripts or Jupyter Notebooks, and how to smoothly manage the deluge of files in their workflow.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Data-based: Going Beyond the Dataframe\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FData_based_f32745b461.png&w=3840&q=75)](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n[2025-11-20](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n### [Data-based: Going Beyond the Dataframe](https://motherduck.com/videos/going-beyond-the-dataframe)\n\nLearn how to turbocharge your Python data work using DuckDB and MotherDuck with Pandas. We walk through performance comparisons, exploratory data analysis on bigger datasets, and an end-to-end ML feature engineering pipeline.\n\nWebinar\n\nPython\n\nAI, ML and LLMs\n\n[![\"DuckLake: Making BIG DATA feel small (Coalesce 2025)\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Fmaxresdefault_1_7f1e9ebbca.jpg&w=3840&q=75)](https://motherduck.com/videos/ducklake-big-data-small-coalesce-2025/)\n\n[2025-10-14](https://motherduck.com/videos/ducklake-big-data-small-coalesce-2025/)\n\n### [DuckLake: Making BIG DATA feel small (Coalesce 2025)](https://motherduck.com/videos/ducklake-big-data-small-coalesce-2025)\n\nMotherDuck\u2019s managed DuckLake data lakehouse blends the cost efficiency, scale, and openness of a lakehouse with the speed of a warehouse for truly joyful dbt pipelines. They will show you how!\n\nTalk\n\n[![\"What can Postgres learn from DuckDB? (PGConf.dev 2025)\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_06_13_at_3_52_19_PM_470b0f71b1.png&w=3840&q=75)\\\\\n\\\\\n20:44](https://motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025/)\n\n[2025-06-13](https://motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025/)\n\n### [What can Postgres learn from DuckDB? (PGConf.dev 2025)](https://motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025)\n\nDuckDB an open source SQL analytics engine that is quickly growing in popularity. This begs the question: What can Postgres learn from DuckDB?\n\nYouTube\n\nEcosystem\n\nTalk\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 155,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 155
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search for more specific information about file format performance and efficiency:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ parquet.*performance|parquet.*efficient|parquet.*compress|parquet.*fast

  [96mğŸ”§ TOOL:[0m Grep â†’ columnar format|columnar storage|compression|query performance
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": ".kurt/sources/motherduck.com/learn-more.md:83:Learn what the Parquet file format is, its key benefits like columnar storage and compression, and how it compares to alternatives like CSV, Avro, Iceberg, and Delta Lake.\\\\\n.kurt/sources/motherduck.com/index.md:584:This benchmark uses plain parquet files and COUNT distinct to truly measure performance without skipping any data so we know the differences are correct.\\\\\n.kurt/sources/motherduck.com/index.md:797:This benchmark uses plain parquet files and COUNT distinct to truly measure performance without skipping any data so we know the differences are correct.\\\\\n.kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md:81:You have a Parquet file, but a legacy tool or a colleague needs it in CSV format. Or maybe you have a massive CSV that you want to compress into the more efficient Parquet format. What's your go-to move? Do you spin up a Jupyter notebook and `import pandas`? Do you write a quick, one-off Python script? Or do you resort to a sketchy online file converter?\n.kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md:85:Enter DuckDB. While you might know it as a powerful embedded analytical database, it's also a versatile Swiss army knife for your data. Because it's a lightweight, serverless tool that speaks fluent SQL and natively understands formats like Parquet, CSV, and JSON, it's the perfect utility for lightning-fast file conversions directly from your command line.\n.kurt/sources/motherduck.com/videos.md:297:Tame \"file zoos\" with DuckDB! This talk reveals how DuckDB seamlessly integrates with Python to read/write diverse files (CSV, Parquet), organize data efficiently, enable larger-than-memory analysis, and streamline your workflows.\n.kurt/sources/motherduck.com/glossary/storage layer.md:16:[Omitted long matching line]\n.kurt/sources/motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai.md:200:Since our source files are Parquet, running a simple command to get the schema is fast, easy, and cheap using DuckDB: `DESCRIBE SELECT * FROM read_parquet('s3://path/to/your/data/*.parquet');`\n.kurt/sources/motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai.md:252:- **Use sampling for large datasets (Optional):** When dealing with very large datasets, consider using MCP to create a smaller, local sample (`CREATE TABLE local_sample AS SELECT * FROM read_parquet('s3://...') LIMIT 1000;`). Iterating on this faster local sample can significantly speed up development before applying logic to the full dataset.\n.kurt/sources/motherduck.com/glossary/DuckDB.md:21:[Omitted long matching line]\n.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md:66:[Omitted long matching line]\n.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md:98:Matt and Zach generated seven synthetic Parquet datasets in DuckDB and benchmarked both engines by reading each file and computing COUNT(DISTINCT rand\\_str) to force full-file scans. Results showed DuckDB was faster in every run, including the largest 23\u202fGB dataset that exceeded the machine RAM.\n.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md:108:### [Turn Thousands of Messy JSON Files into One Parquet: DuckDB for Fast Data Warehouse Ingestion](https://www.dumky.net/posts/turn-thousands-of-messy-json-files-into-one-parquet-duckdb-for-fast-data-warehouse-ingestion/)\n.kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md:124:Data Export | Scheduled job running `COPY my_table TO 's3://my-dr-bucket/...' (FORMAT PARQUET, PARTITION_BY (load_date));` | RPO Definition: Exports data in an open, efficient format. Run nightly for a 24-hour Recovery Point Objective (RPO) or hourly for a 1-hour RPO. |\n.kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md:204:[Omitted long matching line]\n.kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md:343:Absolutely. Instead of complex, expensive platforms like Snowflake or BigQuery, startups can adopt a lean stack using open formats like Parquet on S3 combined with a serverless analytics engine. This approach eliminates infrastructure management and high costs, and platforms like MotherDuck are designed specifically for this modern, efficient model, offering a 10x cheaper and faster experience.\n.kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md:375:Yes. This is a key innovation of the modern, hybrid data stack. With DuckDB and MotherDuck, you can write a single SQL query that seamlessly joins a local CSV or Parquet file on your laptop with a large dataset stored in the MotherDuck cloud. This capability eliminates complex ETL steps and enables incredibly fast, iterative analysis across both local and shared data.\n.kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md:71:[Omitted long matching line]\n.kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md:182:The most efficient way to get data out of BigQuery is to export it to a columnar format like [Parquet](https://parquet.apache.org/) in an object storage bucket. Parquet is highly compressed and performs exceptionally well with DuckDB. You can do this with a single SQL command in the BigQuery console.\n.kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md:219:DuckDB will automatically infer the schema from the Parquet files, parallelize the download from S3, and load the data efficiently into your new table in MotherDuck's managed storage. This simple, SQL-based approach avoids complex ingestion pipelines and allows you to move terabytes of data with just a few commands.\n.kurt/sources/motherduck.com/learn-more/what-is-duckdb.md:171:- Use the Parquet file format for large datasets to benefit from its columnar storage and compression capabilities\n.kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-1.md:234:We can export our tables to Parquet files for safekeeping and easier storage and processing in other ways. Parquet as a columnar format compresses better, includes the schema and supports optimized reading with column selection and predicate pushdown.\n.kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md:25:In data engineering, efficient storage and processing of massive datasets is often very important. As data volumes continue to grow, organizations are turning to innovative file formats and table structures to optimize their data warehousing and analytics workflows. Among these, the Apache Parquet file format has emerged as a popular choice for its columnar storage layout and impressive compression capabilities.\n.kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md:27:Parquet, designed for use with large-scale data processing frameworks like Apache Spark and Hadoop, has gained significant traction in both the \"big data\" and \"small data\" communities. Its ability to enable efficient data storage and retrieval, coupled with its support for complex data types and schema evolution, makes it a compelling option for a wide range of data-intensive applications.\n.kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md:33:- **What is Parquet**: An open-source columnar storage file format for efficient analytics.\n.kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md:36:- **DuckDB & MotherDuck**: Parquet integrates seamlessly with DuckDB for high-performance SQL queries directly on Parquet files.\n.kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md:40:[Omitted long matching line]\n.kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md:44:One of the key characteristics of Parquet is its columnar storage layout. Unlike traditional row-based formats, Parquet organizes data by columns rather than rows. This means that all values for a particular column are stored contiguously on disk, enabling faster retrieval and better compression ratios.\n.kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md:48:The columnar storage format also allows for more efficient compression techniques. Since values within a column tend to be of the same data type and exhibit similar patterns, Parquet can apply appropriate compression algorithms to achieve higher compression ratios compared to row-based formats. This reduction in storage footprint translates to lower storage costs and faster query execution, as less data needs to be read from disk.\n.kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md:62:Parquet stands out in data processing environments for its ability to efficiently manage large datasets while ensuring high performance. Its unique approach to data storage enhances retrieval speeds and optimizes storage efficiency. This is achieved through intelligent data organization and compression strategies that minimize unnecessary data handling.",
  [2m  â”‚[0m   "numLines": 30,
  [2m  â”‚[0m   "appliedLimit": 30
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/panel-fixing-the-data-engineering-lifecycle-coalesce-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/leveraging-duckdb-and-delta-lake-together.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/monte-carlo-simulations-with-duckdb-ft-james-mcneill.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-bootstrap-a-data-warehouse-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ducklake-big-data-small-coalesce-2025.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 15,
  [2m  â”‚[0m   "appliedLimit": 15
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me read the key source document about Parquet:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/leveraging-duckdb-and-delta-lake-together.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     "content": "---\ntitle: why-choose-parquet-table-file-format\ncontent_type: tutorial\nsource_url: https://motherduck.com/learn-more/why-choose-parquet-table-file-format\nindexed_at: '2025-11-25T09:56:57.618325'\ncontent_hash: 2fb6399ae13aa7e1\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO LEARN](https://motherduck.com/learn-more/)\n\n# Parquet File Format: What It Is, Benefits, and Alternatives\n\n11 min readBY\n\n[Aditya Somani](https://motherduck.com/authors/aditya-aomani/)\n\n![Parquet File Format: What It Is, Benefits, and Alternatives](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdataframe_3_3e66a00165.png&w=3840&q=75)\n\nIn data engineering, efficient storage and processing of massive datasets is often very important. As data volumes continue to grow, organizations are turning to innovative file formats and table structures to optimize their data warehousing and analytics workflows. Among these, the Apache Parquet file format has emerged as a popular choice for its columnar storage layout and impressive compression capabilities.\n\nParquet, designed for use with large-scale data processing frameworks like Apache Spark and Hadoop, has gained significant traction in both the \"big data\" and \"small data\" communities. Its ability to enable efficient data storage and retrieval, coupled with its support for complex data types and schema evolution, makes it a compelling option for a wide range of data-intensive applications.\n\nIn this article, we will dive deep into the world of Parquet, exploring its key features, benefits, and use cases. We will also compare Parquet to other popular file formats and table structures, such as Apache Iceberg and Delta Lake, to help you make informed decisions when designing your data architecture.\n\n## Key Takeaways\n\n- **What is Parquet**: An open-source columnar storage file format for efficient analytics.\n- **Core Benefits**: Superior compression, faster query performance (via column pruning and predicate pushdown), and schema evolution support.\n- **Common Alternatives**: Compared to row-based (CSV, Avro), columnar (ORC), and table formats (Iceberg, Delta Lake).\n- **DuckDB & MotherDuck**: Parquet integrates seamlessly with DuckDB for high-performance SQL queries directly on Parquet files.\n\n## What is Parquet?\n\nParquet is a [columnar storage file format](https://motherduck.com/learn-more/columnar-storage-guide/). When data engineers ask 'what is a Parquet file?', the simple answer is that it's a file that stores data in columns, not rows. This Parquet data format is designed for efficient data processing, particularly in the context of big data applications. Developed as part of the Apache Hadoop ecosystem, Parquet has gained widespread adoption due to its ability to optimize storage and query performance.\n\n### Columnar Storage Format\n\nOne of the key characteristics of Parquet is its columnar storage layout. Unlike traditional row-based formats, Parquet organizes data by columns rather than rows. This means that all values for a particular column are stored contiguously on disk, enabling faster retrieval and better compression ratios.\n\nBy storing data in a columnar fashion, Parquet excels at analytical queries that involve reading a subset of columns from a large dataset. This is particularly beneficial for data warehousing and business intelligence scenarios, where queries often focus on specific columns rather than entire rows.\n\nThe columnar storage format also allows for more efficient compression techniques. Since values within a column tend to be of the same data type and exhibit similar patterns, Parquet can apply appropriate compression algorithms to achieve higher compression ratios compared to row-based formats. This reduction in storage footprint translates to lower storage costs and faster query execution, as less data needs to be read from disk.\n\n![Post Image](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fstorage_comparison_1_5c87b9f5c1.svg&w=3840&q=75)\n\n### Open Source and Widely Supported\n\nParquet is an open-source project governed by the Apache Software Foundation, ensuring its transparency, community-driven development, and long-term sustainability. The open-source nature of Parquet has fostered a vibrant ecosystem, with contributions from industry leaders and a growing user base.\n\nOne of the key advantages of Parquet's open-source status is its wide support across various big data processing frameworks. Major platforms like Apache Spark, Apache Hadoop, and Presto have native support for reading and writing Parquet files, making it easy to integrate Parquet into existing data pipelines.\n\nThis broad compatibility ensures interoperability between different tools and systems, allowing organizations to leverage Parquet across their data ecosystem. It also mitigates the risk of vendor lock-in, as Parquet can be used with a variety of open-source and commercial solutions.\n\n## Why Use Parquet?\n\nParquet stands out in data processing environments for its ability to efficiently manage large datasets while ensuring high performance. Its unique approach to data storage enhances retrieval speeds and optimizes storage efficiency. This is achieved through intelligent data organization and compression strategies that minimize unnecessary data handling.\n\n### Efficient Compression and Encoding\n\nParquet's architecture benefits from exceptional data compression capabilities, essential for managing extensive datasets. Instead of treating data uniformly, Parquet leverages specialized encoding techniques such as dictionary, run-length, and delta encoding to optimize storage. These methods reduce the data footprint, translating into cost savings and improved access speeds. By minimizing I/O operations, Parquet enhances query performance, making it ideal for data-intensive tasks.\n\n![Post Image](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fencoding_compression_e1bd66680f.svg&w=3840&q=75)\n\n### Schema Evolution and Nested Data Types\n\nParquet is designed to handle evolving data structures with ease, supporting seamless schema modifications. This flexibility allows for the addition or alteration of columns without disrupting existing workflows, ensuring continuous data integrity. Parquet's proficiency with complex data structures\u2014like nested fields\u2014facilitates versatile data modeling. Its adaptability is vital for accommodating dynamic business requirements and integrating diverse datasets.\n\n### Predicate Pushdown and Column Pruning\n\nParquet effectively supports techniques like predicate pushdown and column pruning, which are crucial for optimizing data queries. By bringing filtering and aggregation operations closer to the storage layer, Parquet reduces the amount of data that needs processing. This approach not only speeds up queries but also lowers computational demands, enabling swift responses. Consequently, Parquet allows data professionals to execute complex queries on extensive datasets efficiently, providing timely and actionable insights.\n\n## Parquet Alternatives and Comparisons\n\nNavigating the landscape of data storage solutions reveals a variety of formats, each offering unique strengths tailored to specific needs. While Parquet remains a prominent choice, exploring its alternatives can provide valuable insights into selecting the best fit for particular data environments.\n\n### CSV Files\n\nCSV files are a straightforward, text-based format that organizes data in a tabular form with rows and columns separated by delimiters. This simplicity makes CSV highly portable and easy to use for data exchange across different systems. However, it lacks advanced functionalities like compression and schema management, which limits its efficiency in handling large-scale datasets.\n\n### Apache ORC\n\nApache ORC stands out with its columnar storage capabilities, optimized for high-performance data processing tasks. It excels in compressing and managing large datasets efficiently, offering features like ACID transactions that ensure data integrity during updates and queries. ORC's tight integration with Hive-specific functionalities makes it an appealing choice for Hadoop ecosystems, enabling seamless operations within Hive data warehouses.\n\n### Apache Avro\n\nApache Avro offers a flexible, row-based format that emphasizes efficient data serialization and schema evolution. Avro provides robust support for evolving schemas, allowing changes without requiring data rewrites\u2014ideal for applications needing frequent schema updates and cross-system data exchange. Its compact binary serialization format enhances data transmission efficiency across distributed systems.\n\n### Delta Lake and Apache Iceberg\n\nDelta Lake and Apache Iceberg build on the strengths of Parquet, introducing advanced table management features. Delta Lake, with its seamless Spark integration, offers capabilities like ACID transactions and data versioning, supporting both batch and streaming data processing. This makes it suitable for environments requiring consistent data updates and real-time analytics.\n\nApache Iceberg is crafted to optimize large-scale data lake operations, providing comprehensive support for multiple file formats, including Parquet. It facilitates complex data management with features like schema evolution and time travel, ensuring data consistency and adaptability across diverse processing engines and storage solutions.\n\n## Choosing the Right Format\n\nSelecting the optimal data format necessitates a keen understanding of your data access patterns. For workloads requiring extensive data scans with a focus on specific data attributes, leveraging formats designed for efficient data retrieval can significantly enhance performance. In contrast, for scenarios where frequent updates or point-specific data access is essential, a format that facilitates rapid row-level operations may be more advantageous.\n\n### Ecosystem Compatibility\n\nThe integration of a data format with existing systems is paramount for operational efficiency. Formats that align well with current data processing tools and frameworks simplify the implementation process and reduce potential disruptions. Evaluating the collective expertise of your team with a given format can inform the decision-making process, ensuring a smooth transition and effective utilization of the chosen technology.\n\n### Data Volume and Scalability\n\nAnticipating the trajectory of data growth and volume is critical in format selection. For environments managing substantial datasets, selecting a format that balances storage efficiency with retrieval speed is crucial. Formats equipped with advanced scalability features, such as those designed for extensive data management, offer robust solutions to handle burgeoning data lakes. These options ensure data integrity while maintaining high performance as data scales.\n\n## How you read and write parquet files in DuckDB\n\nDuckDB provides extensive support for Parquet files, seamlessly integrating them into workflows that demand high-performance data analysis. Users can take advantage of DuckDB's capabilities while preserving the structural and performance benefits of Parquet.\n\n### Reading Parquet Files\n\nWith DuckDB, accessing Parquet files becomes a streamlined process, eliminating the need for data importation. This approach offers:\n\n- **Direct File Access**: DuckDB enables SQL queries directly on Parquet files, allowing immediate data exploration without additional data loading steps.\n- **Optimized Column Retrieval**: By leveraging Parquet's columnar nature, DuckDB efficiently processes column-specific queries, ensuring rapid data retrieval and minimizing unnecessary data scans.\n\nThese features facilitate efficient and effective data analysis, maximizing the performance advantages inherent in Parquet's design.\n\n### Writing Parquet Files\n\nExporting data to Parquet format in DuckDB is straightforward, ensuring that the advantages of Parquet's compression and structuring are retained:\n\n- **SQL-Driven Export**: Users can export results to Parquet with simple SQL commands, preserving the data's integrity and organization.\n- **Advanced Compression Options**: DuckDB supports the use of Parquet's compression methods during export, optimizing file size for storage efficiency.\n\nThese functionalities enable seamless data management from analysis to storage, ensuring that DuckDB users can fully leverage the benefits of both technologies in their data workflows.\n\n## How the support for Parquet files in DuckDB differs from the support for other open file formats like Iceberg and Delta Lake\n\nDuckDB excels in its support for Parquet files, offering direct integration that maximizes the potential of Parquet\u2019s columnar storage benefits. This integration underscores DuckDB\u2019s focus on performance and efficiency. In comparison, its handling of other formats like Iceberg and Delta Lake involves additional considerations due to their distinct capabilities in data management and transactional features.\n\n### Parquet Integration in DuckDB\n\nDuckDB\u2019s architecture is purpose-built to leverage the column-oriented design of Parquet. This allows for seamless data processing and minimizes overhead, enhancing query performance. The optimized handling of Parquet files means that DuckDB can efficiently execute analytical tasks by taking full advantage of Parquet\u2019s compression and encoded structures, making it an ideal choice for applications requiring swift data interrogation.\n\n### Handling Iceberg and Delta Lake\n\nWhen it comes to supporting Iceberg and Delta Lake, DuckDB must navigate the complexities introduced by these formats\u2019 advanced features. Both formats provide robust table management functionalities that extend beyond simple storage solutions and may not be fully supported.\n\n### Performance Considerations\n\nThe performance dynamics between DuckDB and these formats are influenced by the specific use cases they address. Parquet\u2019s design aligns well with DuckDB\u2019s strengths, facilitating efficient data retrieval processes. In contrast, utilizing Iceberg or Delta Lake may necessitate additional performance considerations, particularly when dealing with their advanced metadata and transaction management capabilities, which may impact processing efficiency based on workload demands.\n\nDuckDB\u2019s approach to these formats showcases its flexibility and adaptability, offering robust support for Parquet while accommodating the advanced features of Iceberg and Delta Lake for more complex data management needs.\n\nThe ongoing transformation in data storage and management highlights Parquet's role as a cornerstone of [modern data strategies](https://motherduck.com/learn-more/modern-data-warehouse-playbook/). Its architecture aligns with the demands of large-scale data operations, offering efficient solutions that meet the needs of complex analytics. This adaptability ensures Parquet remains integral across diverse computing environments, valued for its performance and reliability.\n\nEmerging table formats, including Delta Lake and Apache Iceberg, introduce enhancements that build on Parquet's capabilities. These formats offer advanced features like transactional support and time travel, catering to sophisticated data management requirements. Their ability to efficiently handle massive datasets makes them indispensable for organizations seeking comprehensive data governance solutions.\n\nDuckDB's integration of Parquet reflects its commitment to high-performance data processing, optimizing Parquet's strengths for analytical workflows. The platform navigates the complexities of newer formats like Iceberg and Delta Lake, ensuring robust support while maintaining efficient data operations. This synergy exemplifies how DuckDB leverages Parquet's strengths alongside the advanced capabilities of modern table formats to deliver effective data solutions.\n\nAs the data landscape continues to evolve, understanding the intricacies of Parquet and its alternatives is crucial for making informed decisions about your data architecture. By leveraging the strengths of Parquet and integrating it with powerful tools like DuckDB, you can unlock the full potential of your data and drive meaningful insights. If you're ready to experience the benefits of Parquet and explore a collaborative data warehousing solution, [join us at MotherDuck and get started today](https://app.motherduck.com/?auth_flow=signup).\n\n### TABLE OF CONTENTS\n\n[Key Takeaways](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#key-takeaways)\n\n[What is Parquet?](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#what-is-parquet)\n\n[Why Use Parquet?](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#why-use-parquet)\n\n[Parquet Alternatives and Comparisons](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#parquet-alternatives-and-comparisons)\n\n[Choosing the Right Format](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#choosing-the-right-format)\n\n[How you read and write parquet files in DuckDB](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#how-you-read-and-write-parquet-files-in-duckdb)\n\n[How the support for Parquet files in DuckDB differs from the support for other open file formats like Iceberg and Delta Lake](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/#how-the-support-for-parquet-files-in-duckdb-differs-from-the-support-for-other-open-file-formats-like-iceberg-and-delta-lake)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\n## FAQS\n\n### What is the Parquet file format?\n\nApache Parquet is an open-source columnar storage file format designed for efficient data processing. Unlike row-based formats like CSV, it stores data by columns, enabling faster query performance and superior compression for large-scale analytical workloads.\n\n### What are the main benefits of using Parquet files?\n\nParquet's main benefits include efficient data compression and encoding, schema evolution support, and performance optimization. It uses techniques like predicate pushdown and column pruning to speed up analytical queries by only reading the data that is needed.\n\n### What are the alternatives to Parquet?\n\nCommon alternatives to Parquet include row-based formats like CSV and Avro, and other columnar formats like Apache ORC. Newer table formats like Delta Lake and Apache Iceberg often use Parquet as their underlying file storage format while adding transactional guarantees.\n\n## Additional Resources\n\n[Docs\\\\\n\\\\\nWhat is a Data Warehouse?](https://motherduck.com/learn-more/what-is-a-data-warehouse/) [Docs\\\\\n\\\\\nWhat is OLAP?](https://motherduck.com/learn-more/what-is-OLAP/) [Docs\\\\\n\\\\\nLoading Data into MotherDuck with Parquet](https://motherduck.com/docs/key-tasks/loading-data-into-motherduck/loading-data-md-python/) [Docs\\\\\n\\\\\nUse DuckDB to Convert CSV to Parquet](https://duckdbsnippets.com/snippets/6/quickly-convert-a-csv-to-parquet-bash-function) [Video\\\\\n\\\\\nQuerying Parquet Files on S3 with DuckDB](https://www.youtube.com/watch?v=fZj6kTwXN1U)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 212,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 212
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/leveraging-duckdb-and-delta-lake-together.md",
  [2m  â”‚[0m     "content": "---\ntitle: leveraging-duckdb-and-delta-lake-together\ncontent_type: event\nsource_url: https://motherduck.com/videos/leveraging-duckdb-and-delta-lake-together\nindexed_at: '2025-11-25T20:43:46.309156'\ncontent_hash: 67aa2584b6d25b06\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nLeveraging DuckDB and Delta Lake Together - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Leveraging DuckDB and Delta Lake Together](https://www.youtube.com/watch?v=WzTRW_j-dpI)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=WzTRW_j-dpI&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 1:02:11\n\n\u2022Live\n\n\u2022\n\nYouTubeSourcesQuack & Code\n\n# Leveraging DuckDB and Delta Lake Together\n\n2024/07/24\n\nQuack & Code on table formats and Delta Lake with Holly Smith from Databricks. Can you use DuckDB and MotherDuck to query your data lakehouse based on Delta Lake? Yes! And Delta Lake provides a better format than Parquet.\n\n## Related Videos\n\n[![\" pg_duckdb: Ducking awesome analytics in Postgres\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2F0kc_W5o0tcg_HD_03880f61fb.jpg&w=3840&q=75)](https://motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres/)\n\n[2025-06-12](https://motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres/)\n\n### [pg\\_duckdb: Ducking awesome analytics in Postgres](https://motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres)\n\nSupercharge your Postgres analytics! This talk shows how the pg\\_duckdb extension accelerates your slowest queries instantly, often with zero code changes. Learn practical tips and how to use remote columnar storage for even more speed.\n\nTalk\n\nSources\n\n[![\"Can DuckDB revolutionize the data lake experience?\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmaxresdefault_2074c1c09e.jpg&w=3840&q=75)\\\\\n\\\\\n16:37](https://motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience/)\n\n[2024-11-22](https://motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience/)\n\n### [Can DuckDB revolutionize the data lake experience?](https://motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience)\n\nMehdi explores DuckDB as a catalog for Data Lake and Lakehouse pattern. He'll define what we mean by \"data catalog\", gives clear examples on how they work and dive into a pragmatic use case with DuckDB & MotherDuck.\n\nYouTube\n\nData Pipelines\n\nSources\n\n[![\"A new paradigm for data visualization with just SQL + Markdown\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmaxresdefault_4b5916378f.jpg&w=3840&q=75)\\\\\n\\\\\n1:00:53](https://motherduck.com/videos/a-new-paradigm-for-data-visualization-with-just-sql-markdown/)\n\n[2024-09-24](https://motherduck.com/videos/a-new-paradigm-for-data-visualization-with-just-sql-markdown/)\n\n### [A new paradigm for data visualization with just SQL + Markdown](https://motherduck.com/videos/a-new-paradigm-for-data-visualization-with-just-sql-markdown)\n\nCome to this Quack&Code where Mehdi will discuss data visualization with DuckDB/MotherDuck, specifically focusing on Evidence! Archie, who is building evidence.dev, will join us to share his wisdom on charts ;-)\n\nBI & Visualization\n\nYouTube\n\nQuack & Code\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 127,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 127
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search for more specific information about data loading efficiency and storage optimization:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ DuckDB.*storage|native storage|MotherDuck storage

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     "content": "**Step 1: Export Data from BigQuery to Google Cloud Storage (GCS)**\n\nThe most efficient way to get data out of BigQuery is to export it to a columnar format like [Parquet](https://parquet.apache.org/) in an object storage bucket. Parquet is highly compressed and performs exceptionally well with DuckDB. You can do this with a single SQL command in the BigQuery console.\n\nCopy code\n\n```\nEXPORT DATA\nOPTIONS(\nuri='gs://your-gcs-bucket/path/to/export/data_*.parquet',\nformat='PARQUET',\noverwrite=true\n) AS\nSELECT * FROM your_project.your_dataset.your_table;\n```\n\n\nThis command will export the contents of `your_table`\n\ninto one or more Parquet files in the specified GCS bucket.\n\n**Step 2: Transfer Data to a Compatible Object Store**\n\nWhile DuckDB can read directly from GCS, for loading data into MotherDuck's managed storage, it is often easiest to use a cloud object store like Amazon S3 or Cloudflare R2. You can use a tool like [ rclone](https://rclone.org/) or cloud-native transfer services to move the Parquet files from your GCS bucket to an S3 bucket.\n\n**Step 3: Load Data into MotherDuck**\n\nOnce your data is in S3, loading it into a MotherDuck table is a simple `CREATE TABLE AS`\n\nstatement. From your local DuckDB CLI or Python script connected to MotherDuck, you can run the following command. You will first need to configure DuckDB with your AWS credentials to access the S3 bucket. For more details, see our documentation on [loading data from S3](https://motherduck.com/docs/integrations/cloud-storage/amazon-s3/).\n\nCopy code\n\n```\nCREATE TABLE my_new_table AS\nSELECT * FROM 's3://your-s3-bucket/path/to/export/data_*.parquet';\n```\n\n\nDuckDB will automatically infer the schema from the Parquet files, parallelize the download from S3, and load the data efficiently into your new table in MotherDuck's managed storage. This simple, SQL-based approach avoids complex ingestion pipelines and allows you to move terabytes of data with just a few commands.\n\n## The Right Tool for Your Data Scale\n\nBigQuery is a powerful and impressive technology, but its architecture and pricing are optimized for true \"big data.\" For the vast and growing world of \"medium data,\" from gigabytes to tens of terabytes, its model can introduce unnecessary cost and complexity. MotherDuck, built on the fast-growing DuckDB ecosystem, offers an architecture designed specifically for this scale. Its focus on compute locality, developer experience, and interactive performance provides a simpler, faster, and more cost-effective solution for many common analytical workloads.\n\nThe choice is not about finding a universal replacement, but about right-sizing your tools. Use the platform whose cost model and performance profile align with your data's scale and your application's requirements. The flexibility of the DuckDB ecosystem is its portability, speed, and analytical power that runs anywhere from a laptop to the cloud, giving lean teams the ability to build powerful data applications without the overhead of a massive data platform.\n\nTo see the difference for yourself:\n\n[Sign up for a free MotherDuck account](https://app.motherduck.com/?auth_flow=signup).- Use the playbook above to load one of your medium-sized Parquet files.",
  [2m  â”‚[0m     "numLines": 50,
  [2m  â”‚[0m     "startLine": 180,
  [2m  â”‚[0m     "totalLines": 258
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": ".kurt/sources/motherduck.com/duckdb-book-summary-chapter2.md:48:[Omitted long matching line]\n.kurt/sources/motherduck.com/product/duckdb-users.md:103:More Details](https://motherduck.com/product/duckdb-users/cloud-database-storage/)\n.kurt/sources/motherduck.com/index.md:179:MotherDuck employs a per-user tenancy and vertical scaling strategy. Users connect to their own MotherDuck Ducklings (DuckDB instances), which are sized (pulse, standard, jumbo, mega, giga) to meet their specific needs. There is also the option for additional Ducklings, through read scaling (explained below), to ensure flexible resource allocation. Ultimately, each Duckling establishes a connection with the central Data Warehouse storage.\n.kurt/sources/motherduck.com/product/app-developers.md:83:DuckDB is designed for analytical workloads where low latency query performance is crucial. With MotherDuck, DuckDB\u2019s small footprint delivers remarkable efficiency without the need for complex infrastructure or configuration. Data is stored in a columnar format that streamlines analytical queries by reducing the data read from storage and into memory.\n.kurt/sources/motherduck.com/videos.md:263:Supercharge your Postgres analytics! This talk shows how the pg\\_duckdb extension accelerates your slowest queries instantly, often with zero code changes. Learn practical tips and how to use remote columnar storage for even more speed.\n.kurt/sources/motherduck.com/glossary/C/C++.md:21:[Omitted long matching line]\n.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md:87:Supercharge your Postgres analytics! This talk shows how the pg\\_duckdb extension accelerates your slowest queries instantly, often with zero code changes. Learn practical tips and how to use remote columnar storage for even more speed.\n.kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md:183:MotherDuck | JDBC, Go, SQLAlchemy, DuckDB SDK | Yes, 10 GB storage, 10 CU hours/month | Interactive analytics, BI, local-first development. | Very Low |\n.kurt/sources/motherduck.com/videos/how-to-bootstrap-a-data-warehouse-with-duckdb.md:113:Supercharge your Postgres analytics! This talk shows how the pg\\_duckdb extension accelerates your slowest queries instantly, often with zero code changes. Learn practical tips and how to use remote columnar storage for even more speed.\n.kurt/sources/motherduck.com/ecosystem.md:477:LlamaIndex.ai integrates with MotherDuck by leveraging DuckDB's capabilities to query structured data efficiently within the MotherDuck cloud data warehouse environment. This integration allows users to combine unstructured data handling from LlamaIndex.ai with the high-performance querying and storage solutions provided by MotherDuck, enabling comprehensive data analysis workflows.](https://motherduck.com/ecosystem/llamaindex/)\n.kurt/sources/motherduck.com/ecosystem/llamaindex.md:25:LlamaIndex.ai integrates with MotherDuck by leveraging DuckDB's capabilities to query structured data efficiently within the MotherDuck cloud data warehouse environment. This integration allows users to combine unstructured data handling from LlamaIndex.ai with the high-performance querying and storage solutions provided by MotherDuck, enabling comprehensive data analysis workflows.\n.kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md:88:So what's MotherDuck? We've taken the open-source DuckDB and built a serverless cloud service around it. Unlike distributed systems that scale out, we scale up on single nodes. This approach makes practical sense when you consider that single-node hardware capabilities (AWS instances with up to 32TB RAM and nearly 900 vCPUs) are growing faster than most datasets. With MotherDuck, you get DuckDB's local speed plus remote querying, persistent storage, and collaboration in the cloud.\n.kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md:25:SQLite inspired the creation of DuckDB, which is a [columnar database](https://motherduck.com/learn-more/columnar-storage-guide/) with vectorized execution enabling large-scale aggregation queries important for dashboarding and business intelligence. DuckDB is often referred to as the \u201cSQLite for Analytics.\"\n.kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md:35:DuckDB is an embedded database management system designed for fast analytical queries and complex workloads. It leverages vectorized query execution and a columnar storage format optimized for [OLAP](https://motherduck.com/learn-more/what-is-OLAP/) (analytical) scenarios. DuckDB offers native integration with popular data science tools like Python, R, and Julia for seamless data analysis.\n.kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md:59:DuckDB outperforms SQLite significantly for analytical queries involving aggregations, joins, and large datasets. SQLite's [performance](https://www.lukas-barth.net/blog/sqlite-duckdb-benchmark/) is optimized for point queries and transactional workloads, while DuckDB shines in complex analytics. DuckDB's vectorized execution and columnar storage enable efficient processing of data in memory and on disk.\n.kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md:89:[Omitted long matching line]\n.kurt/sources/motherduck.com/glossary/storage layer.md:2:title: 'DuckDB Data Engineering Glossary: storage layer'\n.kurt/sources/motherduck.com/learn-more/what-is-duckdb.md:104:- **vs SQLite**: Both DuckDB and SQLite embed easily into applications, but they serve different purposes. DuckDB optimizes for analytical queries and large dataset processing, while SQLite targets transactional workloads and serves as a local data storage solution for applications.\n.kurt/sources/motherduck.com/authors/simon-spati.md:122:Learn how DuckDB and open table formats like Iceberg power a fast, composable analytics stack on affordable cloud storage\n.kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md:113:Supercharge your Postgres analytics! This talk shows how the pg\\_duckdb extension accelerates your slowest queries instantly, often with zero code changes. Learn practical tips and how to use remote columnar storage for even more speed.",
  [2m  â”‚[0m   "numLines": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now I have enough information to synthesize the answer. Let me write the markdown file with the findings:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_1.md

  [96mğŸ”§ TOOL:[0m Bash â†’ cat > /tmp/answer_without_kg_1.md << 'EOF'
# Most Efficient File Formats for Loading Data into MotherDuck

Based on MotherDuck's documentation and content, **Parquet** is the most efficient file format for loading data into MotherDuck, followed by other columnar formats and the native DuckDB format.

## Parquet: The Optimal Choice

Parquet is the recommended format for loading data into MotherDuck for several key reasons:

### Superior Compression and Storage Efficiency
- Parquet uses specialized encoding techniques (dictionary, run-length, and delta encoding) to optimize storage
- The columnar format achieves higher compression ratios compared to row-based formats like CSV
- Reduced storage footprint translates to cost savings and improved access speeds
- Minimizes I/O operations, which enhances query performance

### Performance Optimization
- **Column pruning**: Only reads the specific columns needed for a query, not entire rows
- **Predicate pushdown**: Filters data at the storage layer before processing, reducing data movement
- Columnar storage enables faster retrieval when queries focus on specific columns rather than entire rows
- DuckDB can automatically infer schema, parallelize downloads from S3, and load Parquet files efficiently

### Direct Integration with DuckDB/MotherDuck
- DuckDB's architecture is purpose-built to leverage Parquet's column-oriented design
- Seamless data processing with minimal overhead
- Can query Parquet files directly without importing them first
- Simple SQL-based loading: `CREATE TABLE my_table AS SELECT * FROM 's3://bucket/data_*.parquet';`

## Alternative Efficient Formats

### DuckDB Native Format
- Highly compressed columnar file format
- Can persist many large tables in the same file
- Provides ACID transactional safety and parallel processing
- Supports storing processing logic in views and functions
- Ideal for organizing and managing multiple tables together

### MotherDuck Native Storage
According to MotherDuck's documentation on loading data with Estuary:
- **2-10x faster query performance** compared to DuckLake/Parquet
- Optimized specifically for read/write throughput
- Better caching and regional performance
- Best for silver/gold layer data where maximum query speed is needed

### Delta Lake and Apache Iceberg
- Build on Parquet's strengths while adding advanced table management features
- Provide ACID transactions and data versioning
- Delta Lake noted as providing "a better format than Parquet" for data lakehouse scenarios
- Support schema evolution and time travel capabilities

## Less Efficient Formats

### CSV Files
- Simple and portable but lacks compression and schema management
- Less efficient for large-scale datasets
- DuckDB has a "world-class" CSV reader that can handle even messy CSV files
- Useful for compatibility but should be converted to Parquet for better performance
- Can easily convert: `duckdb -c "COPY (SELECT * FROM 'data.csv') TO 'data.parquet';"`

### JSON Files
- Can be read by DuckDB but less efficient than columnar formats
- Better suited for semi-structured data requirements
- Consider converting to Parquet for analytical workloads

## Best Practices for Loading Data

1. **Export to Parquet first**: When migrating from systems like BigQuery, export to Parquet format in object storage (S3, GCS)
2. **Use object storage**: Store Parquet files in S3 or compatible cloud storage for efficient parallel loading
3. **Leverage DuckDB's native capabilities**: Let DuckDB automatically infer schemas and parallelize downloads
4. **Consider storage location**: Keep raw data in DuckLake/Parquet for openness; move processed data to MotherDuck native storage for maximum query performance
5. **For large CSVs**: Convert to Parquet format to benefit from compression and faster processing

## Sources

- [Parquet File Format: What It Is, Benefits, and Alternatives](https://motherduck.com/learn-more/why-choose-parquet-table-file-format)
- [Simple way to convert CSV and Parquet files](https://motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files)
- [How to Efficiently Load Data into DuckLake with Estuary](https://motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary)
- [BigQuery Alternative: MotherDuck](https://motherduck.com/learn-more/bigquery-alternative-motherduck)
- [Taming file zoos: Data science with DuckDB database files](https://motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files)
- [Leveraging DuckDB and Delta Lake Together](https://motherduck.com/videos/leveraging-duckdb-and-delta-lake-together)
EOF
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m 
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ ls -lh /tmp/answer_without_kg_1.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m -rw-------  1 julien  wheel   4.5K Nov 27 13:59 /tmp/answer_without_kg_1.md
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! I've completed the task. Here's a summary of what I found:
  [94mâ”‚[0m 
  [94mâ”‚[0m ## Summary
  [94mâ”‚[0m 
  [94mâ”‚[0m I searched through the `.kurt/sources/` directory containing 874 documents from motherduck.com and synthesized an answer about the most efficient file formats for loading data into MotherDuck.
  [94mâ”‚[0m 
  [94mâ”‚[0m **Key Finding: Parquet is the most efficient format** for loading data into MotherDuck due to:
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. **Superior compression** - Uses specialized encoding (dictionary, run-length, delta) for better compression than row-based formats
  [94mâ”‚[0m 2. **Performance optimization** - Supports column pruning and predicate pushdown, which minimizes data reading
  [94mâ”‚[0m 3. **Native integration** - DuckDB's architecture is purpose-built for Parquet's columnar format
  [94mâ”‚[0m 
  [94mâ”‚[0m **Other efficient options include:**
  [94mâ”‚[0m - DuckDB native format (highly compressed, ACID transactions)
  [94mâ”‚[0m - MotherDuck native storage (2-10x faster than Parquet for queries)
  [94mâ”‚[0m - Delta Lake/Iceberg (add transactional features on top of Parquet)
  [94mâ”‚[0m 
  [94mâ”‚[0m **Less efficient formats:**
  [94mâ”‚[0m - CSV (simple but lacks compression)
  [94mâ”‚[0m - JSON (better for semi-structured data)
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer has been written to `/tmp/answer_without_kg_1.md` with a comprehensive Sources section citing 6 relevant MotherDuck documents.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 5,371 | Cost: $0.3079
     Cumulative: 5,371 tokens | $0.3079
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent has presented a comprehensive summary and indicated that the task is complete, which signals that no further user input is needed.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 15 | Duration: 94.5s
â•‘    Tokens: 5,371 | Cost: $0.3079
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  LLM Judge score: 1.00

======================================================================
â“ Question 2/10
How does MotherDuck integrate with DuckDB?
======================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 2
â”‚ Search through the .kurt/sources/ directory to answer this question:

How does MotherDuck integrate with DuckDB?

Steps:
1. Search for the most relevant markdown files in .kurt/sources/
2. Read those files and synthesize the answer
3. Write a markdown answer (with a ## Sources section) to: /tmp/answer_without_kg_2.md
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I'll help you find information about how MotherDuck integrates with DuckDB by searching through the source files.
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ MotherDuck.*DuckDB|DuckDB.*MotherDuck|integrate|integration|connect

  [96mğŸ”§ TOOL:[0m Grep â†’ attach.*motherduck|md:|token
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/scaling-duckdb-panel-ondemand.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/take-flight-with-the-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/data-discoverability-secoda-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-power-of-wasm-for-analytics-duckdb-in-the-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-surprising-birth-of-duckdb-ft-co-creator-hannes-muhleisen.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/an-evolving-dag-for-the-llm-world-julia-schottenstein-of-langchain-at-small-data-sf-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/build-bigger-with-small-ai-running-small-models-locally.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-use-cases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/hack-night.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/index.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/community-and-open-source.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/kultura-capital.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/tableau-cloud-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-future-of-bi-bi-as-code-duckdb-impact.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search for more specific content about the integration and also look for documentation files:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Glob â†’ **/*{getting-started,quickstart,connect,integration,docs}*.md

  [96mğŸ”§ TOOL:[0m Grep â†’ ATTACH|attach motherduck|md:my_db|service token
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/blog/getting-started-gis-duckdb.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/blog/getting-started-ducklake-table-format.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/blog/motherduck-vercel-marketplace-native-integration.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/blog/secoda-motherduck-integration-modern-duck-stack.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/blog/spark-ducklake-getting-started.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/getting-started-with-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/product/postgres-integration.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/getting-started-with-motherduck-webinar.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "durationMs": 10,
  [2m  â”‚[0m   "numFiles": 10,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/spark-ducklake-getting-started.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streaming-data-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/secoda-motherduck-integration-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/redshift-files-hunt-for-big-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg to motherduck at motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/postgres-duckdb-options.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/preswald-health-data-analysis.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 15,
  [2m  â”‚[0m   "appliedLimit": 15
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! Now let me read the most relevant files:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/getting-started-with-motherduck.md

  [96mğŸ”§ TOOL:[0m Read â†’ /private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md

  [96mğŸ”§ TOOL:[0m Read â†’ /private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md

  [96mğŸ”§ TOOL:[0m Read â†’ /private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/product/duckdb-users.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/getting-started-with-motherduck.md",
  [2m  â”‚[0m     "content": "---\ntitle: getting-started-with-motherduck\ncontent_type: event\nsource_url: https://motherduck.com/getting-started-with-motherduck\nindexed_at: '2025-11-25T20:36:55.713062'\ncontent_hash: eca528b7b79fabc6\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n# Product Demo: Getting Started with MotherDuck\n\n## Join us for a live demo to learn how to get started with MotherDuck, the cloud SQL analytics data warehouse extending DuckDB to the cloud.\n\nMotherDuck is now Generally Available! In this session, MotherDuck Co-Founder Ryan Boyd and Product Expert Nathaniel Thompson will show you how to maximize MotherDuck's analytics and collaboration features to take flight with your analytics workloads and applications.\n\n### Tuesday, June 18th @ 10AM PDT\n\nRegister now. We're looking forward to seeing you there!\n\n![Getting Started with MotherDuck](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGetting_Started_with_MD_437c0d3476.png&w=3840&q=75)\n\nGetting Started with MotherDuck\n\n## Getting Started with MotherDuck\n\nEmail address \\*\n\nNext\n\nAlready registered?Join here\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 37,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 37
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     "content": "---\ntitle: duckdb-motherduck-for-beginners-your-ultimate-guide\ncontent_type: event\nsource_url: https://motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide\nindexed_at: '2025-11-25T20:44:57.276294'\ncontent_hash: 9ac8939ea24395a7\nhas_code_examples: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nDuckDB & MotherDuck for Beginners: Your Ultimate Guide - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[DuckDB & MotherDuck for Beginners: Your Ultimate Guide](https://www.youtube.com/watch?v=WYV8hvJOAQE)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Why am I seeing this?](https://support.google.com/youtube/answer/9004474?hl=en)\n\n[Watch on](https://www.youtube.com/watch?v=WYV8hvJOAQE&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 36:27\n\n\u2022Live\n\n\u2022\n\nYouTube\n\n# DuckDB & MotherDuck for Beginners: Your Ultimate Guide\n\n2025/02/21\n\n## Why DuckDB is Revolutionizing Data Analytics\n\nDuckDB has experienced explosive growth in popularity, with download statistics showing remarkable adoption rates, particularly in the Python ecosystem. This open-source analytical database is designed to handle everything from quick data exploration to complex data pipelines, and it's already trusted by multiple companies in production environments.\n\n## Understanding the Small and Medium Data Revolution\n\nThe tech industry has long operated under the assumption that analytics requires big data infrastructure. However, this paradigm is being challenged by three key insights:\n\n1. **Most workloads aren't big data**: According to data from AWS Redshift and other cloud analytical databases, approximately 83% of users work with query sizes up to 1TB, and 94% work with data below 10TB.\n\n2. **Modern hardware is incredibly powerful**: Today's single machines can handle up to 24TB of memory on AWS, making distributed systems unnecessary for most use cases.\n\n3. **Distributed systems are expensive**: They require significant IO network traffic for coordination, and the development experience is considerably more complex.\n\n\nDuckDB capitalizes on these realities by enabling users to work with large datasets on a single machine, whether locally on a laptop or in the cloud via a scale-up strategy.\n\n## How DuckDB Works: The In-Process Advantage\n\nDuckDB is an open-source, in-process analytical database written in C++, designed as a single self-contained binary with all dependencies included. This architecture sets it apart from traditional databases.\n\n### Traditional Database Architecture\n\nDatabases typically fall into two categories:\n\n- **OLTP (Online Transaction Processing)**: Databases like PostgreSQL and MySQL, optimized for handling transactions with small datasets. Query times typically range from 1-10 milliseconds.\n\n- **OLAP (Online Analytical Processing)**: Databases like BigQuery and Snowflake, built for analytical queries processing large datasets. Query times can range from 100 milliseconds to several minutes.\n\n\nMost traditional databases use a client-server architecture where the database runs as a separate process, and applications connect to it through SQL queries.\n\n### The In-Process Revolution\n\nIn-process databases run directly within the application process itself. While SQLite pioneered this approach for OLTP workloads, DuckDB introduces something new: an in-process OLAP database optimized for analytical workloads.\n\nThis design enables DuckDB to:\n\n- Run on virtually any platform (laptops to cloud workflows)\n- Integrate seamlessly with any programming language\n- Execute in web browsers via WebAssembly\n- Eliminate network overhead for local operations\n\n## Getting Started with DuckDB\n\n### Installation and Basic Usage\n\nDuckDB can be installed through various methods:\n\n- Direct binary download for CLI usage\n- Package managers (Homebrew for macOS)\n- Language-specific packages (Python, R, Java, etc.)\n\nThe CLI provides a powerful interface for data exploration:\n\n```sql\nCopy code\n\n-- Simple query reading from S3\nFROM 's3://bucket/path/to/file.parquet' LIMIT 5;\n```\n\n### Key Features in Action\n\n**Friendly SQL Dialect**: DuckDB extends standard SQL with productivity enhancements, such as the FROM-first syntax shown above.\n\n**Automatic File Format Detection**: DuckDB automatically detects and handles various file formats including Parquet, CSV, JSON, Iceberg, and Delta Lake.\n\n**Extension System**: DuckDB's functionality is modular through extensions. Core extensions (like HTTPFS for S3 access) are auto-loaded when needed, while community extensions can be installed manually.\n\n## Data Persistence and the DuckDB File Format\n\nBy default, DuckDB operates in-memory, but it offers powerful persistence options:\n\n### Creating and Managing Databases\n\n```sql\nCopy code\n\n-- Attach or create a database\nATTACH 'mydatabase.ddb';\n\n-- Create a table from a query\nCREATE TABLE mytable AS SELECT * FROM source_data;\n```\n\n### The DuckDB File Format\n\nDuckDB's native file format (.ddb or .db) is:\n\n- Self-contained (all tables and metadata in one file)\n- ACID-compliant\n- Highly compressed\n- Optimized for analytical workloads\n\n### Exporting Data\n\nDuckDB supports seamless data export:\n\n```sql\nCopy code\n\n-- Export to CSV\nCOPY (SELECT * FROM mytable) TO 'output.csv';\n```\n\n## Managing Secrets and Authentication\n\nDuckDB includes a comprehensive secret management system for secure cloud access:\n\n```sql\nCopy code\n\n-- Create temporary secret using AWS credential chain\nCREATE SECRET (\n    TYPE S3,\n    PROVIDER credential_chain\n);\n```\n\nThis approach supports:\n\n- AWS SSO authentication\n- Temporary and persistent secrets\n- Multiple cloud providers\n- Secure credential storage\n\n## Scaling to the Cloud with MotherDuck\n\nMotherDuck supercharges DuckDB by transforming it from a single-player to a multiplayer analytics experience. The integration is remarkably simple:\n\n### Connecting to MotherDuck\n\n```sql\nCopy code\n\n-- Connect to MotherDuck with one command\nATTACH 'md:';\n```\n\nAuthentication requires only a MotherDuck token, which can be set as an environment variable.\n\n### Key MotherDuck Features\n\n**Dual Execution**: MotherDuck enables intelligent query execution, automatically determining whether to run computations locally or in the cloud based on data location and query requirements.\n\n**Database Sharing**: Create and share cloud databases with simple commands:\n\n```sql\nCopy code\n\n-- Create a share\nCREATE SHARE myshare FROM mydatabase;\n```\n\n**Performance Benefits**: Leveraging cloud infrastructure provides:\n\n- High-bandwidth connections to cloud storage\n- Elimination of local network bottlenecks\n- Seamless collaboration features\n\n### Cloud Storage Integration\n\nMotherDuck dramatically improves performance when querying cloud storage. In benchmarks, queries that take 11 seconds locally can complete in just 2 seconds when leveraging MotherDuck's cloud infrastructure and network proximity to storage.\n\n## The Complete Analytics Toolkit\n\nDuckDB and MotherDuck together provide:\n\n- **Simplicity**: One-command installation and cloud connection\n- **Flexibility**: Run anywhere from browsers to cloud environments\n- **Performance**: Lightning-fast analytical queries on small to medium datasets\n- **Integration**: Native support for numerous file formats and cloud providers\n- **Collaboration**: Easy data sharing and team workflows\n\nWhether you're analyzing data on your laptop, scaling computations in the cloud, or building production data pipelines, DuckDB offers a modern approach to analytics that challenges traditional big data assumptions while delivering exceptional performance and developer experience.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Data-based: Going Beyond the Dataframe\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FData_based_f32745b461.png&w=3840&q=75)](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n[2025-11-20](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n### [Data-based: Going Beyond the Dataframe](https://motherduck.com/videos/going-beyond-the-dataframe)\n\nLearn how to turbocharge your Python data work using DuckDB and MotherDuck with Pandas. We walk through performance comparisons, exploratory data analysis on bigger datasets, and an end-to-end ML feature engineering pipeline.\n\nWebinar\n\nPython\n\nAI, ML and LLMs\n\n[![\"Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FHex_Webinar_778e3959e4.png&w=3840&q=75)](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n[2025-11-14](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n### [Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck)\n\nAI isn't here to replace data work, it's here to make it better. Watch this webinar to see how Hex and MotherDuck build AI workflows that prioritize context, iteration, and real-world impact.\n\nWebinar\n\nAI, ML and LLMs\n\n[![\"Lies, Damn Lies, and Benchmarks\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FLies_Damn_Lies_and_Benchmarks_Thumbnail_404db1bf46.png&w=3840&q=75)](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n[2025-10-31](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n### [Lies, Damn Lies, and Benchmarks](https://motherduck.com/videos/lies-damn-lies-and-benchmarks)\n\nWhy do database benchmarks so often mislead? MotherDuck CEO Jordan Tigani discusses the pitfalls of performance benchmarking, lessons from BigQuery, and why your own workload is the only benchmark that truly matters.\n\nStream\n\nInterview\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 301,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 301
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     "content": "---\ntitle: bringing-duckdb-to-the-cloud-dual-execution-explained\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained\nindexed_at: '2025-11-25T20:45:20.295187'\ncontent_hash: ab4030806c409492\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nBringing DuckDB to the Cloud: Dual Execution Explained - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Bringing DuckDB to the Cloud: Dual Execution Explained](https://www.youtube.com/watch?v=2eVNm4A8sLw)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=2eVNm4A8sLw&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 1:08:11\n\n\u2022Live\n\n\u2022\n\nYouTubeQuack & CodeMotherDuck Features\n\n# Bringing DuckDB to the Cloud: Dual Execution Explained\n\n2024/06/28\n\nBringing DuckDB's analytical power to the cloud requires more than just running it on a server. While DuckDB excels at processing data on a local machine, the dynamics of data analysis change when dealing with cloud-scale data, collaboration, and shared resources. This shift introduces challenges around [security](https://motherduck.com/learn-more/secure-startup-data-warehouse/), concurrent access, and performance, which is where a purpose-built cloud architecture becomes essential.\n\nMotherDuck, a modern cloud data warehouse, is built on DuckDB to solve these challenges. It extends DuckDB\u2019s capabilities without forking the open source project, creating a hybrid system that intelligently balances local and cloud computing. In a conversation with MotherDuck founding engineer Stephanie, she explained the architecture and the innovative query model, known as **dual execution**, that makes this possible.\n\n* * *\n\n## The MotherDuck Architecture: Beyond Hosted DuckDB\n\nA common misconception is that MotherDuck is simply DuckDB hosted in the cloud. The reality is a more sophisticated system designed to overcome the limitations of using a locally-optimized engine in a distributed environment. MotherDuck\u2019s architecture is built on three key components and a core philosophy of not forking DuckDB.\n\nInstead of maintaining a separate version, MotherDuck leverages DuckDB\u2019s powerful **extension system**. This allows MotherDuck to add new capabilities at multiple layers, including the SQL parser, optimizer, and storage interface, while staying current with the latest open source DuckDB releases. This tight integration means users benefit from the rapid innovation of the DuckDB community almost immediately.\n\nThe architecture can be broken down into three main layers:\n\n1. **The Client Extension**: This is how MotherDuck integrates with the DuckDB ecosystem. Whether a user is working in the CLI, a Python script, or a JDBC connection, a simple `ATTACH` command connects their local DuckDB instance to the MotherDuck cloud. This extension is even used to run DuckDB in the browser via WASM for the MotherDuck UI, enabling client-side processing to reduce latency and cloud compute costs.\n2. **The Compute Layer**: In the cloud, queries are processed by containerized DuckDB instances, fondly called \"ducklings.\" These compute resources are scaled based on user needs, providing the necessary CPU and memory to handle complex analytical queries on large datasets.\n3. **The Storage Layer**: DuckDB's native file format is optimized for a single writer on a local file system. This model is not well-suited for the cloud, where multiple users need to read and write to the same database concurrently. To solve this, MotherDuck implemented a differential storage system that maps a logical database file to append-only snapshot layers in cloud object storage. This design is cloud-friendly, enabling efficient in-place updates and forming the foundation for features like database sharing and time travel.\n\n* * *\n\n## Enabling Collaboration with Database Sharing and Secure Credentials\n\nThis unique architecture transforms DuckDB from a \"single-player\" tool into a collaborative \"multiplayer\" platform. One of the most significant advantages is **database sharing**. Instead of emailing SQL scripts or passing around large database files, team members within the same organization can grant query access to their databases. This streamlines collaboration, ensuring everyone works from a consistent and up-to-date version of the data.\n\nSecurity is another critical aspect of any cloud data platform. MotherDuck provides a **centralized Secret Manager**, co-designed with the DuckDB team. Users can create persistent, encrypted secrets for accessing external data sources like AWS S3, Google Cloud Storage, or Delta Lake. Once a secret is created in MotherDuck, it can be reused across different clients and sessions without needing to expose or reconfigure credentials on each local machine.\n\n* * *\n\n## Understanding Dual Execution\n\nThe most innovative feature of MotherDuck's architecture is its **dual execution model**. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\n\nThe process was demonstrated with a query joining two tables. When both tables reside in a MotherDuck database, the query is executed entirely in the cloud. An `EXPLAIN` plan for this query reveals that all operators, from the table scans to the final join, are marked as **remote**. The only local operation is downloading the final result set to the client.\n\nThe real power of dual execution becomes apparent when a query involves both local and cloud data. In the demonstration, one of the remote tables was copied to a local Parquet file. The query was then modified to join this local file with the table that remained in the cloud.\n\nThe `EXPLAIN` plan for this new query showed a mix of local and remote operations. The scan of the Parquet file and its associated processing happened **locally**, leveraging the user's machine. The scan of the cloud table happened **remotely** on MotherDuck's compute. The system then efficiently transferred only the necessary intermediate data to complete the join. This hybrid approach avoids needlessly uploading the local file to the cloud or downloading the entire remote table, resulting in faster and more efficient queries.\n\n* * *\n\n## Taking Control with Manual Execution Overrides\n\nWhile MotherDuck's optimizer is designed to make the most efficient choice automatically, some situations require manual control. For this, users can use the `md_run` parameter within scan functions like `read_parquet()` or `read_csv()`.\n\nBy setting `md_run = 'local'`, a user can force the scan of a remote S3 file to be executed on their local client. This involves downloading the data, which might be desirable if the data needs to be processed near the user's location or if they prefer using local credentials. Conversely, setting `md_run = 'remote'` forces the operation to run in the MotherDuck cloud.\n\nThis control is especially useful for performance tuning. A direct comparison showed that querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds). By pushing the computation to the data's location, MotherDuck minimizes network I/O and delivers results more quickly.\n\n* * *\n\n## A Symbiotic Future for Local and Cloud Analytics\n\nMotherDuck's architecture is a thoughtful extension of DuckDB's core principles, adapting its local-first power for the demands of the cloud. The dual execution model provides a flexible and efficient bridge between a user's laptop and cloud data, optimizing workloads in a way that pure-cloud or pure-local systems cannot.\n\nThis relationship is symbiotic. As the largest production user of DuckDB, MotherDuck continuously pushes the engine to its limits, uncovering opportunities for improvement and contributing enhancements back to the open source project. This collaboration ensures that both DuckDB and MotherDuck will continue to evolve, offering a powerful and seamless analytical experience that spans from local exploration to cloud-scale production.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Ficeberg_video_thumbnail_adfdabe3d6.jpg&w=3840&q=75)\\\\\n\\\\\n46:26](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n[2025-06-12](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n### [Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse)\n\nBuilding a data stack means choosing between easy SaaS and complex open-source. Apache Iceberg is a middle ground, but its catalog is a hurdle. New tools now simplify using Iceberg with DuckDB to create a flexible, local data lakehouse.\n\nMotherDuck Features\n\n[![\"Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FG_Se_B_Sox_AW_Fg_HD_f4fddaa9ab.jpg&w=3840&q=75)](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n[2025-04-23](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n### [Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly)\n\nHamilton Ulmer shares insights from MotherDuck's Instant SQL Mode, exploring how real-time query result previews eliminate the traditional write-run-debug cycle through client-side parsing and DuckDB-WASM caching.\n\nSQL\n\nTalk\n\nMotherDuck Features\n\n[![\"DuckDB: Run dbt build with sub-second execution times\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_03_26_at_3_40_16_PM_c6e3a8096f.png&w=3840&q=75)\\\\\n\\\\\n24:06](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n[2025-03-13](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n### [DuckDB: Run dbt build with sub-second execution times](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times)\n\nWhether you're new to DuckDB or looking to optimize your workflows, this session will provide practical insights to help you leverage its full potential.\n\nYouTube\n\nData Pipelines\n\ndbt\n\nMotherDuck Features\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 184,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 184
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     "content": "---\ntitle: duckdb-users\ncontent_type: product_page\nsource_url: https://motherduck.com/product/duckdb-users/\nindexed_at: '2025-11-25T20:15:52.565114'\ncontent_hash: dfb4ed9cd3383b9d\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n# local + Cloud analytics,  backed by the same database\n\nA duckdb-powered data warehouse for production grade analytics\n\n[Get Started](https://motherduck.com/get-started/)\n\nLEARN MORE\n\nWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuck\n\n## Transform Solo DuckDB Into a Collaborative Powerhouse\n\nDuckDB's in-process nature means your data stays trapped on your machine. MotherDuck eliminates this limitation with a simple \"md:\" prefix.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fshare-data-not-files.db8baefd.png&w=3840&q=75)\n\n### Share Data, Not Files\n\nNo more exporting files or managing S3 buckets to share your work. Create database shares with specific team members while maintaining the familiar DuckDB experience.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmultiple-users-one-database.0b9d7dc7.png&w=3840&q=75)\n\n### Multiple Users, One Database\n\nEnable simultaneous access for Data Engineers, Analytics teams, and Scientists to the same datasets. Each user gets their own dedicated compute instance \u201cduckling\u201d for concurrent queries without conflicts.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-bring-the-flock-together.be57c6e9.png&w=2048&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-bring-the-flock-together.be57c6e9.png&w=2048&q=75)\n\n## Local Speed with Cloud-Scale Production Power\n\nEnjoy the best of both worlds with a seamless dev/prod workflow for your data. Develop locally for immediate feedback, then deploy identical workloads to the cloud with zero code changes.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Frapid-iteration-zero-cost.ff94f539.png&w=3840&q=75)\n\n### Rapid Iteration at Zero Cost\n\nExperiment locally with lightning-fast feedback cycles without spending a dime. When your solution is ready, your exact same queries work in the cloud by simply adding \"md:\" to your connection.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftest-safely-deploy-confidently.ed329fbf.png&w=3840&q=75)\n\n### Test Safely, Deploy Confidently\n\nPrototype complex data transformations without risking expensive cloud resources. This local dev, cloud prod model ensures what works locally will perform at scale in production.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fship-data-solutions-faster.bdfdd521.png&w=3840&q=75)\n\n### Ship Data Solutions Faster\n\nEliminate the friction between development and deployment environments. Your team can innovate with quick local tests and push proven solutions to cloud production seamlessly.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-scalable-node-architecture.91c200dd.png&w=1920&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-scalable-node-architecture.91c200dd.png&w=1920&q=75)\n\n## Familiar Interface with Intelligent Superpowers\n\nMotherDuck maintains the DuckDB interface you love while adding thoughtful, AI-backed intelligent assistance. Experience enhanced productivity with features designed for SQL experts and business users.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fvisual-insights-at-a-glance.60ec6358.png&w=3840&q=75)\n\n### Visual Insights at a Glance\n\nBeautiful visualizations help you understand complex data patterns immediately. MotherDuck's design affordances make exploration intuitive while preserving the power of DuckDB's analytical capabilities.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fautomatic-error-correction.fe59afe0.png&w=3840&q=75)\n\n### Automatic Error Correction\n\n\"Fix It\" repairs common syntax errors with a single click, eliminating frustration from missing commas or quotes. This intelligent assistance ensures everyone on your team can work productively with SQL, regardless of expertise level.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fnatural-language-to-sql.72331bc3.png&w=3840&q=75)\n\n### Natural Language to SQL\n\nNever written SQL before? Non-technical team members can contribute with \"PROMPT QUERY\" turning plain English into correct SQL. This feature democratizes data access by removing the barrier of SQL syntax knowledge.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-ergonomic-and-effortless.b3f51a46.png&w=3840&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-ergonomic-and-effortless.b3f51a46.png&w=3840&q=75)\n\n## Features\n\n[![Cloud database storage feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-cloud.3e9c0a0a.png&w=640&q=75)\\\\\n**Cloud database storage** \\\\\nManage your DuckDB database catalog in the cloud for easy scaling and collaboration.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/cloud-database-storage/)\n\n[![SQL analytics engine feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-analytics.f04308e0.png&w=640&q=75)\\\\\n**SQL analytics engine** \\\\\nEfficiently run the same SQL queries on data stored locally, in MotherDuck or in your data lake.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/sql-analytics-engine/)\n\n[![Simplified database sharing feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-db.11e966ef.png&w=640&q=75)\\\\\n**Simplified database sharing** \\\\\nCreate a shareable snapshot of your data, which your colleagues can easily attach and query in MotherDuck.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/simplified-database-sharing/)\n\n[![Dual query execution feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-query.28419698.png&w=640&q=75)\\\\\n**Dual query execution** \\\\\nUse idle compute on your laptop in concert with the cloud to increase speed and lower cost.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/dual-query-execution/)\n\n[![Notebook-like UI feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-ui.c08b01e4.png&w=1080&q=75)\\\\\n**Notebook-like UI** \\\\\nUse the web to browse the data catalog, write SQL, filter and sort results and share data.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/notebook-like-ui/)\n\n[![Strong DuckDB ecosystem feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-ecosystem.0ab95f7c.png&w=640&q=75)\\\\\n**Strong DuckDB ecosystem** \\\\\nUse with 25+ tools in the modern data stack for import, orchestration and business intelligence.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/strong-duckdb-ecosystem/)\n\n## Architecture\n\nManaged DuckDB-in-the-cloud\n\n## Ecosystem\n\nModern Duck Stack\n\n### CLOUD DATA WAREHOUSE\n\n### Sources\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpostgres-sql.d737f4f5.png&w=750&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Famazon-s3.72386dfc.png&w=640&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsalesforce.35d52b31.png&w=384&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fstripe.2160e881.png&w=384&q=75)\n\n[**Business Intelligence**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftableau.0ec523e2.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpower-bi.f3563826.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fomni.cb7aa381.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmetabase.5f8fe44e.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Business+Intelligence) [**Ingestion**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffivetran.26f3817d.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fairbyte.8371d2f0.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Festuary.babad369.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlthub.e9892b97.png&w=384&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Ingestion) [**Data Science & AI**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhex.275dad7d.png&w=256&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fjupyter.2a6af3de.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fcolab.30ab10af.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmarimo.b699c73d.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Data+Science+%26+AI) [**Reverse ETL**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fcensus.352f1d69.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Reverse+ETL) [**Transformation**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdbt.fd2184d1.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftobiko.d0e3d1e5.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsql-mesh.6fceb012.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Transformation) [**Dev Tools**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdatagrip.f48eba23.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpuppy-graph.848400c6.png&w=750&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdagster.e1970a7c.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Dev+Tools)\n\n[**Orchestration**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fairflow.7f70081a.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdagster.e1970a7c.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fprefect.fd6371b3.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fkestra.aa10acfc.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Orchestration)\n\n[**Data Quality**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmonte-carlo.2143f962.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsecoda.9b7e86fb.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsoda.59e5aa02.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fgreat-expectations.c544f1ef.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Data+Quality)\n\n## Case Studies\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nOur data pipelines used to take eight hours. Now they're taking eight minutes, and I see a world where they take eight seconds. This is why we made the big bet on DuckDB and MotherDuck. It's only possible with DuckDB and MotherDuck.\n\n[READ MORE](https://motherduck.com/case-studies/finqore/)\n\n![Jim O\u2019Neill's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fjim_oneil_photo_930c637f49.jpg&w=3840&q=75)\n\nJim O\u2019Neill\n\nCo-founder and CTO at FinQore\n\nOur data pipelines used to take eight hours. Now they're taking eight minutes, and I see a world where they take eight seconds. This is why we made the big bet on DuckDB and MotherDuck. It's only possible with DuckDB and MotherDuck.\n\n[READ MORE](https://motherduck.com/case-studies/finqore/)\n\nWith MotherDuck, it only took an hour to get an app up and running, which was pretty cool. It uses 18 million rows of data, but once it\u2019s loaded, we have all this interactivity right on the website. And since it's all coming from MotherDuck, we can create a shareable snapshot of our data that others can attach to and query.\n\n[READ MORE](https://motherduck.com/case-studies/dominik-moritz/)\n\n![Dominik Moritz's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdominik_berlin_f8a31170af.jpg&w=3840&q=75)\n\nDominik Moritz\n\nProfessor at Carnegie Mellon University\n\nWith MotherDuck, it only took an hour to get an app up and running, which was pretty cool. It uses 18 million rows of data, but once it\u2019s loaded, we have all this interactivity right on the website. And since it's all coming from MotherDuck, we can create a shareable snapshot of our data that others can attach to and query.\n\n[READ MORE](https://motherduck.com/case-studies/dominik-moritz/)\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nOur data pipelines used to take eight hours. Now they're taking eight minutes, and I see a world where they take eight seconds. This is why we made the big bet on DuckDB and MotherDuck. It's only possible with DuckDB and MotherDuck.\n\n[READ MORE](https://motherduck.com/case-studies/finqore/)\n\n![Jim O\u2019Neill's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fjim_oneil_photo_930c637f49.jpg&w=3840&q=75)\n\nJim O\u2019Neill\n\nCo-founder and CTO at FinQore\n\nOur data pipelines used to take eight hours. Now they're taking eight minutes, and I see a world where they take eight seconds. This is why we made the big bet on DuckDB and MotherDuck. It's only possible with DuckDB and MotherDuck.\n\n[READ MORE](https://motherduck.com/case-studies/finqore/)\n\nWith MotherDuck, it only took an hour to get an app up and running, which was pretty cool. It uses 18 million rows of data, but once it\u2019s loaded, we have all this interactivity right on the website. And since it's all coming from MotherDuck, we can create a shareable snapshot of our data that others can attach to and query.\n\n[READ MORE](https://motherduck.com/case-studies/dominik-moritz/)\n\n![Dominik Moritz's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdominik_berlin_f8a31170af.jpg&w=3840&q=75)\n\nDominik Moritz\n\nProfessor at Carnegie Mellon University\n\nWith MotherDuck, it only took an hour to get an app up and running, which was pretty cool. It uses 18 million rows of data, but once it\u2019s loaded, we have all this interactivity right on the website. And since it's all coming from MotherDuck, we can create a shareable snapshot of our data that others can attach to and query.\n\n[READ MORE](https://motherduck.com/case-studies/dominik-moritz/)\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n## Get in Touch\n\nWe\u2019d love to learn more about what you\u2019re building. A Product Expert will fly right back to you with resources to help you take flight.\n\nFirst Name\n\nLast Name\n\nE-mail\n\nCompany\n\nLeave us a message.\n\nThis site is protected by reCAPTCHA and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply.\n\nSubmit\n\n![Duck looking up](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduck-looking-up.1c763865.png&w=3840&q=75)\n\nAuthorization Response\n\nreCAPTCHA\n\nRecaptcha requires verification.\n\n[Privacy](https://www.google.com/intl/en/policies/privacy/) \\- [Terms](https://www.google.com/intl/en/policies/terms/)\n\nprotected by **reCAPTCHA**\n\n[Privacy](https://www.google.com/intl/en/policies/privacy/) \\- [Terms](https://www.google.com/intl/en/policies/terms/)",
  [2m  â”‚[0m     "numLines": 403,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 403
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me read a couple more files to get comprehensive information:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/index.md

  [96mğŸ”§ TOOL:[0m Grep â†’ extension.*motherduck|client.*integration|simple.*connect
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/index.md",
  [2m  â”‚[0m     "content": "---\ntitle: motherduck.com\ncontent_type: event\nsource_url: https://motherduck.com/\nindexed_at: '2025-11-25T20:45:26.921185'\ncontent_hash: 121e46675911459e\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/#)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n# Making Big Data Feel Small\n\nDuckDB cloud data warehouse scaling to terabytes\nfor customer-facing analytics and BI\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\n[LEARN MORE](https://motherduck.com/product/)\n\n[BOOK A DEMO](https://motherduck.com/contact-us/product-expert/)\n\n[instant sql blog post](https://motherduck.com/blog/introducing-instant-sql/)\n\n![DuckDB Book in the clouds](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduckdb-book-in-the-clouds.d9791904.png&w=3840&q=75)\n\n## \"DuckDB In Action\" Book for Free\n\nGet the complete book for free in your inbox!\n\nWork Email\n\nSubscribe to MotherDuck news\n\nSubscribe to DuckDB ecosystem newsletter\n\nSUBMIT\n\nFINALLY:\n\nA database you don't hate\n\n## Why It's Better\n\n![Scales Vertically and Horizontally to handle Spikey Workloads](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-why-motherduck-scales.b290b9bc.png&w=3840&q=75)\n\nScales Vertically and Horizontally to handle Spikey Workloads\n\n![Reads YOUR data.  (plaintext, json, parquet, iceberg, xls, csv)](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-why-motherduck-reads.51fdba51.png&w=3840&q=75)\n\nReads YOUR data. (plaintext, json, parquet, iceberg, xls, csv)\n\n![Fast, columnar central data storage optimized for analytics](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-why-motherduck-fast.bf89edac.png&w=3840&q=75)\n\nFast, columnar central data storage optimized for analytics\n\n![Run locally, deploy to the cloud for reliability and collaboration. Fits into your workflow](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-why-motherduck-run-locally.3c9900e8.png&w=3840&q=75)\n\nRun locally, deploy to the cloud for reliability and collaboration. Fits into your workflow\n\n## Who is it for?\n\nAnalytics that works for everyone\n\n[![Software Engineers](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-coding-duck.c6c8efa9.png&w=3840&q=75)\\\\\n**Software Engineers** \\\\\nWho ended up with a big data problem](https://motherduck.com/product/app-developers) [![Data Scientists](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-scientist-duck.5805adc0.png&w=3840&q=75)\\\\\n**Data Scientists** \\\\\nWho ended up having to do data engineering](https://motherduck.com/product/duckdb-users) [![Data Engineers](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-plumber-duck.27784453.png&w=3840&q=75)\\\\\n**Data Engineers** \\\\\nWith slow, brittle pipelines](https://motherduck.com/product/data-teams)\n\n## Who is it for?\n\nAnalytics that works for everyone\n\n[![Software Engineers](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-coding-duck.c6c8efa9.png&w=3840&q=75)\\\\\n**Software Engineers** \\\\\nWho ended up with a big data problem](https://motherduck.com/product/app-developers) [![Data Scientists](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-scientist-duck.5805adc0.png&w=3840&q=75)\\\\\n**Data Scientists** \\\\\nWho ended up having to do data engineering](https://motherduck.com/product/duckdb-users) [![Data Engineers](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhomepage-plumber-duck.27784453.png&w=3840&q=75)\\\\\n**Data Engineers** \\\\\nWith slow, brittle pipelines](https://motherduck.com/product/data-teams)\n\nUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse Cases\n\n## Use Cases\n\n!['For Data Teams' duck illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdata-teams-ducks.6fea331d.png&w=3840&q=75)!['For App Devs' duck illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fapp-devs-ducks.51eb4ea9.png&w=3840&q=75)\n\n![For Data Teams ducks](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdata-teams-ducks.6fea331d.png&w=3840&q=75)\n\n### Data Warehousing\n\nIs your data all over the place? Start making sense of your business by bringing it together for internal business intelligence and analytics. Build pure SQL pipelines, share data and quickly collaborate with your team.\n\n[How it Works](https://motherduck.com/docs/getting-started/data-warehouse/)\n\n[Case Studies](https://motherduck.com/case-studies/)\n\n![For App Devs ducks](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fapp-devs-ducks.51eb4ea9.png&w=3840&q=75)\n\n### Customer-facing Analytics\n\nUnlike traditional BI, customer-facing analytics is built directly into your product for end users. It delivers near real-time, low-latency insights at scale \u2014 think milliseconds, not minutes \u2014 and must handle thousands to millions of concurrent queries. MotherDuck's architecture, from per-user tenancy to Wasm support, is designed for the unique requirements of Customer-Facing Analytics to drive increased user engagement directly in your app experience.\n\n[How it Works](https://motherduck.com/docs/getting-started/customer-facing-analytics/)\n\n[Case Studies](https://motherduck.com/case-studies/)\n\n## How We Scale\n\n### Duckling Sizes\n\nMotherDuck\u2019s per-user tenancy model gives each user an isolated\nduckling\n\n![Tooltip Duckling illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftooltip-duckling-illus.32805477.png&w=3840&q=75)\n\nA Duckling is a dedicated DuckDB instance for each user, ensuring optimal performance and scalability in data analytics.\n\n(DuckDB instance) in one of five sizes to enable individual, user-level configuration.\n\n![MotherDuck Assets illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmotherduck-assets-illus.f80662b9.png&w=3840&q=75)\n\n![Pulse Duckling](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduckling-1-placeholder.8fd25135.png&w=3840&q=75)\n\n![Pulse Instance type illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Finstance-type-illus-pulse.8c39b9cc.png&w=3840&q=75)\n\nPulse\n\nOur smallest instance, perfect for ad-hoc analytics tasks\n\n![Standard Duckling](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduckling-2-placeholder.e3942696.png&w=3840&q=75)\n\n![Standard Instance type illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Finstance-type-illus-standard.3d0ae9e9.png&w=3840&q=75)\n\nStandard\n\nBuilt to handle common data warehouse workloads, including loads and transforms\n\n![Jumbo Duckling](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduckling-3-placeholder.9e54ea44.png&w=3840&q=75)\n\n![Jumbo Instance type illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Finstance-type-illus-jumbo.b77c7bca.png&w=3840&q=75)\n\nJumbo\n\nFor larger data warehouse workloads with many transformations or complex aggregations\n\n![Mega Duckling](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduckling-4-placeholder.356f25ee.png&w=3840&q=75)\n\n![Mega Instance type illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Finstance-type-illus-mega.cd4cd44f.png&w=3840&q=75)\n\nMega\n\nAn extremely large instance for when you need complex transformations done quickly\n\n![Giga Duckling](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduckling-5-placeholder.7ca13555.png&w=3840&q=75)\n\n![Giga Instance type illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Finstance-type-illus-giga.3228dda6.png&w=3840&q=75)\n\nGiga\n\nLargest instances enable the toughest transformations to run faster\n\n[Contact us](https://motherduck.com/contact-us/product-expert/)\n\n![Magnifying glass duckling sizes illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmagnifying-glass-duckling-sizes.5db83c77.png&w=3840&q=75)\n\nTake a closer look at\n\n[DUCKLING SIZES](https://motherduck.com/product/pricing/#instance-types)\n\n### Per-user tenancy   and vertical scaling\n\nMotherDuck employs a per-user tenancy and vertical scaling strategy. Users connect to their own MotherDuck Ducklings (DuckDB instances), which are sized (pulse, standard, jumbo, mega, giga) to meet their specific needs. There is also the option for additional Ducklings, through read scaling (explained below), to ensure flexible resource allocation. Ultimately, each Duckling establishes a connection with the central Data Warehouse storage.\n\n![Per-user tenancy      and vertical scaling](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhow-we-scale-per-user-tenancy.3641ecda.png&w=3840&q=75)\n\n### Read Scaling\n\nMotherDuck's read scaling capabilities allow users to connect via a BI Tool to dedicated Ducklings that function as read replicas. These read replicas can be provisioned in various sizes (pulse, standard, jumbo, mega or giga) to accommodate different needs. Ultimately, these read replicas connect to the Data Warehouse storage, enabling efficient handling of read operations.\n\n![Read Scaling](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhow-we-scale-read-scaling.a2333854.png&w=3840&q=75)\n\n## Ecosystem\n\nModern Duck Stack\n\n### CLOUD DATA WAREHOUSE\n\n### Sources\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpostgres-sql.d737f4f5.png&w=750&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Famazon-s3.72386dfc.png&w=640&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsalesforce.35d52b31.png&w=384&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fstripe.2160e881.png&w=384&q=75)\n\n[**Business Intelligence**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftableau.0ec523e2.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpower-bi.f3563826.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fomni.cb7aa381.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmetabase.5f8fe44e.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Business+Intelligence) [**Ingestion**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffivetran.26f3817d.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fairbyte.8371d2f0.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Festuary.babad369.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlthub.e9892b97.png&w=384&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Ingestion) [**Data Science & AI**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhex.275dad7d.png&w=256&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fjupyter.2a6af3de.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fcolab.30ab10af.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmarimo.b699c73d.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Data+Science+%26+AI) [**Reverse ETL**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fcensus.352f1d69.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Reverse+ETL) [**Transformation**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdbt.fd2184d1.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftobiko.d0e3d1e5.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsql-mesh.6fceb012.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Transformation) [**Dev Tools**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdatagrip.f48eba23.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpuppy-graph.848400c6.png&w=750&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdagster.e1970a7c.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Dev+Tools)\n\n[**Orchestration**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fairflow.7f70081a.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdagster.e1970a7c.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fprefect.fd6371b3.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fkestra.aa10acfc.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Orchestration)\n\n[**Data Quality**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmonte-carlo.2143f962.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsecoda.9b7e86fb.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsoda.59e5aa02.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fgreat-expectations.c544f1ef.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Data+Quality)\n\nMotherDuck with DuckDB was by far the fastest of the OLAP platforms we evaluated - both in the cloud and run on our developer's machines - bridging price and performance and greatly increasing productivity.\n\n[READ MORE](https://motherduck.com/case-studies/saasworks/)\n\n![Jim O\u2019Neill's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fjim_oneil_photo_930c637f49.jpg&w=3840&q=75)\n\nJim O\u2019Neill\n\nCTO and Co-founder at FinQore\n\nMotherDuck with DuckDB was by far the fastest of the OLAP platforms we evaluated - both in the cloud and run on our developer's machines - bridging price and performance and greatly increasing productivity.\n\n[READ MORE](https://motherduck.com/case-studies/saasworks/)\n\nWe used to do analytics in a MySQL database with all of our daily device, telemetry, and image processing data. There was no way to scale that further using MySQL. With MotherDuck, we\u2019re finally starting to find patterns in our data to help customers grow produce more effectively.\n\n[READ MORE](https://motherduck.com/case-studies/gardyn/)\n\n![Rob Teeuwen's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2F1689248039646_d0c68f49b7.jpeg&w=3840&q=75)\n\nRob Teeuwen\n\nData Scientist at Gardyn\n\nWe used to do analytics in a MySQL database with all of our daily device, telemetry, and image processing data. There was no way to scale that further using MySQL. With MotherDuck, we\u2019re finally starting to find patterns in our data to help customers grow produce more effectively.\n\n[READ MORE](https://motherduck.com/case-studies/gardyn/)\n\nBringing MotherDuck into our data stack has been a game changer. We're reducing friction and supercharging our dev and ops experience by leveraging the DuckDB interoperability across local and cloud.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nBringing MotherDuck into our data stack has been a game changer. We're reducing friction and supercharging our dev and ops experience by leveraging the DuckDB interoperability across local and cloud.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nMoving to MotherDuck, a billed by-the-second cloud offering is a no-brainer for us, considering the elegance and efficiency of a single node system compared to traditional OLAP solutions.\n\n[ATM.COM](https://atm.com/)\n\n![Nico Ritschel's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fnico_ritschel_photo_41ba132b33.jpg&w=3840&q=75)\n\nNico Ritschel\n\nDirector of Engineering at atm.com\n\nMoving to MotherDuck, a billed by-the-second cloud offering is a no-brainer for us, considering the elegance and efficiency of a single node system compared to traditional OLAP solutions.\n\n[ATM.COM](https://atm.com/)\n\nInstant SQL in MotherDuck will save me the misery of having to try and wrangle SQL in my BI tool where iteration speed can be very slow. This lets me get the data right earlier in the process and with faster feedback than waiting for a chart to render, or having to clear an analytics cache.\n\n[DashFuel](https://www.getdashfuel.com/)\n\n![Mike McClannahan's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmike_e96491a8eb.jpeg&w=3840&q=75)\n\nMike McClannahan\n\nCTO, DashFuel\n\nInstant SQL in MotherDuck will save me the misery of having to try and wrangle SQL in my BI tool where iteration speed can be very slow. This lets me get the data right earlier in the process and with faster feedback than waiting for a chart to render, or having to clear an analytics cache.\n\n[DashFuel](https://www.getdashfuel.com/)\n\nMotherDuck proving once again they are not constrained by what is easy or common. Instant SQL will not only enable complex query construction for those who find the task daunting (everyone?), it'll help past me communicate with future me.\n\n[DashFuel](https://www.getdashfuel.com/)\n\n![SJ Browne's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fsj_contemplating_6411ae5dcf.jpg&w=3840&q=75)\n\nSJ Browne\n\nSoftware Engineer at DashFuel\n\nMotherDuck proving once again they are not constrained by what is easy or common. Instant SQL will not only enable complex query construction for those who find the task daunting (everyone?), it'll help past me communicate with future me.\n\n[DashFuel](https://www.getdashfuel.com/)\n\nMotherDuck is the GOAT\n\n[READ MORE](https://motherduck.com/case-studies/reflex-careers-gig-economy-retail/)\n\n![Nate Hamm's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimage_15820c5f78.png&w=3840&q=75)\n\nNate Hamm\n\nSr Software Developer at Reflex\n\nMotherDuck is the GOAT\n\n[READ MORE](https://motherduck.com/case-studies/reflex-careers-gig-economy-retail/)\n\nWe found that DuckDB and MotherDuck are amazing tools for small data teams like ours.\n\n[READ MORE](https://motherduck.com/case-studies/dosomething-non-profit-tco-cost-savings/)\n\n![Dave Crusoe's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2F1515814825678_e2e8b2e5e7.jpeg&w=3840&q=75)\n\nDave Crusoe\n\nVP of Product & Engineering at DoSomething\n\nWe found that DuckDB and MotherDuck are amazing tools for small data teams like ours.\n\n[READ MORE](https://motherduck.com/case-studies/dosomething-non-profit-tco-cost-savings/)\n\nDuckDB has a lot of folks in the data community excited\u2014it's a new and unique tool in practitioners' toolboxes and has captured the imaginations of many. MotherDuck's efforts bringing DuckDB to the cloud will broaden the reach of this promising technology.\n\n[DBT LABS](https://www.getdbt.com/)\n\n![Tristan Handy's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Ftristan_handy_photo_ef2233a588.jpg&w=3840&q=75)\n\nTristan Handy\n\nCEO & Founder at dbt Labs\n\nDuckDB has a lot of folks in the data community excited\u2014it's a new and unique tool in practitioners' toolboxes and has captured the imaginations of many. MotherDuck's efforts bringing DuckDB to the cloud will broaden the reach of this promising technology.\n\n[DBT LABS](https://www.getdbt.com/)\n\nMotherDuck is insanely performant, and there\u2019s no infrastructure to manage. The cost of experimentation is incredibly low \u2014 which means we can afford to ask more questions and build more features, faster.\n\n[READ MORE](https://motherduck.com/case-studies/kultura-capital/)\n\n![Kristov Paulus's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_05_08_at_10_29_30_AM_ac663c67b9.png&w=3840&q=75)\n\nKristov Paulus\n\nFounder, Chief Investment Officer at Kultura Capital\n\nMotherDuck is insanely performant, and there\u2019s no infrastructure to manage. The cost of experimentation is incredibly low \u2014 which means we can afford to ask more questions and build more features, faster.\n\n[READ MORE](https://motherduck.com/case-studies/kultura-capital/)\n\nI just onboarded some non-technical users to MotherDuck, and I can\u2019t imagine having done so in a different system with this level of ease and lack of intimidation. Between the \u2018Filter\u2019 button, Column Explorer, and the FROM syntax - they feel empowered to answer many of their own questions! \ud83d\ude42\n\n[READ MORE](https://motherduck.com/blog/dosomething-motherduck-data-warehouse-ROI/)\n\n![Sahil Gupta's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fsahil_portrait_600bd2844e.png&w=3840&q=75)\n\nSahil Gupta\n\nSenior Data Engineer at dosomething.org\n\nI just onboarded some non-technical users to MotherDuck, and I can\u2019t imagine having done so in a different system with this level of ease and lack of intimidation. Between the \u2018Filter\u2019 button, Column Explorer, and the FROM syntax - they feel empowered to answer many of their own questions! \ud83d\ude42\n\n[READ MORE](https://motherduck.com/blog/dosomething-motherduck-data-warehouse-ROI/)\n\nMotherDuck with DuckDB was by far the fastest of the OLAP platforms we evaluated - both in the cloud and run on our developer's machines - bridging price and performance and greatly increasing productivity.\n\n[READ MORE](https://motherduck.com/case-studies/saasworks/)\n\n![Jim O\u2019Neill's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fjim_oneil_photo_930c637f49.jpg&w=3840&q=75)\n\nJim O\u2019Neill\n\nCTO and Co-founder at FinQore\n\nMotherDuck with DuckDB was by far the fastest of the OLAP platforms we evaluated - both in the cloud and run on our developer's machines - bridging price and performance and greatly increasing productivity.\n\n[READ MORE](https://motherduck.com/case-studies/saasworks/)\n\nWe used to do analytics in a MySQL database with all of our daily device, telemetry, and image processing data. There was no way to scale that further using MySQL. With MotherDuck, we\u2019re finally starting to find patterns in our data to help customers grow produce more effectively.\n\n[READ MORE](https://motherduck.com/case-studies/gardyn/)\n\n![Rob Teeuwen's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2F1689248039646_d0c68f49b7.jpeg&w=3840&q=75)\n\nRob Teeuwen\n\nData Scientist at Gardyn\n\nWe used to do analytics in a MySQL database with all of our daily device, telemetry, and image processing data. There was no way to scale that further using MySQL. With MotherDuck, we\u2019re finally starting to find patterns in our data to help customers grow produce more effectively.\n\n[READ MORE](https://motherduck.com/case-studies/gardyn/)\n\nBringing MotherDuck into our data stack has been a game changer. We're reducing friction and supercharging our dev and ops experience by leveraging the DuckDB interoperability across local and cloud.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nBringing MotherDuck into our data stack has been a game changer. We're reducing friction and supercharging our dev and ops experience by leveraging the DuckDB interoperability across local and cloud.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nMoving to MotherDuck, a billed by-the-second cloud offering is a no-brainer for us, considering the elegance and efficiency of a single node system compared to traditional OLAP solutions.\n\n[ATM.COM](https://atm.com/)\n\n![Nico Ritschel's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fnico_ritschel_photo_41ba132b33.jpg&w=3840&q=75)\n\nNico Ritschel\n\nDirector of Engineering at atm.com\n\nMoving to MotherDuck, a billed by-the-second cloud offering is a no-brainer for us, considering the elegance and efficiency of a single node system compared to traditional OLAP solutions.\n\n[ATM.COM](https://atm.com/)\n\nInstant SQL in MotherDuck will save me the misery of having to try and wrangle SQL in my BI tool where iteration speed can be very slow. This lets me get the data right earlier in the process and with faster feedback than waiting for a chart to render, or having to clear an analytics cache.\n\n[DashFuel](https://www.getdashfuel.com/)\n\n![Mike McClannahan's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmike_e96491a8eb.jpeg&w=3840&q=75)\n\nMike McClannahan\n\nCTO, DashFuel\n\nInstant SQL in MotherDuck will save me the misery of having to try and wrangle SQL in my BI tool where iteration speed can be very slow. This lets me get the data right earlier in the process and with faster feedback than waiting for a chart to render, or having to clear an analytics cache.\n\n[DashFuel](https://www.getdashfuel.com/)\n\nMotherDuck proving once again they are not constrained by what is easy or common. Instant SQL will not only enable complex query construction for those who find the task daunting (everyone?), it'll help past me communicate with future me.\n\n[DashFuel](https://www.getdashfuel.com/)\n\n![SJ Browne's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fsj_contemplating_6411ae5dcf.jpg&w=3840&q=75)\n\nSJ Browne\n\nSoftware Engineer at DashFuel\n\nMotherDuck proving once again they are not constrained by what is easy or common. Instant SQL will not only enable complex query construction for those who find the task daunting (everyone?), it'll help past me communicate with future me.\n\n[DashFuel](https://www.getdashfuel.com/)\n\n## Our Community\n\n[![MotherDuck profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmedia.licdn.com%2Fdms%2Fimage%2Fv2%2FD560BAQEODuOUOAdoQg%2Fcompany-logo_100_100%2FB56ZfmzIGPG0AQ-%2F0%2F1751923851982%2Fmotherduck_logo%3Fe%3D1762992000%26v%3Dbeta%26t%3DelhKxwqBElgX_PNnhA3vQAu8QdPUILGxxa4doqv60-w&w=96&q=75)\\\\\n\\\\\nMotherDuck\\\\\n\\\\\n@motherduck\\\\\n\\\\\n@duckdb is empowering machine learning use cases! \ud83e\udd16\\\\\nAny good model training starts with some good data exploration.\\\\\n\\\\\n@huggingface now enables SQL queries using DuckDB on any dataset stored on the Hub!](https://twitter.com/motherduck/status/1666804451552935938)\n\n[![DuckDB profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1274363897676521474%2FqgbqYYuV_x96.jpg&w=96&q=75)\\\\\n\\\\\nDuckDB\\\\\n\\\\\n@duckdb\\\\\n\\\\\nDuckDB Co-Creator @hfmuehleisen and MotherDuck Founder @jrdntgn in this new podcast - among other things about the origin story of DuckDB and later MotherDuck:](https://twitter.com/duckdb/status/1649305101570306048)\n\n[![Garrett McClintock profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1438016780060950531%2FCn11Ke8O_400x400.jpg&w=96&q=75)\\\\\n\\\\\nGarrett McClintock\\\\\n\\\\\n@GJMcClintock\\\\\n\\\\\njust built a pipeline using @dltHub and @motherduck and it's very nice. 10/10 experience once you get the hang of it and very scalable.](https://x.com/GJMcClintock/status/1800962608939303002)\n\n[![Andrew Madson profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2F1706413992317_120c2864d9.jpeg&w=96&q=75)\\\\\n\\\\\nAndrew Madson\\\\\n\\\\\nandrew-madsen\\\\\n\\\\\nInstant SQL! This is one of the highlights of Data Council... Instant SQL caches a sample of your table instantly, and synchronously updates the query results, AS YOU TYPE!](https://www.linkedin.com/posts/andrew-madson_duckdb-instant-sql-this-is-one-of-the-activity-7321023818074894336-sbar?utm_source=share&utm_medium=member_desktop&rcm=ACoAAACw0KEBx7lMe9uvdRJxJyQifAb-I4X7yQA)\n\n[![KKovacs [\u30b9] profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F995607718508589058%2FtQPO3yiD_400x400.jpg&w=96&q=75)\\\\\n\\\\\nKKovacs \\[\u30b9\\]\\\\\n\\\\\n@kkovacs\\\\\n\\\\\nI don\u2019t want to say that #DuckDB is a silver bullet because there is no such thing, but it\u2019s surprisingly projectile-shaped, very shiny, and killed all the various werewolves (ok, data problems) I\u2019ve thrown at it](https://twitter.com/kkovacs/status/1801607688889012587)\n\n[![evidence profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1699896475864510464%2F_XFRsLSv_400x400.jpg&w=96&q=75)\\\\\n\\\\\nevidence\\\\\n\\\\\n@evidence\\_dev\\\\\n\\\\\nDuckDB is increasingly being built into every data tool, but DuckDB isn't designed to be used as a cloud data warehouse\\\\\n\\\\\n@motherduck\\\\\nis solving that problem, and they\u2019re doing it in a delightful, uniquely DuckDB way...](https://twitter.com/evidence_dev/status/1801630536252768584)\n\n[![Zach Wilson profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmedia.licdn.com%2Fdms%2Fimage%2Fv2%2FD5603AQGLd4zOUXaJJg%2Fprofile-displayphoto-shrink_200_200%2FB56ZcbuHVJHUAY-%2F0%2F1748516763636%3Fe%3D1762992000%26v%3Dbeta%26t%3DXmqD42PALO0Cur0sGLHL8Wo_3RMwSo9tGHycq0jpFI8&w=96&q=75)\\\\\n\\\\\nZach Wilson\\\\\n\\\\\neczachly\\\\\n\\\\\nMatt Martin and I did a runtime benchmark of Apache Spark vs DuckDB (by MotherDuck)\\\\\nWe found that data sets under 20 gigabytes ran about 100 times faster on DuckDB than they did on Apache Spark!\\\\\n\\\\\nThis benchmark uses plain parquet files and COUNT distinct to truly measure performance without skipping any data so we know the differences are correct.\\\\\n\\\\\nHere's the complete writeup: https://lnkd.in/gXuWGWyy](https://www.linkedin.com/posts/eczachly_matt-martin-and-i-did-a-runtime-benchmark-activity-7376304145978310657-GzkR/)\n\n[![Daniel Beach profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2F22616816.fs1.hubspotusercontent-na1.net%2Fhubfs%2F22616816%2Fdaniel%2520beach.jpg&w=96&q=75)\\\\\n\\\\\nDaniel Beach\\\\\n\\\\\n@dataengineeringcentral\\\\\n\\\\\nThis MotherDuck thingy is such a beauty it brings a tear to me old eye. You have no idea how many new tools (maybe you do) I agonize over while trying to bring you the good, the bad, and the ugly.](https://dataengineeringcentral.substack.com/p/honest-review-of-motherduck)\n\n[![Ryan Dolley profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fryan_dolley_a1178e8a04.jpeg&w=96&q=75)\\\\\n\\\\\nRyan Dolley\\\\\n\\\\\nryandolley\\\\\n\\\\\nMotherDuck ... bringing big innovation to SQL to make it more like a real language and less like a 'groovy ancient programming language from the 70s.' Check it out.](https://www.linkedin.com/posts/ryandolley_duckdb-activity-7320941264818884609-1fij?utm_source=share&utm_medium=member_desktop&rcm=ACoAAACw0KEBx7lMe9uvdRJxJyQifAb-I4X7yQA)\n\n[![Robert Lancer profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Frobert_lancer_e5d68edd81.jpeg&w=96&q=75)\\\\\n\\\\\nRobert Lancer\\\\\n\\\\\nrobertlancer\\\\\n\\\\\nThe progress on the MotherDuck UI is amazing! It went from minimal utility to my preferred place to write SQL in a matter of months](https://www.linkedin.com/feed/update/urn:li:ugcPost:7163558312741982208?commentUrn=ur[%E2%80%A6]89847054557184%2Curn%3Ali%3AugcPost%3A7163558312741982208%29)\n\n[![Caleb Fahlgren profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1716604301563289600%2FYycgFNAn_400x400.jpg&w=96&q=75)\\\\\n\\\\\nCaleb Fahlgren\\\\\n\\\\\n@calebfahlgren\\\\\n\\\\\nThe amount of data I am storing in \\\\\n@motherduck rn for a new project is criminal \ud83e\udd23\\\\\nDecided to try it out since I am a big \\\\\n@duckdb user. Is pretty good so far. Have few hundred million records.](https://x.com/calebfahlgren)\n\n[![Simon Sp\u00e4ti \ud83c\udfd4\ufe0f profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1137678291605676032%2FITc3M_ah_400x400.png&w=96&q=75)\\\\\n\\\\\nSimon Sp\u00e4ti \ud83c\udfd4\ufe0f\\\\\n\\\\\n@sspaeti\\\\\n\\\\\nI'm just checking a little about the hosted DuckDB by @motherduck. What a nice user experience. It automatically detects errors, and \\`accept & run\\` fixes common errors. Love it.](https://x.com/sspaeti/status/1798290369110749458)\n\n[![Jacob Matson profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1278773580122054656%2F3SG9rUwl_x96.jpg&w=96&q=75)\\\\\n\\\\\nJacob Matson\\\\\n\\\\\n@matsonj\\\\\n\\\\\nusing sql for data cleaning vs excel (reason 879 to learn sql)\\\\\n\\\\\nlevenstein distance between 590strings in excel (350k combinations): \\\\\nexcel: 25 mins of spinning. \\\\\nduckdb: 1.2s](https://twitter.com/matsonj/status/1669434599095816192)\n\n[![Bob Currier profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fbob_currier_d620fb0c2d.jpeg&w=96&q=75)\\\\\n\\\\\nBob Currier\\\\\n\\\\\nbobcurrier\\\\\n\\\\\nIf you're not familiar with DuckDB, you definitely want to give it a look. I've recently converted all my existing code from using CSV files and SQLite databases for exploratory analysis to parquet files, DuckDB and MotherDuck. The transformation in my workflow has been nothing short of astounding. 10GB of CSV files turned into 1GB of parquet files that are now stored in S3 buckets. Using DuckDB and MotherDuck, I can run exploratory queries seamlessly thanks to the DuckStack with dual execution with local processing and cloud scale. My query development time has decreased by 90%. And I'm loving the team feature of the MotherDuck platform. OCEANCODA is rocking the small data life! #duckdb #python #motherduck #eda #ai #ml (Graphic courtesy of Manning Publications Co. book 'DuckDB in Action.' ) Check it out.](https://www.linkedin.com/feed/update/urn:li:activity:7221573493295910912/)\n\n[![evidence profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1699896475864510464%2F_XFRsLSv_400x400.jpg&w=96&q=75)\\\\\n\\\\\nevidence\\\\\n\\\\\n@evidence\\_dev\\\\\n\\\\\nDuckDB is increasingly being built into every data tool, but DuckDB isn't designed to be used as a cloud data warehouse\\\\\n\\\\\n@motherduck is solving that problem, and they\u2019re doing it in a delightful, uniquely DuckDB way](https://x.com/evidence_dev/status/1801630536252768584)\n\n[![Ethan profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1530999951609606144%2FPhNCpNOa_400x400.jpg&w=96&q=75)\\\\\n\\\\\nEthan\\\\\n\\\\\n@@ethanf\\_17\\\\\n\\\\\nDuckDB is my new best friend for analytics. It's so fast, makes it easy to manipulate dataframes, ingests anything, and exports it to whatever form I want. \\\\\n\\\\\nParquet, Arrow dataframes, and DuckDB are the future for analytics.](https://x.com/ethanf_17/status/1809835395695407529)\n\n[![uwe geercken profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1297147628%2FMyAvatar_x96.png&w=96&q=75)\\\\\n\\\\\nuwe geercken\\\\\n\\\\\n@uweeegeee\\\\\n\\\\\n#duckdb is really impressive: good terminal client, solid, easy and fast import of csv data and impressive speed on a 20mio data set. With automatic schema creation. Easy export to parquet and more. I think I am gonna spend more time on it...](https://twitter.com/uweeegeee/status/1611663518033498117)\n\n[![Etienne Posthumus profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1564546125352271882%2FxFZq7oWV.jpg&w=96&q=75)\\\\\n\\\\\nEtienne Posthumus\\\\\n\\\\\n@epoz\\\\\n\\\\\nI have been kicking the tyres of @motherduck - and it is \ud83e\udd2f\\\\\n\\\\\nBeing able to do hybrid queries over local and humongous remote datasets are incredible.\\\\\n\\\\\nThe potential applications are making my head spin with awe and excitement. Well done.](https://twitter.com/epoz/status/1688924943806013441)\n\n[![Jamin Ball profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1346111891618189312%2FSQzYMMst_x96.jpg&w=96&q=75)\\\\\n\\\\\nJamin Ball\\\\\n\\\\\n@jaminball\\\\\n\\\\\nAnother awesome & insightful post from the @motherduck team. The last few decades have been about scale-out. The next decade will be about scale-up! What you can accomplish with a single machine these days is incredibly impressive](https://twitter.com/jaminball/status/1659324081848864768)\n\n[![Danny Blumenthal profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FDanny_Blumenthal_7bb40ab793.jpeg&w=96&q=75)\\\\\n\\\\\nDanny Blumenthal\\\\\n\\\\\ndpblumenthal\\\\\n\\\\\nOur friends at MotherDuck stand out in the data space for how consistently they keep shipping awesome new features.](https://www.linkedin.com/posts/dpblumenthal_instant-sql-is-here-activity-7321571222222217218-mAm5/?utm_source=share&utm_medium=member_desktop&rcm=ACoAAACw0KEBx7lMe9uvdRJxJyQifAb-I4X7yQA)\n\n[![Nico Ritschel profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1671183451771527169%2Fg6RdaL5q_x96.jpg&w=96&q=75)\\\\\n\\\\\nNico Ritschel\\\\\n\\\\\n@nicoritschel\\\\\n\\\\\nI just deployed to Rill cloud backed by Motherduck and it worked great\u2014 wonderful experience...\\\\\nIn-browser CLI auth with Rill Cloud, added my Motherduck token, invited some viewers, I'm live. Took 3 minutes.](https://twitter.com/nicoritschel/status/1686595084454961152)\n\n[![Damon Cortesi profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1551732159416635392%2FIFOUWmsD_x96.jpg&w=96&q=75)\\\\\n\\\\\nDamon Cortesi\\\\\n\\\\\n@dacort\\\\\n\\\\\nBecause, I don't have enough time on my hands, I created a DuckDB extension that allows you to import data by scanning an Athena table. \\\\\n\\\\\nhttps://github.com/dacort/duckdb-athena-extension](https://twitter.com/dacort/status/1633952720561516544)\n\n[![George Fraser profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1571011203736174593%2FobtbMbnD_x96.jpg&w=96&q=75)\\\\\n\\\\\nGeorge Fraser\\\\\n\\\\\n@frasergeorgew\\\\\n\\\\\nHow fast is DuckDB compared to the best commercial data warehouses? I decided to benchmark it myself. Short version: very fast! But it's not (yet) great at scaling up to many cores. https://fivetran.com/blog/how-fast-is-duckdb-really](https://twitter.com/frasergeorgew/status/1638314840488923139)\n\n[![MotherDuck profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmedia.licdn.com%2Fdms%2Fimage%2Fv2%2FD560BAQEODuOUOAdoQg%2Fcompany-logo_100_100%2FB56ZfmzIGPG0AQ-%2F0%2F1751923851982%2Fmotherduck_logo%3Fe%3D1762992000%26v%3Dbeta%26t%3DelhKxwqBElgX_PNnhA3vQAu8QdPUILGxxa4doqv60-w&w=96&q=75)\\\\\n\\\\\nMotherDuck\\\\\n\\\\\n@motherduck\\\\\n\\\\\n@duckdb is empowering machine learning use cases! \ud83e\udd16\\\\\nAny good model training starts with some good data exploration.\\\\\n\\\\\n@huggingface now enables SQL queries using DuckDB on any dataset stored on the Hub!](https://twitter.com/motherduck/status/1666804451552935938)\n\n[![DuckDB profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1274363897676521474%2FqgbqYYuV_x96.jpg&w=96&q=75)\\\\\n\\\\\nDuckDB\\\\\n\\\\\n@duckdb\\\\\n\\\\\nDuckDB Co-Creator @hfmuehleisen and MotherDuck Founder @jrdntgn in this new podcast - among other things about the origin story of DuckDB and later MotherDuck:](https://twitter.com/duckdb/status/1649305101570306048)\n\n[![Garrett McClintock profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1438016780060950531%2FCn11Ke8O_400x400.jpg&w=96&q=75)\\\\\n\\\\\nGarrett McClintock\\\\\n\\\\\n@GJMcClintock\\\\\n\\\\\njust built a pipeline using @dltHub and @motherduck and it's very nice. 10/10 experience once you get the hang of it and very scalable.](https://x.com/GJMcClintock/status/1800962608939303002)\n\n[![Andrew Madson profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2F1706413992317_120c2864d9.jpeg&w=96&q=75)\\\\\n\\\\\nAndrew Madson\\\\\n\\\\\nandrew-madsen\\\\\n\\\\\nInstant SQL! This is one of the highlights of Data Council... Instant SQL caches a sample of your table instantly, and synchronously updates the query results, AS YOU TYPE!](https://www.linkedin.com/posts/andrew-madson_duckdb-instant-sql-this-is-one-of-the-activity-7321023818074894336-sbar?utm_source=share&utm_medium=member_desktop&rcm=ACoAAACw0KEBx7lMe9uvdRJxJyQifAb-I4X7yQA)\n\n[![KKovacs [\u30b9] profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F995607718508589058%2FtQPO3yiD_400x400.jpg&w=96&q=75)\\\\\n\\\\\nKKovacs \\[\u30b9\\]\\\\\n\\\\\n@kkovacs\\\\\n\\\\\nI don\u2019t want to say that #DuckDB is a silver bullet because there is no such thing, but it\u2019s surprisingly projectile-shaped, very shiny, and killed all the various werewolves (ok, data problems) I\u2019ve thrown at it](https://twitter.com/kkovacs/status/1801607688889012587)\n\n[![evidence profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1699896475864510464%2F_XFRsLSv_400x400.jpg&w=96&q=75)\\\\\n\\\\\nevidence\\\\\n\\\\\n@evidence\\_dev\\\\\n\\\\\nDuckDB is increasingly being built into every data tool, but DuckDB isn't designed to be used as a cloud data warehouse\\\\\n\\\\\n@motherduck\\\\\nis solving that problem, and they\u2019re doing it in a delightful, uniquely DuckDB way...](https://twitter.com/evidence_dev/status/1801630536252768584)\n\n[![Zach Wilson profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmedia.licdn.com%2Fdms%2Fimage%2Fv2%2FD5603AQGLd4zOUXaJJg%2Fprofile-displayphoto-shrink_200_200%2FB56ZcbuHVJHUAY-%2F0%2F1748516763636%3Fe%3D1762992000%26v%3Dbeta%26t%3DXmqD42PALO0Cur0sGLHL8Wo_3RMwSo9tGHycq0jpFI8&w=96&q=75)\\\\\n\\\\\nZach Wilson\\\\\n\\\\\neczachly\\\\\n\\\\\nMatt Martin and I did a runtime benchmark of Apache Spark vs DuckDB (by MotherDuck)\\\\\nWe found that data sets under 20 gigabytes ran about 100 times faster on DuckDB than they did on Apache Spark!\\\\\n\\\\\nThis benchmark uses plain parquet files and COUNT distinct to truly measure performance without skipping any data so we know the differences are correct.\\\\\n\\\\\nHere's the complete writeup: https://lnkd.in/gXuWGWyy](https://www.linkedin.com/posts/eczachly_matt-martin-and-i-did-a-runtime-benchmark-activity-7376304145978310657-GzkR/)\n\n[![Daniel Beach profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2F22616816.fs1.hubspotusercontent-na1.net%2Fhubfs%2F22616816%2Fdaniel%2520beach.jpg&w=96&q=75)\\\\\n\\\\\nDaniel Beach\\\\\n\\\\\n@dataengineeringcentral\\\\\n\\\\\nThis MotherDuck thingy is such a beauty it brings a tear to me old eye. You have no idea how many new tools (maybe you do) I agonize over while trying to bring you the good, the bad, and the ugly.](https://dataengineeringcentral.substack.com/p/honest-review-of-motherduck)\n\n[![Ryan Dolley profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fryan_dolley_a1178e8a04.jpeg&w=96&q=75)\\\\\n\\\\\nRyan Dolley\\\\\n\\\\\nryandolley\\\\\n\\\\\nMotherDuck ... bringing big innovation to SQL to make it more like a real language and less like a 'groovy ancient programming language from the 70s.' Check it out.](https://www.linkedin.com/posts/ryandolley_duckdb-activity-7320941264818884609-1fij?utm_source=share&utm_medium=member_desktop&rcm=ACoAAACw0KEBx7lMe9uvdRJxJyQifAb-I4X7yQA)\n\n[![Robert Lancer profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Frobert_lancer_e5d68edd81.jpeg&w=96&q=75)\\\\\n\\\\\nRobert Lancer\\\\\n\\\\\nrobertlancer\\\\\n\\\\\nThe progress on the MotherDuck UI is amazing! It went from minimal utility to my preferred place to write SQL in a matter of months](https://www.linkedin.com/feed/update/urn:li:ugcPost:7163558312741982208?commentUrn=ur[%E2%80%A6]89847054557184%2Curn%3Ali%3AugcPost%3A7163558312741982208%29)\n\n[![Caleb Fahlgren profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1716604301563289600%2FYycgFNAn_400x400.jpg&w=96&q=75)\\\\\n\\\\\nCaleb Fahlgren\\\\\n\\\\\n@calebfahlgren\\\\\n\\\\\nThe amount of data I am storing in \\\\\n@motherduck rn for a new project is criminal \ud83e\udd23\\\\\nDecided to try it out since I am a big \\\\\n@duckdb user. Is pretty good so far. Have few hundred million records.](https://x.com/calebfahlgren)\n\n[![Simon Sp\u00e4ti \ud83c\udfd4\ufe0f profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1137678291605676032%2FITc3M_ah_400x400.png&w=96&q=75)\\\\\n\\\\\nSimon Sp\u00e4ti \ud83c\udfd4\ufe0f\\\\\n\\\\\n@sspaeti\\\\\n\\\\\nI'm just checking a little about the hosted DuckDB by @motherduck. What a nice user experience. It automatically detects errors, and \\`accept & run\\` fixes common errors. Love it.](https://x.com/sspaeti/status/1798290369110749458)\n\n[![Jacob Matson profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1278773580122054656%2F3SG9rUwl_x96.jpg&w=96&q=75)\\\\\n\\\\\nJacob Matson\\\\\n\\\\\n@matsonj\\\\\n\\\\\nusing sql for data cleaning vs excel (reason 879 to learn sql)\\\\\n\\\\\nlevenstein distance between 590strings in excel (350k combinations): \\\\\nexcel: 25 mins of spinning. \\\\\nduckdb: 1.2s](https://twitter.com/matsonj/status/1669434599095816192)\n\n[![Bob Currier profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fbob_currier_d620fb0c2d.jpeg&w=96&q=75)\\\\\n\\\\\nBob Currier\\\\\n\\\\\nbobcurrier\\\\\n\\\\\nIf you're not familiar with DuckDB, you definitely want to give it a look. I've recently converted all my existing code from using CSV files and SQLite databases for exploratory analysis to parquet files, DuckDB and MotherDuck. The transformation in my workflow has been nothing short of astounding. 10GB of CSV files turned into 1GB of parquet files that are now stored in S3 buckets. Using DuckDB and MotherDuck, I can run exploratory queries seamlessly thanks to the DuckStack with dual execution with local processing and cloud scale. My query development time has decreased by 90%. And I'm loving the team feature of the MotherDuck platform. OCEANCODA is rocking the small data life! #duckdb #python #motherduck #eda #ai #ml (Graphic courtesy of Manning Publications Co. book 'DuckDB in Action.' ) Check it out.](https://www.linkedin.com/feed/update/urn:li:activity:7221573493295910912/)\n\n[![evidence profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1699896475864510464%2F_XFRsLSv_400x400.jpg&w=96&q=75)\\\\\n\\\\\nevidence\\\\\n\\\\\n@evidence\\_dev\\\\\n\\\\\nDuckDB is increasingly being built into every data tool, but DuckDB isn't designed to be used as a cloud data warehouse\\\\\n\\\\\n@motherduck is solving that problem, and they\u2019re doing it in a delightful, uniquely DuckDB way](https://x.com/evidence_dev/status/1801630536252768584)\n\n[![Ethan profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1530999951609606144%2FPhNCpNOa_400x400.jpg&w=96&q=75)\\\\\n\\\\\nEthan\\\\\n\\\\\n@@ethanf\\_17\\\\\n\\\\\nDuckDB is my new best friend for analytics. It's so fast, makes it easy to manipulate dataframes, ingests anything, and exports it to whatever form I want. \\\\\n\\\\\nParquet, Arrow dataframes, and DuckDB are the future for analytics.](https://x.com/ethanf_17/status/1809835395695407529)\n\nFOLLOW US\n\n[MotherDuck Twitter](https://twitter.com/motherduck) [MotherDuck LinkedIn](https://linkedin.com/company/motherduck/)\n\n## Join the flock\n\n[CHAT WITH US ON SLACK](https://slack.motherduck.com/)\n\n[![Mobile Slack banner](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fslack-banner-mobile.20467522.png&w=2048&q=75)![Desktop Slack banner](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fslack-banner-desktop.7f4b0dc6.png&w=2048&q=75)](https://slack.motherduck.com/)\n\n## SUBSCRIBE\n\n### Subscribe to our newsletter\n\nE-mail\n\nSubscribe to other MotherDuck emails\n\nSubmit\n\n![Newsletter section illus](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fnewsletter-home.c93874dd.png&w=3840&q=75)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 906,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 906
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/is-duckdb-the-secret-to-unlocking-your-gis-potential.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ibis-one-library-to-query-any-backend.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 10,
  [2m  â”‚[0m   "appliedLimit": 10
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! Now I have enough information to synthesize a comprehensive answer. Let me write the markdown file with the answer.
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_2.md

  [96mğŸ”§ TOOL:[0m Bash â†’ cat > /tmp/answer_without_kg_2.md << 'EOF'
# How MotherDuck Integrates with DuckDB

MotherDuck extends DuckDB to the cloud through a sophisticated integration architecture that maintains full compatibility with the open-source database while adding cloud-scale capabilities.

## Integration Architecture

### Three-Layer System

MotherDuck's integration is built on three core components:

1. **Client Extension**: MotherDuck integrates directly with the DuckDB ecosystem through an extension system. Users can connect their local DuckDB instance to MotherDuck with a simple command, regardless of whether they're working in the CLI, Python, JDBC, or even the browser via WebAssembly.

2. **Compute Layer**: Cloud-based queries are processed by containerized DuckDB instances called "ducklings." These compute resources scale based on user needs, providing the necessary CPU and memory for analytical queries.

3. **Storage Layer**: Instead of DuckDB's native single-writer file format, MotherDuck implements a differential storage system that maps logical database files to append-only snapshot layers in cloud object storage. This enables concurrent access, database sharing, and time travel capabilities.

### Non-Fork Philosophy

Critically, MotherDuck does not fork DuckDB. Instead, it leverages DuckDB's powerful extension system to add capabilities at multiple layers including the SQL parser, optimizer, and storage interface. This allows MotherDuck to stay current with the latest open-source DuckDB releases and benefit from rapid community innovation.

## Simple Connection

Connecting to MotherDuck from DuckDB requires only a single command:

```sql
ATTACH 'md:';
```

Users simply need a MotherDuck token (which can be set as an environment variable) to authenticate. Once connected, the same DuckDB interface works seamlessly across local and cloud environments.

## Dual Execution Model

The most innovative feature of MotherDuck's integration is **dual execution** - a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the cloud.

### How It Works

- When all data resides in MotherDuck databases, queries execute entirely in the cloud
- When queries involve both local and cloud data, the system runs a mix of local and remote operations
- The optimizer transfers only the necessary intermediate data to complete operations, avoiding needless data movement

### Performance Benefits

By pushing computation to where the data lives, MotherDuck minimizes network I/O. In demonstrations, querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds).

### Manual Control

Users can override automatic optimization using the `md_run` parameter:
- `md_run = 'local'` forces execution on the local client
- `md_run = 'remote'` forces execution in the MotherDuck cloud

## Local Development, Cloud Production

MotherDuck enables a seamless workflow where developers can:

1. **Develop locally** with lightning-fast feedback cycles at zero cost
2. **Deploy to cloud** by simply adding "md:" to their connection string
3. **Run identical workloads** in both environments with zero code changes

This "local dev, cloud prod" model ensures what works locally will perform at scale in production.

## Collaboration Features

The integration transforms DuckDB from a "single-player" tool into a collaborative "multiplayer" platform:

- **Database Sharing**: Team members can grant query access to their databases without emailing files or managing S3 buckets
- **Concurrent Access**: Multiple users can query the same datasets simultaneously, each getting their own dedicated "duckling" compute instance
- **Centralized Secret Manager**: Create persistent, encrypted credentials for cloud storage that work across different clients and sessions

## Extension Support

MotherDuck's client extension can run DuckDB in various environments:
- CLI interfaces
- Python scripts
- JDBC connections
- Web browsers via WebAssembly (used in the MotherDuck UI for client-side processing)

This flexibility ensures users can work with their preferred tools while seamlessly accessing both local and cloud data.

## Symbiotic Relationship

As the largest production user of DuckDB, MotherDuck continuously pushes the engine to its limits and contributes enhancements back to the open-source project. This ensures both DuckDB and MotherDuck evolve together, offering a powerful analytical experience from local exploration to cloud-scale production.

---

## Sources

- **Getting Started with MotherDuck**: `.kurt/sources/motherduck.com/getting-started-with-motherduck.md`
- **DuckDB & MotherDuck for Beginners Guide**: `.kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md`
- **Bringing DuckDB to the Cloud: Dual Execution Explained**: `.kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md`
- **DuckDB Users Product Page**: `.kurt/sources/motherduck.com/product/duckdb-users.md`
- **MotherDuck Homepage**: `.kurt/sources/motherduck.com/index.md`
EOF
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m 
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! I've successfully completed the task. Here's what I did:
  [94mâ”‚[0m 
  [94mâ”‚[0m ## Summary
  [94mâ”‚[0m 
  [94mâ”‚[0m I searched through the MotherDuck source files in `.kurt/sources/` and synthesized a comprehensive answer about how MotherDuck integrates with DuckDB. 
  [94mâ”‚[0m 
  [94mâ”‚[0m ### Key Findings:
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. **Architecture**: MotherDuck uses a three-layer system (client extension, compute layer, storage layer) without forking DuckDB
  [94mâ”‚[0m 2. **Simple Connection**: Just `ATTACH 'md:';` command connects local DuckDB to the cloud
  [94mâ”‚[0m 3. **Dual Execution**: Intelligent hybrid execution that decides whether to run queries locally or in the cloud
  [94mâ”‚[0m 4. **Collaboration**: Transforms DuckDB from single-player to multiplayer with database sharing and concurrent access
  [94mâ”‚[0m 5. **Performance**: Optimizes by pushing computation to where data lives, significantly reducing query times
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer has been written to `/tmp/answer_without_kg_2.md` with a detailed Sources section referencing the five most relevant markdown files I analyzed from the MotherDuck documentation.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 4,170 | Cost: $0.3263
     Cumulative: 4,170 tokens | $0.3263
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent's message provides a comprehensive summary and indicates that the task is complete, without soliciting any further input from the user.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 12 | Duration: 90.7s
â•‘    Tokens: 4,170 | Cost: $0.3263
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  LLM Judge score: 0.99

======================================================================
â“ Question 3/10
What SQL features from DuckDB are not yet supported in MotherDuck?
======================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 3
â”‚ Search through the .kurt/sources/ directory to answer this question:

What SQL features from DuckDB are not yet supported in MotherDuck?

Steps:
1. Search for the most relevant markdown files in .kurt/sources/
2. Read those files and synthesize the answer
3. Write a markdown answer (with a ## Sources section) to: /tmp/answer_without_kg_3.md
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I'll help you find information about SQL features from DuckDB that are not yet supported in MotherDuck. Let me search through the sources directory.
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ not supported|unsupported|limitations|not available|does not support

  [96mğŸ”§ TOOL:[0m Grep â†’ DuckDB.*feature|feature.*DuckDB

  [96mğŸ”§ TOOL:[0m Grep â†’ SQL.*not.*support|incompatible|differences
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/cultivating-growth-how-gardyn-scaled-its-data-operations-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/an-evolving-dag-for-the-llm-world-julia-schottenstein-of-langchain-at-small-data-sf-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-use.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/privacy-policy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/dataframes.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-OLAP.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/pandas-dataframes-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-lake-vs-data-warehouse-vs-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/index.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/filter.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/fees-addendum.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-brief.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/layers-multi-tenant-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/atm-com-analytics-costs-sql-expressibility.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/summer-data-engineering-roadmap.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-if-sql-queries-returned-results-instantly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/speed-querying-stackoverflow-data-with-duckdb-ft-michael-hunger.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-bad-the-good-and-the-ugly-navigating-the-new-waves-of-machine-learning-ft-adi-polak.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/lies-damn-lies-and-benchmarks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/machine-learning-and-ai-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/more-than-a-vibe-ai-driven-sql-that-actually-works.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/getting-started-with-motherduck-webinar.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/going-beyond-the-dataframe.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search more specifically for MotherDuck limitations and features:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ MotherDuck.*limit|limitation.*MotherDuck

  [96mğŸ”§ TOOL:[0m Grep â†’ currently.*not|not yet|coming soon|roadmap

  [96mğŸ”§ TOOL:[0m Bash â†’ find .kurt/sources -name "*.md" -type f | head -20
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m .kurt/sources/motherduck.com/research/execution-guided-sql-generation.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/research/ssdtrain-framework-paper.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/research/accelerating-machine-learning-lifecycle.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/customer-support.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/datalab.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/secoda.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/cube.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/airbyte.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/marimo.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/zenlytic.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/bruin.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/ascend.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/hex.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/OneSignal.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/datawisp.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/Shearwater.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/astronomer.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/streamkap.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/puppygraph.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/codecentric.md
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/ducklake-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/acid-transactions-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dosomething-non-profit-tco-cost-savings.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/layers-multi-tenant-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/summer-data-engineering-roadmap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/solving-advent-code-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg_duckdb-postgresql-extension-for-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-mooncake-columnstore.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-reflections-sigmod-pods-2024.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-use.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/eu-region.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/fees-addendum.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/candidate-privacy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dexibit.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search for documentation files that might contain information about features and limitations:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ find .kurt/sources -name "*.md" -path "*docs*" -type f | head -30

  [96mğŸ”§ TOOL:[0m Grep â†’ extension|pragma|configuration|setting
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m 
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/spatial-data-management-with-duckdb-ft-mattforrest.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/panel-fixing-the-data-engineering-lifecycle-coalesce-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/is-duckdb-the-secret-to-unlocking-your-gis-potential.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/leveraging-duckdb-and-delta-lake-together.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/monte-carlo-simulations-with-duckdb-ft-james-mcneill.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-bootstrap-a-data-warehouse-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ibis-one-library-to-query-any-backend.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/is-bi-too-big-for-small-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-vs-pandas-vs-polars-for-python-devs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ducklake-big-data-small-coalesce-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/faster-data-pipelines-development-with-mcp-and-duckdb.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 30,
  [2m  â”‚[0m   "appliedLimit": 30
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me look for files that might discuss MotherDuck specifically and its differences from DuckDB:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ ls -la .kurt/sources/motherduck.com/ | head -50

  [96mğŸ”§ TOOL:[0m Grep â†’ DuckDB feature|MotherDuck.*DuckDB|difference between
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m total 1176
  [2m  â”‚[0m drwxr-xr-x   51 julien  staff   1632 Nov 27 13:57 .
  [2m  â”‚[0m drwxr-xr-x    3 julien  staff     96 Nov 27 13:57 ..
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   4118 Nov 26 10:30 about-us.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   2154 Nov 26 10:30 acceptable-use-policy.md
  [2m  â”‚[0m drwxr-xr-x   55 julien  staff   1760 Nov 27 13:57 authors
  [2m  â”‚[0m drwxr-xr-x  178 julien  staff   5696 Nov 27 13:57 blog
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  12059 Nov 26 10:30 blog.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  15483 Nov 26 10:30 candidate-privacy.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  27511 Nov 26 10:30 careers.md
  [2m  â”‚[0m drwxr-xr-x   13 julien  staff    416 Nov 27 13:57 case-studies
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  15955 Nov 26 10:30 case-studies.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  20608 Nov 26 10:30 community-and-open-source.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   1734 Nov 26 10:30 community-code.md
  [2m  â”‚[0m drwxr-xr-x    4 julien  staff    128 Nov 27 13:57 contact-us
  [2m  â”‚[0m -rw-r--r--    1 julien  staff    901 Nov 26 10:30 contact-us.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   2380 Nov 26 10:30 customer-support.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   6132 Nov 26 10:30 duckdb-book-brief.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   4571 Nov 26 10:30 duckdb-book-summary-chapter2.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   4014 Nov 26 10:30 duckdb-book-summary-chapter3.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   4370 Nov 26 10:30 duckdb-book-summary-chapter4.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  11557 Nov 26 10:30 duckdb-news.md
  [2m  â”‚[0m drwxr-xr-x   68 julien  staff   2176 Nov 27 13:57 ecosystem
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  66511 Nov 26 10:30 ecosystem.md
  [2m  â”‚[0m drwxr-xr-x    3 julien  staff     96 Nov 27 13:57 events
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   2525 Nov 26 10:30 events.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   7249 Nov 26 10:30 fees-addendum.md
  [2m  â”‚[0m drwxr-xr-x    3 julien  staff     96 Nov 27 13:57 forms
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   1392 Nov 26 10:30 getting-started-with-motherduck.md
  [2m  â”‚[0m drwxr-xr-x   71 julien  staff   2272 Nov 27 13:57 glossary
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   5549 Nov 26 10:30 hack-night.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  49247 Nov 26 10:30 index.md
  [2m  â”‚[0m drwxr-xr-x   37 julien  staff   1184 Nov 27 13:57 learn-more
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  20455 Nov 26 10:30 learn-more.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   1720 Nov 26 10:30 local-dev-cloud-prod-dagster-motherduck.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff    524 Nov 26 10:30 onboarding-sessions.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  17998 Nov 26 10:30 privacy-policy.md
  [2m  â”‚[0m drwxr-xr-x    8 julien  staff    256 Nov 27 13:57 product
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  26113 Nov 26 10:30 product.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  16979 Nov 26 10:30 quacking-around-the-web.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   4924 Nov 26 10:30 react-components-testing.md
  [2m  â”‚[0m drwxr-xr-x    5 julien  staff    160 Nov 27 13:57 research
  [2m  â”‚[0m -rw-r--r--    1 julien  staff    550 Nov 26 10:30 research.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   2381 Nov 26 10:30 sql-duckdb-book-form.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  12172 Nov 26 10:30 startups.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff    406 Nov 26 10:30 terms-of-service-app.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  39072 Nov 26 10:30 terms-of-service.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  22162 Nov 26 10:30 terms-of-use.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   2528 Nov 26 10:30 trust-and-security.md
  [2m  â”‚[0m drwxr-xr-x   99 julien  staff   3168 Nov 27 13:57 videos
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/scaling-duckdb-panel-ondemand.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/take-flight-with-the-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/data-discoverability-secoda-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-power-of-wasm-for-analytics-duckdb-in-the-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-surprising-birth-of-duckdb-ft-co-creator-hannes-muhleisen.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/spatial-data-management-with-duckdb-ft-mattforrest.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/speed-querying-stackoverflow-data-with-duckdb-ft-michael-hunger.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/sql-ide-safari-harlequin-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/take-flight-with-dbt-and-duckdb-dropping-dev-warehouse-costs-to-zero.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-future-of-bi-exploring-the-impact-of-bi-as-code-tools-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 30,
  [2m  â”‚[0m   "appliedLimit": 30
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me look at some key files that might contain information about MotherDuck vs DuckDB features:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/product.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/getting-started-with-motherduck.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     "content": "---\ntitle: bringing-duckdb-to-the-cloud-dual-execution-explained\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained\nindexed_at: '2025-11-25T20:45:20.295187'\ncontent_hash: ab4030806c409492\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nBringing DuckDB to the Cloud: Dual Execution Explained - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Bringing DuckDB to the Cloud: Dual Execution Explained](https://www.youtube.com/watch?v=2eVNm4A8sLw)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=2eVNm4A8sLw&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 1:08:11\n\n\u2022Live\n\n\u2022\n\nYouTubeQuack & CodeMotherDuck Features\n\n# Bringing DuckDB to the Cloud: Dual Execution Explained\n\n2024/06/28\n\nBringing DuckDB's analytical power to the cloud requires more than just running it on a server. While DuckDB excels at processing data on a local machine, the dynamics of data analysis change when dealing with cloud-scale data, collaboration, and shared resources. This shift introduces challenges around [security](https://motherduck.com/learn-more/secure-startup-data-warehouse/), concurrent access, and performance, which is where a purpose-built cloud architecture becomes essential.\n\nMotherDuck, a modern cloud data warehouse, is built on DuckDB to solve these challenges. It extends DuckDB\u2019s capabilities without forking the open source project, creating a hybrid system that intelligently balances local and cloud computing. In a conversation with MotherDuck founding engineer Stephanie, she explained the architecture and the innovative query model, known as **dual execution**, that makes this possible.\n\n* * *\n\n## The MotherDuck Architecture: Beyond Hosted DuckDB\n\nA common misconception is that MotherDuck is simply DuckDB hosted in the cloud. The reality is a more sophisticated system designed to overcome the limitations of using a locally-optimized engine in a distributed environment. MotherDuck\u2019s architecture is built on three key components and a core philosophy of not forking DuckDB.\n\nInstead of maintaining a separate version, MotherDuck leverages DuckDB\u2019s powerful **extension system**. This allows MotherDuck to add new capabilities at multiple layers, including the SQL parser, optimizer, and storage interface, while staying current with the latest open source DuckDB releases. This tight integration means users benefit from the rapid innovation of the DuckDB community almost immediately.\n\nThe architecture can be broken down into three main layers:\n\n1. **The Client Extension**: This is how MotherDuck integrates with the DuckDB ecosystem. Whether a user is working in the CLI, a Python script, or a JDBC connection, a simple `ATTACH` command connects their local DuckDB instance to the MotherDuck cloud. This extension is even used to run DuckDB in the browser via WASM for the MotherDuck UI, enabling client-side processing to reduce latency and cloud compute costs.\n2. **The Compute Layer**: In the cloud, queries are processed by containerized DuckDB instances, fondly called \"ducklings.\" These compute resources are scaled based on user needs, providing the necessary CPU and memory to handle complex analytical queries on large datasets.\n3. **The Storage Layer**: DuckDB's native file format is optimized for a single writer on a local file system. This model is not well-suited for the cloud, where multiple users need to read and write to the same database concurrently. To solve this, MotherDuck implemented a differential storage system that maps a logical database file to append-only snapshot layers in cloud object storage. This design is cloud-friendly, enabling efficient in-place updates and forming the foundation for features like database sharing and time travel.\n\n* * *\n\n## Enabling Collaboration with Database Sharing and Secure Credentials\n\nThis unique architecture transforms DuckDB from a \"single-player\" tool into a collaborative \"multiplayer\" platform. One of the most significant advantages is **database sharing**. Instead of emailing SQL scripts or passing around large database files, team members within the same organization can grant query access to their databases. This streamlines collaboration, ensuring everyone works from a consistent and up-to-date version of the data.\n\nSecurity is another critical aspect of any cloud data platform. MotherDuck provides a **centralized Secret Manager**, co-designed with the DuckDB team. Users can create persistent, encrypted secrets for accessing external data sources like AWS S3, Google Cloud Storage, or Delta Lake. Once a secret is created in MotherDuck, it can be reused across different clients and sessions without needing to expose or reconfigure credentials on each local machine.\n\n* * *\n\n## Understanding Dual Execution\n\nThe most innovative feature of MotherDuck's architecture is its **dual execution model**. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\n\nThe process was demonstrated with a query joining two tables. When both tables reside in a MotherDuck database, the query is executed entirely in the cloud. An `EXPLAIN` plan for this query reveals that all operators, from the table scans to the final join, are marked as **remote**. The only local operation is downloading the final result set to the client.\n\nThe real power of dual execution becomes apparent when a query involves both local and cloud data. In the demonstration, one of the remote tables was copied to a local Parquet file. The query was then modified to join this local file with the table that remained in the cloud.\n\nThe `EXPLAIN` plan for this new query showed a mix of local and remote operations. The scan of the Parquet file and its associated processing happened **locally**, leveraging the user's machine. The scan of the cloud table happened **remotely** on MotherDuck's compute. The system then efficiently transferred only the necessary intermediate data to complete the join. This hybrid approach avoids needlessly uploading the local file to the cloud or downloading the entire remote table, resulting in faster and more efficient queries.\n\n* * *\n\n## Taking Control with Manual Execution Overrides\n\nWhile MotherDuck's optimizer is designed to make the most efficient choice automatically, some situations require manual control. For this, users can use the `md_run` parameter within scan functions like `read_parquet()` or `read_csv()`.\n\nBy setting `md_run = 'local'`, a user can force the scan of a remote S3 file to be executed on their local client. This involves downloading the data, which might be desirable if the data needs to be processed near the user's location or if they prefer using local credentials. Conversely, setting `md_run = 'remote'` forces the operation to run in the MotherDuck cloud.\n\nThis control is especially useful for performance tuning. A direct comparison showed that querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds). By pushing the computation to the data's location, MotherDuck minimizes network I/O and delivers results more quickly.\n\n* * *\n\n## A Symbiotic Future for Local and Cloud Analytics\n\nMotherDuck's architecture is a thoughtful extension of DuckDB's core principles, adapting its local-first power for the demands of the cloud. The dual execution model provides a flexible and efficient bridge between a user's laptop and cloud data, optimizing workloads in a way that pure-cloud or pure-local systems cannot.\n\nThis relationship is symbiotic. As the largest production user of DuckDB, MotherDuck continuously pushes the engine to its limits, uncovering opportunities for improvement and contributing enhancements back to the open source project. This collaboration ensures that both DuckDB and MotherDuck will continue to evolve, offering a powerful and seamless analytical experience that spans from local exploration to cloud-scale production.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Ficeberg_video_thumbnail_adfdabe3d6.jpg&w=3840&q=75)\\\\\n\\\\\n46:26](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n[2025-06-12](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n### [Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse)\n\nBuilding a data stack means choosing between easy SaaS and complex open-source. Apache Iceberg is a middle ground, but its catalog is a hurdle. New tools now simplify using Iceberg with DuckDB to create a flexible, local data lakehouse.\n\nMotherDuck Features\n\n[![\"Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FG_Se_B_Sox_AW_Fg_HD_f4fddaa9ab.jpg&w=3840&q=75)](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n[2025-04-23](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n### [Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly)\n\nHamilton Ulmer shares insights from MotherDuck's Instant SQL Mode, exploring how real-time query result previews eliminate the traditional write-run-debug cycle through client-side parsing and DuckDB-WASM caching.\n\nSQL\n\nTalk\n\nMotherDuck Features\n\n[![\"DuckDB: Run dbt build with sub-second execution times\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_03_26_at_3_40_16_PM_c6e3a8096f.png&w=3840&q=75)\\\\\n\\\\\n24:06](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n[2025-03-13](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n### [DuckDB: Run dbt build with sub-second execution times](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times)\n\nWhether you're new to DuckDB or looking to optimize your workflows, this session will provide practical insights to help you leverage its full potential.\n\nYouTube\n\nData Pipelines\n\ndbt\n\nMotherDuck Features\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 184,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 184
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/getting-started-with-motherduck.md",
  [2m  â”‚[0m     "content": "---\ntitle: getting-started-with-motherduck\ncontent_type: event\nsource_url: https://motherduck.com/getting-started-with-motherduck\nindexed_at: '2025-11-25T20:36:55.713062'\ncontent_hash: eca528b7b79fabc6\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n# Product Demo: Getting Started with MotherDuck\n\n## Join us for a live demo to learn how to get started with MotherDuck, the cloud SQL analytics data warehouse extending DuckDB to the cloud.\n\nMotherDuck is now Generally Available! In this session, MotherDuck Co-Founder Ryan Boyd and Product Expert Nathaniel Thompson will show you how to maximize MotherDuck's analytics and collaboration features to take flight with your analytics workloads and applications.\n\n### Tuesday, June 18th @ 10AM PDT\n\nRegister now. We're looking forward to seeing you there!\n\n![Getting Started with MotherDuck](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGetting_Started_with_MD_437c0d3476.png&w=3840&q=75)\n\nGetting Started with MotherDuck\n\n## Getting Started with MotherDuck\n\nEmail address \\*\n\nNext\n\nAlready registered?Join here\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 37,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 37
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/product.md",
  [2m  â”‚[0m     "content": "---\ntitle: product\ncontent_type: tutorial\nsource_url: https://motherduck.com/product/\nindexed_at: '2025-11-25T20:15:52.484117'\ncontent_hash: d41fe98ea483c712\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n# Cloud SQL Analytics without the Overhead\n\n[Data Warehouses](https://motherduck.com/product/data-teams/) and [Interactive Apps](https://motherduck.com/product/app-developers/) Powered by DuckDB\n\n[Get Started](https://app.motherduck.com/?auth_flow=signup)\n\nLEARN MORE\n\n![YouTube video preview](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fproduct-hero-thumbnail.5d94bd17.png&w=3840&q=75)\n\nUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse CasesUse Cases\n\n![For App Devs's illus](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fapp-devs-ducks.51eb4ea9.png&w=3840&q=75)\n\n## For App Devs\n\nFast, fresh analytics for data-driven applications and components shouldn\u2019t be an uphill battle with your operational DWH.\n\n[LEARN MORE](https://motherduck.com/product/app-developers/)\n\n![For Data Teams's illus](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdata-teams-ducks.6fea331d.png&w=3840&q=75)\n\n## For Data Teams\n\nThe hardest part of getting value from your data is going from question to insight - it doesn\u2019t have to be this way.\n\n[LEARN MORE](https://motherduck.com/product/data-teams/)\n\n## What Makes MotherDuck Unique?\n\n### The Old World\n\nBin-packed, high latency workloads\n\n### Analytics are too cumbersome\n\nWith all your users in one big, shared box, they\u2019re constantly competing for resources and compute, making load times unbearable...why are we doing this?\n\n### Scaling is resource-intensive\n\nWith a traditional DWH, power users run into latency issues as processing scales. Deciding which users to bin-pack and which to isolate on their own instance is cumbersome and sloooooooow.\n\n### No visibility of user-level usage\n\nIt\u2019s impossible to understand who is driving the lion\u2019s share of usage and how to prevent your database and the applications built on top from grinding to a halt.\n\nThe struggle is real - but there\u2019s finally a better way.\n\n### The MotherDuck World\n\nData warehouse without the overhead\n\n### Simple, straightforward analytics\n\nMotherDuck is the first analytics data warehouse that offers cloud scale and individualized, user-level tenancy. Managing user-level compute limits and cost attribution without cumbersome upfront configuration hasn\u2019t been possible until now.\n\n### The first truly personal data warehouse\n\nEach user automatically gets their own compute instance \u201cduckling\u201d so end users can move quickly and freely from question to insight. Best of all: No more waiting around for queries to process because someone else is using too much compute!\n\n### User-level CPU Visibility by design\n\nBuilt-in user-level CPU visibility ensures a consistent, smooth experience for all your users. Cost attribution at the user-level helps you predict and proactively optimize costs without additional overhead - say goodbye to surprise bills when usage scales up.\n\n## Dual Execution Query Engine\n\nSUPPORT LOCAL ANALYTICS\n\nYour MacBook is faster than a $20-60/hr data warehouse. Why is it acting like a dumb terminal for the mainframe-in-the-cloud? MotherDuck lets you repurpose unused compute on your machine for local analytics. Save the cloud for added scale when needed.\n\nDOUBLE YOUR PROCESSING POWER\n\nMotherDuck\u2019s query planner routes execution plans where they can be completed quickly and efficiently. Queries can process on users\u2019 local machines, in the cloud, or in a hybrid, dual fashion across both locations.\n\nUNLOCK THE FREEDOM TO RUN ANYWHERE\n\nDuckDB\u2019s binary has the freedom to run anywhere, including in the web browser. With MotherDuck, DuckDB\u2019s flexible query engine can finally be used by multiple users in production thanks to org-level sharing and collaboration.\n\nEXTEND DUCKDB TO THE CLOUD\n\nDevelop locally and ship to production with the same, ultra-efficient database on the client and in the cloud. It\u2019s simple, easy, efficient, and it just ducking works.\n\nSUPPORT LOCAL ANALYTICS\n\nYour MacBook is faster than a $20-60/hr data warehouse. Why is it acting like a dumb terminal for the mainframe-in-the-cloud? MotherDuck lets you repurpose unused compute on your machine for local analytics. Save the cloud for added scale when needed.\n\nDOUBLE YOUR PROCESSING POWER\n\nMotherDuck\u2019s query planner routes execution plans where they can be completed quickly and efficiently. Queries can process on users\u2019 local machines, in the cloud, or in a hybrid, dual fashion across both locations.\n\nUNLOCK THE FREEDOM TO RUN ANYWHERE\n\nDuckDB\u2019s binary has the freedom to run anywhere, including in the web browser. With MotherDuck, DuckDB\u2019s flexible query engine can finally be used by multiple users in production thanks to org-level sharing and collaboration.\n\nEXTEND DUCKDB TO THE CLOUD\n\nDevelop locally and ship to production with the same, ultra-efficient database on the client and in the cloud. It\u2019s simple, easy, efficient, and it just ducking works.\n\nSUPPORT LOCAL ANALYTICS\n\nYour MacBook is faster than a $20-60/hr data warehouse. Why is it acting like a dumb terminal for the mainframe-in-the-cloud? MotherDuck lets you repurpose unused compute on your machine for local analytics. Save the cloud for added scale when needed.\n\nDOUBLE YOUR PROCESSING POWER\n\nMotherDuck\u2019s query planner routes execution plans where they can be completed quickly and efficiently. Queries can process on users\u2019 local machines, in the cloud, or in a hybrid, dual fashion across both locations.\n\nUNLOCK THE FREEDOM TO RUN ANYWHERE\n\nDuckDB\u2019s binary has the freedom to run anywhere, including in the web browser. With MotherDuck, DuckDB\u2019s flexible query engine can finally be used by multiple users in production thanks to org-level sharing and collaboration.\n\nEXTEND DUCKDB TO THE CLOUD\n\nDevelop locally and ship to production with the same, ultra-efficient database on the client and in the cloud. It\u2019s simple, easy, efficient, and it just ducking works.\n\n## Architecture\n\nManaged DuckDB-in-the-cloud\n\n## Ecosystem\n\nModern Duck Stack\n\n### CLOUD DATA WAREHOUSE\n\n### Sources\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpostgres-sql.d737f4f5.png&w=750&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Famazon-s3.72386dfc.png&w=640&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsalesforce.35d52b31.png&w=384&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fstripe.2160e881.png&w=384&q=75)\n\n[**Business Intelligence**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftableau.0ec523e2.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpower-bi.f3563826.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fomni.cb7aa381.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmetabase.5f8fe44e.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Business+Intelligence) [**Ingestion**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffivetran.26f3817d.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fairbyte.8371d2f0.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Festuary.babad369.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlthub.e9892b97.png&w=384&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Ingestion) [**Data Science & AI**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhex.275dad7d.png&w=256&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fjupyter.2a6af3de.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fcolab.30ab10af.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmarimo.b699c73d.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Data+Science+%26+AI) [**Reverse ETL**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fcensus.352f1d69.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Reverse+ETL) [**Transformation**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdbt.fd2184d1.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftobiko.d0e3d1e5.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsql-mesh.6fceb012.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Transformation) [**Dev Tools**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdatagrip.f48eba23.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpuppy-graph.848400c6.png&w=750&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdagster.e1970a7c.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Dev+Tools)\n\n[**Orchestration**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fairflow.7f70081a.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdagster.e1970a7c.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fprefect.fd6371b3.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fkestra.aa10acfc.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Orchestration)\n\n[**Data Quality**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmonte-carlo.2143f962.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsecoda.9b7e86fb.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsoda.59e5aa02.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fgreat-expectations.c544f1ef.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Data+Quality)\n\nMotherDuck is insanely performant, and there\u2019s no infrastructure to manage. The cost of experimentation is incredibly low \u2014 which means we can afford to ask more questions and build more features, faster.\n\n[READ MORE](https://motherduck.com/case-studies/kultura-capital/)\n\n![Kristov Paulus's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_05_08_at_10_29_30_AM_ac663c67b9.png&w=3840&q=75)\n\nKristov Paulus\n\nFounder, Chief Investment Officer at Kultura Capital\n\nMotherDuck is insanely performant, and there\u2019s no infrastructure to manage. The cost of experimentation is incredibly low \u2014 which means we can afford to ask more questions and build more features, faster.\n\n[READ MORE](https://motherduck.com/case-studies/kultura-capital/)\n\nInstant SQL in MotherDuck will save me the misery of having to try and wrangle SQL in my BI tool where iteration speed can be very slow. This lets me get the data right earlier in the process and with faster feedback than waiting for a chart to render, or having to clear an analytics cache.\n\n[DashFuel](https://www.getdashfuel.com/)\n\n![Mike McClannahan's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmike_e96491a8eb.jpeg&w=3840&q=75)\n\nMike McClannahan\n\nCTO, DashFuel\n\nInstant SQL in MotherDuck will save me the misery of having to try and wrangle SQL in my BI tool where iteration speed can be very slow. This lets me get the data right earlier in the process and with faster feedback than waiting for a chart to render, or having to clear an analytics cache.\n\n[DashFuel](https://www.getdashfuel.com/)\n\nMoving to MotherDuck, a billed by-the-second cloud offering is a no-brainer for us, considering the elegance and efficiency of a single node system compared to traditional OLAP solutions.\n\n[ATM.COM](https://atm.com/)\n\n![Nico Ritschel's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fnico_ritschel_photo_41ba132b33.jpg&w=3840&q=75)\n\nNico Ritschel\n\nDirector of Engineering at atm.com\n\nMoving to MotherDuck, a billed by-the-second cloud offering is a no-brainer for us, considering the elegance and efficiency of a single node system compared to traditional OLAP solutions.\n\n[ATM.COM](https://atm.com/)\n\nMotherDuck is the GOAT\n\n[READ MORE](https://motherduck.com/case-studies/reflex-careers-gig-economy-retail/)\n\n![Nate Hamm's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimage_15820c5f78.png&w=3840&q=75)\n\nNate Hamm\n\nSr Software Developer at Reflex\n\nMotherDuck is the GOAT\n\n[READ MORE](https://motherduck.com/case-studies/reflex-careers-gig-economy-retail/)\n\nMotherDuck with DuckDB was by far the fastest of the OLAP platforms we evaluated - both in the cloud and run on our developer's machines - bridging price and performance and greatly increasing productivity.\n\n[READ MORE](https://motherduck.com/case-studies/saasworks/)\n\n![Jim O\u2019Neill's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fjim_oneil_photo_930c637f49.jpg&w=3840&q=75)\n\nJim O\u2019Neill\n\nCTO and Co-founder at FinQore\n\nMotherDuck with DuckDB was by far the fastest of the OLAP platforms we evaluated - both in the cloud and run on our developer's machines - bridging price and performance and greatly increasing productivity.\n\n[READ MORE](https://motherduck.com/case-studies/saasworks/)\n\nDuckDB has a lot of folks in the data community excited\u2014it's a new and unique tool in practitioners' toolboxes and has captured the imaginations of many. MotherDuck's efforts bringing DuckDB to the cloud will broaden the reach of this promising technology.\n\n[DBT LABS](https://www.getdbt.com/)\n\n![Tristan Handy's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Ftristan_handy_photo_ef2233a588.jpg&w=3840&q=75)\n\nTristan Handy\n\nCEO & Founder at dbt Labs\n\nDuckDB has a lot of folks in the data community excited\u2014it's a new and unique tool in practitioners' toolboxes and has captured the imaginations of many. MotherDuck's efforts bringing DuckDB to the cloud will broaden the reach of this promising technology.\n\n[DBT LABS](https://www.getdbt.com/)\n\nWe found that DuckDB and MotherDuck are amazing tools for small data teams like ours.\n\n[READ MORE](https://motherduck.com/case-studies/dosomething-non-profit-tco-cost-savings/)\n\n![Dave Crusoe's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2F1515814825678_e2e8b2e5e7.jpeg&w=3840&q=75)\n\nDave Crusoe\n\nVP of Product & Engineering at DoSomething\n\nWe found that DuckDB and MotherDuck are amazing tools for small data teams like ours.\n\n[READ MORE](https://motherduck.com/case-studies/dosomething-non-profit-tco-cost-savings/)\n\nWe used to do analytics in a MySQL database with all of our daily device, telemetry, and image processing data. There was no way to scale that further using MySQL. With MotherDuck, we\u2019re finally starting to find patterns in our data to help customers grow produce more effectively.\n\n[READ MORE](https://motherduck.com/case-studies/gardyn/)\n\n![Rob Teeuwen's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2F1689248039646_d0c68f49b7.jpeg&w=3840&q=75)\n\nRob Teeuwen\n\nData Scientist at Gardyn\n\nWe used to do analytics in a MySQL database with all of our daily device, telemetry, and image processing data. There was no way to scale that further using MySQL. With MotherDuck, we\u2019re finally starting to find patterns in our data to help customers grow produce more effectively.\n\n[READ MORE](https://motherduck.com/case-studies/gardyn/)\n\nMotherDuck proving once again they are not constrained by what is easy or common. Instant SQL will not only enable complex query construction for those who find the task daunting (everyone?), it'll help past me communicate with future me.\n\n[DashFuel](https://www.getdashfuel.com/)\n\n![SJ Browne's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fsj_contemplating_6411ae5dcf.jpg&w=3840&q=75)\n\nSJ Browne\n\nSoftware Engineer at DashFuel\n\nMotherDuck proving once again they are not constrained by what is easy or common. Instant SQL will not only enable complex query construction for those who find the task daunting (everyone?), it'll help past me communicate with future me.\n\n[DashFuel](https://www.getdashfuel.com/)\n\nI just onboarded some non-technical users to MotherDuck, and I can\u2019t imagine having done so in a different system with this level of ease and lack of intimidation. Between the \u2018Filter\u2019 button, Column Explorer, and the FROM syntax - they feel empowered to answer many of their own questions! \ud83d\ude42\n\n[READ MORE](https://motherduck.com/blog/dosomething-motherduck-data-warehouse-ROI/)\n\n![Sahil Gupta's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fsahil_portrait_600bd2844e.png&w=3840&q=75)\n\nSahil Gupta\n\nSenior Data Engineer at dosomething.org\n\nI just onboarded some non-technical users to MotherDuck, and I can\u2019t imagine having done so in a different system with this level of ease and lack of intimidation. Between the \u2018Filter\u2019 button, Column Explorer, and the FROM syntax - they feel empowered to answer many of their own questions! \ud83d\ude42\n\n[READ MORE](https://motherduck.com/blog/dosomething-motherduck-data-warehouse-ROI/)\n\nBringing MotherDuck into our data stack has been a game changer. We're reducing friction and supercharging our dev and ops experience by leveraging the DuckDB interoperability across local and cloud.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nBringing MotherDuck into our data stack has been a game changer. We're reducing friction and supercharging our dev and ops experience by leveraging the DuckDB interoperability across local and cloud.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nMotherDuck is insanely performant, and there\u2019s no infrastructure to manage. The cost of experimentation is incredibly low \u2014 which means we can afford to ask more questions and build more features, faster.\n\n[READ MORE](https://motherduck.com/case-studies/kultura-capital/)\n\n![Kristov Paulus's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_05_08_at_10_29_30_AM_ac663c67b9.png&w=3840&q=75)\n\nKristov Paulus\n\nFounder, Chief Investment Officer at Kultura Capital\n\nMotherDuck is insanely performant, and there\u2019s no infrastructure to manage. The cost of experimentation is incredibly low \u2014 which means we can afford to ask more questions and build more features, faster.\n\n[READ MORE](https://motherduck.com/case-studies/kultura-capital/)\n\nInstant SQL in MotherDuck will save me the misery of having to try and wrangle SQL in my BI tool where iteration speed can be very slow. This lets me get the data right earlier in the process and with faster feedback than waiting for a chart to render, or having to clear an analytics cache.\n\n[DashFuel](https://www.getdashfuel.com/)\n\n![Mike McClannahan's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmike_e96491a8eb.jpeg&w=3840&q=75)\n\nMike McClannahan\n\nCTO, DashFuel\n\nInstant SQL in MotherDuck will save me the misery of having to try and wrangle SQL in my BI tool where iteration speed can be very slow. This lets me get the data right earlier in the process and with faster feedback than waiting for a chart to render, or having to clear an analytics cache.\n\n[DashFuel](https://www.getdashfuel.com/)\n\nMoving to MotherDuck, a billed by-the-second cloud offering is a no-brainer for us, considering the elegance and efficiency of a single node system compared to traditional OLAP solutions.\n\n[ATM.COM](https://atm.com/)\n\n![Nico Ritschel's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fnico_ritschel_photo_41ba132b33.jpg&w=3840&q=75)\n\nNico Ritschel\n\nDirector of Engineering at atm.com\n\nMoving to MotherDuck, a billed by-the-second cloud offering is a no-brainer for us, considering the elegance and efficiency of a single node system compared to traditional OLAP solutions.\n\n[ATM.COM](https://atm.com/)\n\nMotherDuck is the GOAT\n\n[READ MORE](https://motherduck.com/case-studies/reflex-careers-gig-economy-retail/)\n\n![Nate Hamm's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimage_15820c5f78.png&w=3840&q=75)\n\nNate Hamm\n\nSr Software Developer at Reflex\n\nMotherDuck is the GOAT\n\n[READ MORE](https://motherduck.com/case-studies/reflex-careers-gig-economy-retail/)\n\nMotherDuck with DuckDB was by far the fastest of the OLAP platforms we evaluated - both in the cloud and run on our developer's machines - bridging price and performance and greatly increasing productivity.\n\n[READ MORE](https://motherduck.com/case-studies/saasworks/)\n\n![Jim O\u2019Neill's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fjim_oneil_photo_930c637f49.jpg&w=3840&q=75)\n\nJim O\u2019Neill\n\nCTO and Co-founder at FinQore\n\nMotherDuck with DuckDB was by far the fastest of the OLAP platforms we evaluated - both in the cloud and run on our developer's machines - bridging price and performance and greatly increasing productivity.\n\n[READ MORE](https://motherduck.com/case-studies/saasworks/)\n\nDuckDB has a lot of folks in the data community excited\u2014it's a new and unique tool in practitioners' toolboxes and has captured the imaginations of many. MotherDuck's efforts bringing DuckDB to the cloud will broaden the reach of this promising technology.\n\n[DBT LABS](https://www.getdbt.com/)\n\n![Tristan Handy's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Ftristan_handy_photo_ef2233a588.jpg&w=3840&q=75)\n\nTristan Handy\n\nCEO & Founder at dbt Labs\n\nDuckDB has a lot of folks in the data community excited\u2014it's a new and unique tool in practitioners' toolboxes and has captured the imaginations of many. MotherDuck's efforts bringing DuckDB to the cloud will broaden the reach of this promising technology.\n\n[DBT LABS](https://www.getdbt.com/)\n\n## FAQS\n\n### How much does MotherDuck cost?\n\nMotherDuck has a generous Free Plan for dabbling and experimenting with analytics.\nFor full details on our Pricing Plans, you can learn more [here](https://motherduck.com/pricing/).\n\n### Can I use MotherDuck to query my data in S3?\n\nYes, MotherDuck plays well with AWS S3 as well as other object storage systems that are API-compatible with S3. Head over our documentation to learn how to [authenticate](https://motherduck.com/docs/integrations/cloud-storage/amazon-s3/) and [query](https://motherduck.com/docs/key-tasks/cloud-storage/querying-s3-files/) data in your S3-compatible data lake.\n\n### Can I use something other than SQL to query MotherDuck?\n\nThere are multiple clients like [Golang](https://motherduck.com/docs/integrations/language-apis-and-drivers/go-driver/) or [Python](https://motherduck.com/docs/integrations/language-apis-and-drivers/python/python-overview/) supported by DuckDB that work out of the box with MotherDuck. You can also [use natural language with our AI Functions](https://motherduck.com/docs/key-tasks/writing-sql-with-ai/) to query your data.\n\n### I use dbt. Can I use it with MotherDuck?\n\ndbt users can take advantage of the [DuckDB dbt adapter](https://github.com/duckdb/dbt-duckdb), which works well with MotherDuck dbt users. Head over to [our documentation](https://motherduck.com/docs/integrations/transformation/dbt/) for a full guide.\n\n### Can I do data visualization with MotherDuck?\n\nMotherDuck supports a wide variety of BI and data visualization tools like Omni, Hex, Tableau, PowerBI, and more. Learn more about our [Ecosystem](https://motherduck.com/ecosystem/?category=Business+Intelligence) of 40+ integrations and tools.\n\n## Get in Touch\n\nWe\u2019d love to learn more about what you\u2019re building. A Product Expert will fly right back to you with resources to help you take flight.\n\nFirst Name\n\nLast Name\n\nE-mail\n\nCompany\n\nLeave us a message.\n\nThis site is protected by reCAPTCHA and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply.\n\nSubmit\n\n![Duck looking up](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduck-looking-up.1c763865.png&w=3840&q=75)\n\nreCAPTCHA\n\nRecaptcha requires verification.\n\n[Privacy](https://www.google.com/intl/en/policies/privacy/) \\- [Terms](https://www.google.com/intl/en/policies/terms/)\n\nprotected by **reCAPTCHA**\n\n[Privacy](https://www.google.com/intl/en/policies/privacy/) \\- [Terms](https://www.google.com/intl/en/policies/terms/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 517,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 517
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search for blog posts or documentation that might discuss specific features or limitations:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ attach|storage|transaction|write|update|delete|insert

  [96mğŸ”§ TOOL:[0m Bash â†’ ls .kurt/sources/motherduck.com/blog/*.md | wc -l
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m      176
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-everybody-hates-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-learn-sql-in-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streamlining-ai-agents-duckdb-rag-solutions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/summer-data-engineering-roadmap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/tableau-cloud-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-future-of-bi-bi-as-code-duckdb-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-simple-joys-of-scaling-up.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/unstructured-analysis-tensorlake-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vector-technologies-ai-data-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vibe-coding-sql-cursor.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-workshops-agenda.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/solving-advent-code-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/spark-ducklake-getting-started.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-embeddings-for-semantic-meaning-in-text-and-rag.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-keyboard-shortcuts-for-joyful-querying.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-llm-prompt-function-gpt-models.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streaming-data-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streaming-oracle-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streamkap-mysql-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/secoda-motherduck-integration-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/semantic-layer-duckdb-tutorial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/simplifying-iot-analytics-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/six-reasons-duckdb-slaps.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-manifesto.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-recap-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-recap.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 30,
  [2m  â”‚[0m   "appliedLimit": 30
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search more specifically for topics related to DuckDB features and MotherDuck compatibility:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ ATTACH|pragma|persistent|in-memory|file format|\.db|database file
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/local-dev-cloud-prod-dagster-motherduck.md-22-\n.kurt/sources/motherduck.com/local-dev-cloud-prod-dagster-motherduck.md-23-![local dev, cloud prod](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FDagster_Mother_Duck_sized_full_text_a05f3b4eb2.png&w=3840&q=75)\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/local-dev-cloud-prod-dagster-motherduck.md-24-\n.kurt/sources/motherduck.com/local-dev-cloud-prod-dagster-motherduck.md:25:Hear from Alex Monahan, Forward Deployed Engineer at MotherDuck, and Colton Padden, Data Engineer & Developer Advocate at Dagster Labs, to learn how to aggregate your data in the cloud with MotherDuck's managed offering of DuckDB. We'll then persist our data to a local, in-memory instance of our DB within Dagster for follow-up processing.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/local-dev-cloud-prod-dagster-motherduck.md-26-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/local-dev-cloud-prod-dagster-motherduck.md-27-We're looking forward to seeing you there!\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/local-dev-cloud-prod-dagster-motherduck.md-28-\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/learn-more.md-80-\n.kurt/sources/motherduck.com/learn-more.md-81-[![Parquet File Format: What It Is, Benefits, and Alternatives](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdataframe_3_3e66a00165.png&w=3840&q=75)\\\\\n.kurt/sources/motherduck.com/learn-more.md-82-**Parquet File Format: What It Is, Benefits, and Alternatives** \\\\\n.kurt/sources/motherduck.com/learn-more.md:83:Learn what the Parquet file format is, its key benefits like columnar storage and compression, and how it compares to alternatives like CSV, Avro, Iceberg, and Delta Lake.\\\\\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/learn-more.md-84-\\\\\n.kurt/sources/motherduck.com/learn-more.md-85-Learn More](https://motherduck.com/learn-more/why-choose-parquet-table-file-format)\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/learn-more.md-86-\n.kurt/sources/motherduck.com/learn-more.md-87-[![Fast Data Exports with DuckDB's Partitioned Writes and DuckLake](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FDatabases_63cb14e85e.png&w=3840&q=75)\\\\\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/learn-more.md-88-**Fast Data Exports with DuckDB's Partitioned Writes and DuckLake** \\\\\n.kurt/sources/motherduck.com/learn-more.md:89:[Omitted long matching line]\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/learn-more.md-90-\\\\\n.kurt/sources/motherduck.com/learn-more.md-91-Learn More](https://motherduck.com/learn-more/partitioned-writes-parquet-ducklake)\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/learn-more.md-92-\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/duckdb-book-brief.md-43-\n.kurt/sources/motherduck.com/duckdb-book-brief.md-44-### [An introduction to DuckDB](https://motherduck.com/duckdb-book-summary-chapter1)\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/duckdb-book-brief.md-45-\n.kurt/sources/motherduck.com/duckdb-book-brief.md:46:- Why DuckDB, a single node in-memory database, emerged in the era of big data\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/duckdb-book-brief.md-47-- DuckDB\u2019s capabilities\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/duckdb-book-brief.md-48-- How DuckDB works and fits into your data pipeline\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/duckdb-book-brief.md-49-\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md-74-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md-75-2024/05/07\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md-76-\n.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md:77:In this discussion with Mehdi Ouazza and Pedro Holanda from DuckDB, they talk about all things CSVs and dive into a pragmatic example of how the CSV parser works in DuckDB. Enjoy!\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md-78-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md-79-## Related Videos\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md-80-\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md-98-\n.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md-99-### [Can DuckDB revolutionize the data lake experience?](https://motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience)\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md-100-\n.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md:101:Mehdi explores DuckDB as a catalog for Data Lake and Lakehouse pattern. He'll define what we mean by \"data catalog\", gives clear examples on how they work and dive into a pragmatic use case with DuckDB & MotherDuck.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md-102-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md-103-YouTube\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md-104-\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/product/duckdb-users.md-26-\n.kurt/sources/motherduck.com/product/duckdb-users.md-27-DuckDB's in-process nature means your data stays trapped on your machine. MotherDuck eliminates this limitation with a simple \"md:\" prefix.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/product/duckdb-users.md-28-\n.kurt/sources/motherduck.com/product/duckdb-users.md:29:![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fshare-data-not-files.db8baefd.png&w=3840&q=75)",
  [2m  â”‚[0m   "numLines": 50,
  [2m  â”‚[0m   "appliedLimit": 50
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me try a more targeted search for feature limitations or unsupported features:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ does not support|doesn't support|cannot|can't|won't work|not work|incompatible|restriction
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/glossary/column.md-41-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/glossary/column.md-42-## Data Types\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/glossary/column.md-43-\n.kurt/sources/motherduck.com/glossary/column.md:44:Each column must have a consistent data type - for example, a date column can't contain text strings. DuckDB supports standard SQL types like `INTEGER`, `VARCHAR`, `TIMESTAMP` but also modern types like `JSON`, `MAP`, and `STRUCT` for more complex data structures.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/glossary/column.md-45-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/glossary/column.md-46-## Best Practices\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/glossary/column.md-47-\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/glossary/primary key.md-51-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/glossary/primary key.md-52-## Key Characteristics\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/glossary/primary key.md-53-\n.kurt/sources/motherduck.com/glossary/primary key.md:54:Primary keys must contain unique values and cannot be NULL. DuckDB will automatically enforce these constraints by rejecting any insertions or updates that would violate them. While some databases allow nullable primary keys, DuckDB follows the SQL standard strictly in this regard.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/glossary/primary key.md-55-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/glossary/primary key.md-56-## Best Practices\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/glossary/primary key.md-57-\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/fees-addendum.md-27-| **Free Account** | N/A | 10 GB of storage | N/A | 10 CU-hours | N/A |\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/fees-addendum.md-28-| **Billed Account** | $25 | 3041.66667 GB-days | $0.0025685 per GB-day | 100 CU-hours | $0.25 per CU-hour |\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/fees-addendum.md-29-\n.kurt/sources/motherduck.com/fees-addendum.md:30:1. **Free Accounts**. Free Accounts are subject to limitations and restrictions which may change at any time, and MotherDuck may suspend a Free Account at any time and for any reason. Without limiting the foregoing, the following additional terms apply to Free Accounts:\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/fees-addendum.md-31-1. Limited Features. Certain features available to Commercial Accounts may be unavailable to Free Accounts.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/fees-addendum.md-32-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/fees-addendum.md-33-\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/fees-addendum.md-37-3. Internal Business Use Only. Customers having Free Accounts may use the Services solely for internal business purposes, and may not use the Services to deliver, or incorporate any part of the Services into, any product, service, or technology that is made available to third parties (including without limitation any commercial product or service).\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/fees-addendum.md-38-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/fees-addendum.md-39-\n.kurt/sources/motherduck.com/fees-addendum.md:40:4. No Circumvention. Customer shall not create multiple Free Accounts or accounts participating in Trials and Betas on the Services, including for the purposes of obtaining additional Storage Volume or Query Volume or otherwise circumventing any usage restrictions established by MotherDuck.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/fees-addendum.md-41-2. **Commercial Accounts**\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/fees-addendum.md-42-1. Billed Accounts. If Customer\u2019s account is a Billed Account, then Customer is responsible for all consumption of Query Volume and Storage Volume by all accounts on the Services associated with Billed Account\u2019s Organization. Once the Included Volume is exceeded, Customer will pay the Additional Usage Fee corresponding to consumption by the Billed Account and all associated Managed Accounts in accordance with Section 2.3 below.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/fees-addendum.md-43-\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/candidate-privacy.md-132-- Rectify inaccurate Personal Information and complete it where it is incomplete;\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/candidate-privacy.md-133-- Transmit your Personal Information to you or to another company;\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/candidate-privacy.md-134-- Erase Personal Information as permitted under applicable law;\n.kurt/sources/motherduck.com/candidate-privacy.md:135:- Obtain restriction of processing pending your objections and challenges to our processing activities, for the time necessary to answer such objections and challenges;\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/candidate-privacy.md-136-- Object to the processing of your Personal Information based on our legitimate interests and to decisions resulting from automated processing; and\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/candidate-privacy.md-137-- Withdraw any consent that you have given to MotherDuck in connection with the processing of your Personal Information.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/candidate-privacy.md-138-\n--",
  [2m  â”‚[0m   "numLines": 40,
  [2m  â”‚[0m   "appliedLimit": 40
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me use the Kurt CLI to search for content about limitations or unsupported features:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ kurt content list --limit 20 | grep -i "limit\|support\|feature"
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m â”‚ 935a1f6f... â”‚ support           â”‚ ERROR  â”‚     3 â”‚     0 â”‚ https://motherduâ€¦ â”‚
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search through the blog posts for content about limitations or differences:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md

  [96mğŸ”§ TOOL:[0m Grep â†’ shared database|multiple writer|concurrent|write.*write|ACID
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     "content": "---\ntitle: separating-storage-compute-duckdb\ncontent_type: blog\nsource_url: https://motherduck.com/blog/separating-storage-compute-duckdb\nindexed_at: '2025-11-25T19:56:19.076306'\ncontent_hash: 1946e34812cd5b95\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Separating Storage and Compute in DuckDB\n\n2024/12/17 - 15 min read\n\nBY\n\n[Jordan Tigani](https://motherduck.com/authors/jordan-tigani/)\n,\n[Boaz Leskes](https://motherduck.com/authors/boaz-leskes/)\n\n## What is Separation of Storage and Compute?\n\nThe most celebrated architectural improvement in the first wave of Cloud Data Warehouses was that storage and compute were decoupled. Instead of storing the data on the same machine that was running queries, data was stored in a remote object store. While this may seem like a relatively narrow technical difference, it removed a number of constraints in how systems were run.\n\nWhen you separate storage and compute, the first, most obvious benefit is that you can now scale compute and storage independently. In the past, the storage to compute ratios were limited by the amount of CPUs and disks you could squeeze into a server (or a small cluster of servers). It was rare that this ratio was exactly right, and if you wanted to change it, you had to buy different hardware and rebalance your data. This becomes even a bigger problem as data accumulates over time, since data tends to grow faster than the need for CPUs. But this can be hard to satisfy in a system that combines storage and compute.\n\n![Separating Storage and Compute](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fseparating_storage_compute_diagram_bg_605d13303b.png&w=3840&q=75)\n\nRunning in the cloud allows you to to benefit from using elastic services such as containerized compute and object store. If you have very heavy query workloads but not a lot of data, you can spin up additional compute nodes when you need them. And if your data size grows, you can accumulate it in an object store and even if you don't need more compute nodes to handle it. With dynamic resource allocation, we already do not need to provision based on peak usage, but rather as you go. Separating your storage from your compute means that if your compute needs peak, you only need to provision compute. As your storage needs increase, only more storage is needed.\n\nBy decoupling storage resources from the compute ones, we can now use specialized hardware. We no longer need to carefully choose the Cloud VM type use, balancing just the right mix of storage capabilities and compute power. We can now use the dedicated storage services, like the inexpensive object storage like S3. These object stores generally have very high throughput by distributing the data across thousands or millions of disks. The resulting bandwidth is thus orders of magnitude higher than more traditional systems where the storage was attached to the compute node running the query. Similarly, we can optimize compute hardware, using GPU heavy VMs for AI workloads, or 256-cores machines for heavy real time analytics.\n\nUsing dedicated services for storage also helps with both availability and durability. If your storage is attached to the local instance, you can lose data when a machine crashes. Cloud object stores usually have almost infinite data durability. The disks attached to individual instances are far less durable. And even if you don\u2019t lose data, if a node with attached storage crashes, you\u2019ll have to wait until it restarts before you can query it again, so availability can suffer.\n\n|  | Local Disk | Object Store |\n| --- | --- | --- |\n| Durability | One data loss event / 5 years per disk | 1 data loss event / 1 billion years per object |\n| Availability | Lifetime of your VM (99%?) | Availability of storage service (99.999%?) |\n| Cost | $$$$ | $ |\n\nThe first cloud data warehouse to separate storage and compute was BigQuery, and was outlined in the Dremel paper in 2008. Yes, Jordan is a little bitter about this because Snowflake claimed to have invented it several years later. Of course, as one does, when Jordan mentioned this to some database guru, he was immediately corrected and told that IBM had been separating storage and compute in the \u201880s. So there is really [nothing new](https://db.cs.cmu.edu/papers/2024/whatgoesaround-sigmodrec2024.pdf) under the sun.\n\nIn practice, separation of storage and compute allowed storage sizes to increase while compute needs grew a lot more slowly. This is one of the key assertions of \u201c [Big Data is Dead](https://motherduck.com/blog/big-data-is-dead/)\u201d: most data is \u201ccold\u201d, and so you might have ten years worth of logs but you only need to provision compute for the [\u201chot\u201d data that you query every day](https://motherduck.com/learn-more/modern-data-warehouse-use-cases/).\n\n## Why would you want to separate storage in DuckDB?\n\nMotherDuck is a single-node system, so we don\u2019t need to add additional compute nodes to handle larger queries and don\u2019t have to suffer the overhead of a distributed system. That said, storage and compute separation is still useful.\n\nFirst, we do want to be able to scale out to multiple users of the same data. For example, you might want to share data with other people in your company, which is a [key feature of MotherDuck](https://motherduck.com/docs/key-tasks/sharing-data/sharing-overview/) . DuckDB, as it is designed for single users, assumes sole ownership of its database file. This only gives you two options: either you copy the data and send it over (looking at you, CSV-over-e-mail), or send everyone to use the same single DuckDB instance. Sharing is caring, but neither seems to be a practical solution. By separating the Storage layer from the DuckDB instances, MotherDuck can share the data through [modern zero-copy techniques](https://motherduck.com/blog/differential-storage-building-block-for-data-warehouse/), while giving each user a dedicated and fast DuckDB instance.\n\nHere is an example to show how separation of storage and storage and compute helps users in MotherDuck.\n\n![Post asset](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdiagram_1_57e6161a24.png&w=3840&q=75)\n\nIn this diagram we can see that Alice creates and manages a sales database, under the covers it is stored in two files. She creates a share that can be attached by folks in her team, and they can then use it to query the same data. No copies of the data needed to be made.\n\n![Post asset](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdiagram_2_e07b5211e8.png&w=3840&q=75)\n\nNext, we can see that Alice has added data for December, which under the covers gets stored in a separate file (more on that is described below). But she has not yet validated the data, so she doesn\u2019t want it to appear to users and doesn\u2019t update the share that Bob and the rest of the company uses for reporting.\n\n![Post asset](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdiagram_3_cf9acc3360.png&w=3840&q=75)\n\nFinally, once Alice completes verification of the data, she updates the share, and the data becomes available to clients, including Bob.\n\nSeparating storage allowed us to build a dedicated storage system that allows sharing. In similar fashion, having a separate compute layer gives us the opportunity to get the most of DuckDB\u2019s versatility. We can scale up to give heavy workloads large dedicated nodes. On the other end of the spectrum, DuckDB\u2019s ability to deliver value even in extremely low resource environments (try shell.duckdb.org in your mobile browser) means we can scale down quite low.\n\nDuckDB\u2019s millisecond start up times (together with a shared cache on the storage layer) means that we can scale down to 0 quickly when the service is not used. Cold start is so fast that shutting down instances between queries becomes feasible. As long as you can spin them up again quickly, users won\u2019t be any wiser. At MotherDuck, we aim for the time-to-first query to be less than 200 ms, which is faster than most cloud databases can run a query, and within human reaction time.\n\n## How does MotherDuck separate storage and compute with DuckDB?\n\nThere are two main parts to separating storage and compute in DuckDB; first we want to be able to write to a disaggregated storage system in a way that DucKDB can mutate the data. This actually separates the storage from the compute. Second, we want to add synchronization and data sharing mechanisms to allow other DuckDB instances independently read a coherent view of the same data, even while it\u2019s being mutated.\n\nThe first part, writing to disaggregated storage is important, because we don\u2019t want to tie the availability of the data to the availability of a particular machine. That means we need to look at one of the ready-made Cloud services or build our own from scratch. External block storage services like EBS have an attractive price but can only be attached to one machine. Distributed file systems like EFS address all the technical needs but tend to be expensive, especially at the scale we\u2019re aiming for. Lastly there are object stores like S3.\n\nIf you are building a storage service on top of a cloud object stores like S3; you get a bunch of advantages out of the box. They are able to handle multiple readers, have high throughput, and are inexpensive. However, they have a problem: data in cloud object stores are immutable; that is, once you write a file, you can\u2019t modify it afterwards. This is fundamentally at odds with a database system, like DuckDB that updates data in place.\n\nFor databases that use a write-ahead log (WAL), Log Shipping is a common technique for building separation of storage and compute. This means you take the log and replay it somewhere else to generate replicas. However, this doesn\u2019t work with DuckDB, because DuckDB often skips the WAL for batch updates. This is a pretty significant performance optimization for analytics, which often deals with a lot of big updates. If those big updates had to be written to the WAL, it would require duplicating the work as well as bottleneck writes. If we tried to separate storage and compute using log shipping, we would dramatically reduce performance of updates.\n\nRather than make deep changes to how DuckDB does its writes, we decided to implement separation of storage and compute at a lower level that made changes transparent to the database. We built a Differential Storage engine which sits at the filesystem layer. We built a FUSE-based filesystem that only does appends under the covers, but it makes it look like the data has been updated. To do this, the filesystem keeps track of metadata indicating which blocks are active at a particular time. One of our engineers described it in detail in a blog post [here](https://motherduck.com/blog/differential-storage-building-block-for-data-warehouse/).\n\nTo summarize, our differential storage system works by aggregating writes into a single append-only file. We then use a transactional database (currently Postgres), to keep track of metadata like which blocks are live and where to find them. When a block is overwritten, we mark it stale and update the metadata to point to the new location of the data. When DuckDB wants to read a block, all we need to do is resolve what location to read from and then perform a direct read. Since DuckDB always flushes full pages ( [sized 256KB](https://github.com/duckdb/duckdb/blob/895fb8f/src/include/duckdb/storage/storage_info.hpp#L27)) this performs pretty well. When writing large blocks of consecutive data it works even better because they can be tracked using ranges rather than individual blocks.\n\nWriting to append-only files has another neat side effect. Since the underlying data is immutable, sharing data and copying files are now just a question of making a copy of the metadata at a specific point in time. There\u2019s no need to copy the underlying data files. They never change. That property allows explaining the name \u201cdifferential storage\u201d. If two files share common ancestry, the files only have unique reference to the difference between them while sharing the common data.\n\nLast, when an append only file is no longer, a garbage collection process can clean them up. Similarly, we run other maintenance processes, like compaction, to keep the metadata nice and tight.\n\nOne of the big advantages of the differential storage system is that it allows someone to read a consistent view of the data even while it is being changed; you just need to look at the metadata as it was at a particular time. We already hinted before that that\u2019s how we do zero-copy and sharing. However it has some other nice side effects. If you\u2019re familiar with functional programming and [immutable data structures](https://www.cs.cmu.edu/~rwh/students/okasaki.pdf), using immutable trees is a great way to provide writes and reads concurrently without having to use locks. We\u2019re effectively doing the same thing with on-disk persistent data; the metadata mapping which file ranges are active is effectively an on-disk version of Okasaki\u2019s immutable trees.\n\n## MotherDuck sharing model\n\nMany data tasks involve teams of multiple people; data engineers load and transform the data, analysts and data scientists dig for insights, business users interact with dashboards. Some sort of data sharing is required in order to allow these tasks to flow smoothly. However, DuckDB is an in-process, single-user analytical database, without a concept of users or access control. If you want to use DuckDB in a collaborative setting, you need to figure out how to make it easy to collaborate.\n\nMotherDuck was founded on the idea that you can scale up a single node to handle virtually any workload; however, when you have lots of people using the system, a single node solution may not be able to handle all of their workloads at the same time. So in adapting DuckDB to run in the cloud, we decided to scale it out in a different way; every user gets their own DuckDB instance. This way we don\u2019t have to force the concept of users inside DuckDB, and each user would be able to take advantage of the full power of their own DuckDB instance.\n\nIn order to allow different users, each of whom has their own DuckDB instance, to share the same data, we push much of the work of collaboration to the storage layer. We rely on our differential storage engine to give a point in time consistent snapshot of any database. We ensure that each reader of a database sees a \u201cclean\u201d view of the database, allowing us to work around limitations in DuckDB regarding simultaneous readers and writers. Each user can scale up or down to the size of their workload in isolation from other users, while allowing access to data created by other users.\n\nMotherDuck treats all databases as private by default. That is, when you create a database, no one else can access it until you deliberately share it; that is, you create a share. Shares in MotherDuck operate very much like Google Docs; you can choose to share via URL, which means that anyone with the link can access it. You can also share with your organization, which means that anyone in your org with the link can access it. Users can also browse organization-wide shares and discover them on their own. You can also share just with specific users.\n\nThere are still some restrictions that remain; in MotherDuck, only one user can have write access to a database. We\u2019ve solved the reading while writing part, but the multiple writer problem remains. This does somewhat limit what kinds of writes you can do, but in practice, very few workloads require simultaneous writes from different users. Generally the model that we tend to see is that data ingestion and transformation is done by a shared service account, whereas reading can be done by lots of different users. It is also often the case that data writes are to separate data universes, so these can be cleanly split between service accounts, giving more write throughput. That data can be then shared with multiple users and combined using DuckDB\u2019s multi-database support. All of these are made possible with the MotherDuck access model.\n\nThere is a further caveat; in order to give readers in other instances a clean snapshot of the data, they might not be able to see the up-to-the-moment changes that are being written by the owner of the database. If you create a share with the `AUTOUPDATE` flag, any changes will be published to readers of the share as soon as possible. However, there can be a small delay before readers see changes.\n\nSometimes a delay between changes being written and being visible to readers is useful. The writer may be making a handful of changes that they want to appear together. Imagine a pipeline that updates a number of tables and then runs unit tests; they only want to publish the results after the unit tests pass. In this case, they\u2019d create the share as a non-auto-update share, and then call the `UPDATE SHARE` command when the changes are ready to be published. Once the `UPDATE SHARE` runs, all changes will be immediately available to readers.\n\n## Future Work\n\nToday, MotherDuck supports two modes for users to publish changes to the data they have shared. In the first mode, a user can explicitly commit changes to be able to be seen in the share (via the `UPDATE SHARE` SQL command). This gives users control but also requires explicit commands. Alternatively, users can have the shares be eventually consistent, having to wait until a periodic checkpoint operation occurs. This can create some delay if you rely on readers being able to see up-to-the-moment data. We are working to reduce this gap, and will be introducing upper bounds guarantees to how long it takes to publish the data.\n\nFuture work in MotherDuck will allow multi-writer by routing writes to a single backend. That is, even if DuckDB doesn\u2019t allow multiple writers, MotherDuck can simulate it by routing updates from multiple different users to the same instance. On the read side, we can do something similar by using a scalable snapshot but also reading deltas from a live instance and directly applying it to another. This would allow us to avoid the heavy flushing and reload of memory on close and reopen of the database.\n\nAdditionally, the immutable nature of the underlying storage makes it easy to add support for features like time travel and branching. We will likely be adding those features soon. We will also be doing more work on providing caching to provide faster \u2018warm start\u2019 access to data.\n\n### Conclusion\n\nSeparation of storage and compute is useful for more than just being able handle larger datasets; it also helps you decouple workloads from physical machines and enables new data architectures. Retrofitting Separation of Storage and compute on a database that wasn\u2019t designed for it can be tricky, but also can deliver a ton of benefits.\n\nMotherDuck is standing on the shoulders of [giant ducks](https://www.snexplores.org/article/weird-new-dino-looked-more-duck), namely the DuckDB team, and they move and grow very quickly. We work very hard to keep up with them, and to continue to push the limits of what DuckDB can do.\n\n### TABLE OF CONTENTS\n\n[What is Separation of Storage and Compute?](https://motherduck.com/blog/separating-storage-compute-duckdb/#what-is-separation-of-storage-and-compute)\n\n[Why would you want to separate storage in DuckDB?](https://motherduck.com/blog/separating-storage-compute-duckdb/#why-would-you-want-to-separate-storage-in-duckdb)\n\n[How does MotherDuck separate storage and compute with DuckDB?](https://motherduck.com/blog/separating-storage-compute-duckdb/#how-does-motherduck-separate-storage-and-compute-with-duckdb)\n\n[MotherDuck sharing model](https://motherduck.com/blog/separating-storage-compute-duckdb/#motherduck-sharing-model)\n\n[Future Work](https://motherduck.com/blog/separating-storage-compute-duckdb/#future-work)\n\n!['DuckDB In Action' book cover](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduckdb-book-full-cover.68e4f598.png&w=3840&q=75)\n\nGet your free book!\n\nE-mail\n\nSubscribe to other MotherDuck news\n\nSubmit\n\nFree Book!\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![MotherDuck is Landing in Europe! Announcing our EU Region](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Feu_launch_blog_b165ff2751.png&w=3840&q=75)](https://motherduck.com/blog/motherduck-in-europe/)\n\n[2025/09/24 - Garrett O'Brien, Sheila Sitaram](https://motherduck.com/blog/motherduck-in-europe/)\n\n### [MotherDuck is Landing in Europe! Announcing our EU Region](https://motherduck.com/blog/motherduck-in-europe)\n\nServerless analytics built on DuckDB, running entirely in the EU.\n\n[![DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_4_1_b6209aca06.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n[2025/10/09 - Alex Monahan, Garrett O'Brien](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n### [DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/blog/announcing-duckdb-141-motherduck)\n\nMotherDuck now supports DuckDB 1.4.1 and DuckLake 0.3, with new SQL syntax, faster sorting, Iceberg interoperability, and more. Read on for the highlights from these major releases.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 180,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 180
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Great! I found important information. Let me read a few more relevant files:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/learn-more/acid-transactions-sql.md

  [96mğŸ”§ TOOL:[0m Grep â†’ extension|\.load|install|pragma
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/learn-more/acid-transactions-sql.md",
  [2m  â”‚[0m     "content": "---\ntitle: 'ACID Transactions in Databases: A Data Engineer''s Guide'\ncontent_type: guide\ndescription: Learn how ACID transactions ensure data consistency, integrity, and reliability\n  in SQL databases like DuckDB and SQL Server. This guide covers atomicity, consistency,\n  isolation, and durability with real-world SQL examples. Perfect for data engineers\n  and analysts building robust ETL pipelines and analytics workflows. Discover how\n  modern tools like MotherDuck bring ACID guarantees to the cloud.\npublished_date: '2025-07-30T00:00:00'\nsource_url: https://motherduck.com/learn-more/acid-transactions-sql\nindexed_at: '2025-11-25T20:37:04.970349'\ncontent_hash: 96b7cbdddd203c46\nhas_code_examples: true\nhas_step_by_step: true\nhas_narrative: true\n---\n\nEver found yourself glaring at a dashboard where the numbers just don't add up? Or maybe you've watched in horror as a data pipeline crashed halfway through, leaving your tables in complete disarray? If these scenarios sound painfully familiar, you've encountered one of the fundamental challenges in the field: keeping data consistent when the world seems determined to mess it up.\n\nThis guide walks you through ACID transactions \u2013 the reliable foundation that keeps database operations dependable even when things go wrong.\n\nIn this post, you'll learn:\n\n- What ACID transactions are (beyond just a memorable acronym)\n- Why they matter specifically for data engineers and analysts\n- How they work in SQL databases like SQL Server (and we'll look at DuckDB examples)\n- Their evolving role in the NoSQL world\n- Practical code examples you can adapt to your work\n\nThink of this as your comprehensive guide to ensuring your database changes stick when they should, disappear when they shouldn't, and generally don't leave you with a mess to clean up. Let's dive in!\n\n## What Exactly Are ACID Transactions?\n\nAt its core, ACID stands for **A**tomicity, **C**onsistency, **I**solation, and **D**urability \u2013 four properties that collectively ensure your database transactions are processed reliably. A transaction in database terms is simply a sequence of operations treated as a single logical unit of work.\n\nWhen you bundle your database operations (inserts, updates, deletes) into a transaction, the database essentially makes a promise to uphold these ACID guarantees. This is critical in any system where data integrity is non-negotiable \u2013 banking, inventory management, or really any serious data application.\n\n### A: Atomicity (All or Nothing)\n\nAtomicity ensures that a transaction is treated as a single, indivisible unit. Either all operations within the transaction succeed, or none of them do. If any part fails due to errors or crashes, the entire transaction rolls back, returning the database to its previous state.\n\nWhy care? Imagine transferring $100 from Account A to Account B. This requires two operations: debiting A and crediting B. Atomicity guarantees you won't end up in a state where A is debited but B never gets credited if something goes wrong midway.\n\n#### Code Example in DuckDB\n\nCopy code\n\n```\n-- First, let's create a sample Accounts table if it doesn't exist\nCREATE TABLE IF NOT EXISTS Accounts (\nAccountID INTEGER PRIMARY KEY,\nBalance DECIMAL(10,2)\n);\n-- Insert sample data if table is empty\nINSERT OR IGNORE INTO Accounts VALUES\n(1, 500.00),\n(2, 300.00);\n-- Check balances before transfer\nSELECT * FROM Accounts WHERE AccountID IN (1, 2);\n-- Start the transaction\nBEGIN TRANSACTION;\n-- Step 1: Debit Account A\nUPDATE Accounts\nSET Balance = Balance - 100\nWHERE AccountID = 1;\n-- Step 2: Credit Account B\nUPDATE Accounts\nSET Balance = Balance + 100\nWHERE AccountID = 2;\n-- Commit the transaction to make changes permanent\nCOMMIT;\n-- Check balances after transfer\nSELECT * FROM Accounts WHERE AccountID IN (1, 2);\n-- Note: If you need to cancel the transaction, you would use:\n-- ROLLBACK;\n-- instead of COMMIT;\n```\n\n\n#### Code Explanation\n\n`BEGIN TRANSACTION;`\n\nmarks the start of our atomic unit.- The first\n`UPDATE`\n\nstatement debits money from Account A. The second`UPDATE`\n\nstatement credits money to Account B. `COMMIT;`\n\nfinalizes the transaction, making the changes permanent if both operations succeeded.`ROLLBACK;`\n\n(when needed) undoes all changes if any error occurs between`BEGIN`\n\nand`COMMIT`\n\n.\n\n**Practical Tip:** In production systems, it's typical to wrap these operations in TRY...CATCH blocks (or the equivalent in the chosen language/framework) to automatically trigger a `ROLLBACK`\n\nif any error occurs within the transaction.\n\n### C: Consistency (Keeping it Valid)\n\nConsistency ensures that a transaction brings the database from one valid state to another. This means the data not only makes logical sense (like the total money in our bank example staying the same), but also adheres to all defined database rules and constraints (`NOT NULL`\n\n, `UNIQUE`\n\n, `CHECK`\n\n, `FOREIGN KEY`\n\n).\n\nWhy care? This prevents bad data from entering your database. If you have a rule that account balances cannot be negative, a transaction attempting to overdraw an account would fail consistency checks and roll back.\n\n#### Code Example (Adding Constraints in DuckDB)\n\nCopy code\n\n```\n-- Example showing constraints in DuckDB\n-- Important: DuckDB does not support ADD CONSTRAINT and DROP CONSTRAINT in ALTER TABLE statements\n-- Since we can't add constraints after table creation, we need to create the table with all constraints\nDROP TABLE IF EXISTS Accounts;\n-- Create the table with the CHECK constraint included from the beginning\nCREATE TABLE Accounts (\nAccountID INTEGER PRIMARY KEY,\nBalance DECIMAL(10,2) CHECK (Balance >= 0) -- Constraint defined at table creation time\n);\n-- Insert sample data\nINSERT INTO Accounts VALUES\n(1, 100.00), -- Account 1 has exactly $100\n(2, 300.00);\n-- Show initial account balances\nSELECT * FROM Accounts WHERE AccountID IN (1, 2);\n-- Now, let's try a transaction that violates this consistency rule\nBEGIN TRANSACTION;\n-- Try to debit $150 from Account A which only has $100\n-- This will fail due to the CHECK constraint\nUPDATE Accounts\nSET Balance = Balance - 150\nWHERE AccountID = 1;\n-- The following line won't execute if the UPDATE fails\n-- but we include it for completeness\nSELECT 'After update attempt (this may not run if constraint fails)' AS Status,\n* FROM Accounts WHERE AccountID = 1;\n-- This COMMIT won't happen automatically if the constraint fails\n-- DuckDB will roll back automatically on constraint violation\nCOMMIT;\n-- Check final account balances - should be unchanged due to rollback\nSELECT 'Final balances after constraint violation' AS Status,\n* FROM Accounts WHERE AccountID IN (1, 2);\n-- Let's try a valid transaction now\nBEGIN TRANSACTION;\n-- Withdraw just $50 instead (which is valid)\nUPDATE Accounts\nSET Balance = Balance - 50\nWHERE AccountID = 1;\n-- Show the pending change\nSELECT 'Valid update in progress' AS Status,\n* FROM Accounts WHERE AccountID = 1;\n-- Commit the valid transaction\nCOMMIT;\n-- Show final state after valid transaction\nSELECT 'Final balances after valid transaction' AS Status,\n* FROM Accounts WHERE AccountID IN (1, 2);\n```\n\n\n#### Code Explanation\n\n`CREATE TABLE ... CHECK ...`\n\ndefines a rule at the database level when the table is created (unlike some other databases, DuckDB requires constraints to be defined at creation time, not with`ALTER TABLE`\n\n).- Our\n`TRANSACTION`\n\nattempts an operation that would result in a negative balance. - The database engine enforces the\n`CHECK`\n\nconstraint, causing the`UPDATE`\n\nstatement to fail. - Since the transaction cannot complete while satisfying all constraints, it is automatically rolled back by DuckDB.\n- A subsequent transaction with a valid withdrawal amount succeeds because it maintains the constraint.\n\n[does not currently support](https://duckdb.org/docs/stable/sql/statements/alter_table.html#add--drop-constraint)adding or dropping constraints after table creation with `ALTER TABLE` statements. All constraints must be defined during the initial `CREATE TABLE` statement, unlike some other database systems that allow constraints to be modified later.\n\n### I: Isolation (Playing Nicely with Others)\n\nIsolation ensures that concurrent transactions don't interfere with each other. From any single transaction's perspective, it should appear as if it's the only one operating on the database at that moment.\n\nThis prevents phenomena like:\n\n**Dirty Reads:**Reading data another transaction has modified but not yet committed**Non-Repeatable Reads:**Getting different results when reading the same row multiple times within a transaction**Phantom Reads:**Getting different results when querying a range of rows because another transaction inserted or deleted matching rows\n\nWhy care? Without isolation, imagine running a financial report that sums up sales while another transaction is actively adding new sales records. Your report could end up with inconsistent numbers depending on timing.\n\nDatabases implement isolation using mechanisms like locking or multiversion concurrency control (MVCC). SQL defines standard [transaction isolation levels](https://en.wikipedia.org/wiki/Isolation_(database_systems)) (`READ UNCOMMITTED`\n\n, `READ COMMITTED`\n\n, `REPEATABLE READ`\n\n, `SERIALIZABLE`\n\n) that let you balance consistency against performance.\n\n#### Code Example (Isolation in DuckDB)\n\nCopy code\n\n```\n-- Drop and recreate the Accounts table\nDROP TABLE IF EXISTS Accounts;\nCREATE TABLE Accounts (\nAccountID INTEGER PRIMARY KEY,\nBalance DECIMAL(10,2) CHECK (Balance >= 0)\n);\n-- Insert sample data\nINSERT INTO Accounts VALUES\n(1, 100.00),\n(2, 300.00);\n-- DuckDB uses snapshot isolation by default, which is equivalent to SERIALIZABLE\n-- and provides the highest isolation level\n-- Start a transaction\nBEGIN TRANSACTION;\n-- With DuckDB's snapshot isolation, if you read rows,\n-- you'll get a consistent snapshot of the database at the start of the transaction\n-- regardless of other concurrent transactions\n-- Take a snapshot of the total balance at the beginning of the transaction\nSELECT 'Initial snapshot of total balance' AS QueryInfo, SUM(Balance) AS TotalBalance\nFROM Accounts;\n-- Some other operations...\n-- Let's simulate some time passing by doing another operation\nSELECT COUNT(*) AS TotalAccounts FROM Accounts;\n-- Rerun the same query - guaranteed to get the same result\n-- even if another transaction added or modified rows in between (due to snapshot)\nSELECT 'Second snapshot (should match initial)' AS QueryInfo, SUM(Balance) AS TotalBalance\nFROM Accounts;\n-- Commit the transaction\nCOMMIT;\n```\n\n\n#### Code Explanation\n\n- In DuckDB, transaction isolation is handled automatically with snapshot isolation (equivalent to SERIALIZABLE in the SQL standard), so there's no need to explicitly set isolation levels.\n- When a transaction begins with\n`BEGIN TRANSACTION`\n\n, DuckDB creates a consistent snapshot of the database that remains stable throughout the transaction's lifetime. - This snapshot isolation guarantees that all reads within the transaction see a consistent view of the database as it existed at the start of the transaction, regardless of changes made by other concurrent transactions.\n- The database enforces these isolation rules automatically to prevent concurrency issues while providing the highest level of transaction integrity.\n\n### D: Durability (Making it Stick)\n\nDurability guarantees that once a transaction has been successfully committed, its changes are permanent and will survive system failures like power outages or server crashes.\n\nWhy care? This is your ultimate safety net. If your `COMMIT TRANSACTION`\n\ncall returns successfully, you have the database's promise that the data is safely stored and won't vanish even if the power goes out immediately afterward.\n\nDatabases typically achieve durability using techniques like [write-ahead logging (WAL)](https://en.wikipedia.org/wiki/Write-ahead_logging). Changes are written to a transaction log file on stable storage before the actual database files are modified. If a crash occurs, the database can use these logs during recovery.\n\n#### Code Example\n\nCopy code\n\n```\nBEGIN TRANSACTION;\n-- Perform critical updates\n-- Deduct $50 from Account 1\nUPDATE Accounts SET Balance = Balance - 50.00 WHERE AccountID = 1;\n-- Add $50 to Account 2\nUPDATE Accounts SET Balance = Balance + 50.00 WHERE AccountID = 2;\n-- This is the point of no return.\n-- Once COMMIT succeeds, the changes are durable.\nCOMMIT TRANSACTION;\n-- If the server crashes after this COMMIT returns success,\n-- the changes are guaranteed to persist through recovery.\n```\n\n\n#### Code Explanation\n\n- The\n`COMMIT TRANSACTION`\n\nsignals the successful end of the transaction and ensures that all updates \u2014 such as transferring funds between accounts \u2014 are made durable. - DuckDB, like other ACID-compliant systems, handles the underlying durability mechanisms (e.g., write-ahead logging) automatically. Once\n`COMMIT`\n\ncompletes, the changes are safely persisted and will survive a crash or restart.\n\n## What About ACID Transactions in NoSQL?\n\nHistorically, many NoSQL databases prioritized availability and scalability over strict ACID guarantees, often following BASE (Basically Available, Soft state, Eventually consistent) principles.\n\nHowever, the landscape has evolved significantly:\n\n- Many NoSQL databases now offer ACID guarantees for operations within a single document or record\n- Several modern NoSQL systems (MongoDB, RavenDB, FaunaDB, certain DynamoDB configurations) support multi-document ACID transactions\n- Implementation details and performance characteristics may differ from traditional relational databases\n\nSo, can NoSQL be ACID-compliant? Increasingly, yes \u2013 but check the specific capabilities of your chosen database.\n\n## MotherDuck and ACID Compliance\n\nSpeaking of modern database solutions, [MotherDuck](https://motherduck.com/) \u2013 the cloud service for [DuckDB](https://duckdb.org/) \u2013 brings ACID transaction support to analytics workloads. As a managed service built on DuckDB's foundation, MotherDuck maintains DuckDB's ACID compliance while adding cloud-native features. This means data engineers can confidently run their analytical queries and transformations with the same transactional guarantees we've discussed, even at scale. When you're processing large analytical datasets, having these ACID guarantees prevents the kind of partial updates or inconsistent results that can waddle their way into your dashboards and reports. For data teams looking to maintain data integrity across local and cloud environments, MotherDuck's approach ensures your analytical pipelines don't sacrifice reliability for performance.\n\n## Why Should Data Engineers and Analysts Care About ACID?\n\nUnderstanding ACID is essential for data professionals:\n\n**Data Integrity:**Understanding ACID helps you design data pipelines and transformation logic that preserves correctness and reliability. No more mysterious inconsistencies from partial updates in failed batch jobs.**Troubleshooting:**When things go wrong (and they will), knowledge of transactions provides a framework for debugging. Was data lost? Check durability. Seeing weird intermediate values? Suspect isolation issues. Pipeline failed mid-way? Thank atomicity for rolling things back cleanly.**System Design:**When choosing databases or designing data flows, understanding trade-offs between ACID and other models helps you select the right tool based on consistency needs versus performance requirements.**Writing Robust Code:**Explicitly using`BEGIN`\n\n,`COMMIT`\n\n, and`ROLLBACK`\n\nconstructs makes your code significantly more reliable and easier to reason about.\n\n## Wrapping Up\n\nACID transactions aren't just theoretical database concepts \u2013 they're fundamental pillars ensuring the reliability of data we work with daily. Whether you're building ETL pipelines, modifying data in SQL databases like DuckDB or SQL Server, or even working with modern NoSQL databases, understanding Atomicity, Consistency, Isolation, and Durability helps you build more robust systems and avoid those frustrating data anomalies.\n\nKeep these ACID properties in your toolkit, and your data will remain as organized as ducks in a row. Your future self (and your data consumers) will thank you!\n\n**Frequently Asked Questions (FAQ)**\n\n**1. What is ACID in simple terms?**\n\nACID stands for Atomicity, Consistency, Isolation, and Durability. It's a set of four properties that guarantee database transactions are processed reliably. In simple terms, it's a contract that ensures your data remains correct and uncorrupted, even when multiple operations happen at once or if the system crashes.\n\n**2. Why are ACID transactions important for data integrity?**\n\nACID transactions are the foundation of data integrity in relational databases. They prevent common data corruption scenarios:\n\n**Atomicity**stops partial updates from failed pipelines. This means that a transaction is an \"all or nothing\" event; either the entire transaction completes successfully, or it leaves the database unchanged.**Consistency**enforces data quality rules (e.g., a product must have a price), ensuring the database transitions from one valid state to another.**Isolation**prevents analytics queries from reading incomplete or \"dirty\" data by making sure concurrent transactions do not interfere with each other.**Durability**ensures that once data is written, it's saved permanently, even in the event of a system failure.\n\nWithout these guarantees, it would be nearly impossible to trust the data in your dashboards, reports, or applications.\n\n**3. Is DuckDB fully ACID compliant?**\n\nYes, [DuckDB is fully ACID compliant](https://duckdb.org/2024/09/25/changing-data-with-confidence-and-acid.html). It supports serializable transactions, which is the highest level of isolation. This is achieved through a custom, bulk-optimized Multi-Version Concurrency Control (MVCC) system. This makes it a reliable tool for data professionals who need to perform complex, multi-step analytical queries and transformations with the same data integrity guarantees found in larger server-based databases like PostgreSQL or SQL Server.\n\n**4. Do NoSQL databases use ACID?**\n\nIt's a mixed bag. Historically, many NoSQL databases sacrificed strict ACID compliance for higher availability and scalability, a model often described by the acronym BASE (Basically Available, Soft state, Eventual consistency). However, many modern NoSQL databases, such as [MongoDB](https://www.mongodb.com/resources/products/capabilities/acid-compliance), now offer full ACID compliance, especially for multi-document transactions, as the demand for data reliability has grown.\n\n**5. What's a real-world example of an ACID transaction?**\n\nA classic example is an e-commerce order placement. The process involves multiple steps that must be treated as a single transaction:\n\n- Decrease the product's stock level in the inventory table.\n- Create a new order in the orders table.\n- Process the payment via a payment gateway.\n\nIf the payment fails (Step 3), **atomicity** ensures the inventory is restocked and the order is canceled (rolled back). **Consistency** ensures the stock level can't go below zero. **Isolation** prevents another customer from buying the last item while your order is being processed. **Durability** ensures that once your order is confirmed, it remains in the system even if the server reboots.\n\nStart using MotherDuck now!",
  [2m  â”‚[0m     "numLines": 361,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 361
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": ".kurt/sources/motherduck.com/learn-more.md-207-[![DuckDB Python Quickstart (Part 1): Your Complete Guide to Fast Data Analytics](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Flearn_more_duck_db_df1112cb47_151aeeba16.png&w=3840&q=75)\\\\\n.kurt/sources/motherduck.com/learn-more.md-208-**DuckDB Python Quickstart (Part 1): Your Complete Guide to Fast Data Analytics** \\\\\n.kurt/sources/motherduck.com/learn-more.md:209:Get started with DuckDB and Python in this complete quickstart guide. Learn how to install, connect, and run incredibly fast SQL queries directly on CSVs, Parquet files, and more: no server needed.\\\\\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/learn-more.md-210-\\\\\n.kurt/sources/motherduck.com/learn-more.md-211-Learn More](https://motherduck.com/learn-more/duckdb-python-quickstart-part1)\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/glossary/metadata.md-27-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/glossary/metadata.md-28--- Get column statistics\n.kurt/sources/motherduck.com/glossary/metadata.md:29:SELECT * FROM pragma_table_info('mytable');\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/glossary/metadata.md-30-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/glossary/metadata.md-31--- View system-wide metadata\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/index.md-720-@dacort\\\\\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/index.md-721-\\\\\n.kurt/sources/motherduck.com/index.md:722:Because, I don't have enough time on my hands, I created a DuckDB extension that allows you to import data by scanning an Athena table. \\\\\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/index.md-723-\\\\\n.kurt/sources/motherduck.com/index.md:724:https://github.com/dacort/duckdb-athena-extension](https://twitter.com/dacort/status/1633952720561516544)\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/index.md-725-\n.kurt/sources/motherduck.com/index.md-726-[![George Fraser profile picture](https://motherduck.com/_next/image/?url=https%3A%2F%2Fpbs.twimg.com%2Fprofile_images%2F1571011203736174593%2FobtbMbnD_x96.jpg&w=96&q=75)\\\\\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/product/pricing.md-562-## Support with a heart\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/product/pricing.md-563-\n.kurt/sources/motherduck.com/product/pricing.md:564:Great technology deserves a remarkable customer experience to match. We believe a thoughtful, human-first approach to delivering great support is as important as building great products. Our Customer team genuinely cares about your success and strives to act as an extension of your own team.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/product/pricing.md-565-\n.kurt/sources/motherduck.com/product/pricing.md-566-[LEARN MORE](https://motherduck.com/customer-support/)\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md-75-2024/05/07\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md-76-\n.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md:77:In this discussion with Mehdi Ouazza and Pedro Holanda from DuckDB, they talk about all things CSVs and dive into a pragmatic example of how the CSV parser works in DuckDB. Enjoy!\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md-78-",
  [2m  â”‚[0m   "numLines": 30,
  [2m  â”‚[0m   "appliedLimit": 30
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search for more specific information about extensions and features:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md

  [96mğŸ”§ TOOL:[0m Grep â†’ custom.*extension|user.*defined|UDF
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md",
  [2m  â”‚[0m     "content": "---\ntitle: the-magic-of-duckdb-extensions-for-data-engineering-and-analytics\ncontent_type: event\nsource_url: https://motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics\nindexed_at: '2025-11-25T20:44:16.174430'\ncontent_hash: 842f826631864bf3\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nThe magic of #duckdb extensions \ud83e\ude84 #dataengineering #database #shorts - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[The magic of #duckdb extensions \ud83e\ude84 #dataengineering #database #shorts](https://www.youtube.com/watch?v=Hgdbm-4IsC0)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=Hgdbm-4IsC0&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 0:57\n\n\u2022Live\n\n\u2022\n\nYouTubeShort\n\n# The magic of DuckDB extensions for data engineering and analytics \ud83e\ude84\n\n2023/04/23\n\nThis short from Mehdi Ouazza talks about DuckDB for OLAP and how DuckDB extensions are installed and loaded.\n\n## Related Videos\n\n[![\"One data tool with all its dependencies: DuckDB and extensions\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fvideo_we_RMT_Aj_Udic_9e9f5c6d41.jpg&w=3840&q=75)\\\\\n\\\\\n0:00:55](https://motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions/)\n\n[2024-06-21](https://motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions/)\n\n### [One data tool with all its dependencies: DuckDB and extensions](https://motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions)\n\nLearn about DuckDB extensions, including the ability to query data in your AWS S3-powered data lake.\n\nYouTube\n\nShort\n\n[![\"No SQL standard for 7 years ?!\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fvideo_Joro_Ss_Keq_Sk_9a6deced1a.jpg&w=3840&q=75)\\\\\n\\\\\n0:00:47](https://motherduck.com/videos/no-sql-standard-for-7-years/)\n\n[2024-01-22](https://motherduck.com/videos/no-sql-standard-for-7-years/)\n\n### [No SQL standard for 7 years ?!](https://motherduck.com/videos/no-sql-standard-for-7-years)\n\nThis short of Mehdi with Alex Monahan discusses the lack of evolution of the SQL standard. \u201c(SQL) stands for Structured Query Language, not Standard Query Language\u201d\n\nYouTube\n\nShort\n\nSQL\n\n[![\"Simple way to convert CSV and Parquet files\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fvideo_Y_G_Xdbet9_Gk_6fc9d7bdd0.jpg&w=3840&q=75)\\\\\n\\\\\n0:01:00](https://motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files/)\n\n[2024-01-17](https://motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files/)\n\n### [Simple way to convert CSV and Parquet files](https://motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files)\n\nWant convert CSV to Parquet or vice versa? In this YouTube short, Mehdi shows you how to convert using DuckDB as a Swiss Army Knife to convert data formats.\n\nYouTube\n\nShort\n\nTutorial\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 129,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 129
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/product-analytics-motherduck-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/web-assembly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 15,
  [2m  â”‚[0m   "appliedLimit": 15
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! Now I have enough information. Let me write the answer based on what I've found:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_3.md

  [96mğŸ”§ TOOL:[0m Bash â†’ touch /tmp/answer_without_kg_3.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m 
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_3.md

  [96mğŸ”§ TOOL:[0m Read â†’ /tmp/answer_without_kg_3.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/tmp/answer_without_kg_3.md",
  [2m  â”‚[0m     "content": "# SQL Features from DuckDB Not Yet Supported in MotherDuck\n\nBased on the MotherDuck documentation, there are several SQL features available in DuckDB that are not yet fully supported in MotherDuck's cloud environment. While MotherDuck maintains strong compatibility with DuckDB and uses the same engine both locally and in the cloud, some advanced features remain unavailable on the server-side.\n\n## Key Unsupported Features\n\n### 1. Custom User-Defined Functions (UDFs)\n\n**Python UDFs and Native UDFs** are not supported on the MotherDuck server-side. This includes:\n- Custom Python functions that extend DuckDB's capabilities\n- Native/compiled UDFs written in C/C++\n\n**Workaround**: Perform these transformations upstream (on the client-side or in your data pipeline) before loading data into MotherDuck.\n\n### 2. Server-Side `ATTACH` Operations\n\nThe **`ATTACH` statement** for connecting to other databases (such as PostgreSQL, MySQL, or SQLite) is not available on the MotherDuck server. DuckDB locally supports attaching to various databases to query them directly, but this capability is not yet enabled in MotherDuck's cloud environment.\n\n**Workaround**: Export data from external databases and load it into MotherDuck, or perform cross-database queries locally using DuckDB's dual execution model.\n\n### 3. Custom Extensions\n\n**Custom DuckDB extensions** cannot be loaded on the MotherDuck server. While DuckDB has a rich ecosystem of extensions for various data formats, protocols, and functionality (spatial data, HTTP, JSON, etc.), custom or third-party extensions beyond what MotherDuck provides are not supported in the cloud environment.\n\n**Workaround**: Use client-side DuckDB with the required extensions for preprocessing, or leverage MotherDuck's supported built-in extensions.\n\n## Architectural Context\n\nIt's important to understand that these limitations stem from MotherDuck's serverless, managed architecture. MotherDuck prioritizes:\n- **Security**: Preventing arbitrary code execution in the cloud environment\n- **Performance**: Maintaining predictable, optimized query execution\n- **Stability**: Ensuring consistent behavior across all users\n\nThe dual-execution model allows many of these operations to be performed locally on the client side, where full DuckDB functionality is available, while still benefiting from MotherDuck's cloud-scale storage and compute for large datasets.\n\n## Other Considerations\n\n### Regional Availability\nWhile not a SQL feature limitation, MotherDuck currently operates in:\n- AWS `us-east-1` (US East)\n- AWS `eu-central-1` (Frankfurt, Europe)\n\nTeams with strict data residency requirements outside these regions will need to consider this constraint.\n\n### Workload Limitations\nMotherDuck is optimized for **OLAP (analytical)** workloads, not **OLTP (transactional)** workloads. This means:\n- Not designed for thousands of concurrent, low-latency writes and updates per second\n- Database-level permissions rather than fine-grained row-level security\n- Not currently designed to serve thousands of simultaneous public-facing analytical queries\n\n## Compatibility Strengths\n\nDespite these limitations, MotherDuck maintains excellent compatibility with DuckDB for the vast majority of analytical SQL operations, including:\n- Full SQL query syntax support\n- Columnar storage and vectorized execution\n- ACID transactions\n- Complex joins and aggregations\n- Direct querying of Parquet, CSV, and JSON files\n- Integration with cloud object storage (S3, GCS, Azure Blob Storage)\n- Hybrid execution (joining local and cloud data)\n\n## Summary\n\nThe main SQL features from DuckDB not yet supported in MotherDuck are:\n1. **Custom Python/Native User-Defined Functions (UDFs)**\n2. **Server-side `ATTACH` to external databases**\n3. **Custom DuckDB extensions**\n\nThese limitations are architectural choices that enable MotherDuck to provide a secure, performant, and serverless analytics platform. For most analytical workloads, the combination of MotherDuck's cloud capabilities and local DuckDB's full feature set (via dual execution) provides a powerful and flexible solution.\n\n---\n\n## Sources\n\n- **Fix Slow BI Dashboards Guide** - [motherduck.com/learn-more/fix-slow-bi-dashboards](https://motherduck.com/learn-more/fix-slow-bi-dashboards)\n  - States: \"MotherDuck does not yet support the full range of DuckDB's SQL features. Notably, custom Python/native User-Defined Functions (UDFs), server-side `ATTACH` to other databases (like Postgres), and custom extensions are not yet available.\"\n\n- **MotherDuck: A BigQuery Alternative** - [motherduck.com/learn-more/bigquery-alternative-motherduck](https://motherduck.com/learn-more/bigquery-alternative-motherduck)\n  - Discusses MotherDuck's hybrid architecture and the distinction between client-side and server-side capabilities\n  - Highlights known trade-offs including limitations for high-throughput transactional workloads and fine-grained row-level security\n\n- **Hybrid Analytics Guide** - [motherduck.com/learn-more/hybrid-analytics-guide](https://motherduck.com/learn-more/hybrid-analytics-guide)\n  - Explains the dual execution model and when to use local vs. cloud execution\n  - Discusses scenarios where the hybrid approach may not be suitable\n\n- **DuckDB Users Product Page** - [motherduck.com/product/duckdb-users](https://motherduck.com/product/duckdb-users/)\n  - Overview of MotherDuck's compatibility with DuckDB and its cloud-scale production capabilities\n",
  [2m  â”‚[0m     "numLines": 88,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 88
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Great! The file already exists with content. Now let me update it with my more comprehensive findings:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Edit â†’ /tmp/answer_without_kg_3.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_3.md",
  [2m  â”‚[0m   "oldString": "# SQL Features from DuckDB Not Yet Supported in MotherDuck\n\nBased on the MotherDuck documentation, there are several SQL features available in DuckDB that are not yet fully supported in MotherDuck's cloud environment. While MotherDuck maintains strong compatibility with DuckDB and uses the same engine both locally and in the cloud, some advanced features remain unavailable on the server-side.\n\n## Key Unsupported Features\n\n### 1. Custom User-Defined Functions (UDFs)\n\n**Python UDFs and Native UDFs** are not supported on the MotherDuck server-side. This includes:\n- Custom Python functions that extend DuckDB's capabilities\n- Native/compiled UDFs written in C/C++\n\n**Workaround**: Perform these transformations upstream (on the client-side or in your data pipeline) before loading data into MotherDuck.\n\n### 2. Server-Side `ATTACH` Operations\n\nThe **`ATTACH` statement** for connecting to other databases (such as PostgreSQL, MySQL, or SQLite) is not available on the MotherDuck server. DuckDB locally supports attaching to various databases to query them directly, but this capability is not yet enabled in MotherDuck's cloud environment.\n\n**Workaround**: Export data from external databases and load it into MotherDuck, or perform cross-database queries locally using DuckDB's dual execution model.\n\n### 3. Custom Extensions\n\n**Custom DuckDB extensions** cannot be loaded on the MotherDuck server. While DuckDB has a rich ecosystem of extensions for various data formats, protocols, and functionality (spatial data, HTTP, JSON, etc.), custom or third-party extensions beyond what MotherDuck provides are not supported in the cloud environment.\n\n**Workaround**: Use client-side DuckDB with the required extensions for preprocessing, or leverage MotherDuck's supported built-in extensions.\n\n## Architectural Context\n\nIt's important to understand that these limitations stem from MotherDuck's serverless, managed architecture. MotherDuck prioritizes:\n- **Security**: Preventing arbitrary code execution in the cloud environment\n- **Performance**: Maintaining predictable, optimized query execution\n- **Stability**: Ensuring consistent behavior across all users\n\nThe dual-execution model allows many of these operations to be performed locally on the client side, where full DuckDB functionality is available, while still benefiting from MotherDuck's cloud-scale storage and compute for large datasets.\n\n## Other Considerations\n\n### Regional Availability\nWhile not a SQL feature limitation, MotherDuck currently operates in:\n- AWS `us-east-1` (US East)\n- AWS `eu-central-1` (Frankfurt, Europe)\n\nTeams with strict data residency requirements outside these regions will need to consider this constraint.\n\n### Workload Limitations\nMotherDuck is optimized for **OLAP (analytical)** workloads, not **OLTP (transactional)** workloads. This means:\n- Not designed for thousands of concurrent, low-latency writes and updates per second\n- Database-level permissions rather than fine-grained row-level security\n- Not currently designed to serve thousands of simultaneous public-facing analytical queries\n\n## Compatibility Strengths\n\nDespite these limitations, MotherDuck maintains excellent compatibility with DuckDB for the vast majority of analytical SQL operations, including:\n- Full SQL query syntax support\n- Columnar storage and vectorized execution\n- ACID transactions\n- Complex joins and aggregations\n- Direct querying of Parquet, CSV, and JSON files\n- Integration with cloud object storage (S3, GCS, Azure Blob Storage)\n- Hybrid execution (joining local and cloud data)\n\n## Summary\n\nThe main SQL features from DuckDB not yet supported in MotherDuck are:\n1. **Custom Python/Native User-Defined Functions (UDFs)**\n2. **Server-side `ATTACH` to external databases**\n3. **Custom DuckDB extensions**\n\nThese limitations are architectural choices that enable MotherDuck to provide a secure, performant, and serverless analytics platform. For most analytical workloads, the combination of MotherDuck's cloud capabilities and local DuckDB's full feature set (via dual execution) provides a powerful and flexible solution.",
  [2m  â”‚[0m   "newString": "# SQL Features from DuckDB Not Yet Supported in MotherDuck\n\nBased on the MotherDuck documentation, there are several SQL features and capabilities from DuckDB that have limitations or are not yet fully supported in MotherDuck due to its cloud architecture. While MotherDuck maintains strong compatibility with DuckDB and uses the same engine both locally and in the cloud, some advanced features remain unavailable on the server-side.\n\n## Key Unsupported Features\n\n### 1. Multiple Simultaneous Writers\n\n**Limitation:** MotherDuck currently **only supports one writer per database** at a time. While multiple users can read from the same shared database simultaneously, only one user can have write access.\n\n**Why:** This is a fundamental architectural constraint of MotherDuck's storage system. Even though MotherDuck has solved the \"reading while writing\" problem through its differential storage engine and snapshot isolation, the multiple writer problem remains unsolved.\n\n**Impact:** Workloads requiring simultaneous writes from different users need to be structured carefully. In practice, the typical pattern is:\n- Data ingestion and transformation done by a shared service account\n- Reading can be done by many different users\n- Data writes to separate data universes can be split between service accounts\n\n**Future:** MotherDuck is working on multi-writer support by routing writes to a single backend, simulating multiple writers even though DuckDB doesn't natively allow it.\n\n### 2. Real-Time Write Visibility for Shared Databases\n\n**Limitation:** When databases are shared with other users, there can be a **delay before readers see the latest changes** made by the database owner.\n\n**Two Modes:**\n- **AUTOUPDATE shares:** Changes are published to readers as soon as possible, but there can still be a small delay\n- **Manual shares:** Require explicit `UPDATE SHARE` commands to publish changes to readers\n\n**Why:** To give readers a \"clean snapshot\" of the data, MotherDuck ensures consistency at the storage layer. Readers might not be able to see up-to-the-moment changes being written by the owner.\n\n**Use Case:** Sometimes this delay is actually useful - for example, when making a handful of changes that should appear together, or running unit tests before publishing results.\n\n**Future:** MotherDuck is working to reduce this gap and will introduce upper bounds guarantees on how long it takes to publish data.\n\n### 3. Custom User-Defined Functions (UDFs)\n\n**Python UDFs and Native UDFs** are not supported on the MotherDuck server-side. This includes:\n- Custom Python functions that extend DuckDB's capabilities\n- Native/compiled UDFs written in C/C++\n\n**Workaround**: Perform these transformations upstream (on the client-side or in your data pipeline) before loading data into MotherDuck.\n\n### 4. Server-Side `ATTACH` Operations\n\nThe **`ATTACH` statement** for connecting to other databases (such as PostgreSQL, MySQL, or SQLite) is not available on the MotherDuck server. DuckDB locally supports attaching to various databases to query them directly, but this capability is not yet enabled in MotherDuck's cloud environment.\n\n**Workaround**: Export data from external databases and load it into MotherDuck, or perform cross-database queries locally using DuckDB's dual execution model.\n\n### 5. Custom Extensions\n\n**Custom DuckDB extensions** cannot be loaded on the MotherDuck server. While DuckDB has a rich ecosystem of extensions for various data formats, protocols, and functionality (spatial data, HTTP, JSON, etc.), custom or third-party extensions beyond what MotherDuck provides are not supported in the cloud environment.\n\n**Why:** MotherDuck uses DuckDB's extension system to add its own capabilities, but for security and stability reasons in a multi-tenant cloud environment, not all custom or third-party extensions may be supported.\n\n**Workaround**: Use client-side DuckDB with the required extensions for preprocessing, or leverage MotherDuck's supported built-in extensions.\n\n### 6. Certain DuckDB Configuration Settings\n\n**Limitation:** Some DuckDB PRAGMA settings or configuration options that control local behavior (like memory limits, thread counts for local execution, or file system paths) may work differently or not be available in MotherDuck's cloud environment.\n\n**Why:** MotherDuck manages compute resources through its containerized \"ducklings\" and handles resource allocation at the platform level.\n\n## Architectural Context and Differences\n\nIt's important to understand that these limitations stem from MotherDuck's serverless, managed architecture. MotherDuck prioritizes:\n- **Security**: Preventing arbitrary code execution in the cloud environment\n- **Performance**: Maintaining predictable, optimized query execution\n- **Stability**: Ensuring consistent behavior across all users\n\nThe dual-execution model allows many of these operations to be performed locally on the client side, where full DuckDB functionality is available, while still benefiting from MotherDuck's cloud-scale storage and compute for large datasets.\n\n### Storage Model Differences\n\nWhile not \"unsupported features,\" it's important to understand these architectural differences:\n\n1. **In-Place File Mutations:** DuckDB's native file format is optimized for a single writer on a local file system. MotherDuck doesn't support this model directly because cloud object stores like S3 have immutable files. MotherDuck uses a **differential storage system** that sits at the filesystem layer, making it appear like data has been updated in-place while actually using append-only files underneath.\n\n2. **Write-Ahead Log (WAL):** DuckDB often skips the WAL for batch updates as a performance optimization. MotherDuck's architecture handles this differently by implementing separation at a lower filesystem layer using differential storage rather than relying on WAL-based replication.\n\n3. **Database File Model:** MotherDuck uses a differential storage engine with append-only files and metadata tracking, rather than DuckDB's single-file model. This enables features like zero-copy data sharing and database snapshots.\n\n## Other Considerations\n\n### Regional Availability\nWhile not a SQL feature limitation, MotherDuck currently operates in:\n- AWS `us-east-1` (US East)\n- AWS `eu-central-1` (Frankfurt, Europe)\n\nTeams with strict data residency requirements outside these regions will need to consider this constraint.\n\n### Workload Limitations\nMotherDuck is optimized for **OLAP (analytical)** workloads, not **OLTP (transactional)** workloads. This means:\n- Not designed for thousands of concurrent, low-latency writes and updates per second\n- Database-level permissions rather than fine-grained row-level security\n- Not currently designed to serve thousands of simultaneous public-facing analytical queries\n\n## Compatibility Strengths\n\nDespite these limitations, MotherDuck maintains excellent compatibility with DuckDB for the vast majority of analytical SQL operations, including:\n- Full SQL query syntax support\n- Columnar storage and vectorized execution\n- ACID transactions (with serializable isolation)\n- Complex joins and aggregations\n- Direct querying of Parquet, CSV, and JSON files\n- Integration with cloud object storage (S3, GCS, Azure Blob Storage)\n- Hybrid execution (joining local and cloud data)\n\n## Future Roadmap\n\nMotherDuck is actively working on:\n- Multi-writer support by routing writes to a single backend\n- Reduced latency for share updates with guaranteed upper bounds\n- Time travel and branching features (made easy by immutable storage)\n- Improved caching for faster 'warm start' access\n\n## Summary\n\nMost SQL features from DuckDB **are supported** in MotherDuck. The main limitations are:\n1. **Single writer per database** (no concurrent writes)\n2. **Eventual consistency for shared databases** (small delay in seeing changes)\n3. **Custom Python/Native User-Defined Functions (UDFs)**\n4. **Server-side `ATTACH` to external databases**\n5. **Custom DuckDB extensions**\n6. **Different underlying storage model** (append-only vs. in-place updates)\n\nThese limitations are trade-offs made to enable MotherDuck's key benefits: cloud scale, collaboration, data sharing, and separation of storage and compute while maintaining ACID compliance. For most analytical workloads, the combination of MotherDuck's cloud capabilities and local DuckDB's full feature set (via dual execution) provides a powerful and flexible solution.",
  [2m  â”‚[0m   "originalFile": "# SQL Features from DuckDB Not Yet Supported in MotherDuck\n\nBased on the MotherDuck documentation, there are several SQL features available in DuckDB that are not yet fully supported in MotherDuck's cloud environment. While MotherDuck maintains strong compatibility with DuckDB and uses the same engine both locally and in the cloud, some advanced features remain unavailable on the server-side.\n\n## Key Unsupported Features\n\n### 1. Custom User-Defined Functions (UDFs)\n\n**Python UDFs and Native UDFs** are not supported on the MotherDuck server-side. This includes:\n- Custom Python functions that extend DuckDB's capabilities\n- Native/compiled UDFs written in C/C++\n\n**Workaround**: Perform these transformations upstream (on the client-side or in your data pipeline) before loading data into MotherDuck.\n\n### 2. Server-Side `ATTACH` Operations\n\nThe **`ATTACH` statement** for connecting to other databases (such as PostgreSQL, MySQL, or SQLite) is not available on the MotherDuck server. DuckDB locally supports attaching to various databases to query them directly, but this capability is not yet enabled in MotherDuck's cloud environment.\n\n**Workaround**: Export data from external databases and load it into MotherDuck, or perform cross-database queries locally using DuckDB's dual execution model.\n\n### 3. Custom Extensions\n\n**Custom DuckDB extensions** cannot be loaded on the MotherDuck server. While DuckDB has a rich ecosystem of extensions for various data formats, protocols, and functionality (spatial data, HTTP, JSON, etc.), custom or third-party extensions beyond what MotherDuck provides are not supported in the cloud environment.\n\n**Workaround**: Use client-side DuckDB with the required extensions for preprocessing, or leverage MotherDuck's supported built-in extensions.\n\n## Architectural Context\n\nIt's important to understand that these limitations stem from MotherDuck's serverless, managed architecture. MotherDuck prioritizes:\n- **Security**: Preventing arbitrary code execution in the cloud environment\n- **Performance**: Maintaining predictable, optimized query execution\n- **Stability**: Ensuring consistent behavior across all users\n\nThe dual-execution model allows many of these operations to be performed locally on the client side, where full DuckDB functionality is available, while still benefiting from MotherDuck's cloud-scale storage and compute for large datasets.\n\n## Other Considerations\n\n### Regional Availability\nWhile not a SQL feature limitation, MotherDuck currently operates in:\n- AWS `us-east-1` (US East)\n- AWS `eu-central-1` (Frankfurt, Europe)\n\nTeams with strict data residency requirements outside these regions will need to consider this constraint.\n\n### Workload Limitations\nMotherDuck is optimized for **OLAP (analytical)** workloads, not **OLTP (transactional)** workloads. This means:\n- Not designed for thousands of concurrent, low-latency writes and updates per second\n- Database-level permissions rather than fine-grained row-level security\n- Not currently designed to serve thousands of simultaneous public-facing analytical queries\n\n## Compatibility Strengths\n\nDespite these limitations, MotherDuck maintains excellent compatibility with DuckDB for the vast majority of analytical SQL operations, including:\n- Full SQL query syntax support\n- Columnar storage and vectorized execution\n- ACID transactions\n- Complex joins and aggregations\n- Direct querying of Parquet, CSV, and JSON files\n- Integration with cloud object storage (S3, GCS, Azure Blob Storage)\n- Hybrid execution (joining local and cloud data)\n\n## Summary\n\nThe main SQL features from DuckDB not yet supported in MotherDuck are:\n1. **Custom Python/Native User-Defined Functions (UDFs)**\n2. **Server-side `ATTACH` to external databases**\n3. **Custom DuckDB extensions**\n\nThese limitations are architectural choices that enable MotherDuck to provide a secure, performant, and serverless analytics platform. For most analytical workloads, the combination of MotherDuck's cloud capabilities and local DuckDB's full feature set (via dual execution) provides a powerful and flexible solution.\n\n---\n\n## Sources\n\n- **Fix Slow BI Dashboards Guide** - [motherduck.com/learn-more/fix-slow-bi-dashboards](https://motherduck.com/learn-more/fix-slow-bi-dashboards)\n  - States: \"MotherDuck does not yet support the full range of DuckDB's SQL features. Notably, custom Python/native User-Defined Functions (UDFs), server-side `ATTACH` to other databases (like Postgres), and custom extensions are not yet available.\"\n\n- **MotherDuck: A BigQuery Alternative** - [motherduck.com/learn-more/bigquery-alternative-motherduck](https://motherduck.com/learn-more/bigquery-alternative-motherduck)\n  - Discusses MotherDuck's hybrid architecture and the distinction between client-side and server-side capabilities\n  - Highlights known trade-offs including limitations for high-throughput transactional workloads and fine-grained row-level security\n\n- **Hybrid Analytics Guide** - [motherduck.com/learn-more/hybrid-analytics-guide](https://motherduck.com/learn-more/hybrid-analytics-guide)\n  - Explains the dual execution model and when to use local vs. cloud execution\n  - Discusses scenarios where the hybrid approach may not be suitable\n\n- **DuckDB Users Product Page** - [motherduck.com/product/duckdb-users](https://motherduck.com/product/duckdb-users/)\n  - Overview of MotherDuck's compatibility with DuckDB and its cloud-scale production capabilities\n",
  [2m  â”‚[0m   "structuredPatch": [
  [2m  â”‚[0m     {
  [2m  â”‚[0m       "oldStart": 1,
  [2m  â”‚[0m       "oldLines": 31,
  [2m  â”‚[0m       "newStart": 1,
  [2m  â”‚[0m       "newLines": 66,
  [2m  â”‚[0m       "lines": [
  [2m  â”‚[0m         " # SQL Features from DuckDB Not Yet Supported in MotherDuck",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-Based on the MotherDuck documentation, there are several SQL features available in DuckDB that are not yet fully supported in MotherDuck's cloud environment. While MotherDuck maintains strong compatibility with DuckDB and uses the same engine both locally and in the cloud, some advanced features remain unavailable on the server-side.",
  [2m  â”‚[0m         "+Based on the MotherDuck documentation, there are several SQL features and capabilities from DuckDB that have limitations or are not yet fully supported in MotherDuck due to its cloud architecture. While MotherDuck maintains strong compatibility with DuckDB and uses the same engine both locally and in the cloud, some advanced features remain unavailable on the server-side.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         " ## Key Unsupported Features",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### 1. Custom User-Defined Functions (UDFs)",
  [2m  â”‚[0m         "+### 1. Multiple Simultaneous Writers",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "+**Limitation:** MotherDuck currently **only supports one writer per database** at a time. While multiple users can read from the same shared database simultaneously, only one user can have write access.",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+**Why:** This is a fundamental architectural constraint of MotherDuck's storage system. Even though MotherDuck has solved the \"reading while writing\" problem through its differential storage engine and snapshot isolation, the multiple writer problem remains unsolved.",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+**Impact:** Workloads requiring simultaneous writes from different users need to be structured carefully. In practice, the typical pattern is:",
  [2m  â”‚[0m         "+- Data ingestion and transformation done by a shared service account",
  [2m  â”‚[0m         "+- Reading can be done by many different users",
  [2m  â”‚[0m         "+- Data writes to separate data universes can be split between service accounts",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+**Future:** MotherDuck is working on multi-writer support by routing writes to a single backend, simulating multiple writers even though DuckDB doesn't natively allow it.",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+### 2. Real-Time Write Visibility for Shared Databases",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+**Limitation:** When databases are shared with other users, there can be a **delay before readers see the latest changes** made by the database owner.",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+**Two Modes:**",
  [2m  â”‚[0m         "+- **AUTOUPDATE shares:** Changes are published to readers as soon as possible, but there can still be a small delay",
  [2m  â”‚[0m         "+- **Manual shares:** Require explicit `UPDATE SHARE` commands to publish changes to readers",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+**Why:** To give readers a \"clean snapshot\" of the data, MotherDuck ensures consistency at the storage layer. Readers might not be able to see up-to-the-moment changes being written by the owner.",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+**Use Case:** Sometimes this delay is actually useful - for example, when making a handful of changes that should appear together, or running unit tests before publishing results.",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+**Future:** MotherDuck is working to reduce this gap and will introduce upper bounds guarantees on how long it takes to publish data.",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+### 3. Custom User-Defined Functions (UDFs)",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         " **Python UDFs and Native UDFs** are not supported on the MotherDuck server-side. This includes:",
  [2m  â”‚[0m         " - Custom Python functions that extend DuckDB's capabilities",
  [2m  â”‚[0m         " - Native/compiled UDFs written in C/C++",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         " **Workaround**: Perform these transformations upstream (on the client-side or in your data pipeline) before loading data into MotherDuck.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### 2. Server-Side `ATTACH` Operations",
  [2m  â”‚[0m         "+### 4. Server-Side `ATTACH` Operations",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         " The **`ATTACH` statement** for connecting to other databases (such as PostgreSQL, MySQL, or SQLite) is not available on the MotherDuck server. DuckDB locally supports attaching to various databases to query them directly, but this capability is not yet enabled in MotherDuck's cloud environment.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         " **Workaround**: Export data from external databases and load it into MotherDuck, or perform cross-database queries locally using DuckDB's dual execution model.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### 3. Custom Extensions",
  [2m  â”‚[0m         "+### 5. Custom Extensions",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         " **Custom DuckDB extensions** cannot be loaded on the MotherDuck server. While DuckDB has a rich ecosystem of extensions for various data formats, protocols, and functionality (spatial data, HTTP, JSON, etc.), custom or third-party extensions beyond what MotherDuck provides are not supported in the cloud environment.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "+**Why:** MotherDuck uses DuckDB's extension system to add its own capabilities, but for security and stability reasons in a multi-tenant cloud environment, not all custom or third-party extensions may be supported.",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         " **Workaround**: Use client-side DuckDB with the required extensions for preprocessing, or leverage MotherDuck's supported built-in extensions.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Architectural Context",
  [2m  â”‚[0m         "+### 6. Certain DuckDB Configuration Settings",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "+**Limitation:** Some DuckDB PRAGMA settings or configuration options that control local behavior (like memory limits, thread counts for local execution, or file system paths) may work differently or not be available in MotherDuck's cloud environment.",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+**Why:** MotherDuck manages compute resources through its containerized \"ducklings\" and handles resource allocation at the platform level.",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+## Architectural Context and Differences",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         " It's important to understand that these limitations stem from MotherDuck's serverless, managed architecture. MotherDuck prioritizes:",
  [2m  â”‚[0m         " - **Security**: Preventing arbitrary code execution in the cloud environment",
  [2m  â”‚[0m         " - **Performance**: Maintaining predictable, optimized query execution"
  [2m  â”‚[0m       ]
  [2m  â”‚[0m     },
  [2m  â”‚[0m     {
  [2m  â”‚[0m       "oldStart": 33,
  [2m  â”‚[0m       "oldLines": 6,
  [2m  â”‚[0m       "newStart": 68,
  [2m  â”‚[0m       "newLines": 16,
  [2m  â”‚[0m       "lines": [
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         " The dual-execution model allows many of these operations to be performed locally on the client side, where full DuckDB functionality is available, while still benefiting from MotherDuck's cloud-scale storage and compute for large datasets.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "+### Storage Model Differences",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+While not \"unsupported features,\" it's important to understand these architectural differences:",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+1. **In-Place File Mutations:** DuckDB's native file format is optimized for a single writer on a local file system. MotherDuck doesn't support this model directly because cloud object stores like S3 have immutable files. MotherDuck uses a **differential storage system** that sits at the filesystem layer, making it appear like data has been updated in-place while actually using append-only files underneath.",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+2. **Write-Ahead Log (WAL):** DuckDB often skips the WAL for batch updates as a performance optimization. MotherDuck's architecture handles this differently by implementing separation at a lower filesystem layer using differential storage rather than relying on WAL-based replication.",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+3. **Database File Model:** MotherDuck uses a differential storage engine with append-only files and metadata tracking, rather than DuckDB's single-file model. This enables features like zero-copy data sharing and database snapshots.",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         " ## Other Considerations",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         " ### Regional Availability"
  [2m  â”‚[0m       ]
  [2m  â”‚[0m     },
  [2m  â”‚[0m     {
  [2m  â”‚[0m       "oldStart": 53,
  [2m  â”‚[0m       "oldLines": 20,
  [2m  â”‚[0m       "newStart": 98,
  [2m  â”‚[0m       "newLines": 31,
  [2m  â”‚[0m       "lines": [
  [2m  â”‚[0m         " Despite these limitations, MotherDuck maintains excellent compatibility with DuckDB for the vast majority of analytical SQL operations, including:",
  [2m  â”‚[0m         " - Full SQL query syntax support",
  [2m  â”‚[0m         " - Columnar storage and vectorized execution",
  [2m  â”‚[0m         "-- ACID transactions",
  [2m  â”‚[0m         "+- ACID transactions (with serializable isolation)",
  [2m  â”‚[0m         " - Complex joins and aggregations",
  [2m  â”‚[0m         " - Direct querying of Parquet, CSV, and JSON files",
  [2m  â”‚[0m         " - Integration with cloud object storage (S3, GCS, Azure Blob Storage)",
  [2m  â”‚[0m         " - Hybrid execution (joining local and cloud data)",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "+## Future Roadmap",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+MotherDuck is actively working on:",
  [2m  â”‚[0m         "+- Multi-writer support by routing writes to a single backend",
  [2m  â”‚[0m         "+- Reduced latency for share updates with guaranteed upper bounds",
  [2m  â”‚[0m         "+- Time travel and branching features (made easy by immutable storage)",
  [2m  â”‚[0m         "+- Improved caching for faster 'warm start' access",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         " ## Summary",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-The main SQL features from DuckDB not yet supported in MotherDuck are:",
  [2m  â”‚[0m         "-1. **Custom Python/Native User-Defined Functions (UDFs)**",
  [2m  â”‚[0m         "-2. **Server-side `ATTACH` to external databases**",
  [2m  â”‚[0m         "-3. **Custom DuckDB extensions**",
  [2m  â”‚[0m         "+Most SQL features from DuckDB **are supported** in MotherDuck. The main limitations are:",
  [2m  â”‚[0m         "+1. **Single writer per database** (no concurrent writes)",
  [2m  â”‚[0m         "+2. **Eventual consistency for shared databases** (small delay in seeing changes)",
  [2m  â”‚[0m         "+3. **Custom Python/Native User-Defined Functions (UDFs)**",
  [2m  â”‚[0m         "+4. **Server-side `ATTACH` to external databases**",
  [2m  â”‚[0m         "+5. **Custom DuckDB extensions**",
  [2m  â”‚[0m         "+6. **Different underlying storage model** (append-only vs. in-place updates)",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-These limitations are architectural choices that enable MotherDuck to provide a secure, performant, and serverless analytics platform. For most analytical workloads, the combination of MotherDuck's cloud capabilities and local DuckDB's full feature set (via dual execution) provides a powerful and flexible solution.",
  [2m  â”‚[0m         "+These limitations are trade-offs made to enable MotherDuck's key benefits: cloud scale, collaboration, data sharing, and separation of storage and compute while maintaining ACID compliance. For most analytical workloads, the combination of MotherDuck's cloud capabilities and local DuckDB's full feature set (via dual execution) provides a powerful and flexible solution.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         " ---",
  [2m  â”‚[0m         " "
  [2m  â”‚[0m       ]
  [2m  â”‚[0m     }
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "userModified": false,
  [2m  â”‚[0m   "replaceAll": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me update the sources section:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Edit â†’ /tmp/answer_without_kg_3.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_3.md",
  [2m  â”‚[0m   "oldString": "---\n\n## Sources\n\n- **Fix Slow BI Dashboards Guide** - [motherduck.com/learn-more/fix-slow-bi-dashboards](https://motherduck.com/learn-more/fix-slow-bi-dashboards)\n  - States: \"MotherDuck does not yet support the full range of DuckDB's SQL features. Notably, custom Python/native User-Defined Functions (UDFs), server-side `ATTACH` to other databases (like Postgres), and custom extensions are not yet available.\"\n\n- **MotherDuck: A BigQuery Alternative** - [motherduck.com/learn-more/bigquery-alternative-motherduck](https://motherduck.com/learn-more/bigquery-alternative-motherduck)\n  - Discusses MotherDuck's hybrid architecture and the distinction between client-side and server-side capabilities\n  - Highlights known trade-offs including limitations for high-throughput transactional workloads and fine-grained row-level security\n\n- **Hybrid Analytics Guide** - [motherduck.com/learn-more/hybrid-analytics-guide](https://motherduck.com/learn-more/hybrid-analytics-guide)\n  - Explains the dual execution model and when to use local vs. cloud execution\n  - Discusses scenarios where the hybrid approach may not be suitable\n\n- **DuckDB Users Product Page** - [motherduck.com/product/duckdb-users](https://motherduck.com/product/duckdb-users/)\n  - Overview of MotherDuck's compatibility with DuckDB and its cloud-scale production capabilities",
  [2m  â”‚[0m   "newString": "---\n\n## Sources\n\n1. **Separating Storage and Compute in DuckDB** - [motherduck.com/blog/separating-storage-compute-duckdb](https://motherduck.com/blog/separating-storage-compute-duckdb/)\n   - Comprehensive blog post explaining MotherDuck's architecture, storage model, and sharing limitations\n   - Details the single-writer constraint: \"There are still some restrictions that remain; in MotherDuck, only one user can have write access to a database.\"\n   - Explains the differential storage system and how it differs from DuckDB's native file format\n   - Discusses eventual consistency for shared databases and future work on multi-writer support\n\n2. **Bringing DuckDB to the Cloud: Dual Execution Explained** - [motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained/)\n   - Video and article explaining MotherDuck's architecture, extension system, and storage layer\n   - Covers the dual execution model and how queries are routed between local and cloud compute\n   - Explains database sharing and secure credentials management\n\n3. **ACID Transactions in Databases: A Data Engineer's Guide** - [motherduck.com/learn-more/acid-transactions-sql](https://motherduck.com/learn-more/acid-transactions-sql)\n   - Guide explaining DuckDB's ACID compliance and how MotherDuck maintains these guarantees\n   - Discusses serializable transactions and snapshot isolation in DuckDB\n\n4. **Fix Slow BI Dashboards Guide** - [motherduck.com/learn-more/fix-slow-bi-dashboards](https://motherduck.com/learn-more/fix-slow-bi-dashboards)\n   - States: \"MotherDuck does not yet support the full range of DuckDB's SQL features. Notably, custom Python/native User-Defined Functions (UDFs), server-side `ATTACH` to other databases (like Postgres), and custom extensions are not yet available.\"\n\n5. **MotherDuck: A BigQuery Alternative** - [motherduck.com/learn-more/bigquery-alternative-motherduck](https://motherduck.com/learn-more/bigquery-alternative-motherduck)\n   - Discusses MotherDuck's hybrid architecture and the distinction between client-side and server-side capabilities\n   - Highlights known trade-offs including limitations for high-throughput transactional workloads\n\n6. **MotherDuck Product Page** - [motherduck.com/product](https://motherduck.com/product/)\n   - Overview of MotherDuck's architecture and features, including the dual execution query engine",
  [2m  â”‚[0m   "originalFile": "# SQL Features from DuckDB Not Yet Supported in MotherDuck\n\nBased on the MotherDuck documentation, there are several SQL features and capabilities from DuckDB that have limitations or are not yet fully supported in MotherDuck due to its cloud architecture. While MotherDuck maintains strong compatibility with DuckDB and uses the same engine both locally and in the cloud, some advanced features remain unavailable on the server-side.\n\n## Key Unsupported Features\n\n### 1. Multiple Simultaneous Writers\n\n**Limitation:** MotherDuck currently **only supports one writer per database** at a time. While multiple users can read from the same shared database simultaneously, only one user can have write access.\n\n**Why:** This is a fundamental architectural constraint of MotherDuck's storage system. Even though MotherDuck has solved the \"reading while writing\" problem through its differential storage engine and snapshot isolation, the multiple writer problem remains unsolved.\n\n**Impact:** Workloads requiring simultaneous writes from different users need to be structured carefully. In practice, the typical pattern is:\n- Data ingestion and transformation done by a shared service account\n- Reading can be done by many different users\n- Data writes to separate data universes can be split between service accounts\n\n**Future:** MotherDuck is working on multi-writer support by routing writes to a single backend, simulating multiple writers even though DuckDB doesn't natively allow it.\n\n### 2. Real-Time Write Visibility for Shared Databases\n\n**Limitation:** When databases are shared with other users, there can be a **delay before readers see the latest changes** made by the database owner.\n\n**Two Modes:**\n- **AUTOUPDATE shares:** Changes are published to readers as soon as possible, but there can still be a small delay\n- **Manual shares:** Require explicit `UPDATE SHARE` commands to publish changes to readers\n\n**Why:** To give readers a \"clean snapshot\" of the data, MotherDuck ensures consistency at the storage layer. Readers might not be able to see up-to-the-moment changes being written by the owner.\n\n**Use Case:** Sometimes this delay is actually useful - for example, when making a handful of changes that should appear together, or running unit tests before publishing results.\n\n**Future:** MotherDuck is working to reduce this gap and will introduce upper bounds guarantees on how long it takes to publish data.\n\n### 3. Custom User-Defined Functions (UDFs)\n\n**Python UDFs and Native UDFs** are not supported on the MotherDuck server-side. This includes:\n- Custom Python functions that extend DuckDB's capabilities\n- Native/compiled UDFs written in C/C++\n\n**Workaround**: Perform these transformations upstream (on the client-side or in your data pipeline) before loading data into MotherDuck.\n\n### 4. Server-Side `ATTACH` Operations\n\nThe **`ATTACH` statement** for connecting to other databases (such as PostgreSQL, MySQL, or SQLite) is not available on the MotherDuck server. DuckDB locally supports attaching to various databases to query them directly, but this capability is not yet enabled in MotherDuck's cloud environment.\n\n**Workaround**: Export data from external databases and load it into MotherDuck, or perform cross-database queries locally using DuckDB's dual execution model.\n\n### 5. Custom Extensions\n\n**Custom DuckDB extensions** cannot be loaded on the MotherDuck server. While DuckDB has a rich ecosystem of extensions for various data formats, protocols, and functionality (spatial data, HTTP, JSON, etc.), custom or third-party extensions beyond what MotherDuck provides are not supported in the cloud environment.\n\n**Why:** MotherDuck uses DuckDB's extension system to add its own capabilities, but for security and stability reasons in a multi-tenant cloud environment, not all custom or third-party extensions may be supported.\n\n**Workaround**: Use client-side DuckDB with the required extensions for preprocessing, or leverage MotherDuck's supported built-in extensions.\n\n### 6. Certain DuckDB Configuration Settings\n\n**Limitation:** Some DuckDB PRAGMA settings or configuration options that control local behavior (like memory limits, thread counts for local execution, or file system paths) may work differently or not be available in MotherDuck's cloud environment.\n\n**Why:** MotherDuck manages compute resources through its containerized \"ducklings\" and handles resource allocation at the platform level.\n\n## Architectural Context and Differences\n\nIt's important to understand that these limitations stem from MotherDuck's serverless, managed architecture. MotherDuck prioritizes:\n- **Security**: Preventing arbitrary code execution in the cloud environment\n- **Performance**: Maintaining predictable, optimized query execution\n- **Stability**: Ensuring consistent behavior across all users\n\nThe dual-execution model allows many of these operations to be performed locally on the client side, where full DuckDB functionality is available, while still benefiting from MotherDuck's cloud-scale storage and compute for large datasets.\n\n### Storage Model Differences\n\nWhile not \"unsupported features,\" it's important to understand these architectural differences:\n\n1. **In-Place File Mutations:** DuckDB's native file format is optimized for a single writer on a local file system. MotherDuck doesn't support this model directly because cloud object stores like S3 have immutable files. MotherDuck uses a **differential storage system** that sits at the filesystem layer, making it appear like data has been updated in-place while actually using append-only files underneath.\n\n2. **Write-Ahead Log (WAL):** DuckDB often skips the WAL for batch updates as a performance optimization. MotherDuck's architecture handles this differently by implementing separation at a lower filesystem layer using differential storage rather than relying on WAL-based replication.\n\n3. **Database File Model:** MotherDuck uses a differential storage engine with append-only files and metadata tracking, rather than DuckDB's single-file model. This enables features like zero-copy data sharing and database snapshots.\n\n## Other Considerations\n\n### Regional Availability\nWhile not a SQL feature limitation, MotherDuck currently operates in:\n- AWS `us-east-1` (US East)\n- AWS `eu-central-1` (Frankfurt, Europe)\n\nTeams with strict data residency requirements outside these regions will need to consider this constraint.\n\n### Workload Limitations\nMotherDuck is optimized for **OLAP (analytical)** workloads, not **OLTP (transactional)** workloads. This means:\n- Not designed for thousands of concurrent, low-latency writes and updates per second\n- Database-level permissions rather than fine-grained row-level security\n- Not currently designed to serve thousands of simultaneous public-facing analytical queries\n\n## Compatibility Strengths\n\nDespite these limitations, MotherDuck maintains excellent compatibility with DuckDB for the vast majority of analytical SQL operations, including:\n- Full SQL query syntax support\n- Columnar storage and vectorized execution\n- ACID transactions (with serializable isolation)\n- Complex joins and aggregations\n- Direct querying of Parquet, CSV, and JSON files\n- Integration with cloud object storage (S3, GCS, Azure Blob Storage)\n- Hybrid execution (joining local and cloud data)\n\n## Future Roadmap\n\nMotherDuck is actively working on:\n- Multi-writer support by routing writes to a single backend\n- Reduced latency for share updates with guaranteed upper bounds\n- Time travel and branching features (made easy by immutable storage)\n- Improved caching for faster 'warm start' access\n\n## Summary\n\nMost SQL features from DuckDB **are supported** in MotherDuck. The main limitations are:\n1. **Single writer per database** (no concurrent writes)\n2. **Eventual consistency for shared databases** (small delay in seeing changes)\n3. **Custom Python/Native User-Defined Functions (UDFs)**\n4. **Server-side `ATTACH` to external databases**\n5. **Custom DuckDB extensions**\n6. **Different underlying storage model** (append-only vs. in-place updates)\n\nThese limitations are trade-offs made to enable MotherDuck's key benefits: cloud scale, collaboration, data sharing, and separation of storage and compute while maintaining ACID compliance. For most analytical workloads, the combination of MotherDuck's cloud capabilities and local DuckDB's full feature set (via dual execution) provides a powerful and flexible solution.\n\n---\n\n## Sources\n\n- **Fix Slow BI Dashboards Guide** - [motherduck.com/learn-more/fix-slow-bi-dashboards](https://motherduck.com/learn-more/fix-slow-bi-dashboards)\n  - States: \"MotherDuck does not yet support the full range of DuckDB's SQL features. Notably, custom Python/native User-Defined Functions (UDFs), server-side `ATTACH` to other databases (like Postgres), and custom extensions are not yet available.\"\n\n- **MotherDuck: A BigQuery Alternative** - [motherduck.com/learn-more/bigquery-alternative-motherduck](https://motherduck.com/learn-more/bigquery-alternative-motherduck)\n  - Discusses MotherDuck's hybrid architecture and the distinction between client-side and server-side capabilities\n  - Highlights known trade-offs including limitations for high-throughput transactional workloads and fine-grained row-level security\n\n- **Hybrid Analytics Guide** - [motherduck.com/learn-more/hybrid-analytics-guide](https://motherduck.com/learn-more/hybrid-analytics-guide)\n  - Explains the dual execution model and when to use local vs. cloud execution\n  - Discusses scenarios where the hybrid approach may not be suitable\n\n- **DuckDB Users Product Page** - [motherduck.com/product/duckdb-users](https://motherduck.com/product/duckdb-users/)\n  - Overview of MotherDuck's compatibility with DuckDB and its cloud-scale production capabilities\n",
  [2m  â”‚[0m   "structuredPatch": [
  [2m  â”‚[0m     {
  [2m  â”‚[0m       "oldStart": 128,
  [2m  â”‚[0m       "oldLines": 16,
  [2m  â”‚[0m       "newStart": 128,
  [2m  â”‚[0m       "newLines": 27,
  [2m  â”‚[0m       "lines": [
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         " ## Sources",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-- **Fix Slow BI Dashboards Guide** - [motherduck.com/learn-more/fix-slow-bi-dashboards](https://motherduck.com/learn-more/fix-slow-bi-dashboards)",
  [2m  â”‚[0m         "-  - States: \"MotherDuck does not yet support the full range of DuckDB's SQL features. Notably, custom Python/native User-Defined Functions (UDFs), server-side `ATTACH` to other databases (like Postgres), and custom extensions are not yet available.\"",
  [2m  â”‚[0m         "+1. **Separating Storage and Compute in DuckDB** - [motherduck.com/blog/separating-storage-compute-duckdb](https://motherduck.com/blog/separating-storage-compute-duckdb/)",
  [2m  â”‚[0m         "+   - Comprehensive blog post explaining MotherDuck's architecture, storage model, and sharing limitations",
  [2m  â”‚[0m         "+   - Details the single-writer constraint: \"There are still some restrictions that remain; in MotherDuck, only one user can have write access to a database.\"",
  [2m  â”‚[0m         "+   - Explains the differential storage system and how it differs from DuckDB's native file format",
  [2m  â”‚[0m         "+   - Discusses eventual consistency for shared databases and future work on multi-writer support",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-- **MotherDuck: A BigQuery Alternative** - [motherduck.com/learn-more/bigquery-alternative-motherduck](https://motherduck.com/learn-more/bigquery-alternative-motherduck)",
  [2m  â”‚[0m         "-  - Discusses MotherDuck's hybrid architecture and the distinction between client-side and server-side capabilities",
  [2m  â”‚[0m         "-  - Highlights known trade-offs including limitations for high-throughput transactional workloads and fine-grained row-level security",
  [2m  â”‚[0m         "+2. **Bringing DuckDB to the Cloud: Dual Execution Explained** - [motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained/)",
  [2m  â”‚[0m         "+   - Video and article explaining MotherDuck's architecture, extension system, and storage layer",
  [2m  â”‚[0m         "+   - Covers the dual execution model and how queries are routed between local and cloud compute",
  [2m  â”‚[0m         "+   - Explains database sharing and secure credentials management",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-- **Hybrid Analytics Guide** - [motherduck.com/learn-more/hybrid-analytics-guide](https://motherduck.com/learn-more/hybrid-analytics-guide)",
  [2m  â”‚[0m         "-  - Explains the dual execution model and when to use local vs. cloud execution",
  [2m  â”‚[0m         "-  - Discusses scenarios where the hybrid approach may not be suitable",
  [2m  â”‚[0m         "+3. **ACID Transactions in Databases: A Data Engineer's Guide** - [motherduck.com/learn-more/acid-transactions-sql](https://motherduck.com/learn-more/acid-transactions-sql)",
  [2m  â”‚[0m         "+   - Guide explaining DuckDB's ACID compliance and how MotherDuck maintains these guarantees",
  [2m  â”‚[0m         "+   - Discusses serializable transactions and snapshot isolation in DuckDB",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-- **DuckDB Users Product Page** - [motherduck.com/product/duckdb-users](https://motherduck.com/product/duckdb-users/)",
  [2m  â”‚[0m         "-  - Overview of MotherDuck's compatibility with DuckDB and its cloud-scale production capabilities",
  [2m  â”‚[0m         "+4. **Fix Slow BI Dashboards Guide** - [motherduck.com/learn-more/fix-slow-bi-dashboards](https://motherduck.com/learn-more/fix-slow-bi-dashboards)",
  [2m  â”‚[0m         "+   - States: \"MotherDuck does not yet support the full range of DuckDB's SQL features. Notably, custom Python/native User-Defined Functions (UDFs), server-side `ATTACH` to other databases (like Postgres), and custom extensions are not yet available.\"",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+5. **MotherDuck: A BigQuery Alternative** - [motherduck.com/learn-more/bigquery-alternative-motherduck](https://motherduck.com/learn-more/bigquery-alternative-motherduck)",
  [2m  â”‚[0m         "+   - Discusses MotherDuck's hybrid architecture and the distinction between client-side and server-side capabilities",
  [2m  â”‚[0m         "+   - Highlights known trade-offs including limitations for high-throughput transactional workloads",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+6. **MotherDuck Product Page** - [motherduck.com/product](https://motherduck.com/product/)",
  [2m  â”‚[0m         "+   - Overview of MotherDuck's architecture and features, including the dual execution query engine"
  [2m  â”‚[0m       ]
  [2m  â”‚[0m     }
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "userModified": false,
  [2m  â”‚[0m   "replaceAll": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! Now let me verify the file was created successfully:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ ls -lh /tmp/answer_without_kg_3.md && wc -l /tmp/answer_without_kg_3.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m -rw-------  1 julien  wheel    11K Nov 27 14:04 /tmp/answer_without_kg_3.md
  [2m  â”‚[0m      154 /tmp/answer_without_kg_3.md
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Excellent! The answer has been successfully written. Let me provide a summary:
  [94mâ”‚[0m 
  [94mâ”‚[0m ## Task Completed Successfully âœ“
  [94mâ”‚[0m 
  [94mâ”‚[0m I've searched through the 874 MotherDuck documents in the `.kurt/sources/` directory and synthesized a comprehensive answer about SQL features from DuckDB that are not yet supported in MotherDuck.
  [94mâ”‚[0m 
  [94mâ”‚[0m ### Answer Summary
  [94mâ”‚[0m 
  [94mâ”‚[0m The research revealed **6 main limitations** in MotherDuck compared to DuckDB:
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. **Multiple Simultaneous Writers** - Only one writer per database at a time
  [94mâ”‚[0m 2. **Real-Time Write Visibility** - Small delay in seeing changes in shared databases
  [94mâ”‚[0m 3. **Custom User-Defined Functions (UDFs)** - Python/native UDFs not supported server-side
  [94mâ”‚[0m 4. **Server-Side ATTACH Operations** - Cannot attach to external databases in the cloud
  [94mâ”‚[0m 5. **Custom Extensions** - Third-party extensions restricted for security
  [94mâ”‚[0m 6. **DuckDB Configuration Settings** - Some PRAGMA settings work differently
  [94mâ”‚[0m 
  [94mâ”‚[0m ### Key Sources Used
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer draws from several authoritative sources:
  [94mâ”‚[0m - **"Separating Storage and Compute in DuckDB"** blog post - detailed architectural explanation
  [94mâ”‚[0m - **"Bringing DuckDB to the Cloud: Dual Execution Explained"** video - architecture and features
  [94mâ”‚[0m - **ACID Transactions guide** - transaction guarantees
  [94mâ”‚[0m - **Fix Slow BI Dashboards guide** - explicit list of unsupported features
  [94mâ”‚[0m 
  [94mâ”‚[0m ### Output Location
  [94mâ”‚[0m 
  [94mâ”‚[0m **File:** `/tmp/answer_without_kg_3.md` (154 lines, 11KB)
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer includes detailed explanations of each limitation, why they exist, workarounds, future roadmap items, and proper source citations.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 13,287 | Cost: $0.7276
     Cumulative: 13,287 tokens | $0.7276
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: heuristic: Explicit completion signal detected

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 31 | Duration: 230.0s
â•‘    Tokens: 13,287 | Cost: $0.7276
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  LLM Judge score: 1.00

======================================================================
â“ Question 4/10
Why might a query run slower on MotherDuck than locally?
======================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 4
â”‚ Search through the .kurt/sources/ directory to answer this question:

Why might a query run slower on MotherDuck than locally?

Steps:
1. Search for the most relevant markdown files in .kurt/sources/
2. Read those files and synthesize the answer
3. Write a markdown answer (with a ## Sources section) to: /tmp/answer_without_kg_4.md
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I'll help you find information about query performance differences between MotherDuck and local execution. Let me search through the sources directory.
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ slow|performance|faster|latency|query.*speed

  [96mğŸ”§ TOOL:[0m Grep â†’ MotherDuck.*local|local.*MotherDuck

  [96mğŸ”§ TOOL:[0m Grep â†’ network|latency|overhead|remote
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/take-flight-with-the-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-power-of-wasm-for-analytics-duckdb-in-the-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-surprising-birth-of-duckdb-ft-co-creator-hannes-muhleisen.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-if-sql-queries-returned-results-instantly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/speed-querying-stackoverflow-data-with-duckdb-ft-michael-hunger.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/panel-fixing-the-data-engineering-lifecycle-coalesce-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/leveraging-duckdb-and-delta-lake-together.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/monte-carlo-simulations-with-duckdb-ft-james-mcneill.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-bootstrap-a-data-warehouse-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ducklake-big-data-small-coalesce-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/is-bi-too-big-for-small-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/faster-data-pipelines-development-with-mcp-and-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-dbt-accelerating-the-developer-experience-with-local-power.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/build-bigger-with-small-ai-running-small-models-locally.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/trust-and-security.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-new-paradigm-for-data-visualization-with-just-sql-markdown.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search for more specific terms about query execution and performance differences:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ dual.execution|hybrid|where.*run|execution.*mode

  [96mğŸ”§ TOOL:[0m Grep â†’ cloud.*execution|remote.*execution|when.*use.*motherduck
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-ga-produck-tour.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-use.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/privacy-policy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/pandas-dataframes-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/product-analytics-motherduck-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/an-evolving-dag-for-the-llm-world-julia-schottenstein-of-langchain-at-small-data-sf-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/build-bigger-with-small-ai-running-small-models-locally.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/postgres-integration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-OLAP.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/product-analytics-motherduck-duckdb.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Good! Let me read the most relevant files about dual execution and performance:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     "content": "---\ntitle: bringing-duckdb-to-the-cloud-dual-execution-explained\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained\nindexed_at: '2025-11-25T20:45:20.295187'\ncontent_hash: ab4030806c409492\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nBringing DuckDB to the Cloud: Dual Execution Explained - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Bringing DuckDB to the Cloud: Dual Execution Explained](https://www.youtube.com/watch?v=2eVNm4A8sLw)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=2eVNm4A8sLw&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 1:08:11\n\n\u2022Live\n\n\u2022\n\nYouTubeQuack & CodeMotherDuck Features\n\n# Bringing DuckDB to the Cloud: Dual Execution Explained\n\n2024/06/28\n\nBringing DuckDB's analytical power to the cloud requires more than just running it on a server. While DuckDB excels at processing data on a local machine, the dynamics of data analysis change when dealing with cloud-scale data, collaboration, and shared resources. This shift introduces challenges around [security](https://motherduck.com/learn-more/secure-startup-data-warehouse/), concurrent access, and performance, which is where a purpose-built cloud architecture becomes essential.\n\nMotherDuck, a modern cloud data warehouse, is built on DuckDB to solve these challenges. It extends DuckDB\u2019s capabilities without forking the open source project, creating a hybrid system that intelligently balances local and cloud computing. In a conversation with MotherDuck founding engineer Stephanie, she explained the architecture and the innovative query model, known as **dual execution**, that makes this possible.\n\n* * *\n\n## The MotherDuck Architecture: Beyond Hosted DuckDB\n\nA common misconception is that MotherDuck is simply DuckDB hosted in the cloud. The reality is a more sophisticated system designed to overcome the limitations of using a locally-optimized engine in a distributed environment. MotherDuck\u2019s architecture is built on three key components and a core philosophy of not forking DuckDB.\n\nInstead of maintaining a separate version, MotherDuck leverages DuckDB\u2019s powerful **extension system**. This allows MotherDuck to add new capabilities at multiple layers, including the SQL parser, optimizer, and storage interface, while staying current with the latest open source DuckDB releases. This tight integration means users benefit from the rapid innovation of the DuckDB community almost immediately.\n\nThe architecture can be broken down into three main layers:\n\n1. **The Client Extension**: This is how MotherDuck integrates with the DuckDB ecosystem. Whether a user is working in the CLI, a Python script, or a JDBC connection, a simple `ATTACH` command connects their local DuckDB instance to the MotherDuck cloud. This extension is even used to run DuckDB in the browser via WASM for the MotherDuck UI, enabling client-side processing to reduce latency and cloud compute costs.\n2. **The Compute Layer**: In the cloud, queries are processed by containerized DuckDB instances, fondly called \"ducklings.\" These compute resources are scaled based on user needs, providing the necessary CPU and memory to handle complex analytical queries on large datasets.\n3. **The Storage Layer**: DuckDB's native file format is optimized for a single writer on a local file system. This model is not well-suited for the cloud, where multiple users need to read and write to the same database concurrently. To solve this, MotherDuck implemented a differential storage system that maps a logical database file to append-only snapshot layers in cloud object storage. This design is cloud-friendly, enabling efficient in-place updates and forming the foundation for features like database sharing and time travel.\n\n* * *\n\n## Enabling Collaboration with Database Sharing and Secure Credentials\n\nThis unique architecture transforms DuckDB from a \"single-player\" tool into a collaborative \"multiplayer\" platform. One of the most significant advantages is **database sharing**. Instead of emailing SQL scripts or passing around large database files, team members within the same organization can grant query access to their databases. This streamlines collaboration, ensuring everyone works from a consistent and up-to-date version of the data.\n\nSecurity is another critical aspect of any cloud data platform. MotherDuck provides a **centralized Secret Manager**, co-designed with the DuckDB team. Users can create persistent, encrypted secrets for accessing external data sources like AWS S3, Google Cloud Storage, or Delta Lake. Once a secret is created in MotherDuck, it can be reused across different clients and sessions without needing to expose or reconfigure credentials on each local machine.\n\n* * *\n\n## Understanding Dual Execution\n\nThe most innovative feature of MotherDuck's architecture is its **dual execution model**. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\n\nThe process was demonstrated with a query joining two tables. When both tables reside in a MotherDuck database, the query is executed entirely in the cloud. An `EXPLAIN` plan for this query reveals that all operators, from the table scans to the final join, are marked as **remote**. The only local operation is downloading the final result set to the client.\n\nThe real power of dual execution becomes apparent when a query involves both local and cloud data. In the demonstration, one of the remote tables was copied to a local Parquet file. The query was then modified to join this local file with the table that remained in the cloud.\n\nThe `EXPLAIN` plan for this new query showed a mix of local and remote operations. The scan of the Parquet file and its associated processing happened **locally**, leveraging the user's machine. The scan of the cloud table happened **remotely** on MotherDuck's compute. The system then efficiently transferred only the necessary intermediate data to complete the join. This hybrid approach avoids needlessly uploading the local file to the cloud or downloading the entire remote table, resulting in faster and more efficient queries.\n\n* * *\n\n## Taking Control with Manual Execution Overrides\n\nWhile MotherDuck's optimizer is designed to make the most efficient choice automatically, some situations require manual control. For this, users can use the `md_run` parameter within scan functions like `read_parquet()` or `read_csv()`.\n\nBy setting `md_run = 'local'`, a user can force the scan of a remote S3 file to be executed on their local client. This involves downloading the data, which might be desirable if the data needs to be processed near the user's location or if they prefer using local credentials. Conversely, setting `md_run = 'remote'` forces the operation to run in the MotherDuck cloud.\n\nThis control is especially useful for performance tuning. A direct comparison showed that querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds). By pushing the computation to the data's location, MotherDuck minimizes network I/O and delivers results more quickly.\n\n* * *\n\n## A Symbiotic Future for Local and Cloud Analytics\n\nMotherDuck's architecture is a thoughtful extension of DuckDB's core principles, adapting its local-first power for the demands of the cloud. The dual execution model provides a flexible and efficient bridge between a user's laptop and cloud data, optimizing workloads in a way that pure-cloud or pure-local systems cannot.\n\nThis relationship is symbiotic. As the largest production user of DuckDB, MotherDuck continuously pushes the engine to its limits, uncovering opportunities for improvement and contributing enhancements back to the open source project. This collaboration ensures that both DuckDB and MotherDuck will continue to evolve, offering a powerful and seamless analytical experience that spans from local exploration to cloud-scale production.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Ficeberg_video_thumbnail_adfdabe3d6.jpg&w=3840&q=75)\\\\\n\\\\\n46:26](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n[2025-06-12](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n### [Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse)\n\nBuilding a data stack means choosing between easy SaaS and complex open-source. Apache Iceberg is a middle ground, but its catalog is a hurdle. New tools now simplify using Iceberg with DuckDB to create a flexible, local data lakehouse.\n\nMotherDuck Features\n\n[![\"Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FG_Se_B_Sox_AW_Fg_HD_f4fddaa9ab.jpg&w=3840&q=75)](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n[2025-04-23](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n### [Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly)\n\nHamilton Ulmer shares insights from MotherDuck's Instant SQL Mode, exploring how real-time query result previews eliminate the traditional write-run-debug cycle through client-side parsing and DuckDB-WASM caching.\n\nSQL\n\nTalk\n\nMotherDuck Features\n\n[![\"DuckDB: Run dbt build with sub-second execution times\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_03_26_at_3_40_16_PM_c6e3a8096f.png&w=3840&q=75)\\\\\n\\\\\n24:06](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n[2025-03-13](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n### [DuckDB: Run dbt build with sub-second execution times](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times)\n\nWhether you're new to DuckDB or looking to optimize your workflows, this session will provide practical insights to help you leverage its full potential.\n\nYouTube\n\nData Pipelines\n\ndbt\n\nMotherDuck Features\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 184,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 184
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     "content": "---\ntitle: stay-in-flow-with-motherducks-instant-sql\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql\nindexed_at: '2025-11-25T20:44:49.950311'\ncontent_hash: e6211ffb372b0740\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nStay in Flow with MotherDuck's Instant SQL - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Stay in Flow with MotherDuck's Instant SQL](https://www.youtube.com/watch?v=T3gmsbohn48)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=T3gmsbohn48&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 57:59\n\n\u2022Live\n\n\u2022\n\nYouTube\n\n# Stay in Flow with MotherDuck's Instant SQL\n\n2025/05/21\n\n## What is MotherDuck?\n\nMotherDuck is a cloud data warehouse built on top of and powered by DuckDB, focused on making big data feel small. Unlike traditional big data systems that emerged in the early 2000s with technologies like Hadoop and Spark, MotherDuck takes a different approach by recognizing two key changes in the modern data landscape:\n\n- **Modern hardware is significantly more powerful**: Today's laptops have multiple cores and substantial RAM, with some EC2 instances offering hundreds of cores and terabytes of memory\n- **Most queries aren't actually \"big data\"**: Analysis shows that 99% of queries on systems like Redshift and Snowflake can fit on a single large node, with most under a gigabyte\n\nMotherDuck's architecture is built on three core pillars:\n\n1. **Effortless and serverless**: Pay only for what you use without managing resources or clusters\n2. **Dedicated instances**: Each user gets their own \"duckling\" (DuckDB instance) to avoid noisy neighbor problems\n3. **Dual execution query engine**: Run queries locally, in the cloud, or combine both for optimal performance\n\n## The Problem with Traditional SQL Workflows\n\nTraditional SQL development follows a frustrating pattern that breaks flow state:\n\n1. Write your query\n2. Hit the run button\n3. Wait for results\n4. Debug errors\n5. Repeat\n\nThis write-wait-debug cycle can happen hundreds or thousands of times when developing new queries. Each wait forces a context switch that exhausts mental energy and disrupts concentration. As Hamilton, a front-end engineer at MotherDuck, explains, this interaction model hasn't fundamentally changed since the 1970s.\n\nThe lack of observability tools in SQL makes debugging particularly challenging:\n\n- **CTE debugging**: Common Table Expressions are notoriously difficult to debug, requiring manual commenting and isolation\n- **Complex expressions**: Breaking apart column expressions to identify issues requires writing more SQL\n- **No immediate feedback**: Unlike modern development environments, SQL lacks real-time validation and preview capabilities\n\n## Introducing Instant SQL\n\nInstant SQL transforms SQL development by providing immediate feedback on every keystroke, similar to how digital audio workstations (DAWs) work in music production. The system is guided by Brett Victor's principle: \"Creators need an immediate connection to what they create.\"\n\n### Key Features\n\n**Real-time Query Execution**\n\n- Results update with every keystroke (50-100ms latency)\n- Powered by DuckDB's local execution capabilities\n- Smart caching and query rewriting for performance\n\n**Advanced Observability**\n\n- Click on any column to decompose complex expressions\n- Navigate through CTEs with instant result previews\n- Parser-aware syntax highlighting showing query structure\n\n**AI Integration**\n\n- Context-aware suggestions based on cursor position\n- Real-time preview of AI-generated changes\n- Semantic understanding of query intent\n\n## Technical Implementation\n\nInstant SQL leverages several technical innovations:\n\n### Parser-Powered Intelligence\n\nDuckDB exposes its query parser through SQL, allowing Instant SQL to:\n\n- Generate abstract syntax trees (AST)\n- Identify expression boundaries and table references\n- Create a semantic path through the query based on cursor position\n\n### Intelligent Caching Strategy\n\nThe system automatically:\n\n- Parses queries to identify table references\n- Builds a directed acyclic graph (DAG) of dependencies\n- Creates optimized caches for interactive modeling\n- Rewrites queries to use cached data\n\n### Dual Execution with MotherDuck\n\nFor large datasets, MotherDuck's architecture enables:\n\n- Server-side scanning and filtering of massive tables\n- Local caching of relevant subsets\n- Seamless coordination between cloud and local execution\n\n## Practical Applications\n\n### Local File Exploration\n\nQuery local files directly without uploading to the cloud:\n\n```sql\nCopy code\n\nSELECT * FROM 'path/to/file.parquet'\n```\n\n### Cross-Database Queries\n\nCombine data from multiple sources:\n\n- MotherDuck tables\n- PostgreSQL replicas\n- Local files\n- S3 object storage\n\n### Interactive Data Modeling\n\n- Modify CTEs and see downstream impacts immediately\n- Test transformations without full query execution\n- Debug complex joins and aggregations in real-time\n\n## Performance at Scale\n\nInstant SQL scales effectively through:\n\n- **Smart sampling**: For terabyte-scale data, it samples intelligently\n- **Filtered caching**: WHERE clauses execute server-side to minimize data transfer\n- **Incremental updates**: Only affected parts of the query are re-executed\n\nThe system works with datasets ranging from local CSV files to 100+ billion row tables in MotherDuck, adapting its caching strategy based on data size and query complexity.\n\n## Getting Started\n\nTo try Instant SQL:\n\n- Sign up for a MotherDuck account at motherduck.com\n- Run `duckdb -i` for local-only exploration without an account\n- Join the community at slack.motherduck.com for support and feedback\n\nInstant SQL represents a fundamental shift in SQL development, transforming a traditionally passive, wait-heavy process into an active, immediate experience that keeps developers in flow state while writing complex analytical queries.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Data-based: Going Beyond the Dataframe\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FData_based_f32745b461.png&w=3840&q=75)](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n[2025-11-20](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n### [Data-based: Going Beyond the Dataframe](https://motherduck.com/videos/going-beyond-the-dataframe)\n\nLearn how to turbocharge your Python data work using DuckDB and MotherDuck with Pandas. We walk through performance comparisons, exploratory data analysis on bigger datasets, and an end-to-end ML feature engineering pipeline.\n\nWebinar\n\nPython\n\nAI, ML and LLMs\n\n[![\"Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FHex_Webinar_778e3959e4.png&w=3840&q=75)](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n[2025-11-14](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n### [Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck)\n\nAI isn't here to replace data work, it's here to make it better. Watch this webinar to see how Hex and MotherDuck build AI workflows that prioritize context, iteration, and real-world impact.\n\nWebinar\n\nAI, ML and LLMs\n\n[![\"Lies, Damn Lies, and Benchmarks\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FLies_Damn_Lies_and_Benchmarks_Thumbnail_404db1bf46.png&w=3840&q=75)](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n[2025-10-31](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n### [Lies, Damn Lies, and Benchmarks](https://motherduck.com/videos/lies-damn-lies-and-benchmarks)\n\nWhy do database benchmarks so often mislead? MotherDuck CEO Jordan Tigani discusses the pitfalls of performance benchmarking, lessons from BigQuery, and why your own workload is the only benchmark that truly matters.\n\nStream\n\nInterview\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 253,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 253
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     "content": "---\ntitle: 'Fix Slow BI Dashboards: A Guide to Sub-Second Analytics'\ncontent_type: guide\ndescription: Frustrated by slow BI dashboards? Learn the causes of dashboard latency,\n  from architectural bottlenecks to lakehouse issues. See how MotherDuck helps startups\n  scale.\npublished_date: '2025-10-17T00:00:00'\nsource_url: https://motherduck.com/learn-more/fix-slow-bi-dashboards\nindexed_at: '2025-11-25T10:52:12.236559'\ncontent_hash: 0e692003fc4f1f77\nhas_step_by_step: true\nhas_narrative: true\n---\n\nAs a startup, your data is your compass. But as you scale, that compass begins to spin. Business Intelligence (BI) dashboards that were once snappy now take minutes to load, turning quick checks into coffee breaks. Queries time out. Your team hesitates to ask new questions of the data because the feedback loop is agonizingly slow. You're facing a critical growth inflection point: your analytics stack can no longer keep up with your ambition.\n\nYou know you need something more powerful, but the thought of migrating to a traditional cloud data warehouse, with its complex setup, opaque pricing, and multi-year contracts, is daunting. What if there was a better way? A path that gives you the power of a massive data warehouse without the overhead, designed specifically for teams who move fast?\n\nThis guide is for the technical founders, first data hires, and lean analytics teams feeling this pain. We'll provide a clear framework for evaluating if MotherDuck is the right solution to solve your dashboard latency issues, scale your analytics, and empower your team to make faster, data-driven decisions.\n\n### What You'll Learn in This Guide\n\n**What Causes Dashboard Latency?**We'll uncover why your current database struggles with analytical queries and identify the key architectural bottlenecks.**How Do You Scale Beyond a Single Node?**Discover the limitations of single-node engines for large datasets and how MotherDuck's hybrid architecture provides a straightforward path to scale.**How Can You Achieve Real-Time Insights?**Learn how to move from stale, batch-updated data to sub-second freshness without overloading your production systems.**What Does a Migration Look Like?**Follow a practical, week-by-week plan to migrate a critical workload to MotherDuck and start seeing results in under 30 days.**Is MotherDuck the Right Fit for Your Startup?**Get a clear comparison of when MotherDuck excels and what limitations to consider for your specific use case.\n\n## Why Are My Startup's BI Dashboards So Slow?\n\nIf your dashboards are grinding to a halt, the problem usually isn't a single query but an architectural mismatch. Most startups begin by running analytics on a replica of their production database, often a system like PostgreSQL or MySQL. These are [ Online Transaction Processing (OLTP)](https://motherduck.com/learn-more/what-is-OLAP/) databases, brilliant for handling thousands of small, fast transactions like creating a user or processing an order.\n\nHowever, analytical queries are a completely different beast. They are **Online Analytical Processing (OLAP)** workloads, which involve scanning millions or billions of rows across a few columns to calculate aggregations. Asking an OLTP database to perform heavy OLAP queries is like asking a race car to haul lumber. It wasn't built for the job.\n\nThe core issue lies in how the data is stored on disk. OLTP databases are **row-oriented**, meaning they store all the data for a single record together. When your BI tool asks for the total number of users by `plan_type`\n\nfrom a table with 50 columns, a row-oriented database is forced to read all 50 columns for every single row, even though it only needs one. This wastes an enormous amount of I/O.\n\nThis is where [ columnar storage](https://motherduck.com/learn-more/columnar-storage-guide/), the foundation of modern analytical databases like DuckDB, makes a big difference. A columnar database stores all values from a single column together. When you run the same query, it reads\n\n*only*the\n\n`plan_type`\n\ncolumn, dramatically reducing the amount of data scanned. This fundamental difference is often the key to unlocking sub-second query performance.### How Do Row and Columnar Databases Compare for Analytics?\n\n| Feature | Row-Oriented (e.g., PostgreSQL) | Column-Oriented (e.g., DuckDB/MotherDuck) |\n|---|---|---|\nPrimary Use Case | OLTP: Fast reads and writes of individual records. | OLAP: Fast scans and aggregations over large datasets. |\nData Layout | Stores all values for a single record contiguously. | Stores all values for a single column contiguously. |\nQuery Performance | Slow for analytical queries that only need a few columns. | Extremely fast for analytical queries. It only reads the required columns. |\nCompression | Less effective, as it stores mixed data types in each row. | Highly effective, leading to smaller storage footprint and faster scans. |\n\n## What Happens When My Analytics Queries Need to Scan Billions of Rows?\n\nAs your startup succeeds, your data volume explodes. The single-node analytics database that was once a perfect solution starts to show its limits. While an engine like DuckDB is incredibly fast and can even process datasets larger than RAM by [\"spilling\" intermediate results to disk](https://duckdb.org/2024/07/09/memory-management.html), it is ultimately constrained by the resources of a single machine.\n\nWhen you run a complex query with multiple joins and aggregations on billions of rows, the intermediate data generated can overwhelm the system's memory. This can lead to slow queries as the engine constantly writes to and reads from disk, or worse, an [ OutOfMemoryException](https://duckdb.org/docs/stable/guides/troubleshooting/oom_errors.html) that kills the query entirely.\n\nFurthermore, a single node has a finite capacity for **concurrency**. As more team members connect with BI tools, the CPU and I/O resources get saturated, and everyone's queries slow down. This is the practical ceiling of a single-node engine. This is precisely the problem MotherDuck was built to solve. It extends the lightning-fast local experience of DuckDB with a serverless cloud backend, giving you a straightforward path to scale.\n\n## How Does MotherDuck's Hybrid Architecture Deliver Speed and Scale?\n\nMotherDuck introduces a novel architecture that gives you the best of both worlds: the zero-latency feel of local development and the on-demand power of the cloud. This is achieved through a few key concepts.\n\n### What is Dual Execution and Why Does it Matter?\n\nThe magic behind MotherDuck is its [ Dual Execution query planner](https://www.cidrdb.org/cidr2024/papers/p46-atwal.pdf). Instead of forcing you to move all your data to the cloud, it intelligently pushes the computation to where the data lives, minimizing network latency and data transfer costs.\n\n**Local Query:**If you query a CSV file on your laptop, the query runs entirely on your local DuckDB instance. The result is instantaneous.**Cloud Query:**If you query a large table stored in MotherDuck, the work is routed to a dedicated, serverless compute instance (a \"Duckling\") in the cloud.**Hybrid Query:**This is where it gets powerful. If you join a local file with a large cloud table, the planner is smart enough to push filters down to your local machine first. It processes the local file, sends only the small, filtered result to the cloud, and then performs the final join. This makes complex queries incredibly efficient.\n\n### How Do You Handle High Concurrency from BI Tools?\n\nA common and dangerous blind spot for startups is how BI tools handle concurrency. Tools like Tableau or Looker often use a single service account, funneling queries from dozens of users through one connection. This can quickly overwhelm a database.\n\n## Case Study: How Layers Solved Its Concurrency Bottleneck\n\nThe SaaS company\n\n[faced this exact problem. Their analytics, running on PostgreSQL, were overwhelmed when their BI tool masked 73 simultaneous users behind a single service account. This exhausted their connection pool and caused a high rate of query timeouts.]LayersAfter migrating to MotherDuck, they used the\n\nRead Scalingfeature. By connecting their BI tool with a special Read Scaling Token, user queries were automatically distributed across a pool of 16 dedicated, read-only DuckDB replicas. This instantly parallelized the workload, providing each user session with isolated compute. The result was a dramatic improvement in stability, with BI query timeouts dropping to virtually zero.\n\nBy issuing a unique, read-only token for each BI integration and using the `session_hint`\n\nparameter, you can ensure user queries are [intelligently load-balanced](https://motherduck.com/docs/key-tasks/authenticating-and-connecting-to-motherduck/read-scaling/), providing a consistently fast experience for everyone.\n\n## Why Might Lakehouse Architectures Have High Latency Issues?\n\nThe lakehouse architecture, which combines a data lake's flexibility with a data warehouse's performance, is a popular choice. However, startups often find that it comes with a significant \"latency tax,\" especially for interactive queries.\n\nThe primary culprit is the [ \"small files problem.\"](https://dev.to/thedanicafine/the-apache-iceberg-small-file-problem-1k2m) Streaming ingestion jobs often write thousands of tiny files to cloud object storage like Amazon S3. To run a query, the engine must make thousands of separate, high-latency HTTP requests just to read the metadata and find the right files. Each request can take 100-200 milliseconds, adding up to seconds of delay before a single byte of data is even processed.\n\nA benchmark demonstrated this vividly: a query scanning a 5 GB Apache Iceberg table with thousands of small files took **5.093 seconds** to run. The same query on MotherDuck's **DuckLake** format, which uses a database-backed index to avoid scanning S3 for metadata, returned in just **0.146 seconds**, a [ 34x speedup](https://motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats/).\n\n### How Does DuckLake Compare to Apache Iceberg?\n\n| Feature | Apache Iceberg (Standard) | MotherDuck DuckLake |\n|---|---|---|\nMetadata Storage | Stored as files (manifests) in the object store (e.g., S3). | Stored in a fast, transactional database, separate from data files. |\nQuery Planning | Requires multiple sequential S3 reads to discover which data files to scan. | A single SQL query to the catalog database returns the exact list of files. |\nSmall File Handling | Creates many small data and metadata files, requiring costly maintenance jobs (compaction). | Dramatically reduces metadata overhead and the need for frequent compaction. |\nPerformance | High latency for interactive queries due to metadata discovery overhead. |\n34x faster |\n\n## How Can I Get Real-Time Data Without Overloading My Production Database?\n\nIs your sales dashboard always 30 minutes out of date? This frustrating lag is almost always due to a reliance on traditional, batch-based ETL (Extract, Transform, Load) jobs that run on a schedule. For a startup that needs to react instantly to user behavior, this is no longer acceptable.\n\nThe modern solution is a streaming architecture built on **Change Data Capture (CDC)**. Instead of querying your production database tables (which adds heavy load), CDC tools like Estuary or [Streamkap](https://motherduck.com/blog/streamkap-mysql-to-motherduck/) monitor the database's internal transaction log. They capture every insert, update, and delete as it happens and stream these events to MotherDuck in near real-time, often with [ less than 100ms of latency](https://estuary.dev/destination/motherduck/).\n\nThis approach provides two critical benefits:\n\n**Sub-Minute Freshness:**Your analytics are always synchronized with reality.**Zero Production Impact:**It completely isolates your analytical workload, ensuring that heavy queries never slow down your customer-facing application.\n\n## What Are the Signs My Startup Has Outgrown Its Current Analytics Stack?\n\nThe signs that you've outgrown your analytics stack are both technical and cultural. Recognizing them early can save you from months of frustration and slow decision-making.\n\n**Technical Red Flags:**\n\n**High Query Latency:**Your p95 query latency for interactive dashboards consistently exceeds 2-5 seconds. Research shows that[beyond 1 second, users lose their flow of thought, and beyond 10 seconds, you lose their attention entirely](https://www.nngroup.com/articles/powers-of-10-time-scales-in-ux/).**Rising Error Rates:**You see an increase in query timeouts, connection failures, or application errors related to database load.**Stale Data:**Business teams complain that dashboards are always out of date, indicating that nightly batch jobs are no longer sufficient.\n\n**Business and Cultural Red Flags:**\n\n**Dashboard Abandonment:**Analysts and business users stop using the BI tool because it's \"too slow.\"**Rising Costs:**Your infrastructure bill is growing faster than the value you're getting from your data, often due to over-provisioning to compensate for poor performance.**Slow Product Velocity:**Engineers spend more time optimizing the database than building features, and product managers can't get timely data to inform their roadmap.\n\nCompanies that see these signs can achieve significant improvements by migrating. **Finqore** reduced 8-hour financial data pipelines to just 8 minutes, while **uDisc** cut query times from minutes to seconds, leading to [a significant lift in daily active analysts](https://motherduck.com/case-studies/).\n\n## How Do I Plan a Migration to MotherDuck?\n\nMigrating to MotherDuck doesn't have to be a massive, disruptive project. You can see value in under 30 days by following a straightforward, phased approach focused on a single, high-pain workload.\n\n### Your 30-Day Migration Roadmap\n\n| Week | Key Tasks | Success Criteria |\n|---|---|---|\nWeek 1: Connect & Ingest | - Create your MotherDuck account and generate service tokens. - Install the DuckDB SDK and connect via `ATTACH 'md:';` . - Perform an initial bulk load of a target dataset (e.g., one large, slow table) into a new MotherDuck database. | - You can successfully query your data in MotherDuck from your local machine and BI tool. - Historical data for one target workload is fully loaded. |\nWeek 2: Mirror a Pilot Workload | - Select one high-pain, low-risk dashboard to migrate. - Set up an incremental CDC pipeline (e.g., using Estuary) to keep MotherDuck in sync with the source. - Re-create the dashboard's data model to point to MotherDuck. | - The mirrored dashboard in MotherDuck is live and updating in near real-time. - p95 query latency is under 2 seconds. |\nWeek 3: Validate & Optimize | - Share the new dashboard with a small group of business users for feedback. - Monitor query performance and cost. - Use optimizations like Read Scaling tokens for the BI tool connection. | - Users confirm the new dashboard is significantly faster and accurate. - The projected cost for the pilot workload is lower than the legacy system. |\nWeek 4: Cut-Over & Expand | - Officially switch all users to the new MotherDuck-powered dashboard. - Decommission the old data models and pipelines for the migrated workload. - Plan the migration of the next set of analytical workloads. | - 100% of users for the pilot workload are using the new dashboard. - A prioritized backlog for the next migration is created. |\n\n## What Are the Limitations of MotherDuck I Should Consider?\n\nNo tool is perfect for every situation. Being honest about limitations is key to making the right choice. MotherDuck is rapidly evolving, but startups should be aware of a few current constraints:\n\n**Regional Availability:**MotherDuck currently operates in the AWS[us-east-1](https://motherduck.com/docs/concepts/architecture-and-capabilities/)and recently started in[eu-central-1](https://motherduck.com/blog/motherduck-in-europe/), hosted in Frankfurt. Teams with strict data residency requirements outside this region will need to consider this.**Partial DuckDB SQL Coverage:**MotherDuck does not yet support the full range of DuckDB's SQL features. Notably, custom Python/native User-Defined Functions (UDFs), server-side`ATTACH`\n\nto other databases (like Postgres), and custom extensions are not yet available. The common workaround is to perform these specific transformations upstream before loading data into MotherDuck.\n\n## Conclusion: Stop Waiting, Start Analyzing\n\nFor a startup, speed is everything: speed to market, speed to insight, and speed of execution. A slow analytics stack is a direct drag on all three. You don't need the complexity and cost of a legacy data warehouse, nor can you afford the performance bottlenecks of an overloaded production database.\n\nMotherDuck offers a third way, purpose-built for the scale and agility of a modern startup. By combining the raw speed of a local-first engine with the on-demand scale of the cloud, it eliminates the trade-offs that have held analytics teams back for years. If your dashboards are slow and your team is frustrated, it's time to evaluate a new approach.\n\n[Ready to experience sub-second analytics? Start building on MotherDuck's free tier today.](https://motherduck.com/startups/)\n\nStart using MotherDuck now!\n\n## FAQS\n\n### How much does MotherDuck cost for a startup?\n\nMotherDuck's pricing is designed for startups. It includes a generous free tier for smaller projects and two pay-as-you-go plans that charge only for storage used and compute-seconds executed. There are no idle cluster charges or minimums, which can lead to [ 70-90% cost savings](https://motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck/) compared to traditional data warehouses.\n\n### Can MotherDuck handle our data volume?\n\nYes. Analysis shows that [over 95% of startup databases are smaller than 1 TB](https://hemantkgupta.medium.com/insight-from-paper-motherduck-duckdb-in-the-cloud-and-in-the-client-e4a73da9dbec), a size range where MotherDuck excels. The hybrid architecture is designed to scale from megabytes on your laptop to tens of terabytes in the cloud, ensuring you have a growth path.\n\n### How does MotherDuck compare to Snowflake or BigQuery for a startup?\n\nWhile large warehouses are powerful, they often come with significant operational complexity and cost overhead that can be burdensome for a startup. MotherDuck offers [a simpler, more cost-effective path](https://motherduck.com/learn-more/modern-data-warehouse-playbook/). Its key differentiators are the serverless, per-second billing model and the unique \"local-first\" hybrid architecture, which provides an excellent development experience and eliminates network latency for many common analytical tasks.\n\n### How do startups decide if MotherDuck fits their analytics stack?\n\nA startup should consider MotherDuck if they experience slow BI dashboards, rising query timeouts, and stale data. It's a strong fit for teams that have outgrown a single database (like PostgreSQL) but want to avoid the cost and complexity of a traditional data warehouse. MotherDuck excels with data volumes from gigabytes to tens of terabytes and is ideal for building fast, interactive analytics without a large data engineering team.\n\n### Why do dashboards struggle when scanning billions of rows?\n\nWhen scanning billions of rows, single-node engines like DuckDB can be constrained by the memory and I/O of a single machine. Complex queries generate large intermediate results that can exceed available RAM, forcing the engine to \"spill\" to a slower disk, which increases latency. High user concurrency can also saturate the CPU, causing all queries to slow down.\n\n### Why do lakehouse architectures often have high query latency?\n\nLakehouse latency is often caused by the **\"small files problem.\"** Streaming jobs write thousands of tiny files to object storage like S3. To run a query, the engine must make many high-latency network requests just to read the metadata and find the right files, adding seconds of delay before the query even starts processing data.\n\n### What are the main causes of slow BI dashboards?\n\nThe primary cause is an architectural mismatch. Startups often run heavy analytical (OLAP) queries on their production (OLTP) database, like PostgreSQL. These row-oriented databases are inefficient for analytics, as they must read entire records instead of just the needed columns. As data grows, this leads to high I/O, slow queries, and dashboard latency.\n\n### How does data modeling impact dashboard performance?\n\nIt has a massive impact. Using a [ star schema](https://motherduck.com/learn-more/star-schema-data-warehouse-guide/), which organizes data into a central \"fact\" table and surrounding \"dimension\" tables, is a proven technique for accelerating analytical queries in columnar databases.",
  [2m  â”‚[0m     "numLines": 189,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 189
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search for more specific information about network overhead and when queries run locally vs remotely:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ network.*transfer|data.*movement|download|upload

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     "content": "---\ntitle: duckdb-motherduck-for-beginners-your-ultimate-guide\ncontent_type: event\nsource_url: https://motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide\nindexed_at: '2025-11-25T20:44:57.276294'\ncontent_hash: 9ac8939ea24395a7\nhas_code_examples: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nDuckDB & MotherDuck for Beginners: Your Ultimate Guide - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[DuckDB & MotherDuck for Beginners: Your Ultimate Guide](https://www.youtube.com/watch?v=WYV8hvJOAQE)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Why am I seeing this?](https://support.google.com/youtube/answer/9004474?hl=en)\n\n[Watch on](https://www.youtube.com/watch?v=WYV8hvJOAQE&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 36:27\n\n\u2022Live\n\n\u2022\n\nYouTube\n\n# DuckDB & MotherDuck for Beginners: Your Ultimate Guide\n\n2025/02/21\n\n## Why DuckDB is Revolutionizing Data Analytics\n\nDuckDB has experienced explosive growth in popularity, with download statistics showing remarkable adoption rates, particularly in the Python ecosystem. This open-source analytical database is designed to handle everything from quick data exploration to complex data pipelines, and it's already trusted by multiple companies in production environments.\n\n## Understanding the Small and Medium Data Revolution\n\nThe tech industry has long operated under the assumption that analytics requires big data infrastructure. However, this paradigm is being challenged by three key insights:\n\n1. **Most workloads aren't big data**: According to data from AWS Redshift and other cloud analytical databases, approximately 83% of users work with query sizes up to 1TB, and 94% work with data below 10TB.\n\n2. **Modern hardware is incredibly powerful**: Today's single machines can handle up to 24TB of memory on AWS, making distributed systems unnecessary for most use cases.\n\n3. **Distributed systems are expensive**: They require significant IO network traffic for coordination, and the development experience is considerably more complex.\n\n\nDuckDB capitalizes on these realities by enabling users to work with large datasets on a single machine, whether locally on a laptop or in the cloud via a scale-up strategy.\n\n## How DuckDB Works: The In-Process Advantage\n\nDuckDB is an open-source, in-process analytical database written in C++, designed as a single self-contained binary with all dependencies included. This architecture sets it apart from traditional databases.\n\n### Traditional Database Architecture\n\nDatabases typically fall into two categories:\n\n- **OLTP (Online Transaction Processing)**: Databases like PostgreSQL and MySQL, optimized for handling transactions with small datasets. Query times typically range from 1-10 milliseconds.\n\n- **OLAP (Online Analytical Processing)**: Databases like BigQuery and Snowflake, built for analytical queries processing large datasets. Query times can range from 100 milliseconds to several minutes.\n\n\nMost traditional databases use a client-server architecture where the database runs as a separate process, and applications connect to it through SQL queries.\n\n### The In-Process Revolution\n\nIn-process databases run directly within the application process itself. While SQLite pioneered this approach for OLTP workloads, DuckDB introduces something new: an in-process OLAP database optimized for analytical workloads.\n\nThis design enables DuckDB to:\n\n- Run on virtually any platform (laptops to cloud workflows)\n- Integrate seamlessly with any programming language\n- Execute in web browsers via WebAssembly\n- Eliminate network overhead for local operations\n\n## Getting Started with DuckDB\n\n### Installation and Basic Usage\n\nDuckDB can be installed through various methods:\n\n- Direct binary download for CLI usage\n- Package managers (Homebrew for macOS)\n- Language-specific packages (Python, R, Java, etc.)\n\nThe CLI provides a powerful interface for data exploration:\n\n```sql\nCopy code\n\n-- Simple query reading from S3\nFROM 's3://bucket/path/to/file.parquet' LIMIT 5;\n```\n\n### Key Features in Action\n\n**Friendly SQL Dialect**: DuckDB extends standard SQL with productivity enhancements, such as the FROM-first syntax shown above.\n\n**Automatic File Format Detection**: DuckDB automatically detects and handles various file formats including Parquet, CSV, JSON, Iceberg, and Delta Lake.\n\n**Extension System**: DuckDB's functionality is modular through extensions. Core extensions (like HTTPFS for S3 access) are auto-loaded when needed, while community extensions can be installed manually.\n\n## Data Persistence and the DuckDB File Format\n\nBy default, DuckDB operates in-memory, but it offers powerful persistence options:\n\n### Creating and Managing Databases\n\n```sql\nCopy code\n\n-- Attach or create a database\nATTACH 'mydatabase.ddb';\n\n-- Create a table from a query\nCREATE TABLE mytable AS SELECT * FROM source_data;\n```\n\n### The DuckDB File Format\n\nDuckDB's native file format (.ddb or .db) is:\n\n- Self-contained (all tables and metadata in one file)\n- ACID-compliant\n- Highly compressed\n- Optimized for analytical workloads\n\n### Exporting Data\n\nDuckDB supports seamless data export:\n\n```sql\nCopy code\n\n-- Export to CSV\nCOPY (SELECT * FROM mytable) TO 'output.csv';\n```\n\n## Managing Secrets and Authentication\n\nDuckDB includes a comprehensive secret management system for secure cloud access:\n\n```sql\nCopy code\n\n-- Create temporary secret using AWS credential chain\nCREATE SECRET (\n    TYPE S3,\n    PROVIDER credential_chain\n);\n```\n\nThis approach supports:\n\n- AWS SSO authentication\n- Temporary and persistent secrets\n- Multiple cloud providers\n- Secure credential storage\n\n## Scaling to the Cloud with MotherDuck\n\nMotherDuck supercharges DuckDB by transforming it from a single-player to a multiplayer analytics experience. The integration is remarkably simple:\n\n### Connecting to MotherDuck\n\n```sql\nCopy code\n\n-- Connect to MotherDuck with one command\nATTACH 'md:';\n```\n\nAuthentication requires only a MotherDuck token, which can be set as an environment variable.\n\n### Key MotherDuck Features\n\n**Dual Execution**: MotherDuck enables intelligent query execution, automatically determining whether to run computations locally or in the cloud based on data location and query requirements.\n\n**Database Sharing**: Create and share cloud databases with simple commands:\n\n```sql\nCopy code\n\n-- Create a share\nCREATE SHARE myshare FROM mydatabase;\n```\n\n**Performance Benefits**: Leveraging cloud infrastructure provides:\n\n- High-bandwidth connections to cloud storage\n- Elimination of local network bottlenecks\n- Seamless collaboration features\n\n### Cloud Storage Integration\n\nMotherDuck dramatically improves performance when querying cloud storage. In benchmarks, queries that take 11 seconds locally can complete in just 2 seconds when leveraging MotherDuck's cloud infrastructure and network proximity to storage.\n\n## The Complete Analytics Toolkit\n\nDuckDB and MotherDuck together provide:\n\n- **Simplicity**: One-command installation and cloud connection\n- **Flexibility**: Run anywhere from browsers to cloud environments\n- **Performance**: Lightning-fast analytical queries on small to medium datasets\n- **Integration**: Native support for numerous file formats and cloud providers\n- **Collaboration**: Easy data sharing and team workflows\n\nWhether you're analyzing data on your laptop, scaling computations in the cloud, or building production data pipelines, DuckDB offers a modern approach to analytics that challenges traditional big data assumptions while delivering exceptional performance and developer experience.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Data-based: Going Beyond the Dataframe\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FData_based_f32745b461.png&w=3840&q=75)](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n[2025-11-20](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n### [Data-based: Going Beyond the Dataframe](https://motherduck.com/videos/going-beyond-the-dataframe)\n\nLearn how to turbocharge your Python data work using DuckDB and MotherDuck with Pandas. We walk through performance comparisons, exploratory data analysis on bigger datasets, and an end-to-end ML feature engineering pipeline.\n\nWebinar\n\nPython\n\nAI, ML and LLMs\n\n[![\"Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FHex_Webinar_778e3959e4.png&w=3840&q=75)](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n[2025-11-14](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n### [Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck)\n\nAI isn't here to replace data work, it's here to make it better. Watch this webinar to see how Hex and MotherDuck build AI workflows that prioritize context, iteration, and real-world impact.\n\nWebinar\n\nAI, ML and LLMs\n\n[![\"Lies, Damn Lies, and Benchmarks\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FLies_Damn_Lies_and_Benchmarks_Thumbnail_404db1bf46.png&w=3840&q=75)](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n[2025-10-31](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n### [Lies, Damn Lies, and Benchmarks](https://motherduck.com/videos/lies-damn-lies-and-benchmarks)\n\nWhy do database benchmarks so often mislead? MotherDuck CEO Jordan Tigani discusses the pitfalls of performance benchmarking, lessons from BigQuery, and why your own workload is the only benchmark that truly matters.\n\nStream\n\nInterview\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 301,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 301
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-use.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-new-paradigm-for-data-visualization-with-just-sql-markdown.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/postgres-integration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! Now I have enough information to write a comprehensive answer. Let me create the markdown document:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_4.md

  [96mğŸ”§ TOOL:[0m Bash â†’ touch /tmp/answer_without_kg_4.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m 
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /tmp/answer_without_kg_4.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/tmp/answer_without_kg_4.md",
  [2m  â”‚[0m     "content": "# Why might a query run slower on MotherDuck than locally?\n\nWhile MotherDuck is built on DuckDB and leverages its high-performance capabilities, there are several scenarios where queries might run slower on MotherDuck compared to running DuckDB locally. Understanding these scenarios helps optimize your workloads for the right execution context.\n\n## 1. Network Latency and Data Transfer\n\nThe most common reason for slower performance on MotherDuck is **network overhead**. When data needs to be transferred between the cloud and your local machine, network transit introduces latency that doesn't exist in purely local execution.\n\nAccording to MotherDuck's dual execution model, the system intelligently decides whether to run queries locally, remotely in the cloud, or in a hybrid fashion. However, when queries involve downloading result sets or intermediate data from the cloud, network I/O becomes a bottleneck. As demonstrated in performance comparisons, querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds) - but this advantage only exists when the data is already in the cloud.\n\n## 2. Cold Storage Access\n\nQueries on MotherDuck may suffer from **\"cold cache\" penalties** when accessing data from cloud object storage for the first time. The MotherDuck documentation on query performance explicitly mentions that when data for a specific entity (like a user_id) is scattered across many files, \"the first time this data is read, it is a 'cold cache' read from slow object storage, which carries the highest latency penalty.\"\n\nCloud object storage like S3 is inherently slower for I/O operations compared to local SSDs. When DuckDB runs locally with data on your machine's disk, it can leverage the local operating system's page cache and achieve extremely low-latency I/O from local SSDs, which are \"orders of magnitude faster than fetching data from remote object storage.\"\n\n## 3. Small Data Workloads\n\nFor very small datasets that fit entirely in local memory, **local execution is typically faster**. MotherDuck's architecture recognizes that \"most queries aren't actually 'big data'\" - their analysis shows that 99% of queries on systems like Redshift and Snowflake can fit on a single large node, with most under a gigabyte.\n\nWhen you have small datasets already on your local machine, the overhead of cloud communication (authentication, network round trips, result transfer) may exceed the actual computation time. Local DuckDB can process small datasets with zero network overhead and direct memory access.\n\n## 4. Data Already Optimized Locally\n\nIf your local DuckDB instance has data that's already been **optimized for your specific query patterns** (properly partitioned, clustered, and stored in efficient Parquet format on fast local storage), it may outperform MotherDuck queries on unoptimized cloud data.\n\nThe performance documentation emphasizes that \"your data layout strategy is your performance strategy.\" Local data that's been carefully sorted and clustered can enable aggressive data skipping, while cloud data that hasn't been optimized forces the engine to scan far more data than necessary.\n\n## 5. Compute Resource Differences\n\nWhile MotherDuck provides dedicated \"duckling\" instances to avoid noisy neighbor problems, the **compute resources allocated** might differ from your local machine. If you're running on a powerful local workstation with many cores and substantial RAM, it may outperform the default MotherDuck instance for certain workloads.\n\nHowever, this cuts both ways - MotherDuck's cloud infrastructure can also be more powerful than typical laptops for larger queries.\n\n## When MotherDuck is Faster\n\nIt's important to note that MotherDuck's dual execution model is specifically designed to optimize performance by running queries where it makes the most sense:\n\n- **For cloud-native data**: When data already resides in cloud storage (S3, GCS), MotherDuck can scan and filter on the server side, transferring only relevant results\n- **For collaborative workloads**: Database sharing and centralized secret management eliminate the overhead of data duplication\n- **For hybrid queries**: When joining local files with cloud tables, dual execution minimizes data movement by processing each source where it lives\n\n## Optimization Strategies\n\nTo minimize performance differences:\n\n1. **Use the `md_run` parameter** to manually control execution location for specific scans\n2. **Optimize data layout** in cloud storage with proper partitioning and clustering\n3. **Leverage caching**: MotherDuck's Instant SQL feature automatically caches relevant data subsets locally for interactive exploration\n4. **Consider data locality**: For repeatedly queried data, cached local copies may provide better performance\n\n## Sources\n\n- [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained/) - MotherDuck's architecture and hybrid execution model\n- [Fix Slow Query: A Developer's Guide to Data Warehouse Performance](https://motherduck.com/learn-more/diagnose-fix-slow-queries) - Performance bottlenecks including I/O, network, and cold storage issues\n- [Stay in Flow with MotherDuck's Instant SQL](https://motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql) - Dual execution and caching strategies for interactive queries\n- [What is DuckDB?](https://motherduck.com/learn-more/what-is-duckdb) - DuckDB's local-first architecture and performance characteristics\n",
  [2m  â”‚[0m     "numLines": 58,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 58
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I can see the file already exists with content. Let me update it with my more comprehensive answer based on the sources I found:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Edit â†’ /tmp/answer_without_kg_4.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_4.md",
  [2m  â”‚[0m   "oldString": "# Why might a query run slower on MotherDuck than locally?\n\nWhile MotherDuck is built on DuckDB and leverages its high-performance capabilities, there are several scenarios where queries might run slower on MotherDuck compared to running DuckDB locally. Understanding these scenarios helps optimize your workloads for the right execution context.\n\n## 1. Network Latency and Data Transfer\n\nThe most common reason for slower performance on MotherDuck is **network overhead**. When data needs to be transferred between the cloud and your local machine, network transit introduces latency that doesn't exist in purely local execution.\n\nAccording to MotherDuck's dual execution model, the system intelligently decides whether to run queries locally, remotely in the cloud, or in a hybrid fashion. However, when queries involve downloading result sets or intermediate data from the cloud, network I/O becomes a bottleneck. As demonstrated in performance comparisons, querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds) - but this advantage only exists when the data is already in the cloud.\n\n## 2. Cold Storage Access\n\nQueries on MotherDuck may suffer from **\"cold cache\" penalties** when accessing data from cloud object storage for the first time. The MotherDuck documentation on query performance explicitly mentions that when data for a specific entity (like a user_id) is scattered across many files, \"the first time this data is read, it is a 'cold cache' read from slow object storage, which carries the highest latency penalty.\"\n\nCloud object storage like S3 is inherently slower for I/O operations compared to local SSDs. When DuckDB runs locally with data on your machine's disk, it can leverage the local operating system's page cache and achieve extremely low-latency I/O from local SSDs, which are \"orders of magnitude faster than fetching data from remote object storage.\"\n\n## 3. Small Data Workloads\n\nFor very small datasets that fit entirely in local memory, **local execution is typically faster**. MotherDuck's architecture recognizes that \"most queries aren't actually 'big data'\" - their analysis shows that 99% of queries on systems like Redshift and Snowflake can fit on a single large node, with most under a gigabyte.\n\nWhen you have small datasets already on your local machine, the overhead of cloud communication (authentication, network round trips, result transfer) may exceed the actual computation time. Local DuckDB can process small datasets with zero network overhead and direct memory access.\n\n## 4. Data Already Optimized Locally\n\nIf your local DuckDB instance has data that's already been **optimized for your specific query patterns** (properly partitioned, clustered, and stored in efficient Parquet format on fast local storage), it may outperform MotherDuck queries on unoptimized cloud data.\n\nThe performance documentation emphasizes that \"your data layout strategy is your performance strategy.\" Local data that's been carefully sorted and clustered can enable aggressive data skipping, while cloud data that hasn't been optimized forces the engine to scan far more data than necessary.\n\n## 5. Compute Resource Differences\n\nWhile MotherDuck provides dedicated \"duckling\" instances to avoid noisy neighbor problems, the **compute resources allocated** might differ from your local machine. If you're running on a powerful local workstation with many cores and substantial RAM, it may outperform the default MotherDuck instance for certain workloads.\n\nHowever, this cuts both ways - MotherDuck's cloud infrastructure can also be more powerful than typical laptops for larger queries.\n\n## When MotherDuck is Faster\n\nIt's important to note that MotherDuck's dual execution model is specifically designed to optimize performance by running queries where it makes the most sense:\n\n- **For cloud-native data**: When data already resides in cloud storage (S3, GCS), MotherDuck can scan and filter on the server side, transferring only relevant results\n- **For collaborative workloads**: Database sharing and centralized secret management eliminate the overhead of data duplication\n- **For hybrid queries**: When joining local files with cloud tables, dual execution minimizes data movement by processing each source where it lives\n\n## Optimization Strategies\n\nTo minimize performance differences:\n\n1. **Use the `md_run` parameter** to manually control execution location for specific scans\n2. **Optimize data layout** in cloud storage with proper partitioning and clustering\n3. **Leverage caching**: MotherDuck's Instant SQL feature automatically caches relevant data subsets locally for interactive exploration\n4. **Consider data locality**: For repeatedly queried data, cached local copies may provide better performance\n\n## Sources\n\n- [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained/) - MotherDuck's architecture and hybrid execution model\n- [Fix Slow Query: A Developer's Guide to Data Warehouse Performance](https://motherduck.com/learn-more/diagnose-fix-slow-queries) - Performance bottlenecks including I/O, network, and cold storage issues\n- [Stay in Flow with MotherDuck's Instant SQL](https://motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql) - Dual execution and caching strategies for interactive queries\n- [What is DuckDB?](https://motherduck.com/learn-more/what-is-duckdb) - DuckDB's local-first architecture and performance characteristics",
  [2m  â”‚[0m   "newString": "# Why Queries May Run Slower on MotherDuck Than Locally\n\nWhile MotherDuck extends DuckDB's powerful local analytics capabilities to the cloud, there are specific scenarios where queries may run slower on MotherDuck compared to local execution. Understanding these performance trade-offs helps you optimize your query execution strategy.\n\n## Network Data Transfer Overhead\n\nThe primary reason queries may run slower on MotherDuck is **network data transfer**. When a query executes remotely in the cloud, the final result set must be transferred over the network back to your local client. This introduces latency that doesn't exist with purely local execution.\n\n### When This Matters Most\n\nNetwork overhead becomes significant when:\n- **Large result sets**: Queries returning millions of rows must transfer all that data over the network\n- **Geographic distance**: Greater distance between your location and MotherDuck's servers (currently in AWS us-east-1 and eu-central-1) increases latency\n- **Limited bandwidth**: Slower internet connections create bottlenecks for data transfer\n\n## Local vs. Remote Execution: The Trade-offs\n\nMotherDuck's **dual execution model** intelligently decides whether to run queries locally or remotely based on where the data lives. However, this optimization isn't always perfect for every scenario.\n\n### When Local is Faster\n\nLocal execution excels when:\n- **Small local files**: Querying a CSV or Parquet file on your laptop runs entirely locally with zero network overhead\n- **Results are immediately available**: The in-process architecture eliminates all network latency\n- **Your machine has sufficient resources**: Modern hardware with multiple cores and substantial RAM can handle many analytical workloads efficiently\n\n### When Remote (MotherDuck) is Faster\n\nRemote execution on MotherDuck is typically faster when:\n- **Data is already in the cloud**: Querying large tables stored in MotherDuck avoids downloading the entire dataset\n- **Cloud storage access**: Accessing data from S3 or other cloud storage is faster from MotherDuck's cloud infrastructure due to high-bandwidth connections and network proximity. In benchmarks, queries that took 11 seconds locally completed in just 2 seconds on MotherDuck\n- **Compute-intensive operations**: MotherDuck's dedicated \"ducklings\" (containerized DuckDB instances) provide scaled compute resources that may exceed your local machine's capabilities\n\n## The Data Download Factor\n\nA critical performance consideration is **where your source data lives**. As demonstrated in MotherDuck's dual execution documentation:\n\n> A direct comparison showed that querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds).\n\nWhen you force a query to run locally against remote data (using `md_run = 'local'`), your client must:\n1. Download the entire dataset from cloud storage\n2. Then process it locally\n\nThis download step adds substantial overhead that makes local execution slower than remote execution in the cloud, where the compute is already near the data.\n\n## Hybrid Query Performance Considerations\n\nMotherDuck's most powerful feature\u2014hybrid queries that join local and cloud data\u2014involves careful performance trade-offs:\n\n- **Intelligent optimization**: The system efficiently transfers only necessary intermediate data to complete joins\n- **Avoids unnecessary transfers**: It doesn't upload your entire local file or download the entire remote table\n- **Network still required**: Some data must move between local and cloud, introducing latency that wouldn't exist in a purely local or purely cloud query\n\n## Distributed System Overhead\n\nWhile MotherDuck minimizes the complexity of distributed systems, some overhead is inherent when coordinating between local and cloud execution:\n\n- **Query planning**: The dual execution optimizer must analyze where data lives and decide execution strategy\n- **Coordination**: Managing the split between local and remote operations requires communication\n- **Result aggregation**: When operations happen in both locations, results must be combined\n\n## Small Data Paradox\n\nFor very small datasets (under a few hundred megabytes), local DuckDB is often faster because:\n- **Setup time**: There's minimal overhead to just read a local file\n- **Transfer costs exceed compute benefits**: The time to transfer even a small result set over the network may exceed the entire local query execution time\n- **In-memory advantages**: DuckDB's in-process architecture has zero network hops\n\nAs MotherDuck's documentation notes, approximately 94% of analytical workloads involve data below 10TB, and modern single machines can handle datasets up to 24TB in memory. For many of these smaller workloads, local execution can be optimal.\n\n## Optimization Strategies\n\nTo minimize slowdowns when using MotherDuck:\n\n1. **Understand dual execution**: Use `EXPLAIN` to see which parts of your query run locally vs. remotely\n2. **Control execution location**: Use the `md_run` parameter (`'local'` or `'remote'`) in scan functions to override automatic decisions when you know better\n3. **Minimize result set size**: Apply filters and aggregations to reduce data transferred over the network\n4. **Consider data locality**: Store frequently-queried data in MotherDuck rather than forcing remote access from local queries\n5. **Use Read Scaling for BI tools**: For high-concurrency scenarios, MotherDuck's Read Scaling feature distributes queries across multiple replicas, avoiding the \"noisy neighbor\" problem\n\n## The Bottom Line\n\nQueries may run slower on MotherDuck than locally primarily due to **network transfer overhead** when moving data between your machine and the cloud. However, this is only part of the story. MotherDuck is actually faster than local execution when:\n- Data already lives in the cloud\n- You're accessing cloud storage (S3, GCS, etc.)\n- You need more compute resources than your local machine provides\n- You're running high-concurrency workloads\n\nMotherDuck's architecture is designed to **minimize data movement** and **push computation to where data lives**\u2014the key principle behind its dual execution model. Understanding when to leverage local vs. remote execution helps you get the best performance for your specific workload.\n\n## Sources\n\n1. **Bringing DuckDB to the Cloud: Dual Execution Explained** - https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained\n   - Explains dual execution architecture and hybrid query optimization\n   - Documents the 2GB S3 file benchmark: 25 seconds remote vs. 36 seconds local\n   - Describes how the system minimizes data movement\n\n2. **Stay in Flow with MotherDuck's Instant SQL** - https://motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql\n   - Covers MotherDuck's architecture built on three core pillars\n   - Explains dual execution query engine capabilities\n   - Notes that 99% of queries fit on a single large node\n\n3. **Fix Slow BI Dashboards: A Guide to Sub-Second Analytics** - https://motherduck.com/learn-more/fix-slow-bi-dashboards\n   - Documents how dual execution intelligently pushes computation to where data lives\n   - Explains Read Scaling for high-concurrency scenarios\n   - Describes network latency minimization strategies\n\n4. **DuckDB & MotherDuck for Beginners: Your Ultimate Guide** - https://motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide\n   - Benchmarks showing queries completing in 2 seconds on MotherDuck vs. 11 seconds locally\n   - Explains in-process architecture and elimination of network overhead for local operations\n   - Notes that 94% of workloads involve data below 10TB",
  [2m  â”‚[0m   "originalFile": "# Why might a query run slower on MotherDuck than locally?\n\nWhile MotherDuck is built on DuckDB and leverages its high-performance capabilities, there are several scenarios where queries might run slower on MotherDuck compared to running DuckDB locally. Understanding these scenarios helps optimize your workloads for the right execution context.\n\n## 1. Network Latency and Data Transfer\n\nThe most common reason for slower performance on MotherDuck is **network overhead**. When data needs to be transferred between the cloud and your local machine, network transit introduces latency that doesn't exist in purely local execution.\n\nAccording to MotherDuck's dual execution model, the system intelligently decides whether to run queries locally, remotely in the cloud, or in a hybrid fashion. However, when queries involve downloading result sets or intermediate data from the cloud, network I/O becomes a bottleneck. As demonstrated in performance comparisons, querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds) - but this advantage only exists when the data is already in the cloud.\n\n## 2. Cold Storage Access\n\nQueries on MotherDuck may suffer from **\"cold cache\" penalties** when accessing data from cloud object storage for the first time. The MotherDuck documentation on query performance explicitly mentions that when data for a specific entity (like a user_id) is scattered across many files, \"the first time this data is read, it is a 'cold cache' read from slow object storage, which carries the highest latency penalty.\"\n\nCloud object storage like S3 is inherently slower for I/O operations compared to local SSDs. When DuckDB runs locally with data on your machine's disk, it can leverage the local operating system's page cache and achieve extremely low-latency I/O from local SSDs, which are \"orders of magnitude faster than fetching data from remote object storage.\"\n\n## 3. Small Data Workloads\n\nFor very small datasets that fit entirely in local memory, **local execution is typically faster**. MotherDuck's architecture recognizes that \"most queries aren't actually 'big data'\" - their analysis shows that 99% of queries on systems like Redshift and Snowflake can fit on a single large node, with most under a gigabyte.\n\nWhen you have small datasets already on your local machine, the overhead of cloud communication (authentication, network round trips, result transfer) may exceed the actual computation time. Local DuckDB can process small datasets with zero network overhead and direct memory access.\n\n## 4. Data Already Optimized Locally\n\nIf your local DuckDB instance has data that's already been **optimized for your specific query patterns** (properly partitioned, clustered, and stored in efficient Parquet format on fast local storage), it may outperform MotherDuck queries on unoptimized cloud data.\n\nThe performance documentation emphasizes that \"your data layout strategy is your performance strategy.\" Local data that's been carefully sorted and clustered can enable aggressive data skipping, while cloud data that hasn't been optimized forces the engine to scan far more data than necessary.\n\n## 5. Compute Resource Differences\n\nWhile MotherDuck provides dedicated \"duckling\" instances to avoid noisy neighbor problems, the **compute resources allocated** might differ from your local machine. If you're running on a powerful local workstation with many cores and substantial RAM, it may outperform the default MotherDuck instance for certain workloads.\n\nHowever, this cuts both ways - MotherDuck's cloud infrastructure can also be more powerful than typical laptops for larger queries.\n\n## When MotherDuck is Faster\n\nIt's important to note that MotherDuck's dual execution model is specifically designed to optimize performance by running queries where it makes the most sense:\n\n- **For cloud-native data**: When data already resides in cloud storage (S3, GCS), MotherDuck can scan and filter on the server side, transferring only relevant results\n- **For collaborative workloads**: Database sharing and centralized secret management eliminate the overhead of data duplication\n- **For hybrid queries**: When joining local files with cloud tables, dual execution minimizes data movement by processing each source where it lives\n\n## Optimization Strategies\n\nTo minimize performance differences:\n\n1. **Use the `md_run` parameter** to manually control execution location for specific scans\n2. **Optimize data layout** in cloud storage with proper partitioning and clustering\n3. **Leverage caching**: MotherDuck's Instant SQL feature automatically caches relevant data subsets locally for interactive exploration\n4. **Consider data locality**: For repeatedly queried data, cached local copies may provide better performance\n\n## Sources\n\n- [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained/) - MotherDuck's architecture and hybrid execution model\n- [Fix Slow Query: A Developer's Guide to Data Warehouse Performance](https://motherduck.com/learn-more/diagnose-fix-slow-queries) - Performance bottlenecks including I/O, network, and cold storage issues\n- [Stay in Flow with MotherDuck's Instant SQL](https://motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql) - Dual execution and caching strategies for interactive queries\n- [What is DuckDB?](https://motherduck.com/learn-more/what-is-duckdb) - DuckDB's local-first architecture and performance characteristics\n",
  [2m  â”‚[0m   "structuredPatch": [
  [2m  â”‚[0m     {
  [2m  â”‚[0m       "oldStart": 1,
  [2m  â”‚[0m       "oldLines": 57,
  [2m  â”‚[0m       "newStart": 1,
  [2m  â”‚[0m       "newLines": 111,
  [2m  â”‚[0m       "lines": [
  [2m  â”‚[0m         "-# Why might a query run slower on MotherDuck than locally?",
  [2m  â”‚[0m         "+# Why Queries May Run Slower on MotherDuck Than Locally",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-While MotherDuck is built on DuckDB and leverages its high-performance capabilities, there are several scenarios where queries might run slower on MotherDuck compared to running DuckDB locally. Understanding these scenarios helps optimize your workloads for the right execution context.",
  [2m  â”‚[0m         "+While MotherDuck extends DuckDB's powerful local analytics capabilities to the cloud, there are specific scenarios where queries may run slower on MotherDuck compared to local execution. Understanding these performance trade-offs helps you optimize your query execution strategy.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## 1. Network Latency and Data Transfer",
  [2m  â”‚[0m         "+## Network Data Transfer Overhead",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-The most common reason for slower performance on MotherDuck is **network overhead**. When data needs to be transferred between the cloud and your local machine, network transit introduces latency that doesn't exist in purely local execution.",
  [2m  â”‚[0m         "+The primary reason queries may run slower on MotherDuck is **network data transfer**. When a query executes remotely in the cloud, the final result set must be transferred over the network back to your local client. This introduces latency that doesn't exist with purely local execution.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-According to MotherDuck's dual execution model, the system intelligently decides whether to run queries locally, remotely in the cloud, or in a hybrid fashion. However, when queries involve downloading result sets or intermediate data from the cloud, network I/O becomes a bottleneck. As demonstrated in performance comparisons, querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds) - but this advantage only exists when the data is already in the cloud.",
  [2m  â”‚[0m         "+### When This Matters Most",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## 2. Cold Storage Access",
  [2m  â”‚[0m         "+Network overhead becomes significant when:",
  [2m  â”‚[0m         "+- **Large result sets**: Queries returning millions of rows must transfer all that data over the network",
  [2m  â”‚[0m         "+- **Geographic distance**: Greater distance between your location and MotherDuck's servers (currently in AWS us-east-1 and eu-central-1) increases latency",
  [2m  â”‚[0m         "+- **Limited bandwidth**: Slower internet connections create bottlenecks for data transfer",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-Queries on MotherDuck may suffer from **\"cold cache\" penalties** when accessing data from cloud object storage for the first time. The MotherDuck documentation on query performance explicitly mentions that when data for a specific entity (like a user_id) is scattered across many files, \"the first time this data is read, it is a 'cold cache' read from slow object storage, which carries the highest latency penalty.\"",
  [2m  â”‚[0m         "+## Local vs. Remote Execution: The Trade-offs",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-Cloud object storage like S3 is inherently slower for I/O operations compared to local SSDs. When DuckDB runs locally with data on your machine's disk, it can leverage the local operating system's page cache and achieve extremely low-latency I/O from local SSDs, which are \"orders of magnitude faster than fetching data from remote object storage.\"",
  [2m  â”‚[0m         "+MotherDuck's **dual execution model** intelligently decides whether to run queries locally or remotely based on where the data lives. However, this optimization isn't always perfect for every scenario.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## 3. Small Data Workloads",
  [2m  â”‚[0m         "+### When Local is Faster",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-For very small datasets that fit entirely in local memory, **local execution is typically faster**. MotherDuck's architecture recognizes that \"most queries aren't actually 'big data'\" - their analysis shows that 99% of queries on systems like Redshift and Snowflake can fit on a single large node, with most under a gigabyte.",
  [2m  â”‚[0m         "+Local execution excels when:",
  [2m  â”‚[0m         "+- **Small local files**: Querying a CSV or Parquet file on your laptop runs entirely locally with zero network overhead",
  [2m  â”‚[0m         "+- **Results are immediately available**: The in-process architecture eliminates all network latency",
  [2m  â”‚[0m         "+- **Your machine has sufficient resources**: Modern hardware with multiple cores and substantial RAM can handle many analytical workloads efficiently",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-When you have small datasets already on your local machine, the overhead of cloud communication (authentication, network round trips, result transfer) may exceed the actual computation time. Local DuckDB can process small datasets with zero network overhead and direct memory access.",
  [2m  â”‚[0m         "+### When Remote (MotherDuck) is Faster",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## 4. Data Already Optimized Locally",
  [2m  â”‚[0m         "+Remote execution on MotherDuck is typically faster when:",
  [2m  â”‚[0m         "+- **Data is already in the cloud**: Querying large tables stored in MotherDuck avoids downloading the entire dataset",
  [2m  â”‚[0m         "+- **Cloud storage access**: Accessing data from S3 or other cloud storage is faster from MotherDuck's cloud infrastructure due to high-bandwidth connections and network proximity. In benchmarks, queries that took 11 seconds locally completed in just 2 seconds on MotherDuck",
  [2m  â”‚[0m         "+- **Compute-intensive operations**: MotherDuck's dedicated \"ducklings\" (containerized DuckDB instances) provide scaled compute resources that may exceed your local machine's capabilities",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-If your local DuckDB instance has data that's already been **optimized for your specific query patterns** (properly partitioned, clustered, and stored in efficient Parquet format on fast local storage), it may outperform MotherDuck queries on unoptimized cloud data.",
  [2m  â”‚[0m         "+## The Data Download Factor",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-The performance documentation emphasizes that \"your data layout strategy is your performance strategy.\" Local data that's been carefully sorted and clustered can enable aggressive data skipping, while cloud data that hasn't been optimized forces the engine to scan far more data than necessary.",
  [2m  â”‚[0m         "+A critical performance consideration is **where your source data lives**. As demonstrated in MotherDuck's dual execution documentation:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## 5. Compute Resource Differences",
  [2m  â”‚[0m         "+> A direct comparison showed that querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds).",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-While MotherDuck provides dedicated \"duckling\" instances to avoid noisy neighbor problems, the **compute resources allocated** might differ from your local machine. If you're running on a powerful local workstation with many cores and substantial RAM, it may outperform the default MotherDuck instance for certain workloads.",
  [2m  â”‚[0m         "+When you force a query to run locally against remote data (using `md_run = 'local'`), your client must:",
  [2m  â”‚[0m         "+1. Download the entire dataset from cloud storage",
  [2m  â”‚[0m         "+2. Then process it locally",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-However, this cuts both ways - MotherDuck's cloud infrastructure can also be more powerful than typical laptops for larger queries.",
  [2m  â”‚[0m         "+This download step adds substantial overhead that makes local execution slower than remote execution in the cloud, where the compute is already near the data.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## When MotherDuck is Faster",
  [2m  â”‚[0m         "+## Hybrid Query Performance Considerations",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-It's important to note that MotherDuck's dual execution model is specifically designed to optimize performance by running queries where it makes the most sense:",
  [2m  â”‚[0m         "+MotherDuck's most powerful feature\u2014hybrid queries that join local and cloud data\u2014involves careful performance trade-offs:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-- **For cloud-native data**: When data already resides in cloud storage (S3, GCS), MotherDuck can scan and filter on the server side, transferring only relevant results",
  [2m  â”‚[0m         "-- **For collaborative workloads**: Database sharing and centralized secret management eliminate the overhead of data duplication",
  [2m  â”‚[0m         "-- **For hybrid queries**: When joining local files with cloud tables, dual execution minimizes data movement by processing each source where it lives",
  [2m  â”‚[0m         "+- **Intelligent optimization**: The system efficiently transfers only necessary intermediate data to complete joins",
  [2m  â”‚[0m         "+- **Avoids unnecessary transfers**: It doesn't upload your entire local file or download the entire remote table",
  [2m  â”‚[0m         "+- **Network still required**: Some data must move between local and cloud, introducing latency that wouldn't exist in a purely local or purely cloud query",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "+## Distributed System Overhead",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+While MotherDuck minimizes the complexity of distributed systems, some overhead is inherent when coordinating between local and cloud execution:",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+- **Query planning**: The dual execution optimizer must analyze where data lives and decide execution strategy",
  [2m  â”‚[0m         "+- **Coordination**: Managing the split between local and remote operations requires communication",
  [2m  â”‚[0m         "+- **Result aggregation**: When operations happen in both locations, results must be combined",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+## Small Data Paradox",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+For very small datasets (under a few hundred megabytes), local DuckDB is often faster because:",
  [2m  â”‚[0m         "+- **Setup time**: There's minimal overhead to just read a local file",
  [2m  â”‚[0m         "+- **Transfer costs exceed compute benefits**: The time to transfer even a small result set over the network may exceed the entire local query execution time",
  [2m  â”‚[0m         "+- **In-memory advantages**: DuckDB's in-process architecture has zero network hops",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+As MotherDuck's documentation notes, approximately 94% of analytical workloads involve data below 10TB, and modern single machines can handle datasets up to 24TB in memory. For many of these smaller workloads, local execution can be optimal.",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         " ## Optimization Strategies",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-To minimize performance differences:",
  [2m  â”‚[0m         "+To minimize slowdowns when using MotherDuck:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-1. **Use the `md_run` parameter** to manually control execution location for specific scans",
  [2m  â”‚[0m         "-2. **Optimize data layout** in cloud storage with proper partitioning and clustering",
  [2m  â”‚[0m         "-3. **Leverage caching**: MotherDuck's Instant SQL feature automatically caches relevant data subsets locally for interactive exploration",
  [2m  â”‚[0m         "-4. **Consider data locality**: For repeatedly queried data, cached local copies may provide better performance",
  [2m  â”‚[0m         "+1. **Understand dual execution**: Use `EXPLAIN` to see which parts of your query run locally vs. remotely",
  [2m  â”‚[0m         "+2. **Control execution location**: Use the `md_run` parameter (`'local'` or `'remote'`) in scan functions to override automatic decisions when you know better",
  [2m  â”‚[0m         "+3. **Minimize result set size**: Apply filters and aggregations to reduce data transferred over the network",
  [2m  â”‚[0m         "+4. **Consider data locality**: Store frequently-queried data in MotherDuck rather than forcing remote access from local queries",
  [2m  â”‚[0m         "+5. **Use Read Scaling for BI tools**: For high-concurrency scenarios, MotherDuck's Read Scaling feature distributes queries across multiple replicas, avoiding the \"noisy neighbor\" problem",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "+## The Bottom Line",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+Queries may run slower on MotherDuck than locally primarily due to **network transfer overhead** when moving data between your machine and the cloud. However, this is only part of the story. MotherDuck is actually faster than local execution when:",
  [2m  â”‚[0m         "+- Data already lives in the cloud",
  [2m  â”‚[0m         "+- You're accessing cloud storage (S3, GCS, etc.)",
  [2m  â”‚[0m         "+- You need more compute resources than your local machine provides",
  [2m  â”‚[0m         "+- You're running high-concurrency workloads",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+MotherDuck's architecture is designed to **minimize data movement** and **push computation to where data lives**\u2014the key principle behind its dual execution model. Understanding when to leverage local vs. remote execution helps you get the best performance for your specific workload.",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         " ## Sources",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-- [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained/) - MotherDuck's architecture and hybrid execution model",
  [2m  â”‚[0m         "-- [Fix Slow Query: A Developer's Guide to Data Warehouse Performance](https://motherduck.com/learn-more/diagnose-fix-slow-queries) - Performance bottlenecks including I/O, network, and cold storage issues",
  [2m  â”‚[0m         "-- [Stay in Flow with MotherDuck's Instant SQL](https://motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql) - Dual execution and caching strategies for interactive queries",
  [2m  â”‚[0m         "-- [What is DuckDB?](https://motherduck.com/learn-more/what-is-duckdb) - DuckDB's local-first architecture and performance characteristics",
  [2m  â”‚[0m         "+1. **Bringing DuckDB to the Cloud: Dual Execution Explained** - https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained",
  [2m  â”‚[0m         "+   - Explains dual execution architecture and hybrid query optimization",
  [2m  â”‚[0m         "+   - Documents the 2GB S3 file benchmark: 25 seconds remote vs. 36 seconds local",
  [2m  â”‚[0m         "+   - Describes how the system minimizes data movement",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+2. **Stay in Flow with MotherDuck's Instant SQL** - https://motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql",
  [2m  â”‚[0m         "+   - Covers MotherDuck's architecture built on three core pillars",
  [2m  â”‚[0m         "+   - Explains dual execution query engine capabilities",
  [2m  â”‚[0m         "+   - Notes that 99% of queries fit on a single large node",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+3. **Fix Slow BI Dashboards: A Guide to Sub-Second Analytics** - https://motherduck.com/learn-more/fix-slow-bi-dashboards",
  [2m  â”‚[0m         "+   - Documents how dual execution intelligently pushes computation to where data lives",
  [2m  â”‚[0m         "+   - Explains Read Scaling for high-concurrency scenarios",
  [2m  â”‚[0m         "+   - Describes network latency minimization strategies",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+4. **DuckDB & MotherDuck for Beginners: Your Ultimate Guide** - https://motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide",
  [2m  â”‚[0m         "+   - Benchmarks showing queries completing in 2 seconds on MotherDuck vs. 11 seconds locally",
  [2m  â”‚[0m         "+   - Explains in-process architecture and elimination of network overhead for local operations",
  [2m  â”‚[0m         "+   - Notes that 94% of workloads involve data below 10TB"
  [2m  â”‚[0m       ]
  [2m  â”‚[0m     }
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "userModified": false,
  [2m  â”‚[0m   "replaceAll": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! I've successfully completed the task. Here's what I did:
  [94mâ”‚[0m 
  [94mâ”‚[0m ## Summary
  [94mâ”‚[0m 
  [94mâ”‚[0m I searched through the `.kurt/sources/` directory to find information about why queries might run slower on MotherDuck than locally. 
  [94mâ”‚[0m 
  [94mâ”‚[0m **Key findings:**
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. **Network data transfer overhead** is the primary reason - when queries execute remotely, results must be transferred back over the network to your local client
  [94mâ”‚[0m 
  [94mâ”‚[0m 2. **Data location matters significantly** - A benchmark showed that querying a 2GB S3 file took 25 seconds remotely in MotherDuck vs. 36 seconds locally (because the local client had to download the data first)
  [94mâ”‚[0m 
  [94mâ”‚[0m 3. **Small data paradox** - For very small datasets, local execution is often faster due to zero network overhead and DuckDB's in-process architecture
  [94mâ”‚[0m 
  [94mâ”‚[0m 4. **Dual execution trade-offs** - While MotherDuck intelligently decides where to run queries, hybrid queries that join local and cloud data still require some network data movement
  [94mâ”‚[0m 
  [94mâ”‚[0m 5. **When MotherDuck is actually faster** - For cloud storage access (S3, GCS), queries can be 5.5x faster on MotherDuck (2 seconds vs. 11 seconds) due to high-bandwidth connections and network proximity to the data
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer has been written to `/tmp/answer_without_kg_4.md` with a comprehensive explanation and a Sources section citing 4 specific MotherDuck documentation pages.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 11,980 | Cost: $0.3346
     Cumulative: 11,980 tokens | $0.3346
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent has clearly signaled task completion and provided a comprehensive summary, indicating that no further user input is required.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 14 | Duration: 140.5s
â•‘    Tokens: 11,980 | Cost: $0.3346
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  LLM Judge score: 0.87

======================================================================
â“ Question 5/10
How do I set up MotherDuck to work with dbt?
======================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 5
â”‚ Search through the .kurt/sources/ directory to answer this question:

How do I set up MotherDuck to work with dbt?

Steps:
1. Search for the most relevant markdown files in .kurt/sources/
2. Read those files and synthesize the answer
3. Write a markdown answer (with a ## Sources section) to: /tmp/answer_without_kg_5.md
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I'll search through the `.kurt/sources/` directory to find information about setting up MotherDuck with dbt.
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ find .kurt/sources -type f -name "*.md" | head -20
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m .kurt/sources/motherduck.com/research/execution-guided-sql-generation.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/research/ssdtrain-framework-paper.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/research/accelerating-machine-learning-lifecycle.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/customer-support.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/datalab.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/secoda.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/cube.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/airbyte.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/marimo.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/zenlytic.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/bruin.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/ascend.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/hex.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/OneSignal.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/datawisp.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/Shearwater.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/astronomer.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/streamkap.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/puppygraph.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/codecentric.md
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ dbt
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/data-discoverability-secoda-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/sql-ide-safari-harlequin-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/take-flight-with-dbt-and-duckdb-dropping-dev-warehouse-costs-to-zero.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-in-100-seconds-by-a-duck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/no-sql-standard-for-7-years.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/panel-fixing-the-data-engineering-lifecycle-coalesce-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ find .kurt/sources -type f -name "*.md" -path "*dbt*"
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/dbt.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/solving-advent-code-duckdb-dbt.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/dual-execution-dbt.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/llm-data-pipelines-prompt-motherduck-dbt.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/videos/duckdb-dbt-end-to-end-data-engineering-project-23.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/videos/take-flight-with-dbt-and-duckdb-dropping-dev-warehouse-costs-to-zero.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/videos/duckdb-dbt-accelerating-the-developer-experience-with-local-power.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/glossary/data build tool (dbt).md
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me read the most relevant files - starting with the ecosystem page and the main blog post about MotherDuck and dbt:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/ecosystem/dbt.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/ecosystem/dbt.md",
  [2m  â”‚[0m     "content": "---\ntitle: dbt\ncontent_type: tutorial\nsource_url: https://motherduck.com/ecosystem/dbt\nindexed_at: '2025-11-25T20:37:37.218572'\ncontent_hash: 596a296ccc2dc11d\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO ECOSYSTEM](https://motherduck.com/ecosystem/)\n\n# dbt Core\n\nTRANSFORMATION\n\ndbt (data build tool) is a command-line tool that enables data analysts and engineers to transform data in their warehouse more effectively. It allows users to write modular SQL queries, test data integrity, and document data transformations, streamlining the analytics engineering workflow. dbt supports version control and collaboration, making it easier for teams to manage and deploy data models.\n\n## dbt + MotherDuck\n\ndbt integrates with MotherDuck by allowing users to perform data transformations and analytics directly within the MotherDuck cloud data warehouse. By connecting dbt to MotherDuck, users can leverage DuckDB's efficient query execution engine for transforming and modeling data, ensuring seamless and optimized data workflows. This integration empowers teams to build, test, and document their data pipelines efficiently while utilizing MotherDuck's scalable and performant data infrastructure.\n\nSince both dbt and DuckDB can run locally, users can do development and testing using the same technologies they run in production.\n\nBlog\n\nVideo\n\nDocs\n\n![dbt Core's logo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdbt_59d5e63eaf.svg&w=3840&q=75)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 38,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 38
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     "content": "---\ntitle: motherduck-duckdb-dbt\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/motherduck-duckdb-dbt\nindexed_at: '2025-11-25T19:58:31.660396'\ncontent_hash: 4233f76fbdf40552\nhas_code_examples: true\nhas_step_by_step: true\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# MotherDuck + dbt: Better Together\n\n2023/09/07 - 5 min read\n\nBY\n\n[Sung Won Chung](https://motherduck.com/authors/sung-won-chung--/)\n\n## My Personal DuckDB Story\n\nDuckDB has been charming to me ever since I wrote [about it a year ago](https://roundup.getdbt.com/p/dbt-learning-to-love-software-engineers).\n\nIt gave me the glimmers of something I\u2019ve been begging for a long time: fast should be measured in seconds, not minutes.\n\n[I kicked the tires a lot when working at dbt Labs](https://github.com/dbt-labs/jaffle_shop_duckdb).\n\n- [And here](https://www.loom.com/share/ed4a6f59957e43158837eb4ba0c5ed67)\n\n- [And most recently here](https://www.loom.com/share/e213768457094a3187663a6cff76a61d?sid=29d6d696-0581-4b50-af45-7132dfb65f80)\n\n\nAnd in all the tire kicking, it has remained true to the glimmers it gave me and so much more. It\u2019s fast, easy, and cheap. And if it\u2019s running on your local computer, it\u2019s free.\n\nI\u2019ve had incredibly asymmetric expectations of how much money, time, and work it takes to make data fast and easy that I think to myself, \u201cOh, of course you\u2019re supposed to pay lots of dollars to run queries on millions/billions of rows per month.\u201d This has pleasantly disrupted that inner anchoring point. I see something more charming at play. Data teams can be productive with data bigger and work faster and save more money than they could have dreamed of 5 years ago. Heck! Even a year ago. So let\u2019s get into it.\n\n## Why use MotherDuck + dbt?\n\nWell, DuckDB and Motherduck\u2019s primary use case is solving analytical problems fast. Because of its columnar design, it\u2019s able to do just that. Even more so, the creators were smart about making integrations with adjacent data tools a first class experience. We see this with reading S3 files without copying them over and querying postgres directly without needing to extract and load it into DuckDB. And you don\u2019t need to define schemas or tedious configurations to make it work! Motherduck enables the multiplayer experience that having a single file on your machine is too tedious to pass around and synchronize with your teammates. Motherduck runs DuckDB on your behalf AND uses your local computer if the query you\u2019re running makes more sense to run there. You get dynamic execution out of the box. And that\u2019s pretty sweet.\n\nBut more than platitudes, let\u2019s get hands-on with working code so you can taste and see for yourself!\n\n## Get Started\n\nYou can follow along with [this repo](https://github.com/sungchun12/jaffle_shop_duckdb/tree/blog-guide):\n\n1. Signup for a [MotherDuck account!](https://motherduck.com/)![signup](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmotherduck_signup_33d4e9cf54.png%3Fupdated_at%3D2023-09-06T12%3A58%3A42.172Z&w=3840&q=75)\nNote : MotherDuck is still under private beta, but I heard you could get an invite if you join their [community slack](https://slack.motherduck.com/) with a good duck pun.\n\n2. Sign in and your screen should look like this minus some of the stuff you\u2019ll be building in the rest of this guide.\n![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fsignin_14a92fa2b0.png%3Fupdated_at%3D2023-09-06T12%3A58%3A41.340Z&w=3840&q=75)\n\n3. Click on the settings in the upper right hand corner and copy your Service Token to the clipboard.\n![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fservice_token_8fec7f0642.png%3Fupdated_at%3D2023-09-06T12%3A58%3A39.304Z&w=3840&q=75)\n\n4. Clone the repo and change directories into it.\n\n\n```bash\nCopy code\n\ngit clone -b blog-guide https://github.com/sungchun12/jaffle_shop_duckdb.git\ncd jaffle_shop_duckdb\n```\n\n5. Follow the detailed instructions to setup your [free AWS account and use S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/GetStartedWithS3.html):\n\n_Note: Feel free to skip this step if you already have an AWS account with S3 setup! Plus, MotherDuck has these data under their public S3 bucket at s3://us-prd-motherduck-open-datasets/jaffle\\_shop/csv/_\n\n6. Take the csv files stored in the git repo [here](https://github.com/sungchun12/jaffle_shop_duckdb/tree/blog-guide/seeds) and upload them into S3:\n![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fseeds_9b3753fd5d.png%3Fupdated_at%3D2023-09-06T12%3A58%3A41.504Z&w=3840&q=75)![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fs3_seeds_adf0454153.png%3Fupdated_at%3D2023-09-06T12%3A58%3A41.673Z&w=3840&q=75)\n\n7. [Copy the AWS S3 access keys to authenticate](https://docs.aws.amazon.com/powershell/latest/userguide/pstools-appendix-sign-up.html) your dbt project for later.\n\n\n## Configure Your dbt project\n\n_Note: Huge thanks to Josh Wills for creating the dbt-duckdb adapter and it works great with both DuckDB and MotherDuck: [https://github.com/jwills/dbt-duckdb](https://github.com/jwills/dbt-duckdb). This demo only works with DuckDB version 0.8.1: [https://motherduck.com/docs/intro](https://motherduck.com/docs/intro)_\n\n1. Adjust your `profiles.yml` for the naming conventions that make sense to you. Specifically, focus on schema.\n\n```yaml\nCopy code\n\njaffle_shop:\n\n  target: dev\n  outputs:\n    dev:\n      type: duckdb\n      schema: dev_sung\n      path: 'md:jaffle_shop'\n      threads: 16\n      extensions:\n        - httpfs\n      settings:\n        s3_region: \"{{ env_var('S3_REGION', 'us-west-1') }}\"\n        s3_access_key_id: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n        s3_secret_access_key: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n\n    dev_public_s3:\n      type: duckdb\n      schema: dev_sung\n      path: 'md:jaffle_shop'\n      threads: 16\n      extensions:\n        - httpfs\n      settings:\n        s3_region: \"{{ env_var('S3_REGION', 'us-east-1') }}\" # default region to make hello_public_s3.sql work correctly!\n        s3_access_key_id: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n        s3_secret_access_key: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n\n    prod:\n      type: duckdb\n      schema: prod_sung\n      path: 'md:jaffle_shop'\n      threads: 16\n      extensions:\n        - httpfs\n      settings:\n        s3_region: us-west-1\n        s3_access_key_id: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n        s3_secret_access_key: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n```\n\n2. Export your motherduck and S3 credentials to the terminal session, so your dbt project can authenticate to both\n\n```shell\nCopy code\n\n# all examples are fake\nexport motherduck_token=<your motherduck token> # aouiweh98229g193g1rb9u1\nexport S3_REGION=<your region> # us-west-1\nexport S3_ACCESS_KEY_ID=<your access key id> # haoiwehfpoiahpwohf\nexport S3_SECRET_ACCESS_KEY=<your secret access key> # jiaowhefa998333\n```\n\n3. Create a python virtual environment and install the packages to run this dbt project\n\n```shell\nCopy code\n\npython3 -m venv venv\nsource venv/bin/activate\npython3 -m pip install --upgrade pip\npython3 -m pip install -r requirements.txt\n```\n\n4. Run `dbt debug` to verify dbt can connect to motherduck and S3\n\n```shell\nCopy code\n\ndbt debug\n```\n\n5. Run `dbt build` to run and test the project!\n\n```shell\nCopy code\n\ndbt build\n```\n\n6. If you're feeling adventurous, run the below to reference a public s3 bucket provided by MotherDuck!\n\nImpacted dbt model\n\n```sql\nCopy code\n\n--filename: hello_public_s3.sql\n{% if target.name == 'dev_public_s3' %}\n\nSELECT * FROM 's3://us-prd-motherduck-open-datasets/jaffle_shop/csv/raw_customers.csv'\n\n{% else %}\n\nselect 1 as id\n\n{% endif %}\n```\n\n```shell\nCopy code\n\ndbt build --target dev_public_s3\n```\n\n7. Now, you should see everything ran with green font everywhere and you should see this in the UI! Including the S3 data you built a dbt model on top of!\n\n![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgreen_logs_ad3ec33dd1.png%3Fupdated_at%3D2023-09-06T12%3A58%3A39.718Z&w=3840&q=75)![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmotherduck_success_5f718670be.png%3Fupdated_at%3D2023-09-06T12%3A58%3A42.460Z&w=3840&q=75)\n\nThat\u2019s it! Ruffle up those feathers and start quacking and slapping those juicy SQL queries together to solve your analytics problems faster and cheaper than ever before!\n\n## Conclusion\n\nWe\u2019re at a really cool place where all I had to give you was a couple instructions to get you up and running with MotherDuck. I really hope the data industry gets to a place where we brag about the things we do NOT have to do vs. pride ourselves on complexity for its own sake. What matters is that we solve problems and spend time, money, and energy doing it where it\u2019s actually worth it to solve those problems. I\u2019m excited to see you all build MotherDuck guides far superior to mine. That\u2019s why this is so fun. We get to sharpen each other!\n\n_Want to know more about MotherDuck and dbt ? Checkout [MotherDuck & dbt documentation](https://motherduck.com/docs/integrations/transformation/dbt/) and have a look at their YouTube tutorial about DuckDB & dbt \ud83d\udc47_\n\nUnleashing DuckDB & dbt for local analytics triumphs - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Unleashing DuckDB & dbt for local analytics triumphs](https://www.youtube.com/watch?v=asxGh2TrNyI)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nFull screen is unavailable. [Learn More](https://support.google.com/youtube/answer/6276924)\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nMore videos\n\n## More videos\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=asxGh2TrNyI&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 8:54\n\n\u2022Live\n\n\u2022\n\n### TABLE OF CONTENTS\n\n[My Personal DuckDB Story](https://motherduck.com/blog/motherduck-duckdb-dbt/#my-personal-duckdb-story)\n\n[Why use MotherDuck + dbt?](https://motherduck.com/blog/motherduck-duckdb-dbt/#why-use-motherduck-dbt)\n\n[Get Started](https://motherduck.com/blog/motherduck-duckdb-dbt/#get-started)\n\n[Configure Your dbt project](https://motherduck.com/blog/motherduck-duckdb-dbt/#configure-your-dbt-project)\n\n[Conclusion](https://motherduck.com/blog/motherduck-duckdb-dbt/#conclusion)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![This Month in the DuckDB Ecosystem: August 2023](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fthumbnail_duckdb_newsletter_1_feb51165aa.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-nine/)\n\n[2023/08/21 - Marcos Ortiz](https://motherduck.com/blog/duckdb-ecosystem-newsletter-nine/)\n\n### [This Month in the DuckDB Ecosystem: August 2023](https://motherduck.com/blog/duckdb-ecosystem-newsletter-nine)\n\nThis Month in the DuckDB Ecosystem: August 2023\n\n[![DuckDB Tutorial For Beginners](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgetting_started_duckdb_thumbnail_70b197b1ab.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n[2024/10/31 - Mehdi Ouazza, Ryan Boyd](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n### [DuckDB Tutorial For Beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners)\n\nGet up to speed quickly with DuckDB, including installation, VSCode workflow integration and your first SQL analytics project.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 314,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 314
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md",
  [2m  â”‚[0m     "content": "---\ntitle: motherduck-dbt-pipelines\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/motherduck-dbt-pipelines\nindexed_at: '2025-11-25T19:57:03.385445'\ncontent_hash: 133a50ff003b0fce\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Performant dbt pipelines with MotherDuck\n\n2024/10/07 - 6 min read\n\nBY\n\n[Jacob Matson](https://motherduck.com/authors/jacob-matson/)\n\n_Ed. note: This blog post is a recap of the dbt+MotherDuck workshop at [Small Data SF](https://smalldatasf.com/). For event info and to learn about the next one, hit the website._\n\n## Quick Summary\n\nIn this blog, we will hit on the learnings and unique bits of kit that are a part of DuckDB & MotherDuck to build performant data pipelines in dbt. The final github repository can be [found here](https://github.com/matsonj/stocks). This article is not introductory level, and assumes that you have some experience with dbt.\n\nThe key bits, in order of DAG execution:\n\n- the read\\_blob() function\n- pre\\_hooks & variables + array\\_agg()\n- incremental models & read\\_csv()\n- unnest() + arg\\_max()\n\nThe goal of this exercise is to read a list of files, and then update the dbt models based on this list. The rough data flow looks like this:\n\n![Screenshot 2024-10-07 at 10.47.47\u202fAM.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2024_10_07_at_10_47_47_AM_f29c3a552e.png&w=3840&q=75)\n\nIn order to build a pipeline that can run on top of our data lake, we need to understand what file operations are available in MotherDuck and how to utilize them best inside of a dbt pipeline.\n\n## The read\\_blob function\n\n[Read Blob](https://duckdb.org/docs/guides/file_formats/read_file.html#read_blob) is the first function required to make this pipeline work. It takes a path as a parameter and returns a table with filenames, file size, schema, and last modified date. To assure that other files do not randomly get inserted into our pipeline while it is running, we are going to materialize this as a table and use it as the starting point for the pipeline.\n\n```sql\nCopy code\n\nselect\n    \"filename\" as \"file\",\n    regexp_extract(\"filename\", 'data/(.+?)_\\d+\\.csv', 1) as entity,\n    last_modified as modified_ts\nfrom read_blob('data/*.csv')\n```\n\nIn this example, DuckDB is inspecting local data. But DuckDB also includes capabilities to interact with [Object Stores](https://duckdb.org/docs/extensions/httpfs/overview.html), which means this functionality can easily be extended to data lakes.\n\n## Pre-hooks & variables + array\\_agg\n\nThe next set of models will be broken into two parts - the pre-hook and then the incremental mode. First we will discuss the pre-hook, which leverages a new concept in Duckdb 1.1, [variables](https://duckdb.org/docs/sql/statements/set_variable.html). Variables allow us to insert arbitrary values into them with the `set variable` command and then pass arbitrary values into sql queries with `getvariable()`. Variables only support scalar values, but since DuckDB supports structs (that is - custom data structures), those can also be used with variables. DuckDB also contains a sets of functions to handle structs, like `array_agg()` which is used to turn a table column into a list.\n\nThese concepts can be used together like the example below.\n\n```sql\nCopy code\n\n{{\n    config(\n        pre_hook=\"\"\"\n            set variable my_list = (\n                select array_agg(file)\n   from {{ ref('files') }}\n   where entity = 'ticker_info'\n            )\n        \"\"\",\n        materialized=\"incremental\",\n        unique_key=\"id\",\n    )\n}}\n```\n\n## Incremental models & read\\_csv\n\ndbt has the notion of \u201c [Incremental Materializations](https://docs.getdbt.com/docs/build/incremental-models)\u201d \\- models that are handled in a different flow and require more explicit definition, and thus can be built incrementally. These models usually require a unique\\_key, if no key is provided, the model is treated as \u201cappend only\u201d.\n\nFurthermore, incremental models must define which pieces of the model run incrementally.\n\nWhen invoked in normal dbt build or dbt run, incremental models will do the following:\n\n1. Insert new data into a temp table based on the defined increment.\n2. Delete any data from the existing model that matches the unique\\_key defined in the config block.\n3. Insert data from the temp table into the existing model.\n\nThis obviously means that changes to the schema of your model need to be carefully considered - new columns mean that the model must be rebuilt entirely. A rebuild of the model is called a \u201cfull refresh\u201d in dbt can be invoked with the full-refresh flag in the CLI.\n\nAs described in the pre\\_hook, the variable `my_list` contains a list of files to process, and the config block also contains the relevant information for the model type and key.\n\n```sql\nCopy code\n\nselect\n    info.symbol || '-' || info.filename as id,\n    info.*,\n    files.modified_ts,\n    now() at time zone 'UTC' as updated_ts\nfrom read_csv(getvariable('my_list'), filename = true, union_by_name = true) as info\nleft join {{ ref(\"files\") }} as files on info.filename = files.file\n{% if is_incremental() %}\n    where not exists (select 1 from {{ this }} ck where ck.filename = info.filename)\n{% endif %}\n```\n\nThis also introduces the concept of [{{ this }}](https://docs.getdbt.com/reference/dbt-jinja-functions/this), which is a dbt relation and is a reference to the current model.\n\n## Unnest + arg\\_max\n\nIn any data warehouse, the presence of duplicate data is almost inevitable. This can occur due to various reasons, but that doesn\u2019t make it any less painful.\n\n- Data Integration: When combining data from multiple sources, inconsistencies and overlaps can lead to duplicates.\n- REST API sources: Many data sources don\u2019t allow for incremental updates, which means that every time you get new data, it difficult or impossible to handle it with creating duplicates.\n\nIn order to handle de-duplication in the dbt models, we can use `arg_max()` and `unnest()`. `Arg_max()` allows users to pass a table reference and a numeric column (including dates & timestamps) and returns a single row as a struct. Since it returns this data type, `unnest()` is used in order to get a single row from the `arg_max()` function.\n\n```sql\nCopy code\n\nwith\n    cte_all_rows as (\n        select\n            symbol,\n            * exclude(id, symbol),\n            modified_ts as ts\n        from {{ ref(\"company_info\") }}\n    )\nselect unnest(arg_max(cte_all_rows, ts))\nfrom cte_all_rows\ngroup by symbol\n```\n\nAs an aside - why use `arg_max()` instead of a window function? The short answer is that `arg_max()` uses Radix sort, which leverages SQL group by to identify the groups in which to find the max. The time complexity of Radix sort is _O (n k)_, whereas comparison- based sorting algorithms have _O (n log n)_ time complexity.\n\n## Closing Thoughts\n\nIn conclusion, dbt and MotherDuck together offer a powerful framework for efficient data transformations and analysis. By leveraging tools like `read_blob()` for data ingestion, utilizing `pre_hooks` and `variables` to streamline logic with functions like `array_agg()`, and implementing incremental models with `read_csv()` for optimal performance, you can significantly enhance your data workflows. Additionally, advanced techniques like `unnest()` combined with `arg_max()` allow for more sophisticated data manipulation, unlocking even greater efficiency in your analyses. When used effectively, dbt & motherduck can transform your approach to data, enabling both speed and accuracy in your models. A working demo & instruction that can be found in [this github repo](https://github.com/matsonj/stocks). Good luck and happy quacking!\n\n### TABLE OF CONTENTS\n\n[Quick Summary](https://motherduck.com/blog/motherduck-dbt-pipelines/#quick-summary)\n\n[The read\\_blob function](https://motherduck.com/blog/motherduck-dbt-pipelines/#the-readblob-function)\n\n[Pre-hooks & variables + array\\_agg](https://motherduck.com/blog/motherduck-dbt-pipelines/#pre-hooks-variables-arrayagg)\n\n[Incremental models & read\\_csv](https://motherduck.com/blog/motherduck-dbt-pipelines/#incremental-models-readcsv)\n\n[Unnest + arg\\_max](https://motherduck.com/blog/motherduck-dbt-pipelines/#unnest-argmax)\n\n[Closing Thoughts](https://motherduck.com/blog/motherduck-dbt-pipelines/#closing-thoughts)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![MotherDuck at Coalesce 2024: Your Ultimate Guide to Quack-tastic Fun!](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FCoalesce_3f73bf385e.png&w=3840&q=75)](https://motherduck.com/blog/guide-to-coalesce-2024/)\n\n[2024/10/01 - Jerel Navarrete](https://motherduck.com/blog/guide-to-coalesce-2024/)\n\n### [MotherDuck at Coalesce 2024: Your Ultimate Guide to Quack-tastic Fun!](https://motherduck.com/blog/guide-to-coalesce-2024)\n\nGet ready to make a splash at Coalesce 2024! \ud83e\udd86 MotherDuck is bringing the fun to Las Vegas, and we can't wait to see you there. Whether you're a seasoned Coalesce pro or a first-timer, we've got everything you need to make this year's event unforgettable.\n\n[![DuckDB Tutorial For Beginners](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgetting_started_duckdb_thumbnail_70b197b1ab.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n[2024/10/31 - Mehdi Ouazza, Ryan Boyd](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n### [DuckDB Tutorial For Beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners)\n\nGet up to speed quickly with DuckDB, including installation, VSCode workflow integration and your first SQL analytics project.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 200,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 200
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me search for any official documentation about setting up dbt with MotherDuck:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ find .kurt/sources -type f -name "*.md" -path "*docs*" | grep -i dbt
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m 
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ dbt-duckdb
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/community-and-open-source.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-everybody-hates-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/solving-advent-code-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-kestra-etl-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-seven.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-six.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-ten.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-three.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-five.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/graham-wetzler.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me also check for any files that discuss profiles.yml or configuration:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ profiles\.yml
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-kestra-etl-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-essential-tools.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 5,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me read one more comprehensive guide:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     "content": "---\ntitle: duckdb-dbt-e2e-data-engineering-project-part-2\ncontent_type: blog\nsource_url: https://motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2\nindexed_at: '2025-11-25T19:58:03.158001'\ncontent_hash: df5e80b6258156d7\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# DuckDB & dbt \\| End-To-End Data Engineering Project (2/3)\n\n2024/03/22 - 10 min read\n\nBY\n\n[Mehdi Ouazza](https://motherduck.com/authors/mehdi-ouazza/)\n\ndbt is a great and straightforward tool for building production-ready data pipelines with SQL. It acts as a toolkit that assists in packaging, testing, and deploying your SQL pipelines. However, there's a common misconception that dbt itself processes data. In reality, it's a client that sends SQL commands to a cloud data warehouse, where the actual computing occurs. As a result, you always depend on this cloud service, and the development loop can sometimes be challenging.\n\nIn this blog, we'll explore how the development experience can be significantly improved through the use of DuckDB and dbt. We'll learn how to streamline your architecture, accelerate certain pipelines, and finally allow you to write genuine unit tests. We'll also cover some best practices for AWS S3 authentication and managing incremental pipelines.\n\nAll the source code is available on [GitHub](https://github.com/mehd-io/pypi-duck-flow). And for those who prefer watching over reading, I've got a video for you.\n\nDuckDB & dbt \\| End-To-End Data Engineering Project (2/3) - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[DuckDB & dbt \\| End-To-End Data Engineering Project (2/3)](https://www.youtube.com/watch?v=SpfEQQXBGMQ)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nFull screen is unavailable. [Learn More](https://support.google.com/youtube/answer/6276924)\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=SpfEQQXBGMQ&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 37:25\n\n\u2022Live\n\n\u2022\n\n## Quick recap on part 1 : ingestion\n\n[In the first part of our end-to-end data engineering project](https://motherduck.com/blog/duckdb-python-e2e-data-engineering-project-part-1/), we gathered data from PyPi to obtain download statistics for a specific Python library, DuckDB, using Python. In this second part, we'll transform this raw data using dbt and DuckDB to prepare a dataset ready for data visualization, which will be the focus of part three in this series.\n\nDon't worry if you haven't completed the first part of the project; we've got you covered. We have some sample raw data available in a public AWS S3 bucket that you can use as input for the transformation pipeline.\n\n## dbt & DuckDB Integration\n\nIn dbt, we connect to various databases through [adapters](https://docs.getdbt.com/reference/dbt-jinja-functions/adapter), which are defined in a YAML file. These adapters make it easy to switch quickly between different environments. Typically, your Python process (dbt) would send the query to the target database.\n\nHowever, since DuckDB is an embedded database and just another Python library to install (without any cloud dependency), we can run the computation within the same Python dbt process!\n\nIn this dbt project, we'll look at two setups (aka targets):\n\n1. Reading and writing from S3 when using dbt and DuckDB locally (our \"dev\" setup)\n2. Reading from S3 and pushing the result back to MotherDuck (our \"prod\" setup)\n\n![arch](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdbt_duckdb_md_excalidraw_11600df402.png&w=3840&q=75)\n\nSince MotherDuck is DuckDB in the cloud, you benefit from a seamless transition from working locally to scaling in the cloud. Moreover, for part 3, as we aim to create a dashboard with a BI tool, which mostly relies on a SQL engine to fetch data, MotherDuck will prove to be very useful.\n\nLet's dive into the code.\n\n## Building the SQL pipeline\n\n### Setup\n\nOur initial repository has a monolithic structure with the first part of the series located under `/ingestion`. We'll create a new folder under `/transform` for the code discussed in this blog.\n\nFirst off, we need to add the dbt package dependency. As of now, MotherDuck supports only one version of DuckDB. We're using Poetry as our package manager, so to install dbt and the appropriate DuckDB version, simply execute:\n\n```csharp\nCopy code\n\npoetry add dbt-duckdb[md]\n```\n\nNext, initiate the dbt repository under `./transform` with:\n\n```csharp\nCopy code\n\ndbt init pypi_metrics\n```\n\nYou should now see a structure with some folders pre-created for you:\n\n```csharp\nCopy code\n\n.\n\u251c\u2500\u2500 analyses\n\u251c\u2500\u2500 dbt_project.yml\n\u251c\u2500\u2500 macros\n\u251c\u2500\u2500 models\n\u251c\u2500\u2500 package-lock.yml\n\u251c\u2500\u2500 packages.yml\n\u251c\u2500\u2500 profiles.yml\n\u251c\u2500\u2500 seeds\n\u251c\u2500\u2500 snapshots\n\u251c\u2500\u2500 target\n\u2514\u2500\u2500 tests\n```\n\n### Exploring the Data and Building the Model\n\nTo start, I want to explore the raw data. You can access a free public sample here: `s3://us-prd-motherduck-open-datasets/pypi/sample_tutorial/pypi_file_downloads/*/*/*.parquet`\n\nA straightforward way to begin is by using the DuckDB CLI. You can [find the installation steps online](https://duckdb.org/docs/installation/). A useful setup I recommend -if you are using VSCode- is opening a terminal in VSCode and configuring a shortcut to send commands from the editor to the terminal (the opened DuckDB CLI).\nI assigned the `cmd+k` shortcut to this specific command in my JSON Keyboard Shortcuts settings.\n\n```json\nCopy code\n\n  {\n    \"key\": \"cmd+k\",\n    \"command\": \"workbench.action.terminal.runSelectedText\"\n  },\n```\n\nThat way, you are building your SQL query directly at the right place, in a SQL file\n![](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fim1_89602c1b13.png&w=3840&q=75)\n\nAs you can see on the above screenshot, you can easily describe a remote parquet file using :\n\n```sql\nCopy code\n\nDESCRIBE TABLE 's3://us-prd-motherduck-open-datasets/pypi/sample_tutorial/pypi_file_downloads/*/*/*.parquet';\n```\n\n![](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2024_03_22_at_16_11_36_0b37ea8ccb.png&w=3840&q=75)\n\nThis data shows each row as a download of a specific Python project, already filtered for the `duckdb` project.\n\nOur transformations should include:\n\n- Selecting only relevant columns and unnesting as necessary.\n- Converting the Python version to include only minor versions (e.g., 3.9.1 -> 3.9) for more meaningful aggregation.\n- Aggregating the download count per day to streamline our insights.\n- Adding a `load_id` (based on a hash) for incremental loading.\n\nThe final model is as follows:\n\n```vbnet\nCopy code\n\nWITH pre_aggregated_data AS (\n    SELECT\n        timestamp :: date as download_date,\n        details.system.name AS system_name,\n        details.system.release AS system_release,\n        file.version AS version,\n        project,\n        country_code,\n        details.cpu,\n        CASE\n            WHEN details.python IS NULL THEN NULL\n            ELSE CONCAT(\n                SPLIT_PART(details.python, '.', 1),\n                '.',\n                SPLIT_PART(details.python, '.', 2)\n            )\n        END AS python_version\n    FROM\n          {{ dbt_unit_testing.source('external_source', 'pypi_file_downloads') }}\n    WHERE\n        download_date >= '{{ var(\"start_date\") }}'\n        AND download_date < '{{ var(\"end_date\") }}'\n)\n\nSELECT\n    MD5(CONCAT_WS('|', download_date, system_name, system_release, version, project, country_code, cpu, python_version)) AS load_id,\n    download_date,\n    system_name,\n    system_release,\n    version,\n    project,\n    country_code,\n    cpu,\n    python_version,\n    COUNT(*) AS daily_download_sum\nFROM\n    pre_aggregated_data\nGROUP BY\n    ALL\n```\n\nNotable points include:\n\n- Filtering is always done between a dbt variable including `start_date` and `end_date` for easy data reprocessing.\n- The source table is abstracted with `{{ dbt_unit_testing.source('external_source', 'pypi_file_downloads') }}` for unit testing purposes (more on that further in the blog).\n\nBefore we get to unit testing, let's review our configuration files, mainly `sources.yml` and `dbt_project.yml` and `profiles.yml`.\n\n### YAML configurations files\n\nSources are defined in `sources.yml` in `/transform/pypi_metrics/models/sources.yml`\n\n```yaml\nCopy code\n\nversion: 2\n\nsources:\n  - name: external_source\n    meta:\n      external_location: \"{{ env_var('TRANSFORM_S3_PATH_INPUT') }}\"\n    tables:\n      - name: pypi_file_downloads\n```\n\nWe're using an external location (AWS S3) with a nickname that we referred to in our model's `FROM` statement earlier.\n\nWe've also made the S3 path flexible so it can be provided through environment variables.\n\nTo manage these environment variables smoothly, we use a `Makefile` along with a `.env` file. At the beginning of the `Makefile`, you'll see:\n\n```bash\nCopy code\n\ninclude .env\nexport\n```\n\nIn the code repository, there's an `env.template` file. You can copy this to create a `.env` file and enter the necessary values.\n\nNext, we initiate the dbt run through an entry in the Makefile named `pypi-transform`:\n\n```makefile\nCopy code\n\npypi-transform:\n\tcd $$DBT_FOLDER && \\\n\tdbt run \\\n\t\t--target $$DBT_TARGET \\\n\t\t--vars '{\"start_date\": \"$(START_DATE)\", \"end_date\": \"$(END_DATE)\"}'\n```\n\nLet's have a look now on our `dbt_project.yml`\n\n```ini\nCopy code\n\nmodels:\n  pypi_metrics:\n    pypi_daily_stats:\n      +materialized: \"{{ 'incremental' if target.name == 'prod' else 'table' }}\"\n      +unique_key: load_id\n      +pre-hook: \"{% if target.name == 'dev' %}CALL load_aws_credentials(){% endif %}\"\n      +post-hook: \"{% if target.name == 'dev' %}{{ export_partition_data('download_date', this.name ) }}{% endif %}\"\n```\n\nAs mentioned before, we have two setups: one for local running and read/writing to AWS S3, and another using MotherDuck, designated as `dev` and `prod` targets, respectively.\n\nThese settings are outlined in our `profiles.yml`:\n\n```yaml\nCopy code\n\npypi_metrics:\n  outputs:\n    dev:\n      type: duckdb\n      path: /tmp/dbt.duckdb\n    prod:\n      type: duckdb\n      path: \"md:\"\n  target: dev\n```\n\nThe only difference between running locally and using MotherDuck is the `path` setting. Using `md:` triggers authentication with MotherDuck, which checks for a token in the `motherduck_token` environment variable. You can get this token from your MotherDuck account settings page.\n\nWe face a few challenges:\n\n- dbt doesn't support incremental loading when writing to an external source like AWS S3.\n- We need to authenticate with AWS S3.\n\nThankfully, DuckDB offers extensions that simplify authentication and read/write operations to AWS S3. To address the first challenge, we write to AWS S3 with partitions, allowing us to process within a specific time frame and overwrite any existing partitions.\n\nWe use a simple macro, `export_partition_data.sql`, for this:\n\n```sql\nCopy code\n\n{% macro export_partition_data(date_column, table) %}\n{% set s3_path = env_var('TRANSFORM_S3_PATH_OUTPUT', 'my-bucket-path') %}\n    COPY (\n        SELECT *,\n            YEAR({{ date_column }}) AS year,\n            MONTH({{ date_column }}) AS month\n        FROM {{ table }}\n    )\n    TO '{{ s3_path }}/{{ table }}'\n     (FORMAT PARQUET, PARTITION_BY (year, month), OVERWRITE_OR_IGNORE 1, COMPRESSION 'ZSTD', ROW_GROUP_SIZE 1000000);\n{% endmacro %}\n```\n\nWith dbt running DuckDB, it creates an internal table from the model, allowing us to easily export this data to any format and remote storage (AWS S3/GCP Cloud storage) using the `COPY` command.\n\nNotable points include:\n\n- The AWS S3 path is set as an environment variable.\n- We use a date column for partition generation. For instance, our data will be stored as `s3://my-bucket/my_data/year=2024/month=04`.\n\nFor authentication, we use another extension and invoke `CALL load_aws_credentials()` as a pre-hook in the `dbt_project.yml`, looking for the default profile under `~/.aws`.\n\nWith all configurations set for different environments, let's dive into unit testing.\n\n## Unit Testing the Model\n\nDuckDB operates in-process, allowing us to iterate quickly on our model since computation occurs locally within the same dbt process. dbt is improving unit tests in its April `1.8` release, but currently, it's challenging to run tests without cloud dependencies. While you could install Postgres locally, it's an additional step.\n\nFor unit testing, we use the `dbt-unit-testing` dbt package, added to a `packages.yml` file at the root of your dbt directory:\n\n```yaml\nCopy code\n\npackages:\n  - git: \"https://github.com/EqualExperts/dbt-unit-testing\"\n    revision: v0.4.12\n```\n\nFirst, install the package by running `dbt deps`. This step allows us to use SQL for defining our mock data, both the input and the expected outcome, and then run the model using `dbt-duckdb` right on our local machine.\n\nNext, dive into the tests folder and craft a new SQL file named `test_pypi_daily_stats.sql`:\n\n```sql\nCopy code\n\n{{ config(tags=['unit-test']) }}\n\n{% call dbt_unit_testing.test ('pypi_daily_stats','check_duckdb_downloads_on_20230402') %}\n\n  {% call dbt_unit_testing.mock_source('external_source', 'pypi_file_downloads') %}\n    SELECT\n      '2023-04-02 14:49:15+02'::timestamp AS timestamp,\n      'US' AS country_code,\n      '/packages/38/5b/...' AS url,\n      'duckdb' AS project,\n      NULL AS file, -- Assuming the 'file' struct is not essential for this test\n      STRUCT_PACK(\n          installer := NULL,\n          python := '3.8.2',\n          implementation := NULL,\n          distro := NULL,\n          system := STRUCT_PACK(name := 'Linux', release := '4.15.0-66-generic'),\n          cpu := 'x86_64',\n          openssl_version := NULL,\n          setuptools_version := NULL,\n          rustc_version := NULL\n      ) AS details,\n      'TLSv1.2' AS tls_protocol,\n      'ECDHE-RSA-AES128-GCM-SHA256' AS tls_cipher\n    UNION ALL\n    SELECT\n      '2023-04-02 14:49:15+02'::timestamp AS timestamp,\n      'US' AS country_code,\n      '/packages/38/5b/...' AS url,\n      'duckdb' AS project,\n      NULL AS file, -- Assuming the 'file' struct is not essential for this test\n      STRUCT_PACK(\n          installer := NULL,\n          python := '3.9.1',\n          implementation := NULL,\n          distro := NULL,\n          system := STRUCT_PACK(name := 'Linux', release := '4.15.0-66-generic'),\n          cpu := 'x86_64',\n          openssl_version := NULL,\n          setuptools_version := NULL,\n          rustc_version := NULL\n      ) AS details,\n      'TLSv1.2' AS tls_protocol,\n      'ECDHE-RSA-AES128-GCM-SHA256' AS tls_cipher\n    -- Add more rows as needed for your test\n  {% endcall %}\n\n{% call dbt_unit_testing.expect() %}\n    SELECT\n      '2023-04-02'::date AS download_date,\n      'duckdb' AS project,\n      '3.8' AS python_version,\n      'x86_64' AS cpu,\n      'Linux' AS system_name,\n      2 AS daily_download_sum -- Adjust this based on the expected outcome of your test\n  {% endcall %}\n\n{% endcall %}\n```\n\nThis test is structured in three key parts:\n\n1. Specifying which model we're testing with `{% call dbt_unit_testing.test('pypi_daily_stats', 'check_duckdb_downloads_on_20230402') %}`.\n2. Creating mock source data using `{% call dbt_unit_testing.mock_source('external_source', 'pypi_file_downloads') %}`, which uses SQL to simulate the data. This method allows for the easy definition of complex data structures, perfect for working with DuckDB.\n3. Defining the expected results with `{% call dbt_unit_testing.expect() %}` to verify our model's output.\n\nRun the test by executing:\n\n```bash\nCopy code\n\ndbt test\n```\n\nOr, use the Makefile shortcut `make pypi-transform-test` to initiate testing directly from the project's root folder.\n![](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2024_03_22_at_13_27_00_abea94f8c1.png&w=3840&q=75)\n\nThe testing process is swift, typically taking less than two seconds!\n\n## A New Developer Experience\n\nThis blog has highlighted the dbt-duckdb adapter's contributions, showcasing it as more than a new dbt destination. It introduces a revitalized developer experience, enabling local prototyping, cloud-independent unit testing, and smooth transitions to cloud deployments with MotherDuck.\nUp next in this series, we'll breathe life into our PyPi dataset by creating a dashboard.\n\nIn the meantimes, keep quacking and keep coding.\n\n### TABLE OF CONTENTS\n\n[Quick recap on part 1 : ingestion](https://motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2/#quick-recap-on-part-1-ingestion)\n\n[dbt & DuckDB Integration](https://motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2/#dbt-duckdb-integration)\n\n[Building the SQL pipeline](https://motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2/#building-the-sql-pipeline)\n\n[Unit Testing the Model](https://motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2/#unit-testing-the-model)\n\n[A New Developer Experience](https://motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2/#a-new-developer-experience)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![Differential Storage: A Key Building Block For A DuckDB-Based Data Warehouse](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdiff_storage_121958b7eb.png&w=3840&q=75)](https://motherduck.com/blog/differential-storage-building-block-for-data-warehouse/)\n\n[2024/03/11 - Joseph Hwang](https://motherduck.com/blog/differential-storage-building-block-for-data-warehouse/)\n\n### [Differential Storage: A Key Building Block For A DuckDB-Based Data Warehouse](https://motherduck.com/blog/differential-storage-building-block-for-data-warehouse)\n\nDifferential Storage: A Key Building Block For A DuckDB-Based Data Warehouse\n\n[![DuckDB Tutorial For Beginners](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgetting_started_duckdb_thumbnail_70b197b1ab.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n[2024/10/31 - Mehdi Ouazza, Ryan Boyd](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n### [DuckDB Tutorial For Beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners)\n\nGet up to speed quickly with DuckDB, including installation, VSCode workflow integration and your first SQL analytics project.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 513,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 513
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! Now I have enough information to write a comprehensive answer. Let me create the markdown file:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_5.md

  [96mğŸ”§ TOOL:[0m Bash â†’ cat > /tmp/answer_without_kg_5.md << 'EOF'
# How to Set Up MotherDuck to Work with dbt

MotherDuck integrates seamlessly with dbt (data build tool) through the `dbt-duckdb` adapter, allowing you to perform data transformations directly within MotherDuck's cloud data warehouse. Here's a comprehensive guide to getting started.

## Prerequisites

Before you begin, you'll need:
- A MotherDuck account (sign up at [motherduck.com](https://app.motherduck.com/?auth_flow=signup))
- Python 3.x installed
- Basic familiarity with dbt and SQL

## Installation

### 1. Install the dbt-duckdb Adapter

Install the dbt-duckdb package with MotherDuck support:

```bash
pip install dbt-duckdb[md]
# or with Poetry
poetry add dbt-duckdb[md]
```

The `[md]` extra installs the necessary dependencies for MotherDuck integration.

### 2. Get Your MotherDuck Service Token

1. Sign in to your MotherDuck account
2. Click on **Settings** in the upper right corner
3. Copy your **Service Token** to your clipboard
4. Export it as an environment variable:

```bash
export motherduck_token=<your_motherduck_token>
```

## Configuration

### 3. Initialize Your dbt Project

If you don't already have a dbt project:

```bash
dbt init your_project_name
```

### 4. Configure profiles.yml

Create or update your `profiles.yml` file with MotherDuck connection details. The key difference between local DuckDB and MotherDuck is the `path` parameter:

**Basic Configuration:**

```yaml
your_project_name:
  outputs:
    dev:
      type: duckdb
      path: /tmp/dbt.duckdb  # Local development
      threads: 16

    prod:
      type: duckdb
      path: md:your_database_name  # MotherDuck cloud
      threads: 16

  target: dev
```

**Advanced Configuration with AWS S3:**

If you're working with data in S3, you'll need additional configuration:

```yaml
your_project_name:
  target: dev
  outputs:
    dev:
      type: duckdb
      schema: dev_schema
      path: 'md:your_database_name'
      threads: 16
      extensions:
        - httpfs
      settings:
        s3_region: "{{ env_var('S3_REGION', 'us-west-1') }}"
        s3_access_key_id: "{{ env_var('S3_ACCESS_KEY_ID') }}"
        s3_secret_access_key: "{{ env_var('S3_SECRET_ACCESS_KEY') }}"

    prod:
      type: duckdb
      schema: prod_schema
      path: 'md:your_database_name'
      threads: 16
      extensions:
        - httpfs
      settings:
        s3_region: us-west-1
        s3_access_key_id: "{{ env_var('S3_ACCESS_KEY_ID') }}"
        s3_secret_access_key: "{{ env_var('S3_SECRET_ACCESS_KEY') }}"
```

### 5. Set Up Environment Variables

For AWS S3 integration, export your credentials:

```bash
export motherduck_token=<your_motherduck_token>
export S3_REGION=<your_region>
export S3_ACCESS_KEY_ID=<your_access_key_id>
export S3_SECRET_ACCESS_KEY=<your_secret_access_key>
```

## Verify Connection

### 6. Test Your Connection

Run `dbt debug` to verify that dbt can connect to MotherDuck:

```bash
dbt debug
```

You should see confirmation that the connection is successful.

### 7. Run Your First Model

Create a simple dbt model or run an existing one:

```bash
dbt run
# or for a specific target
dbt run --target prod
```

## Key Features and Benefits

### Local Development, Cloud Production

Since both dbt and DuckDB can run locally, you can:
- Develop and test using the same technologies you run in production
- Run models locally for fast iteration
- Seamlessly transition to MotherDuck for production workloads

### Dynamic Execution

MotherDuck intelligently runs queries either in the cloud or locally based on what's most efficient, providing "dynamic execution out of the box."

### Working with S3 Data

MotherDuck can directly query data from S3 without copying it:

```sql
-- Reference S3 data directly in your dbt models
SELECT * FROM 's3://your-bucket/path/to/data.csv'
```

Or use MotherDuck's public datasets:

```sql
SELECT * FROM 's3://us-prd-motherduck-open-datasets/jaffle_shop/csv/raw_customers.csv'
```

### Incremental Models

Configure incremental materializations in your `dbt_project.yml`:

```yaml
models:
  your_project:
    your_model:
      +materialized: "{{ 'incremental' if target.name == 'prod' else 'table' }}"
      +unique_key: load_id
```

## Best Practices

1. **Use Environment Variables**: Keep credentials secure by using environment variables rather than hardcoding them
2. **Separate Dev and Prod**: Use different targets for local development and production deployments
3. **Enable httpfs Extension**: This DuckDB extension is essential for working with S3 and remote files
4. **Fast Iteration**: Take advantage of local DuckDB for rapid development before deploying to MotherDuck
5. **Unit Testing**: Since DuckDB runs in-process, you can write genuine unit tests without cloud dependencies

## Troubleshooting

- **Authentication Issues**: Ensure your `motherduck_token` environment variable is set correctly
- **S3 Access**: Verify your AWS credentials are properly configured if working with S3 data
- **Version Compatibility**: Make sure your DuckDB version matches what MotherDuck supports (check MotherDuck documentation for the current supported version)

## Additional Resources

For more advanced patterns and examples:
- Check out the [jaffle_shop_duckdb](https://github.com/sungchun12/jaffle_shop_duckdb) demo repository
- See the [stocks demo](https://github.com/matsonj/stocks) for performance-optimized pipelines
- Review the [pypi-duck-flow](https://github.com/mehd-io/pypi-duck-flow) end-to-end project

## Sources

- [MotherDuck Ecosystem: dbt Core](https://motherduck.com/ecosystem/dbt)
- [MotherDuck + dbt: Better Together](https://motherduck.com/blog/motherduck-duckdb-dbt)
- [Performant dbt pipelines with MotherDuck](https://motherduck.com/blog/motherduck-dbt-pipelines)
- [DuckDB & dbt | End-To-End Data Engineering Project (2/3)](https://motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2)
EOF
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m 
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! I've successfully completed the task. Here's what I did:
  [94mâ”‚[0m 
  [94mâ”‚[0m ## Summary
  [94mâ”‚[0m 
  [94mâ”‚[0m I searched through the `.kurt/sources/` directory and found multiple relevant markdown files about setting up MotherDuck with dbt. I read the most comprehensive sources including:
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. **MotherDuck Ecosystem: dbt Core** - Overview of the integration
  [94mâ”‚[0m 2. **MotherDuck + dbt: Better Together** - Step-by-step setup guide with example `profiles.yml` configuration
  [94mâ”‚[0m 3. **Performant dbt pipelines with MotherDuck** - Advanced patterns for building efficient pipelines
  [94mâ”‚[0m 4. **DuckDB & dbt End-to-End Project** - Comprehensive guide covering local and cloud configurations
  [94mâ”‚[0m 
  [94mâ”‚[0m Based on these sources, I synthesized a complete answer covering:
  [94mâ”‚[0m 
  [94mâ”‚[0m - Prerequisites and installation steps
  [94mâ”‚[0m - Configuration of `profiles.yml` for both basic and advanced (S3) setups
  [94mâ”‚[0m - Environment variable setup
  [94mâ”‚[0m - Connection verification
  [94mâ”‚[0m - Key features and benefits
  [94mâ”‚[0m - Best practices
  [94mâ”‚[0m - Troubleshooting tips
  [94mâ”‚[0m - Links to additional resources
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer has been written to `/tmp/answer_without_kg_5.md` with a sources section documenting the four main documents used to create the guide.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 5,229 | Cost: $0.2829
     Cumulative: 5,229 tokens | $0.2829
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent has clearly outlined the completion of the task and provided a summary of the findings, which indicates that it is not asking for user input.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 12 | Duration: 107.3s
â•‘    Tokens: 5,229 | Cost: $0.2829
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  LLM Judge score: 1.00

======================================================================
â“ Question 6/10
How do I migrate data from a local DuckDB database to MotherDuck?
======================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 6
â”‚ Search through the .kurt/sources/ directory to answer this question:

How do I migrate data from a local DuckDB database to MotherDuck?

Steps:
1. Search for the most relevant markdown files in .kurt/sources/
2. Read those files and synthesize the answer
3. Write a markdown answer (with a ## Sources section) to: /tmp/answer_without_kg_6.md
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I'll help you find the answer to how to migrate data from a local DuckDB database to MotherDuck by searching through the sources.
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ migrate|migration|local.*MotherDuck|DuckDB.*MotherDuck|transfer.*data

  [96mğŸ”§ TOOL:[0m Grep â†’ ATTACH|attach.*database|local database

  [96mğŸ”§ TOOL:[0m Grep â†’ copy.*from|insert.*from|export.*import
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/scaling-duckdb-panel-ondemand.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/take-flight-with-the-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-power-of-wasm-for-analytics-duckdb-in-the-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-surprising-birth-of-duckdb-ft-co-creator-hannes-muhleisen.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ai-powered-bi-can-llms-really-generate-your-dashboards-ft-michael-driscoll.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/privacy-policy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dominik-moritz.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-future-of-bi-bi-as-code-duckdb-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vibe-coding-sql-cursor.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/spark-ducklake-getting-started.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-keyboard-shortcuts-for-joyful-querying.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-use-cases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/INSERT statement.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/JSON.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/CSV.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-future-of-bi-bi-as-code-duckdb-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vibe-coding-sql-cursor.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-faker-duckdb-exploration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg_duckdb-postgresql-extension-for-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ getting.started|quick.start|onboarding|first.steps

  [96mğŸ”§ TOOL:[0m Grep â†’ md:.*local|my_db|sample.*database
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/getting-started-with-motherduck-webinar.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/cultivating-growth-how-gardyn-scaled-its-data-operations-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/build-bigger-with-small-ai-running-small-models-locally.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/sql-duckdb-book-form.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/onboarding-sessions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/ducklake-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/hack-night.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-keyboard-shortcuts-for-joyful-querying.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streaming-oracle-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/postgres-duckdb-options.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/preswald-health-data-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/google-sheets-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/fabi-ai-llm-prompt-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-excel-extension.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/bigquery-to-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-sqlite-databases-duckdb.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me read some of the most relevant files:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/getting-started-with-motherduck-webinar.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     "content": "---\ntitle: duckdb-motherduck-for-beginners-your-ultimate-guide\ncontent_type: event\nsource_url: https://motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide\nindexed_at: '2025-11-25T20:44:57.276294'\ncontent_hash: 9ac8939ea24395a7\nhas_code_examples: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nDuckDB & MotherDuck for Beginners: Your Ultimate Guide - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[DuckDB & MotherDuck for Beginners: Your Ultimate Guide](https://www.youtube.com/watch?v=WYV8hvJOAQE)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Why am I seeing this?](https://support.google.com/youtube/answer/9004474?hl=en)\n\n[Watch on](https://www.youtube.com/watch?v=WYV8hvJOAQE&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 36:27\n\n\u2022Live\n\n\u2022\n\nYouTube\n\n# DuckDB & MotherDuck for Beginners: Your Ultimate Guide\n\n2025/02/21\n\n## Why DuckDB is Revolutionizing Data Analytics\n\nDuckDB has experienced explosive growth in popularity, with download statistics showing remarkable adoption rates, particularly in the Python ecosystem. This open-source analytical database is designed to handle everything from quick data exploration to complex data pipelines, and it's already trusted by multiple companies in production environments.\n\n## Understanding the Small and Medium Data Revolution\n\nThe tech industry has long operated under the assumption that analytics requires big data infrastructure. However, this paradigm is being challenged by three key insights:\n\n1. **Most workloads aren't big data**: According to data from AWS Redshift and other cloud analytical databases, approximately 83% of users work with query sizes up to 1TB, and 94% work with data below 10TB.\n\n2. **Modern hardware is incredibly powerful**: Today's single machines can handle up to 24TB of memory on AWS, making distributed systems unnecessary for most use cases.\n\n3. **Distributed systems are expensive**: They require significant IO network traffic for coordination, and the development experience is considerably more complex.\n\n\nDuckDB capitalizes on these realities by enabling users to work with large datasets on a single machine, whether locally on a laptop or in the cloud via a scale-up strategy.\n\n## How DuckDB Works: The In-Process Advantage\n\nDuckDB is an open-source, in-process analytical database written in C++, designed as a single self-contained binary with all dependencies included. This architecture sets it apart from traditional databases.\n\n### Traditional Database Architecture\n\nDatabases typically fall into two categories:\n\n- **OLTP (Online Transaction Processing)**: Databases like PostgreSQL and MySQL, optimized for handling transactions with small datasets. Query times typically range from 1-10 milliseconds.\n\n- **OLAP (Online Analytical Processing)**: Databases like BigQuery and Snowflake, built for analytical queries processing large datasets. Query times can range from 100 milliseconds to several minutes.\n\n\nMost traditional databases use a client-server architecture where the database runs as a separate process, and applications connect to it through SQL queries.\n\n### The In-Process Revolution\n\nIn-process databases run directly within the application process itself. While SQLite pioneered this approach for OLTP workloads, DuckDB introduces something new: an in-process OLAP database optimized for analytical workloads.\n\nThis design enables DuckDB to:\n\n- Run on virtually any platform (laptops to cloud workflows)\n- Integrate seamlessly with any programming language\n- Execute in web browsers via WebAssembly\n- Eliminate network overhead for local operations\n\n## Getting Started with DuckDB\n\n### Installation and Basic Usage\n\nDuckDB can be installed through various methods:\n\n- Direct binary download for CLI usage\n- Package managers (Homebrew for macOS)\n- Language-specific packages (Python, R, Java, etc.)\n\nThe CLI provides a powerful interface for data exploration:\n\n```sql\nCopy code\n\n-- Simple query reading from S3\nFROM 's3://bucket/path/to/file.parquet' LIMIT 5;\n```\n\n### Key Features in Action\n\n**Friendly SQL Dialect**: DuckDB extends standard SQL with productivity enhancements, such as the FROM-first syntax shown above.\n\n**Automatic File Format Detection**: DuckDB automatically detects and handles various file formats including Parquet, CSV, JSON, Iceberg, and Delta Lake.\n\n**Extension System**: DuckDB's functionality is modular through extensions. Core extensions (like HTTPFS for S3 access) are auto-loaded when needed, while community extensions can be installed manually.\n\n## Data Persistence and the DuckDB File Format\n\nBy default, DuckDB operates in-memory, but it offers powerful persistence options:\n\n### Creating and Managing Databases\n\n```sql\nCopy code\n\n-- Attach or create a database\nATTACH 'mydatabase.ddb';\n\n-- Create a table from a query\nCREATE TABLE mytable AS SELECT * FROM source_data;\n```\n\n### The DuckDB File Format\n\nDuckDB's native file format (.ddb or .db) is:\n\n- Self-contained (all tables and metadata in one file)\n- ACID-compliant\n- Highly compressed\n- Optimized for analytical workloads\n\n### Exporting Data\n\nDuckDB supports seamless data export:\n\n```sql\nCopy code\n\n-- Export to CSV\nCOPY (SELECT * FROM mytable) TO 'output.csv';\n```\n\n## Managing Secrets and Authentication\n\nDuckDB includes a comprehensive secret management system for secure cloud access:\n\n```sql\nCopy code\n\n-- Create temporary secret using AWS credential chain\nCREATE SECRET (\n    TYPE S3,\n    PROVIDER credential_chain\n);\n```\n\nThis approach supports:\n\n- AWS SSO authentication\n- Temporary and persistent secrets\n- Multiple cloud providers\n- Secure credential storage\n\n## Scaling to the Cloud with MotherDuck\n\nMotherDuck supercharges DuckDB by transforming it from a single-player to a multiplayer analytics experience. The integration is remarkably simple:\n\n### Connecting to MotherDuck\n\n```sql\nCopy code\n\n-- Connect to MotherDuck with one command\nATTACH 'md:';\n```\n\nAuthentication requires only a MotherDuck token, which can be set as an environment variable.\n\n### Key MotherDuck Features\n\n**Dual Execution**: MotherDuck enables intelligent query execution, automatically determining whether to run computations locally or in the cloud based on data location and query requirements.\n\n**Database Sharing**: Create and share cloud databases with simple commands:\n\n```sql\nCopy code\n\n-- Create a share\nCREATE SHARE myshare FROM mydatabase;\n```\n\n**Performance Benefits**: Leveraging cloud infrastructure provides:\n\n- High-bandwidth connections to cloud storage\n- Elimination of local network bottlenecks\n- Seamless collaboration features\n\n### Cloud Storage Integration\n\nMotherDuck dramatically improves performance when querying cloud storage. In benchmarks, queries that take 11 seconds locally can complete in just 2 seconds when leveraging MotherDuck's cloud infrastructure and network proximity to storage.\n\n## The Complete Analytics Toolkit\n\nDuckDB and MotherDuck together provide:\n\n- **Simplicity**: One-command installation and cloud connection\n- **Flexibility**: Run anywhere from browsers to cloud environments\n- **Performance**: Lightning-fast analytical queries on small to medium datasets\n- **Integration**: Native support for numerous file formats and cloud providers\n- **Collaboration**: Easy data sharing and team workflows\n\nWhether you're analyzing data on your laptop, scaling computations in the cloud, or building production data pipelines, DuckDB offers a modern approach to analytics that challenges traditional big data assumptions while delivering exceptional performance and developer experience.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Data-based: Going Beyond the Dataframe\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FData_based_f32745b461.png&w=3840&q=75)](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n[2025-11-20](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n### [Data-based: Going Beyond the Dataframe](https://motherduck.com/videos/going-beyond-the-dataframe)\n\nLearn how to turbocharge your Python data work using DuckDB and MotherDuck with Pandas. We walk through performance comparisons, exploratory data analysis on bigger datasets, and an end-to-end ML feature engineering pipeline.\n\nWebinar\n\nPython\n\nAI, ML and LLMs\n\n[![\"Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FHex_Webinar_778e3959e4.png&w=3840&q=75)](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n[2025-11-14](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n### [Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck)\n\nAI isn't here to replace data work, it's here to make it better. Watch this webinar to see how Hex and MotherDuck build AI workflows that prioritize context, iteration, and real-world impact.\n\nWebinar\n\nAI, ML and LLMs\n\n[![\"Lies, Damn Lies, and Benchmarks\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FLies_Damn_Lies_and_Benchmarks_Thumbnail_404db1bf46.png&w=3840&q=75)](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n[2025-10-31](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n### [Lies, Damn Lies, and Benchmarks](https://motherduck.com/videos/lies-damn-lies-and-benchmarks)\n\nWhy do database benchmarks so often mislead? MotherDuck CEO Jordan Tigani discusses the pitfalls of performance benchmarking, lessons from BigQuery, and why your own workload is the only benchmark that truly matters.\n\nStream\n\nInterview\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 301,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 301
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/getting-started-with-motherduck-webinar.md",
  [2m  â”‚[0m     "content": "---\ntitle: getting-started-with-motherduck-webinar\ncontent_type: event\nsource_url: https://motherduck.com/videos/getting-started-with-motherduck-webinar\nindexed_at: '2025-11-25T20:44:42.911566'\ncontent_hash: 7e2a069d42290080\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\n\\[Webinar\\] Getting Started with MotherDuck - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[\\[Webinar\\] Getting Started with MotherDuck](https://www.youtube.com/watch?v=bcU3voqZnCE)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=bcU3voqZnCE&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 41:41\n\n\u2022Live\n\n\u2022\n\nWebinarMotherDuck Features\n\n# Getting Started with MotherDuck Webinar\n\n2024/06/19\n\nMotherDuck co-founder Ryan Boyd and Produck Expert Nathaniel Thompson provide an overview on MotherDuck, featuring DuckDB analyst ergonomics, Column Explorer, Fixit, Tableau visualization, Python data analysis, Wasm and more.\n\n## Related Videos\n\n[![\"Data-based: Going Beyond the Dataframe\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FData_based_f32745b461.png&w=3840&q=75)](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n[2025-11-20](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n### [Data-based: Going Beyond the Dataframe](https://motherduck.com/videos/going-beyond-the-dataframe)\n\nLearn how to turbocharge your Python data work using DuckDB and MotherDuck with Pandas. We walk through performance comparisons, exploratory data analysis on bigger datasets, and an end-to-end ML feature engineering pipeline.\n\nWebinar\n\nPython\n\nAI, ML and LLMs\n\n[![\"Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FHex_Webinar_778e3959e4.png&w=3840&q=75)](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n[2025-11-14](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n### [Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck)\n\nAI isn't here to replace data work, it's here to make it better. Watch this webinar to see how Hex and MotherDuck build AI workflows that prioritize context, iteration, and real-world impact.\n\nWebinar\n\nAI, ML and LLMs\n\n[![\"Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Ficeberg_video_thumbnail_adfdabe3d6.jpg&w=3840&q=75)\\\\\n\\\\\n46:26](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n[2025-06-12](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n### [Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse)\n\nBuilding a data stack means choosing between easy SaaS and complex open-source. Apache Iceberg is a middle ground, but its catalog is a hurdle. New tools now simplify using Iceberg with DuckDB to create a flexible, local data lakehouse.\n\nMotherDuck Features\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 121,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 121
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     "content": "---\ntitle: bringing-duckdb-to-the-cloud-dual-execution-explained\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained\nindexed_at: '2025-11-25T20:45:20.295187'\ncontent_hash: ab4030806c409492\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nBringing DuckDB to the Cloud: Dual Execution Explained - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Bringing DuckDB to the Cloud: Dual Execution Explained](https://www.youtube.com/watch?v=2eVNm4A8sLw)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=2eVNm4A8sLw&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 1:08:11\n\n\u2022Live\n\n\u2022\n\nYouTubeQuack & CodeMotherDuck Features\n\n# Bringing DuckDB to the Cloud: Dual Execution Explained\n\n2024/06/28\n\nBringing DuckDB's analytical power to the cloud requires more than just running it on a server. While DuckDB excels at processing data on a local machine, the dynamics of data analysis change when dealing with cloud-scale data, collaboration, and shared resources. This shift introduces challenges around [security](https://motherduck.com/learn-more/secure-startup-data-warehouse/), concurrent access, and performance, which is where a purpose-built cloud architecture becomes essential.\n\nMotherDuck, a modern cloud data warehouse, is built on DuckDB to solve these challenges. It extends DuckDB\u2019s capabilities without forking the open source project, creating a hybrid system that intelligently balances local and cloud computing. In a conversation with MotherDuck founding engineer Stephanie, she explained the architecture and the innovative query model, known as **dual execution**, that makes this possible.\n\n* * *\n\n## The MotherDuck Architecture: Beyond Hosted DuckDB\n\nA common misconception is that MotherDuck is simply DuckDB hosted in the cloud. The reality is a more sophisticated system designed to overcome the limitations of using a locally-optimized engine in a distributed environment. MotherDuck\u2019s architecture is built on three key components and a core philosophy of not forking DuckDB.\n\nInstead of maintaining a separate version, MotherDuck leverages DuckDB\u2019s powerful **extension system**. This allows MotherDuck to add new capabilities at multiple layers, including the SQL parser, optimizer, and storage interface, while staying current with the latest open source DuckDB releases. This tight integration means users benefit from the rapid innovation of the DuckDB community almost immediately.\n\nThe architecture can be broken down into three main layers:\n\n1. **The Client Extension**: This is how MotherDuck integrates with the DuckDB ecosystem. Whether a user is working in the CLI, a Python script, or a JDBC connection, a simple `ATTACH` command connects their local DuckDB instance to the MotherDuck cloud. This extension is even used to run DuckDB in the browser via WASM for the MotherDuck UI, enabling client-side processing to reduce latency and cloud compute costs.\n2. **The Compute Layer**: In the cloud, queries are processed by containerized DuckDB instances, fondly called \"ducklings.\" These compute resources are scaled based on user needs, providing the necessary CPU and memory to handle complex analytical queries on large datasets.\n3. **The Storage Layer**: DuckDB's native file format is optimized for a single writer on a local file system. This model is not well-suited for the cloud, where multiple users need to read and write to the same database concurrently. To solve this, MotherDuck implemented a differential storage system that maps a logical database file to append-only snapshot layers in cloud object storage. This design is cloud-friendly, enabling efficient in-place updates and forming the foundation for features like database sharing and time travel.\n\n* * *\n\n## Enabling Collaboration with Database Sharing and Secure Credentials\n\nThis unique architecture transforms DuckDB from a \"single-player\" tool into a collaborative \"multiplayer\" platform. One of the most significant advantages is **database sharing**. Instead of emailing SQL scripts or passing around large database files, team members within the same organization can grant query access to their databases. This streamlines collaboration, ensuring everyone works from a consistent and up-to-date version of the data.\n\nSecurity is another critical aspect of any cloud data platform. MotherDuck provides a **centralized Secret Manager**, co-designed with the DuckDB team. Users can create persistent, encrypted secrets for accessing external data sources like AWS S3, Google Cloud Storage, or Delta Lake. Once a secret is created in MotherDuck, it can be reused across different clients and sessions without needing to expose or reconfigure credentials on each local machine.\n\n* * *\n\n## Understanding Dual Execution\n\nThe most innovative feature of MotherDuck's architecture is its **dual execution model**. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\n\nThe process was demonstrated with a query joining two tables. When both tables reside in a MotherDuck database, the query is executed entirely in the cloud. An `EXPLAIN` plan for this query reveals that all operators, from the table scans to the final join, are marked as **remote**. The only local operation is downloading the final result set to the client.\n\nThe real power of dual execution becomes apparent when a query involves both local and cloud data. In the demonstration, one of the remote tables was copied to a local Parquet file. The query was then modified to join this local file with the table that remained in the cloud.\n\nThe `EXPLAIN` plan for this new query showed a mix of local and remote operations. The scan of the Parquet file and its associated processing happened **locally**, leveraging the user's machine. The scan of the cloud table happened **remotely** on MotherDuck's compute. The system then efficiently transferred only the necessary intermediate data to complete the join. This hybrid approach avoids needlessly uploading the local file to the cloud or downloading the entire remote table, resulting in faster and more efficient queries.\n\n* * *\n\n## Taking Control with Manual Execution Overrides\n\nWhile MotherDuck's optimizer is designed to make the most efficient choice automatically, some situations require manual control. For this, users can use the `md_run` parameter within scan functions like `read_parquet()` or `read_csv()`.\n\nBy setting `md_run = 'local'`, a user can force the scan of a remote S3 file to be executed on their local client. This involves downloading the data, which might be desirable if the data needs to be processed near the user's location or if they prefer using local credentials. Conversely, setting `md_run = 'remote'` forces the operation to run in the MotherDuck cloud.\n\nThis control is especially useful for performance tuning. A direct comparison showed that querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds). By pushing the computation to the data's location, MotherDuck minimizes network I/O and delivers results more quickly.\n\n* * *\n\n## A Symbiotic Future for Local and Cloud Analytics\n\nMotherDuck's architecture is a thoughtful extension of DuckDB's core principles, adapting its local-first power for the demands of the cloud. The dual execution model provides a flexible and efficient bridge between a user's laptop and cloud data, optimizing workloads in a way that pure-cloud or pure-local systems cannot.\n\nThis relationship is symbiotic. As the largest production user of DuckDB, MotherDuck continuously pushes the engine to its limits, uncovering opportunities for improvement and contributing enhancements back to the open source project. This collaboration ensures that both DuckDB and MotherDuck will continue to evolve, offering a powerful and seamless analytical experience that spans from local exploration to cloud-scale production.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Ficeberg_video_thumbnail_adfdabe3d6.jpg&w=3840&q=75)\\\\\n\\\\\n46:26](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n[2025-06-12](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n### [Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse)\n\nBuilding a data stack means choosing between easy SaaS and complex open-source. Apache Iceberg is a middle ground, but its catalog is a hurdle. New tools now simplify using Iceberg with DuckDB to create a flexible, local data lakehouse.\n\nMotherDuck Features\n\n[![\"Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FG_Se_B_Sox_AW_Fg_HD_f4fddaa9ab.jpg&w=3840&q=75)](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n[2025-04-23](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n### [Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly)\n\nHamilton Ulmer shares insights from MotherDuck's Instant SQL Mode, exploring how real-time query result previews eliminate the traditional write-run-debug cycle through client-side parsing and DuckDB-WASM caching.\n\nSQL\n\nTalk\n\nMotherDuck Features\n\n[![\"DuckDB: Run dbt build with sub-second execution times\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_03_26_at_3_40_16_PM_c6e3a8096f.png&w=3840&q=75)\\\\\n\\\\\n24:06](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n[2025-03-13](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n### [DuckDB: Run dbt build with sub-second execution times](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times)\n\nWhether you're new to DuckDB or looking to optimize your workflows, this session will provide practical insights to help you leverage its full potential.\n\nYouTube\n\nData Pipelines\n\ndbt\n\nMotherDuck Features\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 184,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 184
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search for more specific information about data migration:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ CREATE TABLE.*AS SELECT|INSERT INTO.*SELECT|COPY.*TO

  [96mğŸ”§ TOOL:[0m Glob â†’ **/*quickstart*.md

  [96mğŸ”§ TOOL:[0m Glob â†’ **/*tutorial*.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "durationMs": 9,
  [2m  â”‚[0m   "numFiles": 2,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/blog/semantic-layer-duckdb-tutorial.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "durationMs": 10,
  [2m  â”‚[0m   "numFiles": 3,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data pipeline.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/dataset.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/ETL.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vibe-coding-sql-cursor.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/spark-ducklake-getting-started.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-faker-duckdb-exploration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/postgres-duckdb-options.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/local-duckdb-ui-visual-data-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/just-enough-sql-for-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/geospatial-for-beginner-duckdb-spatial-motherduck.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md

  [96mğŸ”§ TOOL:[0m Grep â†’ share.*database|CREATE SHARE
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     "content": "---\ntitle: duckdb-python-quickstart-part1\ncontent_type: tutorial\nsource_url: https://motherduck.com/learn-more/duckdb-python-quickstart-part1\nindexed_at: '2025-11-25T09:57:03.972667'\ncontent_hash: a093ddae571df028\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO LEARN](https://motherduck.com/learn-more/)\n\n# DuckDB Python Quickstart (Part 1): Your Complete Guide to Fast Data Analytics\n\n14 min readBY\n\n[Aditya Somani](https://motherduck.com/authors/aditya-aomani/)\n\n![DuckDB Python Quickstart (Part 1): Your Complete Guide to Fast Data Analytics](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Flearn_more_duck_db_df1112cb47_151aeeba16.png&w=3840&q=75)\n\nAlright, let's talk data. How many times have you been handed a CSV file, a Parquet dump, or even just a hefty Pandas DataFrame and thought, \"Okay, how do I query this _right now_ without spinning up a whole database server?\" Maybe you just need to do some quick exploration, filter a few million rows, or calculate some aggregates, but setting up a proper database feels like overkill, and plain old Pandas is starting to chug on the larger datasets.\n\nYou've probably been there. That friction between having data in files or memory and needing database-level query power is a common one. This is where something like DuckDB comes in, and specifically, its fantastic Python integration.\n\nThink of DuckDB as the analytical database that lives _inside_ your Python process. No separate server to manage, no complex setup \u2013 you just `pip install duckdb`, and you're ready to perform fast, analytical SQL queries directly on your data, wherever it lives. It's like having a pocket-sized data warehouse ready to _quack_ into action whenever you need it.\n\nIn this first part of your DuckDB Python quickstart, you're going to dive into the foundational API features. You'll cover everything you need to get up and running quickly:\n\n- **Installation and Connection:** Getting DuckDB into your Python environment and making your first connection.\n- **Basic Querying:** Executing simple SQL statements using both the relational API and DB-API styles.\n- **Data Ingestion:** Reading data directly from common file formats like CSV and Parquet.\n- **The Relational API:** A Pythonic way to build and compose queries, including set operations and joins.\n\nBy the end of this guide, you'll have a solid grasp of how to leverage DuckDB for basic querying and data handling directly within your Python environment. And, by the way, you could also follow this guide using [this notebook](https://github.com/adisomani/duckdb-notebooks/blob/main/duckdb_python_quickstart.ipynb). Let's get your ducks in a row!\n\n## Getting Started: Installation and Connection\n\nFirst things first, you need the DuckDB Python package. If you're using pip, a simple:\n\n```bash\nCopy code\n\npip install duckdb\n```\n\nwill do the trick. If you're more of a Conda person, you can use:\n\n```bash\nCopy code\n\nconda install python-duckdb -c conda-forge\n```\n\nOnce installed, you can import the library and check the version to make sure everything is gravy:\n\n```python\nCopy code\n\nimport duckdb\nprint(duckdb.__version__)\n# Should output something like: '1.3.0'\n```\n\nNow, how do you connect? This is delightfully simple with DuckDB. Since it's an embedded database, it runs directly within your Python process. You don't _have_ to create an explicit connection object right away if you just want to mess around in memory, but it's good practice and necessary if you want a persistent database or specific connection settings.\n\nTo start an **in-memory** database (data disappears when your session ends):\n\n```python\nCopy code\n\nimport duckdb\n\ncon = duckdb.connect(database=':memory:')\n```\n\nTo connect to or create a **persistent database file**:\n\n```python\nCopy code\n\nimport duckdb\n\ncon = duckdb.connect(database='my_local_data.duckdb') # Creates if it doesn't exist\n```\n\nHere's something useful you'll discover: if you don't specify a database name in `duckdb.connect()`, it defaults to in-memory. Even better, the main `duckdb` module provides a default in-memory connection that you can use directly via `duckdb.sql()` without managing a connection object explicitly. This default connection is global to the `duckdb` Python module, which makes it perfect for quick exploratory work.\n\nINFO: Serverless Option: MotherDuck\nWhile DuckDB is embedded and runs locally, you can easily extend its capabilities for serverless, collaborative analytics by \\[connecting to MotherDuck\\](https://motherduck.com/). MotherDuck provides a managed service that works seamlessly with your DuckDB Python client, allowing you to store data persistently in the cloud and share access. Learn more about how to [Connect Python to MotherDuck](https://motherduck.com/docs/category/python/).\n\n## Your First Queries: `sql()` and `execute()`\n\nDuckDB's Python API offers two main approaches for executing SQL. The `sql()` method is typically more convenient for interactive work and integrating with Python objects.\n\nLet's start with a simple query using the default connection:\n\n```python\nCopy code\n\nimport duckdb\n\n# Use the default in-memory connection\nresult = duckdb.sql(\"SELECT 42 AS answer\")\n```\n\nWhat do you get back? Let's check the type:\n\n```python\nCopy code\n\nprint(type(result))\n# Output: <class 'duckdb.duckdb.DuckDBPyRelation'>\n```\n\nAh, a `DuckDBPyRelation`. This is a key concept in DuckDB's relational API. It represents a query result _or_ a data source (like a table or a file), but it doesn't necessarily mean the data has been fully processed or brought into Python memory yet. It's a symbolic representation of a query.\n\nTo see the actual data, you need to _materialize_ the relation. You can do this in several ways:\n\n- `.show()`: Prints a nice tabular representation (great for interactive exploration).\n- `.fetchall()`: Returns all results as a list of tuples (the traditional DB-API way).\n- `.fetchone()`: Returns the next single row as a tuple.\n- `.df()` or `.fetchdf()`: Converts the result into a Pandas DataFrame.\n- `.arrow()` or `.fetcharrow()`: Converts the result into an Apache Arrow Table.\n- `.pl()`: Converts the result into a Polars DataFrame.\n\nLet's try `show()`:\n\n```python\nCopy code\n\nduckdb.sql(\"SELECT 42 AS answer\").show()\n```\n\n```bash\nCopy code\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 answer \u2502\n\u2502 int32  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     42 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nAnd using `fetchall()`:\n\n```python\nCopy code\n\nresult_list = duckdb.sql(\"SELECT 42 AS answer\").fetchall()\nprint(result_list)\n# Output: [(42,)]\n```\n\nThe other primary way to execute SQL is using the `execute()` method, typically on a connection object (`con.execute(...)`). This aligns more closely with the Python DB-API 2.0 standard. It immediately executes the query and returns a cursor-like object that you then use with methods like `fetchone()` or `fetchall()` to retrieve data.\n\n```python\nCopy code\n\ncon = duckdb.connect(database=':memory:')\nresult_cursor = con.execute(\"SELECT 'hello' || ' ' || 'world' AS greeting\")\nprint(result_cursor.fetchone())\n# Output: ('hello world',)\n```\n\nFor most quick analysis and integration with Python data structures, `sql()` is often preferred due to its flexibility and the `DuckDBPyRelation` object it returns, which you'll explore further. The `execute()` method is particularly useful when you need **parameterized queries** (passing values separately from the SQL string).\n\nHere's how you pass parameters using the DB-API style `execute()`, which helps prevent SQL injection and can improve performance for repeated queries:\n\n```python\nCopy code\n\ncon = duckdb.connect(database=':memory:')\ncon.execute(\"CREATE TABLE items (name VARCHAR, value INTEGER)\")\ncon.execute(\"INSERT INTO items VALUES ('apple', 1), ('banana', 2), ('cherry', 3)\")\n\n# Parameterized query using '$param_name' syntax and a dictionary\nitem_name = 'banana'\nresult = con.execute(\"SELECT value FROM items WHERE name = $item_name\", {'item_name': item_name})\nprint(f\"Value for {item_name}: {result.fetchone()}\")\n\n# Parameterized query using '?' syntax and a tuple/list\nitem_value = 1\nresult = con.execute(\"SELECT name FROM items WHERE value = ?\", [item_value])\nprint(f\"Name for value {item_value}: {result.fetchone()}\")\n\ncon.close()\n```\n\n```bash\nCopy code\n\nValue for banana: (2,)\nName for value 1: ('apple',)\n```\n\nThe `$param_name` syntax with a dictionary is often more readable than the `?` syntax with a list/tuple. Both are valid ways to pass parameters.\n\n## Ingesting Data: From Files to Relations\n\nOne of the most common tasks is getting data _into_ your database or query environment. DuckDB shines here by allowing you to query files directly. No need for a `CREATE TABLE` followed by a bulk `INSERT`. You can treat files like tables from the get-go.\n\nLet's say you have a CSV file. DuckDB provides functions like `read_csv`, `read_parquet`, `read_json`, etc., that you can call directly from Python or within SQL.\n\nUsing the Python API, you can read a file and get a `DuckDBPyRelation` back:\n\n```python\nCopy code\n\nimport duckdb\n\n# You'll use a publicly available CSV file\n# Connect to a persistent DB file or use ':memory:'\ncon = duckdb.connect(database='my_local_data.duckdb')\ncon.sql(\"INSTALL httpfs\") # Need httpfs extension to read from URL\ncon.sql(\"LOAD httpfs\")\n\npopulation_relation = con.read_csv(\"https://bit.ly/3KoiZR0\")\n\nprint(type(population_relation))\n# Output: <class 'duckdb.duckdb.DuckDBPyRelation'>\n```\n\nJust like with your simple `SELECT 42` example, this `population_relation` object isn't the data itself, but a representation of the data in the CSV file, ready to be queried. You can then query this relation using either SQL or the Relational API methods.\n\nUsing SQL via `sql()`:\n\n```python\nCopy code\n\ncon.sql(\"SELECT Country, Population FROM population_relation LIMIT 5\").show()\n```\n\n```bash\nCopy code\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Country     \u2502 Population \u2502\n\u2502     varchar     \u2502   int64    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Afghanistan     \u2502   31056997 \u2502\n\u2502 Albania         \u2502    3581655 \u2502\n\u2502 Algeria         \u2502   32930091 \u2502\n\u2502 American Samoa  \u2502      57794 \u2502\n\u2502 Andorra         \u2502      71201 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nThis is incredibly convenient! You're querying the CSV file directly using SQL syntax, without defining a schema or loading it fully into a table first.\n\nIf you find yourself querying the same file repeatedly or need better performance than reading the file anew each time, you can easily persist the data into a DuckDB table using the `.to_table()` method on your relation:\n\n```python\nCopy code\n\n# You should already have the 'con' connection open and httpfs loaded\n\npopulation_relation = con.read_csv(\"https://bit.ly/3KoiZR0\") # Read into a relation\npopulation_relation.to_table(\"population\") # Persist the relation as a table named 'population'\n\n# Now you can query the 'population' table directly\ncon.sql(\"SELECT COUNT(*) FROM population\").show()\n```\n\n```bash\nCopy code\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count_star() \u2502\n\u2502    int64     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502          227 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nINFO: Querying Files Directly\nDuckDB's ability to query file formats like CSV, Parquet, and JSON directly without a formal \\`CREATE TABLE\\` step is a massive productivity boost for data exploration and quick analysis. It leverages optimized readers under the hood. For larger files or repeated access, converting to a DuckDB table (.to\\_table()) is generally recommended for better performance and persistence.\n\nWhile `.to_table()` persists data locally, you might want to [persist data in the cloud with MotherDuck](https://motherduck.com/docs/key-tasks/loading-data-into-motherduck/) for collaboration, accessibility, and serverless scaling.\n\n## The Relational API: Building Queries Programmatically\n\nYou've seen that `duckdb.sql()` and `con.read_csv()` return `DuckDBPyRelation` objects. These objects are not just passive query results; they are _query builders_ themselves. This is DuckDB's Relational API, offering a Pythonic, fluent interface to construct queries.\n\nInstead of writing a single SQL string, you can chain methods on a `DuckDBPyRelation` object, with each method representing a relational operation like `filter`, `project` (select columns), `aggregate`, `order`, `limit`, `except_`, `intersect`, `union`, and `join`.\n\nHere are some key methods you'll use:\n\n- `.filter(condition)`: Adds a `WHERE` clause.\n- `.project(columns)`: Adds a `SELECT` clause (specifies columns).\n- `.limit(n)`: Adds a `LIMIT` clause.\n- `.order(columns)`: Adds an `ORDER BY` clause.\n- `.aggregate(expressions, group_by_columns=None)`: Adds an `AGGREGATE` and optional `GROUP BY`.\n- `.except_(other_relation)`: Returns rows in the current relation that are not in `other_relation`.\n- `.intersect(other_relation)`: Returns rows present in _both_ the current relation and `other_relation`.\n- `.union(other_relation)`: Combines all rows from both relations (like `UNION ALL` in SQL; for `UNION DISTINCT`, you'd typically chain `.distinct()`).\n- `.join(other_relation, on=None, how='inner')`: Performs a join with `other_relation`.\n\nLet's find countries with a population over 10 million, select just the country and population, limit to the first 5, and display the result, all using the relational API methods on the `population_relation` you created earlier:\n\n```python\nCopy code\n\n# You should already have the 'population_relation' relation from the CSV file\n(population_relation\n .filter(\"Population > 10000000\")\n .project(\"Country, Population\")\n .limit(5)\n .show()\n)\n```\n\n```bash\nCopy code\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Country    \u2502 Population \u2502\n\u2502   varchar    \u2502   int64    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Afghanistan  \u2502   31056997 \u2502\n\u2502 Algeria      \u2502   32930091 \u2502\n\u2502 Angola       \u2502   12127071 \u2502\n\u2502 Argentina    \u2502   39921833 \u2502\n\u2502 Australia    \u2502   20264082 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nNotice how you can chain the methods. Each method call returns a _new_`DuckDBPyRelation` representing the result of applying that operation. The actual query execution doesn't happen until a materializing method like `.show()`, `.fetchall()`, or `.df()` is called. This lazy execution is powerful!\n\nThis programmatic approach has a significant advantage over building SQL strings manually:\n\nINFO: Programmatic Query Building\nBuilding queries using the Relational API methods (\\`.filter()\\`, \\`.project()\\`, etc.) is generally safer and more maintainable than constructing SQL queries via string formatting or concatenation, especially when dealing with dynamic conditions or column selections based on user input or program logic. It helps avoid potential SQL injection vulnerabilities (as column/table names cannot be parameterized in standard SQL) and makes your code more readable and composable by breaking down complex queries into smaller, named relational objects.\n\nYou can also save intermediate steps as variables:\n\n```python\nCopy code\n\nover_10m_population = population_relation.filter(\"Population > 10000000\")\n\n# Now reuse 'over_10m_population' for different analyses\n# Find average population per region for these large countries\n# Also include the count of countries per region in the same aggregate\n(over_10m_population\n .aggregate(\"\"\"\n    Region,\n    count(*) AS country_count,\n    CAST(avg(Population) AS int) AS avg_pop\n    \"\"\") # Aggregates on Region and computes count and average\n .order(\"avg_pop DESC\")\n .show()\n)\n```\n\n```bash\nCopy code\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               Region                \u2502 country_count \u2502  avg_pop  \u2502\n\u2502               varchar               \u2502     int64     \u2502   int32   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 ASIA (EX. NEAR EAST)                \u2502            19 \u2502 192779730 \u2502\n\u2502 NORTHERN AMERICA                    \u2502             2 \u2502 165771574 \u2502\n\u2502 LATIN AMER. & CARIB                 \u2502            10 \u2502  48643375 \u2502\n\u2502 C.W. OF IND. STATES                 \u2502             5 \u2502  48487549 \u2502\n\u2502 WESTERN EUROPE                      \u2502             9 \u2502  38955933 \u2502\n\u2502 NORTHERN AFRICA                     \u2502             4 \u2502  38808343 \u2502\n\u2502 NEAR EAST                           \u2502             5 \u2502  32910924 \u2502\n\u2502 SUB-SAHARAN AFRICA                  \u2502            21 \u2502  30941436 \u2502\n\u2502 EASTERN EUROPE                      \u2502             3 \u2502  23691959 \u2502\n\u2502 OCEANIA                             \u2502             1 \u2502  20264082 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 10 rows                                               3 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Relational Set Operations and Joins\n\nThe relational API also includes methods for set operations and joins, allowing you to combine or compare relations programmatically.\n\nLet's find countries with _under_ 10 million population by using the `except_` method to subtract the `over_10m_population` relation from the original `population_relation`:\n\n```python\nCopy code\n\nunder_10m_population = population_relation.except_(over_10m_population)\n\n# Now aggregate the under 10m countries by region\n(under_10m_population\n .aggregate(\"\"\"\n    Region,\n    count(*) AS country_count,\n    CAST(avg(Population) AS int) AS avg_pop\n    \"\"\")\n .order(\"avg_pop DESC\")\n .show()\n)\n```\n\n```bash\nCopy code\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               Region                \u2502 country_count \u2502 avg_pop \u2502\n\u2502               varchar               \u2502     int64     \u2502  int32  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 EASTERN EUROPE                      \u2502             9 \u2502 5426538 \u2502\n\u2502 C.W. OF IND. STATES                 \u2502             7 \u2502 5377686 \u2502\n\u2502 SUB-SAHARAN AFRICA                  \u2502            30 \u2502 3322228 \u2502\n\u2502 NORTHERN AFRICA                     \u2502             2 \u2502 3086881 \u2502\n\u2502 ASIA (EX. NEAR EAST)                \u2502             9 \u2502 2796374 \u2502\n\u2502 NEAR EAST                           \u2502            11 \u2502 2773978 \u2502\n\u2502 WESTERN EUROPE                      \u2502            19 \u2502 2407190 \u2502\n\u2502 BALTICS                             \u2502             3 \u2502 2394991 \u2502\n\u2502 LATIN AMER. & CARIB                 \u2502            35 \u2502 2154024 \u2502\n\u2502 OCEANIA                             \u2502            20 \u2502  643379 \u2502\n\u2502 NORTHERN AMERICA                    \u2502             3 \u2502   43053 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 11 rows                                             3 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nNow, let's find countries that are both in 'EASTERN EUROPE' _and_ have a population over 10 million using `intersect`:\n\n```python\nCopy code\n\n# Filter for Eastern Europe\neastern_europe = population_relation.filter(\"Region ~ '.*EASTERN EUROPE.*'\") # Using SIMILAR TO shorthand\n\n# Find the intersection of Eastern Europe countries and those over 10m\n(eastern_europe\n .intersect(over_10m_population)\n .project(\"Country, Population\")\n .show()\n)\n```\n\n```bash\nCopy code\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Country     \u2502 Population \u2502\n\u2502     varchar     \u2502   int64    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Czech Republic  \u2502   10235455 \u2502\n\u2502 Romania         \u2502   22303552 \u2502\n\u2502 Poland          \u2502   38536869 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nFinally, let's look at a simple `join` example. Suppose you had another relation, maybe containing region nicknames. You can simulate this by creating a relation from Python data (this requires Pandas/Arrow, you'll cover that more later, but the `from_arrow` or `from_df` methods can create relations):\n\n```python\nCopy code\n\nimport pandas as pd\n\n# Create a simple DataFrame of region nicknames\nregion_nicknames_df = pd.DataFrame({\n    'Region': [\\\n        'ASIA (EX. NEAR EAST)     ',\\\n        'NORTHERN AMERICA       ',\\\n        'LATIN AMER. & CARIB      ',\\\n        'C.W. OF IND. STATES      ',\\\n        'WESTERN EUROPE           '\\\n    ],\n    'Nickname': [\\\n        'Asian Tigers',\\\n        'North America',\\\n        'Latin America',\\\n        'CIS States',\\\n        'Western Europe'\\\n    ]\n})\n\n# Convert DataFrame to a DuckDBPyRelation using the correct method\nregion_nicknames_rel = con.from_df(region_nicknames_df)\n\n# Trim spaces in the region names for a clean join using the built-in trim() function.\npopulation_trimmed = population_relation.project(\"Country, Population, trim(Region) as Region\")\nnicknames_trimmed = region_nicknames_rel.project(\"trim(Region) as Region, Nickname\")\n\n# Perform an inner join on the trimmed Region names\n(population_trimmed\n .join(nicknames_trimmed, on=\"Region\")\n .project(\"Country, Population, Nickname\")\n .limit(5)\n .show()\n)\n```\n\n```bash\nCopy code\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      Country       \u2502 Population \u2502    Nickname    \u2502\n\u2502      varchar       \u2502   int64    \u2502    varchar     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Afghanistan        \u2502   31056997 \u2502 Asian Tigers   \u2502\n\u2502 Andorra            \u2502      71201 \u2502 Western Europe \u2502\n\u2502 Anguilla           \u2502      13477 \u2502 Latin America  \u2502\n\u2502 Antigua & Barbuda  \u2502      69108 \u2502 Latin America  \u2502\n\u2502 Argentina          \u2502   39921833 \u2502 Latin America  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nYou can even use these relation objects directly within standard SQL queries executed via `con.sql()`:\n\n```python\nCopy code\n\ncon.sql(\"\"\"\n-- Query the 'over_10m_population' relation directly in SQL\nSELECT Country, \"GDP ($ per capita)\"\nFROM over_10m_population\nWHERE \"GDP ($ per capita)\" > 29000\nLIMIT 5\n\"\"\").show()\n```\n\n```bash\nCopy code\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Country     \u2502 GDP ($ per capita) \u2502\n\u2502    varchar     \u2502       int64        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Belgium        \u2502              29100 \u2502\n\u2502 Canada         \u2502              29800 \u2502\n\u2502 United States  \u2502              37800 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nAh, much better! Belgium, Canada, and the United States fit the bill. The point is, `over_10m_population`, which is a Python object representing a filtered relation, can be referenced directly in the `FROM` clause of a standard SQL query. Pretty slick! This demonstrates how the Relational API and standard SQL can complement each other.\n\n## Wrapping Up\n\nDuckDB's Python integration provides a practical middle ground between the simplicity of working with files and the power of SQL databases. The combination of direct file reading, in-memory processing, and both SQL and programmatic query interfaces makes it a solid tool for data analysis workflows.\n\nThe key concepts you've covered \u2013 connections, the `DuckDBPyRelation` object, direct file ingestion, and the relational API \u2013 form the foundation for most DuckDB operations. These features will streamline many of your data exploration tasks, particularly when working with files that are too large for comfortable Pandas operations but don't warrant setting up a full database server.\n\nINFO: Extend with MotherDuck\nWhile this guide focuses on local DuckDB, \\*\\*MotherDuck\\*\\* extends these capabilities to a serverless, collaborative environment. With Motherduck, you can query data stored in the cloud, share databases with colleagues, and leverage the convenience of a managed service. [Explore MotherDuck](https://motherduck.com/) to see how it complements your DuckDB workflows.\n\nIn the [next part](https://motherduck.com/learn-more/duckdb-python-quickstart-part2), you'll explore DuckDB's integrations with Pandas, Polars, and Arrow, along with Python user-defined functions for custom data transformations. These integrations are where DuckDB really starts to quack up the productivity gains in a typical Python data workflow.\n\n### TABLE OF CONTENTS\n\n[Getting Started: Installation and Connection](https://motherduck.com/learn-more/duckdb-python-quickstart-part1/#getting-started-installation-and-connection)\n\n[Your First Queries: \\`sql\\`](https://motherduck.com/learn-more/duckdb-python-quickstart-part1/#your-first-queries-sql)\n\n[Ingesting Data: From Files to Relations](https://motherduck.com/learn-more/duckdb-python-quickstart-part1/#ingesting-data-from-files-to-relations)\n\n[The Relational API: Building Queries Programmatically](https://motherduck.com/learn-more/duckdb-python-quickstart-part1/#the-relational-api-building-queries-programmatically)\n\n[Wrapping Up](https://motherduck.com/learn-more/duckdb-python-quickstart-part1/#wrapping-up)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 575,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 575
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     "content": "---\ntitle: duckdb-tutorial-for-beginners\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/duckdb-tutorial-for-beginners\nindexed_at: '2025-11-25T19:57:39.343742'\ncontent_hash: 9666dd2fd1ca1c68\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# DuckDB Tutorial For Beginners\n\n2024/10/31 - 12 min read\n\nBY\n\n[Mehdi Ouazza](https://motherduck.com/authors/mehdi-ouazza/)\n,\n[Ryan Boyd](https://motherduck.com/authors/ryan-boyd/)\n\nIf you haven't had the chance to get up to speed with DuckDB, this tutorial is for you! We'll go over the essentials, from installation to workflow, getting to know the command-line interface (CLI), and diving into your first analytics project. If are too lazy to read, I also made a video for this tutorial.\n\nDuckDB Tutorial For Beginners In 12 min - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[DuckDB Tutorial For Beginners In 12 min](https://www.youtube.com/watch?v=ZX5FdqzGT1E)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nFull screen is unavailable. [Learn More](https://support.google.com/youtube/answer/6276924)\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Why am I seeing this?](https://support.google.com/youtube/answer/9004474?hl=en)\n\n[Watch on](https://www.youtube.com/watch?v=ZX5FdqzGT1E&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 11:26\n\n\u2022Live\n\n\u2022\n\nLet's start quacking some code!\n\nEditor's note: this tutorial was originally published 2023-04-26 by Mehdi and has been updated by Ryan to reflect advancements in DuckDB.\n\n\n## What is DuckDB?\n\nDuckDB is an in-process SQL [OLAP](https://motherduck.com/learn-more/what-is-OLAP/) database, which means it is a database optimized for analytics and runs within the same process as the application using it. This unique feature allows DuckDB to offer the advantages of a database without the complexities of managing one. But, as with any software concept, the best way to learn is to dive in and get your hands dirty.\n\nWe\u2019ll be showing examples using the DuckDB command-line client (CLI), but you can also use DuckDB from within Python, R, and other languages, or any tool supporting JDBC or ODBC drivers. There is a community-contributed selection of example queries and code for many of these languages on the [DuckDB Snippets](https://duckdbsnippets.com/) website.\n\n_In the below snippets, any code example prefixed with `$` means that it\u2019s a bash command. Otherwise we assume that these would run within a DuckDB process, which uses a `D` prompt._\n\n## Installation\n\nInstalling DuckDB is a breeze. Visit the [DuckDB documentation](https://duckdb.org/docs/installation/index) and download the binary for your operating system.\n\nFor MacOS and Windows users, you can leverage package managers to make the DuckDB CLI directly available in your PATH, simplifying upgrades and installations.\n\nTo install DuckDB on MacOS using Homebrew, run the following command:\n\n```bash\nCopy code\n\n$ brew install duckdb\n```\n\nTo install DuckDB on Windows using winget, run the following command:\n\n```bash\nCopy code\n\nC:\\> winget install DuckDB.cli\n```\n\nYou can now launch DuckDB by simply calling the `duckdb` CLI command.\n\n```jsx\nCopy code\n\n$ duckdb\nv1.0.0 1f98600c2c\nEnter \".help\" for usage hints.\nConnected to a transient in-memory database.\nUse \".open FILENAME\" to reopen on a persistent database.\nD\n```\n\n## Workflow with VSCode\n\nTo follow along with our exploration of DuckDB, check out this [GitHub repository](https://github.com/mehd-io/duckdb-playground-tutorial). I recommend working with an editor, a SQL file, and sending commands to the terminal for a lightweight setup. This approach offers visibility on all commands, enables you to safely version control them, and allows you to leverage formatting tools and AI friends like Copilot.\n\nIn our example, we'll use Visual Studio Code (VSCode). To configure a custom shortcut to send commands from the editor to the terminal, open the keyboard shortcuts JSON file and add a key binding to the following command :\n\n```jsx\nCopy code\n\n{\n    \"key\": \"shift+enter\",\n    \"command\": \"workbench.action.terminal.runSelectedText\"\n}\n```\n\nOf course, this workflow can be pretty easily replicated with any editor or IDE!\n\n### Data Persistence with DuckDB: Overview\n\nBy default, DuckDB is an in-memory process and won't persist any data. To demonstrate this, let's create a simple table based on a query result:\n\n```sql\nCopy code\n\n$ duckdb\nD CREATE TABLE ducks AS SELECT 3 AS age, 'mandarin' AS breed;\nFROM ducks;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  age  \u2502  breed   \u2502\n\u2502 int32 \u2502 varchar  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     3 \u2502 mandarin \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nThis query creates and populates a \"ducks\" table. However, if we exit the CLI and reopen it, the table will be gone.\n\nUnlike standard SQL, the query above uses the `FROM` statement without any `SELECT *` statement. This is a neat shortcut in DuckDB and there are plenty more [DuckDB SQL shortcuts](https://duckdb.org/2022/05/04/friendlier-sql.html)!\n\n\n### Data Persistence with DuckDB: Creating a Database\n\nTo persist data, you have two options:\n\n1. Provide a path to a database file when starting DuckDB. The file can have any extension, but common choices are `.db`, `.duckdb`, or `.ddb`. If no database exists at the specified path, DuckDB will create one.\n\n\n```bash\nCopy code\n\n\n$ duckdb /data/myawesomedb.db\n```\n\n\nYou can also launch DuckDB with a database in read-only mode to avoid modifying the database:\n\n\n```bash\nCopy code\n\n\n$ duckdb -readonly /data/myawesomedb.db\n```\n\n2. If DuckDB is already running, use the `attach` command to connect to a database at the specified file path.\n\n\n```arduino\nCopy code\n\n\nATTACH DATABASE '/path/to/your/database.db' AS mydb;\n```\n\n\nThe database file uses DuckDB's custom single-file format (all tables are included), which supports transactional ACID compliance and stores data in a compressed columnar format for optimal aggregation performance. DuckDB is [regularly adding](https://duckdb.org/2022/10/28/lightweight-compression.html) new compression algorithms to improve performance.\n\nWhile the DuckDB team often improves the file format with new releases, it is [backward compatible](https://duckdb.org/docs/internals/storage.html) as of DuckDB 1.0, meaning that new releases are able to read files produced by early releases of DuckDB.\n\nIf you use MotherDuck as your cloud data warehouse, it automatically manages the DuckDB databases for you, so you create a MotherDuck database using the familiar [`CREATE DATABASE`](https://motherduck.com/docs/sql-reference/motherduck-sql-reference/create-database/) SQL statement.\n\n## Reading and Displaying Data\n\nNext, let's explore reading and writing data in CSV and Parquet formats. We'll use a small dataset from Kaggle containing daily Netflix Top 10 Movie/TV Show data for the United States from 2020 to March 2022.\n\nTo load the CSV dataset, use the [read\\_csv\\_auto](https://duckdb.org/docs/data/csv/overview#read_csv_auto-function) command, which infers the schema and detects the delimiter. You can also use the `read_csv` command and pass the schema and delimiter as parameters.\n\n```sql\nCopy code\n\nSELECT * FROM read_csv_auto('path/to/your/file.csv');\n```\n\nWhen you use this command, the dataset is read, but an actual table is not created in your DuckDB database. To create a table, use a `CREATE TABLE x AS` (CTAS) statement:\n\n```sql\nCopy code\n\nCREATE TABLE netflix_top10 AS SELECT * FROM read_csv_auto('path/to/your/file.csv');\n```\n\nTo write data to a CSV file, use the `COPY` command and specify the delimiter. For Parquet files, simply specify the file format:\n\n```sql\nCopy code\n\nCOPY ./data/netflix_top10.csv TO 'path/to/your/output/file.csv' WITH (FORMAT 'CSV', DELIMITER ',');\nCOPY ./data/netflix_top10.csv TO 'path/to/your/output/file.parquet' WITH (FORMAT 'PARQUET');\n```\n\nTo read data from a Parquet file, use the `read_parquet` command:\n\n```sql\nCopy code\n\nSELECT * FROM read_parquet('path/to/your/file.parquet');\n```\n\nDuckDB supports a wide variety of different file formats, including the native DuckDB database file used above, CSV, [JSON](https://motherduck.com/blog/analyze-json-data-using-sql/), Parquet, [Iceberg](https://motherduck.com/docs/integrations/file-formats/apache-iceberg/), [Delta Lake](https://motherduck.com/docs/integrations/file-formats/delta-lake/) and more. You can read these files from your local filesystem, a http endpoint or a cloud blob store like AWS S3, Cloudflare R2, Azure Blob Storage or Google Cloud Storage.\n\n## Display Modes, Output Options\n\nDuckDB CLI offers various ways to enhance your experience by customizing the data display and output options.\n\nYou can use the `.mode` command to change the appearance of tables returned in the terminal output. For instance, if you are dealing with long nested JSON, you can change the mode to `line` or `JSON` to have a better view of your data.\n\n```jsx\nCopy code\n\n.mode line\nSELECT * FROM './data/sales.json';\nsales_data = [{'order_id': 1, 'customer': {'id': 101, 'name': John Doe, 'email': john.doe@example.com}, 'items': [{'product_id': 301, 'product_name': Laptop, 'quantity': 1, 'price': 1200}, {'product_id': 302, 'product_name': Mouse, 'quantity': 1, 'price': 25}], 'total_amount': 1225, 'date': 2023-03-24}, {'order_id': 2, 'customer': {'id': 102, 'name': Jane Smith, 'email': jane.smith@example.com}, 'items': [{'product_id': 303, 'product_name': Keyboard, 'quantity': 1, 'price': 50}, {'product_id': 304, 'product_name': Monitor, 'quantity': 1, 'price': 200}], 'total_amount': 250, 'date': 2023-03-25}]\n```\n\nNext to that, you can output elsewhere the data by redirecting the terminal output to a file.\n\nLet\u2019s say you would like to output the result to a Markdown file, you can set the display mode to Markdown with `.mode markdown`. Combine this with the `.output` or `.once` command to write the result directly to a specific file. The `.output` command writes all the output of the different results you run, while `.once` does it just once.\n\n```lua\nCopy code\n\n.mode markdown\n.output myfile.md\n```\n\n## Running Commands and Exiting\n\nDuckDB CLI allows you to run a SQL statement and exit using the `-c` option parameter. For example, if you use a `SELECT` statement to read a Parquet file:\n\n```jsx\nCopy code\n\n$ duckdb -c \"SELECT * FROM read_parquet('path/to/your/file.parquet');\"\n```\n\nThis feature is lightweight, fast, and easy. You can even build your own [bash functions](https://duckdbsnippets.com/snippets/6/quickly-convert-a-csv-to-parquet-bash-function) using the DuckDB CLI for various operations on CSV/Parquet files, such as converting a CSV to Parquet.\n\nDuckDB also offers flags for configuration that you can fine-tune, such as setting the thread count, memory limits, ordering of null values and more. You can find the full list of flag options and their current values from the `duckdb_settings()` table function.\n\n```csharp\nCopy code\n\nFROM duckdb_settings();\n```\n\n## Working with Extensions\n\nExtensions are like packages that you can install within DuckDB to enjoy specific feature. DuckDB supports a number of core extensions. Not all are included by default, but DuckDB has a mechanism for remote extension installation. To view the available core extensions, execute the following statement:\n\n```csharp\nCopy code\n\nFROM duckdb_extensions();\n```\n\nTo install an extension, such as the popular `httpfs` extension that allows reading/writing remote files over HTTPS and S3, use the `INSTALL` command followed by the extension name. Once installed, DuckDB downloads the extension to the `$HOME/.duckdb/` folder (modifiable by setting the `extension_directory` parameter).\n\nNext, load the extension in the DuckDB process with the `LOAD` command.\n\n```ini\nCopy code\n\nINSTALL httpfs;\nLOAD httpfs;\n```\n\nDuckDB supports autoloading of the core extensions, so you often do not need to manually load these. As an example. if you read from a CSV file with a \\`https://\\` scheme, the \\`httpfs\\` extension will be autoloaded.\n\n\nIf you're using a third-party extension or your own extension not bundled by default, set the `allow_unsigned_extensions` flag to `True`, or use the `-unsigned` flag parameter when launching DuckDB.\n\n```jsx\nCopy code\n\n$ duckdb -unsigned\n```\n\nExtensions are powerful and versatile. You can create your own using the [template](https://github.com/duckdb/extension-template) provided by the DuckDB Labs team to kickstart your extension development journey.\n\nThere is now a [Community Extensions repository](https://duckdb.org/docs/extensions/community_extensions.html) for you to share any custom extensions with the wider DuckDB community for easy installation.\n\n## First analytics project\n\nWe have the mentioned Netflix dataset hosted on a public AWS S3 bucket. In this simple project, we will answer the most existential question : what were people in the US binge-watching during the COVID lockdown?\n\nAs the data is sitting on AWS S3, we\u2019ll start by installing the extension httpfs.\n\n```jsx\nCopy code\n\n-- Install extensions\nINSTALL httpfs;\nLOAD httpfs;\n-- Minimum configuration for loading S3 dataset if the bucket is public\nSET s3_region='us-east-1';\n```\n\nWe can now read our dataset :\n\n```jsx\nCopy code\n\nD CREATE TABLE netflix AS SELECT * FROM read_parquet('s3://us-prd-motherduck-open-datasets/netflix/netflix_daily_top_10.parquet');\nFROM netflix;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   As of    \u2502 Rank  \u2502 Year to Date Rank \u2502 \u2026 \u2502 Days In Top 10 \u2502 Viewership Score \u2502\n\u2502    date    \u2502 int64 \u2502      varchar      \u2502   \u2502     int64      \u2502      int64       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2020-04-01 \u2502     1 \u2502 1                 \u2502 \u2026 \u2502              9 \u2502               90 \u2502\n\u2502 2020-04-01 \u2502     2 \u2502 2                 \u2502 \u2026 \u2502              5 \u2502               45 \u2502\n\u2502 2020-04-01 \u2502     3 \u2502 3                 \u2502 \u2026 \u2502              9 \u2502               76 \u2502\n\u2502 2020-04-01 \u2502     4 \u2502 4                 \u2502 \u2026 \u2502              5 \u2502               30 \u2502\n\u2502 2020-04-01 \u2502     5 \u2502 5                 \u2502 \u2026 \u2502              9 \u2502               55 \u2502\n\u2502 2020-04-01 \u2502     6 \u2502 6                 \u2502 \u2026 \u2502              4 \u2502               14 \u2502\n```\n\nFinally, getting the top watched movies as follow :\n\n```jsx\nCopy code\n\n-- Display the most popular TV Shows\nSELECT Title, max(\"Days In Top 10\") from netflix\nwhere Type='Movie'\nGROUP BY Title\nORDER BY max(\"Days In Top 10\") desc\nlimit 5;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502             Title              \u2502 max(\"Days In Top 10\") \u2502\n\u2502            varchar             \u2502         int64         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 The Mitchells vs. The Machines \u2502                    31 \u2502\n\u2502 How the Grinch Stole Christmas \u2502                    29 \u2502\n\u2502 Vivo                           \u2502                    29 \u2502\n\u2502 365 Days                       \u2502                    28 \u2502\n\u2502 Despicable Me 2                \u2502                    27 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n-- Copy the result to CSV\nCOPY (\nSELECT Title, max(\"Days In Top 10\") from netflix\nwhere Type='TV Show'\nGROUP BY Title\nORDER BY max(\"Days In Top 10\") desc\nlimit 5\n) TO 'output.csv' (HEADER, DELIMITER ',');\n```\n\nWhat\u2019s fun is that for both Movies and TV shows, the top 5 mostly include kids show. We all know that kids doesn\u2019t bother to see multiple time the same thing\u2026\n\nDuckDB and MotherDuck also support accessing private S3 buckets by using [`CREATE SECRET`](https://motherduck.com/docs/integrations/cloud-storage/amazon-s3/) to specify and store your credentials.\n\n## Exploring Beyond the Pond\n\nThat\u2019s it for this tutorial! If you're interested in delving deeper into DuckDB, check out these resources:\n\n- The official DuckDB docs : [https://duckdb.org/](https://duckdb.org/)\n- The DuckDB discord : [https://discord.com/invite/tcvwpjfnZx](https://discord.com/invite/tcvwpjfnZx)\n\nTo elevate your experience with DuckDB and scale it with a cloud data warehouse, explore [MotherDuck](https://motherduck.com/product/)! Dive into our [end-to-end tutorial](https://motherduck.com/docs/getting-started/e2e-tutorial) to discover the user-friendly web interface, AI-based SQL query fixing, global and organization-wide data sharing capabilities, and more.\n\nAdditionally, stay tuned to our [monthly newsletter](https://motherduck.com/duckdb-news/) and [YouTube channel](https://youtube.com/@motherduckdb/), where we'll continue to share more DuckDB-related content!\n\nKeep quacking, keep coding.\n\n!['DuckDB In Action' book cover](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduckdb-book-full-cover.68e4f598.png&w=3840&q=75)\n\nGet your free book!\n\nE-mail\n\nSubscribe to other MotherDuck news\n\nSubmit\n\nFree Book!\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![This Month in the DuckDB Ecosystem: March 2023](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fduckdb_ecosystem_monthly_feb_2023_352e669717.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-four/)\n\n[2023/03/23 - Marcos Ortiz](https://motherduck.com/blog/duckdb-ecosystem-newsletter-four/)\n\n### [This Month in the DuckDB Ecosystem: March 2023](https://motherduck.com/blog/duckdb-ecosystem-newsletter-four)\n\nThis month in the DuckDB Ecosystem, by Marcos Ortiz. Includes featured community member Elliana May, Python ecosystem, top links, upcoming events and more.\n\n[![This Month in the DuckDB Ecosystem: April 2023](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fduckdb_ecosystem_monthly_april_2023_bb2015c778.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-five/)\n\n[2023/04/17 - Marcos Ortiz](https://motherduck.com/blog/duckdb-ecosystem-newsletter-five/)\n\n### [This Month in the DuckDB Ecosystem: April 2023](https://motherduck.com/blog/duckdb-ecosystem-newsletter-five)\n\nThis month in the DuckDB Ecosystem, by Marcos Ortiz. Latest updates, including featured community member Josh Wills, upcoming events like webinars and top links.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 465,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 465
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": ".kurt/sources/motherduck.com/product/duckdb-users.md:33:No more exporting files or managing S3 buckets to share your work. Create database shares with specific team members while maintaining the familiar DuckDB experience.\n.kurt/sources/motherduck.com/product.md:60:It\u2019s impossible to understand who is driving the lion\u2019s share of usage and how to prevent your database and the applications built on top from grinding to a halt.\n.kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md:67:Filtered Views | Logical Data Segregation: Within a shared database, create views that pre-filter data (e.g., `CREATE VIEW team_a_data AS SELECT * FROM all_data WHERE team = 'A'` ). | Providing different teams with access to specific slices of a common dataset. |\n.kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md:85:`Catalog Error: error while importing share: Schema with name <local-database-name> does not exist!`\n.kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md:335:To\u00a0[make this data available to others](https://motherduck.com/docs/key-tasks/sharing-data/sharing-overview/), we can use the\u00a0`CREATE SHARE`\u00a0command.\n.kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md:337:If we run it, we will get a shareable link, that others can use with\u00a0`ATTACH`\u00a0to\u00a0[attach our database](https://motherduck.com/docs/key-tasks/sharing-data/sharing-overview/). Currently it takes about a minute to create the share, but in the future it will be a zero-copy operation.\n.kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md:342:-- CREATE SHARE <share name> [FROM <database name>];\n.kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md:343:CREATE SHARE so_2023_05 FROM so;\n.kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md:347:-- ATTACH '<share URL>' [AS <database name>];\n.kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md:355:-- After making changes to the shared database, you need to update the share\n.kurt/sources/motherduck.com/case-studies/dexibit.md:40:[Omitted long matching line]\n.kurt/sources/motherduck.com/blog/data-engineering-toolkit-essential-tools.md:331:[Omitted long matching line]\n.kurt/sources/motherduck.com/blog/announcing-duckdb-snippet-sets-with-motherduck-sharing-databases.md:29:Today, we\u2019ve released a couple features that will make the site even more powerful: the ability to bundle multiple themed snippets together, and the ability to include a [MotherDuck Share](https://motherduck.com/docs/key-tasks/managing-shared-motherduck-database/) of public data with your snippet(s).\n.kurt/sources/motherduck.com/blog/announcing-duckdb-snippet-sets-with-motherduck-sharing-databases.md:33:[MotherDuck Shares](https://motherduck.com/docs/key-tasks/managing-shared-motherduck-database/) give you the power to share an updatable snapshot of an entire DuckDB database with other users by providing them with a secret URL. We\u2019ve seen them be used inside companies to give colleagues access to data and publicly like the authors of the [DuckDB in Action book](https://motherduck.com/duckdb-book-brief/) have chosen to do \\[see page 26 in the free book\\].\n.kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-1.md:36:Then we can use these Parquet files to create the database on MotherDuck and explore it with the new natural language search (AI prompt) features launched last month. To allow you to avoid all the tedious data ingestion work, we use MotherDuck's database [sharing feature](https://motherduck.com/docs/key-tasks/sharing-data/sharing-overview/) to share the database with you.\n.kurt/sources/motherduck.com/glossary/API.md:19:[Omitted long matching line]\n.kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md:84:- **SHARES:** Data can be stored once and shared securely using [`SHARES`](https://motherduck.com/docs/key-tasks/data-warehousing/), which provide read-only access to specific databases. This avoids data duplication entirely.\n.kurt/sources/motherduck.com/blog/json-log-analysis-duckdb-motherduck.md:202:With the extension of MotherDuck, we can simply scale the log analysis in case DuckDB can't handle it, when we want to share quick analytics with a notebook, or when we want to share the data as a shared DuckDB database. You can scale up your parser without making the code more complex, just using a different engine with the same syntax and understanding as DuckDB itself.\n.kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-nov-2024.md:44:[Omitted long matching line]\n.kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md:129:A _share_ is a database that you can easily share, or rather `ATTACH`, to use the appropriate DuckDB term, from any DuckDB client.\n.kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md:130:To access the shared database we prepared for you, you need to have\n.kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md:143:Of course, you can query the shared database from any DuckDB client, which would be Python, Node.JS, and [many more](https://duckdb.org/docs/api/overview)!\n.kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md:189:This example uses a MotherDuck share, which is available by default for all users under the `sample_database` database.\n.kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md:56:[Omitted long matching line]\n.kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md:74:DuckDB\u2019s millisecond start up times (together with a shared cache on the storage layer) means that we can scale down to 0 quickly when the service is not used. Cold start is so fast that shutting down instances between queries becomes feasible. As long as you can spin them up again quickly, users won\u2019t be any wiser. At MotherDuck, we aim for the time-to-first query to be less than 200 ms, which is faster than most cloud databases can run a query, and within human reaction time.\n.kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md:102:[Omitted long matching line]\n.kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md:106:[Omitted long matching line]\n.kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md:106:3. **Better Collaboration:** It transforms the traditionally \"single-player\" DuckDB into a [\"multiplayer\" platform](https://motherduck.com/videos/339/bringing-duckdb-to-the-cloud-dual-execution-explained/) where teams can share databases and work from a single source of truth.\n.kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md:287:Now within PostgreSQL, you can start querying MotherDuck databases or shares. The below query uses a `sample_data` share database accessible by all MotherDuck users.\n.kurt/sources/motherduck.com/blog/read-scaling-preview.md:62:[Omitted long matching line]",
  [2m  â”‚[0m   "numLines": 30,
  [2m  â”‚[0m   "appliedLimit": 30
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me search for more specific information about attaching local databases to MotherDuck:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ ATTACH.*md:|ATTACH.*local|local.*DuckDB.*cloud
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": ".kurt/sources/motherduck.com/quacking-around-the-web.md:183:In this blog post we will walk through building an ELT pipeline using CloudQuery and DuckDB locally and then how to run the same configuration and pipeline with CloudQuery Cloud and MotherDuck (DuckDB Cloud version).\n.kurt/sources/motherduck.com/product/data-teams.md:41:MotherDuck turns users\u2019 laptops into local, easy-to-use execution nodes. DuckDB\u2019s unique portability enables teams to work locally and in the cloud with the same database. Since queries process closer to where data is stored, local compute is saved for analytical processing.\n.kurt/sources/motherduck.com/react-components-testing.md:48:[Omitted long matching line]\n.kurt/sources/motherduck.com/videos.md:987:Develop locally and ship to production confidently with Dagster data pipelines, Evidence dashboards powered by DuckDB and Markdown, and the MotherDuck cloud data warehouse.\n.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md:90:In his review, Daniel details building an Airflow DAG to query a 50GB CSV dataset stored in S3. He emphasizes the simplicity of shifting from a local DuckDB instance to the cloud-based MotherDuck service, noting the only significant code change is the connection string: con = duckdb.connect(f\\\\\\\"md:{MD\\_DB}?motherduck\\_token={MD\\_TOKEN}\\\\\\\").\n.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md:92:While not a formal benchmark, he observed that this data processing task completed in under two minutes within the Airflow pipeline. This gives you the most minimal friction required to scale a local DuckDB script to a cloud execution environment. [Code Snippet](https://github.com/danielbeach/MotherDuckwithApacheAirflow/tree/main)\n.kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md:215:- When your challenges go beyond local files, see how [MotherDuck](https://motherduck.com/) brings the power of DuckDB to the cloud for serverless, collaborative analytics.\n.kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md:138:The core of MotherDuck's anti-lock-in strategy is its [ Dual Execution model](https://motherduck.com/docs/concepts/architecture-and-capabilities/). This unique feature allows a local, open-source DuckDB instance to work together with the MotherDuck cloud service. This means you can develop and test entire data pipelines on a laptop without ever touching the cloud, ensuring your core logic is never tied to a proprietary environment.\n.kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md:202:[Omitted long matching line]\n.kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md:363:DuckDB\u2019s main advantage is its incredible speed for analytical queries on your local machine, eliminating the network latency inherent in cloud-only systems like BigQuery. When combined with a service like MotherDuck, you get the best of both worlds: fast local development with DuckDB and a scalable, serverless cloud backend for production workloads. This hybrid model is far more efficient for iterative analysis than traditional client-server setups.\n.kurt/sources/motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai.md:190:DuckDB's in-process nature and extensive integrations make it highly suitable for data exploration. The MCP exemplifies this by running a local DuckDB instance capable of directly querying various object storage systems, including AWS S3, Google Cloud Storage, and Cloudflare's R2.\n.kurt/sources/motherduck.com/case-studies/dominik-moritz.md:40:[Omitted long matching line]\n.kurt/sources/motherduck.com/learn-more/what-is-duckdb.md:47:**Local Data Processing**: Perform heavy computations on local datasets without a database server. DuckDB shines when you need to analyze data that's too large for memory but not so large that it requires a distributed system, helping you avoid the ['big data tax' of over-provisioned cloud warehouses](https://motherduck.com/learn-more/modern-data-warehouse-playbook/).\n.kurt/sources/motherduck.com/learn-more/pandas-dataframes-guide.md:533:[Omitted long matching line]\n.kurt/sources/motherduck.com/blog/pg to motherduck at motherduck.md:75:ATTACH 'md:MD_DATABASE';\n.kurt/sources/motherduck.com/blog/pg to motherduck at motherduck.md:100:    duck_con.sql(f\"ATTACH 'md:{md_database}'; USE {md_database}\")\n.kurt/sources/motherduck.com/blog/pg to motherduck at motherduck.md:221:duck_con.sql(\"ATTACH 'postgresql://user:pass@localhost:5432/mydb' AS pg (TYPE POSTGRES, READ_ONLY);\")\n.kurt/sources/motherduck.com/blog/pg to motherduck at motherduck.md:224:duck_con.sql(\"ATTACH 'md:mydb' (TOKEN 'your_token'); USE mydb\")\n.kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md:348:ATTACH 'md:_share/so/373594a2-06f7-4c33-814e-cf59028482ca' AS so;\n.kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md:371:[From Local to Cloud - Loading our Database into MotherDuck and Querying it with AI Prompts](https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2/#from-local-to-cloud-loading-our-database-into-motherduck-and-querying-it-with-ai-prompts)",
  [2m  â”‚[0m   "numLines": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! Let me read the article about exploring StackOverflow which seems to have information about loading local data to MotherDuck:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md

  [96mğŸ”§ TOOL:[0m Grep â†’ transfer.*data|upload.*motherduck|sync.*local
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md",
  [2m  â”‚[0m     "content": "Copy code\n\n-- CREATE SHARE <share name> [FROM <database name>];\nCREATE SHARE so_2023_05 FROM so;\n-- share_url = md:_share/so/373594a2-06f7-4c33-814e-cf59028482ca\n-- Run Time (s): real 63.335 user 0.014849 sys 0.013110\n\n-- ATTACH '<share URL>' [AS <database name>];\nATTACH 'md:_share/so/373594a2-06f7-4c33-814e-cf59028482ca' AS so;\n\n-- show the contents of the share\nDESCRIBE SHARE \"so_2023_05\";\n\nLIST SHARES;\n\n-- After making changes to the shared database, you need to update the share\nUPDATE SHARE \"so_2023_05\";\n```\n\nToday we explored the MotherDuck interface, created a database and populated it with tables using Parquet data on S3. That worked really well and you should be able to do this with your own data easily.\n\nThen we tried the new AI prompts on MotherDuck, which work quite well, of course not 100% but often good enough to get a starting point or learn something new. Given the amount of SQL information that was used to the train the LLMs plus the additional schema information, that is not surprising. SQL (derived from structured english query language SEQUEL) is just another langauge for the LLM to translate into, much like Korean or Klingon.\n\nSo while you\u2019re waiting for the third part of the blog series, you can attach our share (which is public) and run your own queries on it.\n\nIn the third part we want to connect to our StackOverflow database on MotherDuck using Python and explore some more ways accessing, querying and visualizing our data.\n\nPlease share any interesting queries or issues on the\u00a0[MotherDuck Slack channel](https://slack.motherduck.com/).\n\n### TABLE OF CONTENTS\n\n[From Local to Cloud - Loading our Database into MotherDuck and Querying it with AI Prompts](https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2/#from-local-to-cloud-loading-our-database-into-motherduck-and-querying-it-with-ai-prompts)\n\n[Getting started with MotherDuck](https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2/#getting-started-with-motherduck)\n\n[Loading our StackOverflow Data into MotherDuck](https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2/#loading-our-stackoverflow-data-into-motherduck)\n\n[Querying the Data with AI \ud83e\udd16](https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2/#querying-the-data-with-ai)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![This Month in the DuckDB Ecosystem: September 2023](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fthumbnail_duckdb_newsletter_september_9b90e1e119.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-ten/)\n\n[2023/09/30 - Marcos Ortiz](https://motherduck.com/blog/duckdb-ecosystem-newsletter-ten/)\n\n### [This Month in the DuckDB Ecosystem: September 2023](https://motherduck.com/blog/duckdb-ecosystem-newsletter-ten)\n\nThis Month in the DuckDB Ecosystem: September 2023\n\n[![DuckDB Tutorial For Beginners](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgetting_started_duckdb_thumbnail_70b197b1ab.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n[2024/10/31 - Mehdi Ouazza, Ryan Boyd](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n### [DuckDB Tutorial For Beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners)\n\nGet up to speed quickly with DuckDB, including installation, VSCode workflow integration and your first SQL analytics project.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 76,
  [2m  â”‚[0m     "startLine": 340,
  [2m  â”‚[0m     "totalLines": 415
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-use.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/privacy-policy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 10,
  [2m  â”‚[0m   "appliedLimit": 10
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md",
  [2m  â”‚[0m     "content": "---\ntitle: exploring-stackoverflow-with-duckdb-on-motherduck-2\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2\nindexed_at: '2025-11-25T19:57:26.723171'\ncontent_hash: e4f1c15b5c200e6d\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Exploring StackOverflow with DuckDB on MotherDuck (Part 2)\n\n2023/10/02 - 11 min read\n\nBY\n\n[Michael Hunger](https://motherduck.com/authors/michael-hunger/)\n\n## From Local to Cloud - Loading our Database into MotherDuck and Querying it with AI Prompts\n\nIn the\u00a0[first part of the series](https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-1/)\u00a0we looked at the full StackOverflow dump as an interesting dataset to explore with DuckDB. We downloaded the data, converted it to CSV and loaded it into DuckDB and explored tags, users and posts a bit before exporting the database to Parquet. Today we want to move from our local evironment to MotherDuck, where we want to look at importing these parquet files into a database, sharing that database with you and exploring the data with the new AI prompt features.\n\n## Getting started with MotherDuck\n\nDuckDB itself is focusing on local, and in-process execution of the analytical database engine. While you can access remote data, it\u2019s downloaded to your machine every time you access the remote files, so you really might want to move your DuckDB execution to where the data lives.\n\nTo make it easier to query data that resides in other, remote locations, MotherDuck offers a managed service that allows you to run DuckDB in the cloud.\n\nWith MotherDuck you can query the data on your cloud storage transparently as if it was local. But what\u2019s even better, is you can join and combine local tables transparently with data in tables residing in the cloud. The MotherDuck UI runs a build of DuckDB WASM in your browser, so the operations in the database that can be executed and rendered locally, are executed inside your web-browser.\n\nHere is a picture of the architecture from the\u00a0[documentation](https://motherduck.com/docs/architecture-and-capabilities/):\n\n![motherduck_hld](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmotherduck_hld_081bc8a023.png%3Fupdated_at%3D2023-10-02T11%3A52%3A29.138Z&w=3840&q=75)\n\nMotherduck also integrates with Python and all the other access libraries and integrations for DuckDB.\n\nIf you already signed up, you can just log-in to MotherDuck, otherwise you can create an account on the\u00a0[homepage](https://motherduck.com/)\u00a0(via Google, GitHub or email auth).\n\nAnywhere you can run DuckDB you can use MotherDuck as it connects through an official DuckDB extension which is downloaded & loaded as soon as you connect to a motherduck database through\u00a0`.open md:`\u00a0or similar commands.\n\n```bash\nCopy code\n\n.open md:\nAttempting to automatically open the SSO authorization page\n   in your default browser.\n1. Please open this link to login into your account:\n    https://auth.motherduck.com/activate\n2. Enter the following code: XXXX-XXXX\n\nToken successfully retrieved \u2705\nYou can store it as an environment variable to avoid having to log in again:\n  $ export motherduck_token='eyJhbGciOiJI..._Jfo'\n```\n\nOnce you have an account you get a\u00a0**motherduck\\_token**, which you need to connect to MotherDuck. Best to set the token as an environment variable, instead of a database variable, because opening a new database wipes the settings in DuckDB (trust me, I tried).\n\nIf you want to explore the MotherDuck UI first, feel free to do so, you can create new databases, upload files and create tables from those. You can run queries and get a nice pivotable, sortable output table with inline frequency charts in the header.\n\n![motherduck-ui1](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmotherduck_ui_1964bb8d8b.png%3Fupdated_at%3D2023-10-02T11%3A52%3A30.426Z&w=3840&q=75)\n\n## Loading our StackOverflow Data into MotherDuck\n\nYou have the option of uploading your local database with single command, which is really neat.\n\n```bash\nCopy code\n\nCREATE DATABASE remote_database_name FROM CURRENT_DATABASE();\n\n-- or more generally\nCREATE DATABASE remote_database_name FROM '<local database name>';\n```\n\nThere are only two caveats,\u00a0**the local and remote name must be different**, otherwise you might get the error below.\n\n`Catalog Error: error while importing share: Schema with name <local-database-name> does not exist!`\n\nAnd for the size of our StackOverflow database and the it took quite some time to finish the upload, around 1 hour, sending 15GB of data for our 11GB database.\n\nSo we can either create the database on the MotherDuck UI and import our tables from our Parquet files on S3, or upload the database from our local system.\n\nFor creating the database and tables from Parquest, we use the web interface or DuckDB on the local machine, connected to MotherDuck. Here are the SQL commands you need to run.\n\n```bash\nCopy code\n\ncreate database so;\n\ncreate table users as\nfrom 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/users.parquet';\n-- Run Time (s): real 10.401 user 0.006417 sys 0.003527\n\ndescribe users;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  column_name   \u2502 column_type \u2502\n\u2502    varchar     \u2502   varchar   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Id             \u2502 BIGINT      \u2502\n\u2502 Reputation     \u2502 BIGINT      \u2502\n\u2502 CreationDate   \u2502 TIMESTAMP   \u2502\n\u2502 DisplayName    \u2502 VARCHAR     \u2502\n\u2502 LastAccessDate \u2502 TIMESTAMP   \u2502\n\u2502 AboutMe        \u2502 VARCHAR     \u2502\n\u2502 Views          \u2502 BIGINT      \u2502\n\u2502 UpVotes        \u2502 BIGINT      \u2502\n\u2502 DownVotes      \u2502 BIGINT      \u2502\n\u2502 Id             \u2502 BIGINT      \u2502\n\u2502 Reputation     \u2502 BIGINT      \u2502\n\u2502 CreationDate   \u2502 TIMESTAMP   \u2502\n\u2502 DisplayName    \u2502 VARCHAR     \u2502\n\u2502 LastAccessDate \u2502 TIMESTAMP   \u2502\n\u2502 AboutMe        \u2502 VARCHAR     \u2502\n\u2502 Views          \u2502 BIGINT      \u2502\n\u2502 UpVotes        \u2502 BIGINT      \u2502\n\u2502 DownVotes      \u2502 BIGINT      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 18 rows                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nRun Time (s): real 0.032 user 0.026184 sys 0.002383\n\n-- do the same for the other tables\n\ncreate table comments as\nfrom 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/comments.parquet';\ncreate table posts as\nfrom 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/posts.parquet';\ncreate table votes as\nfrom 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/votes.parquet';\ncreate table badges as\nfrom 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/badges.parquet';\ncreate table post_links as\nfrom 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/post_links.parquet';\ncreate table tags as\nfrom 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/tags.parquet';\n```\n\nIn the left sidebar of the web interface, now the database\u00a0`so`\u00a0and the tables should show up, if not, refresh the page.\n\n![motherduck-ui](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmotherduck_ui_2_3a7e8b67f8.png%3Fupdated_at%3D2023-10-02T12%3A24%3A30.413Z&w=3840&q=75)\n\n## Querying the Data with AI \ud83e\udd16\n\nA while ago MotherDuck released a new\u00a0[generative AI feature](https://motherduck.com/docs/key-tasks/writing-sql-with-ai/)\u00a0that allows you to\n\n- query your data using natural language\n- generate and fix SQL statements and\n- describe your data.\n\nAs LLMs, GPT and foundational models are\u00a0[close to my heart](https://medium.com/@mesirii), I was really excited to try these out.\n\nIt works actually already quite well, let\u2019s see how it does on this dataset.\n\nThe schema description is a bit uninspiring, I could have seen the same by just looking at the table list. As expected from probabilistic models it returns different results on each run.\n\n```bash\nCopy code\n\nCALL prompt_schema();\n\nsummary = The database contains information related to posts, comments, votes, badges, tags, post links, and users for a platform.\n\nRun Time (s): real 1.476 user 0.001069 sys 0.000778\n\nsummary = The database schema represents a collection of data about various aspects of a community platform, including users, posts, comments, tags, badges, votes, and post links.\n```\n\nOk, let\u2019s try a simple question:\u00a0`What are the most popular tags?`\n\n```bash\nCopy code\n\n.mode duckbox\npragma prompt_query('What are the most popular tags?');\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  TagName   \u2502  Count  \u2502\n\u2502  varchar   \u2502  int64  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 javascript \u2502 2479947 \u2502\n\u2502 python     \u2502 2113196 \u2502\n\u2502 java       \u2502 1889767 \u2502\n\u2502 c#         \u2502 1583879 \u2502\n\u2502 php        \u2502 1456271 \u2502\n\u2502 android    \u2502 1400026 \u2502\n\u2502 html       \u2502 1167742 \u2502\n\u2502 jquery     \u2502 1033113 \u2502\n\u2502 c++        \u2502  789699 \u2502\n\u2502 css        \u2502  787138 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 10 rows    2 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n-- Run Time (s): real 3.763 user 0.124567 sys 0.001716",
  [2m  â”‚[0m     "numLines": 200,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 415
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! Now I have enough information. Let me write the answer to the file:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_6.md

  [96mğŸ”§ TOOL:[0m Bash â†’ touch /tmp/answer_without_kg_6.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m 
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /tmp/answer_without_kg_6.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/tmp/answer_without_kg_6.md",
  [2m  â”‚[0m     "content": "# How to Migrate Data from a Local DuckDB Database to MotherDuck\n\nMigrating data from a local DuckDB database to MotherDuck is straightforward thanks to DuckDB's unified architecture. MotherDuck uses the same DuckDB engine locally and in the cloud, ensuring seamless data transfer and query compatibility.\n\n## Quick Migration Steps\n\n### 1. Connect to Both Databases\n\nThe key is to use the `ATTACH` command to connect both your local DuckDB database and your MotherDuck cloud database simultaneously:\n\n```sql\n-- Attach your local DuckDB database\nATTACH 'local_database.ddb';\n\n-- Connect to MotherDuck (requires MOTHERDUCK_TOKEN environment variable)\nATTACH 'md:';\n```\n\n### 2. Copy Tables to MotherDuck\n\nOnce both databases are attached, you can copy tables using standard SQL:\n\n**Option A: Create new tables**\n```sql\nCREATE TABLE md:cloud_database.table_name AS\nSELECT * FROM local_database.table_name;\n```\n\n**Option B: Use CREATE OR REPLACE for full refreshes**\n```sql\nCREATE OR REPLACE TABLE md:cloud_database.table_name AS\nSELECT * FROM local_database.table_name;\n```\n\n**Option C: Insert into existing tables**\n```sql\nINSERT INTO md:cloud_database.table_name\nSELECT * FROM local_database.table_name;\n```\n\n### 3. Migrate an Entire Database\n\nThe fastest way to migrate an entire local database to MotherDuck is using the `CREATE DATABASE ... FROM` command:\n\n```sql\n-- Attach local database\nATTACH 'local_database.ddb';\n\n-- Connect to MotherDuck\nATTACH 'md:';\n\n-- Upload the entire database to MotherDuck\nCREATE DATABASE cloud_database FROM local_database;\n```\n\nThis single command copies all tables and data from your local database to MotherDuck.\n\n## Complete Python Example\n\nHere's a complete Python script for migrating data:\n\n```python\nimport duckdb\n\n# Create a local DuckDB connection\nduck_con = duckdb.connect()\n\n# Attach your local database\nduck_con.sql(\"ATTACH 'my_local_data.ddb' AS local_db;\")\n\n# Connect to MotherDuck (token from environment variable)\nduck_con.sql(\"ATTACH 'md:my_cloud_db';\")\n\n# Use the cloud database\nduck_con.sql(\"USE my_cloud_db\")\n\n# Copy tables one by one\ntables_to_migrate = ['customers', 'orders', 'products']\n\nfor table in tables_to_migrate:\n    print(f\"Migrating {table}...\")\n    duck_con.sql(f\"CREATE OR REPLACE TABLE {table} AS SELECT * FROM local_db.{table};\")\n    print(f\"\u2713 {table} migrated successfully\")\n\n# Verify the migration\nfor table in tables_to_migrate:\n    count = duck_con.sql(f\"SELECT COUNT(*) FROM {table}\").fetchone()[0]\n    print(f\"{table}: {count} rows\")\n```\n\n## Understanding Dual Execution\n\nMotherDuck's **dual execution model** is what makes this migration seamless. When you run a query that involves both local and cloud data:\n\n1. **Local scan**: The local data is read from your machine\n2. **Minimal data transfer**: Only necessary data is sent to the cloud\n3. **Cloud execution**: The final operations run in MotherDuck\n\nThis means you're not downloading or uploading entire tables unnecessarily\u2014the system intelligently optimizes data movement.\n\n## Migration Methods Comparison\n\n| Method | Use Case | Command |\n|--------|----------|---------|\n| **Whole Database** | Migrate everything at once | `CREATE DATABASE cloud_db FROM local_db` |\n| **Individual Tables** | Selective migration | `CREATE TABLE md:db.table AS SELECT * FROM local_db.table` |\n| **Full Refresh** | Replace existing data | `CREATE OR REPLACE TABLE ...` |\n| **Incremental Load** | Append new data | `INSERT INTO ... SELECT * FROM ...` |\n\n## Authentication Setup\n\nBefore migrating, ensure you have a MotherDuck token:\n\n```bash\n# Set your MotherDuck token as an environment variable\nexport MOTHERDUCK_TOKEN='your_token_here'\n```\n\nYou can get your token from the [MotherDuck web interface](https://app.motherduck.com/).\n\n## Best Practices\n\n1. **Start with a test migration**: Try migrating a small table first to verify your setup\n2. **Use CREATE OR REPLACE**: This ensures clean, repeatable migrations\n3. **Verify row counts**: Always check that source and destination counts match\n4. **Consider data size**: This approach works well for datasets up to hundreds of GB. For very large tables (multi-TB), consider using incremental approaches with timestamps\n5. **Track sync times**: Create a metadata table to record when migrations occurred\n\n## Real-World Performance\n\nAccording to MotherDuck's internal usage, this approach can replicate approximately **150GB of data in about 10 minutes**, making it practical for regular sync operations.\n\n## When to Use This Approach\n\n\u2705 **Good for:**\n- Migrating local development databases to production\n- Regular scheduled syncs (every few hours)\n- Complete table refreshes\n- Datasets from a few MB to hundreds of GB\n\n\u274c **Not ideal for:**\n- Real-time, continuous replication\n- Extremely large tables (10+ TB)\n- High-frequency syncs (every few seconds)\n- When you only need to sync changes (use CDC tools instead)\n\n## Sharing Your Migrated Data\n\nOnce your data is in MotherDuck, you can easily share it with your team:\n\n```sql\n-- Create a shareable snapshot\nCREATE SHARE FROM cloud_database;\n```\n\nThis returns a share URL that colleagues can attach to query your data without duplicating it.\n\n## Sources\n\n- [DuckDB & MotherDuck for Beginners: Your Ultimate Guide](https://motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide) - Overview of DuckDB architecture and MotherDuck connection basics\n- [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained) - Technical deep-dive into MotherDuck's dual execution architecture\n- [Hybrid Analytics: Query Local & Cloud Data Instantly](https://motherduck.com/learn-more/hybrid-analytics-guide) - Guide to joining local and cloud data in hybrid queries\n- [Using MotherDuck at MotherDuck: Loading Data from Postgres with DuckDB](https://motherduck.com/blog/pg%20to%20motherduck%20at%20motherduck) - Real-world example of database migration patterns\n- [CSV Files Persist: DuckDB Solution](https://motherduck.com/blog/csv-files-persist-duckdb-solution) - Demonstrates the `CREATE DATABASE FROM` command for uploading local databases to MotherDuck\n",
  [2m  â”‚[0m     "numLines": 165,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 165
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_6.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "update",
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_6.md",
  [2m  â”‚[0m   "content": "# How to Migrate Data from a Local DuckDB Database to MotherDuck\n\nThere are several methods to migrate data from a local DuckDB database to MotherDuck, ranging from simple one-command uploads to more granular table-by-table approaches.\n\n## Method 1: Upload Entire Local Database (Simplest)\n\nThe easiest way to migrate your local DuckDB database to MotherDuck is using a single command:\n\n```sql\nCREATE DATABASE remote_database_name FROM CURRENT_DATABASE();\n\n-- or more generally\nCREATE DATABASE remote_database_name FROM '<local database name>';\n```\n\n**Important considerations:**\n- The local and remote database names **must be different**, otherwise you'll get an error: `Catalog Error: error while importing share: Schema with name <local-database-name> does not exist!`\n- For large databases, this can take considerable time (e.g., ~1 hour for an 11GB database, transferring 15GB of data)\n- This method uploads all tables and data from your local database to MotherDuck\n\n## Method 2: Connect and Attach Local Database\n\nFirst, connect to MotherDuck by attaching it to your local DuckDB session:\n\n```sql\n-- From DuckDB CLI\n.open md:\n\n-- Or attach MotherDuck\nATTACH 'md:';\n```\n\nWhen you first connect, you'll be prompted to authenticate via SSO. The system will provide a motherduck_token that you should save as an environment variable:\n\n```bash\nexport motherduck_token='eyJhbGciOiJI..._Jfo'\n```\n\n## Method 3: Create Database and Copy Tables from Files\n\nIf you have your data exported to files (like Parquet or CSV), you can create tables directly in MotherDuck from those files:\n\n```sql\n-- Create the database in MotherDuck\nCREATE DATABASE so;\n\n-- Create tables from Parquet files on S3\nCREATE TABLE users AS\nFROM 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/users.parquet';\n\nCREATE TABLE posts AS\nFROM 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/posts.parquet';\n\n-- Or from local files\nCREATE TABLE population AS\nFROM 'local_file.parquet';\n\n-- Repeat for other tables\n```\n\nThis approach works well when:\n- You have data already exported to cloud storage (S3, GCS, etc.)\n- You want to selectively migrate specific tables\n- You want to transform data during migration\n\n## Method 4: Copy Data Table-by-Table Using ATTACH\n\nYou can also use the standard SQL `CREATE TABLE AS SELECT` (CTAS) pattern to copy tables:\n\n```sql\n-- Attach your local DuckDB database\nATTACH 'my_local_data.duckdb' AS local_db;\n\n-- Connect to MotherDuck\nATTACH 'md:' AS motherduck;\n\n-- Create database in MotherDuck (if needed)\nCREATE DATABASE my_remote_db;\nUSE my_remote_db;\n\n-- Copy tables from local to MotherDuck\nCREATE TABLE population AS SELECT * FROM local_db.population;\nCREATE TABLE sales AS SELECT * FROM local_db.sales;\n```\n\n## Understanding DuckDB's Dual Execution Model\n\nMotherDuck uses a \"dual execution\" model that intelligently decides whether to run queries locally or in the cloud:\n\n- When both tables reside in MotherDuck, queries run entirely in the cloud\n- When joining local files with cloud tables, DuckDB processes data where it makes most sense\n- The system minimizes data movement by transferring only necessary intermediate data\n- You can manually control execution location using the `md_run` parameter:\n\n```sql\n-- Force local execution\nSELECT * FROM read_parquet('s3://bucket/file.parquet', md_run='local');\n\n-- Force remote execution in MotherDuck\nSELECT * FROM read_parquet('s3://bucket/file.parquet', md_run='remote');\n```\n\n**Performance example:** Querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to local execution that had to download the data first (36 seconds).\n\n## Data Persistence Options\n\nDuckDB offers two modes:\n1. **In-memory** (default): Data disappears when session ends - use `duckdb.connect(database=':memory:')`\n2. **Persistent**: Data saved to a database file\n\nTo create a persistent local database before uploading:\n\n```bash\n# Start DuckDB with a database file\nduckdb my_local_data.duckdb\n\n# Or from Python\nimport duckdb\ncon = duckdb.connect(database='my_local_data.duckdb')\n```\n\nOr attach an existing database if DuckDB is already running:\n\n```sql\nATTACH DATABASE '/path/to/your/database.db' AS mydb;\n```\n\n## Python Example\n\nHere's a complete Python example for migrating data using the relational API:\n\n```python\nimport duckdb\n\n# Connect to persistent local database\ncon = duckdb.connect(database='my_local_data.duckdb')\n\n# Load data into local database\npopulation_relation = con.read_csv(\"https://bit.ly/3KoiZR0\")\npopulation_relation.to_table(\"population\")  # Persist as table\n\n# Connect to MotherDuck\ncon.sql(\"ATTACH 'md:my_cloud_db'\")\ncon.sql(\"USE my_cloud_db\")\n\n# Copy table to MotherDuck\ncon.sql(\"CREATE TABLE population AS SELECT * FROM my_local_data.population\")\n```\n\n## Best Practices\n\n1. **For small to medium databases**: Use Method 1 (single command upload) for simplicity\n2. **For large databases**: Consider exporting to Parquet files on cloud storage first, then use Method 3 to leverage MotherDuck's high-bandwidth cloud connections\n3. **For selective migration**: Use Method 4 to copy only specific tables\n4. **Set environment variable**: Always export your `motherduck_token` to avoid repeated authentication\n5. **Test connection first**: Verify you can connect to MotherDuck before starting large migrations\n6. **Monitor progress**: Large uploads can take time; plan accordingly\n7. **Leverage cloud proximity**: MotherDuck's cloud infrastructure provides high-bandwidth connections to cloud storage, making it faster to load from S3/GCS than from local files\n\n## Working with Extensions\n\nIf you need to read from cloud storage, make sure to install and load the necessary extensions:\n\n```sql\n-- Install httpfs extension for S3 access\nINSTALL httpfs;\nLOAD httpfs;\n\n-- Configure for public S3 bucket\nSET s3_region='us-east-1';\n\n-- For private buckets, use CREATE SECRET\nCREATE SECRET (\n    TYPE S3,\n    PROVIDER credential_chain\n);\n```\n\n## Sources\n\n- [DuckDB & MotherDuck for Beginners: Your Ultimate Guide](https://motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide) - Comprehensive guide covering DuckDB basics and MotherDuck integration, including how to connect with `ATTACH 'md:'`, data persistence with `.to_table()`, and the dual execution model\n- [Exploring StackOverflow with DuckDB on MotherDuck (Part 2)](https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2) - Tutorial demonstrating the `CREATE DATABASE FROM CURRENT_DATABASE()` method and table creation from Parquet files on S3, with real-world timing examples\n- [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained) - Deep dive into MotherDuck's architecture and dual execution model for hybrid local/cloud queries, with performance comparisons and manual execution control\n- [DuckDB Tutorial For Beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners) - Covers DuckDB fundamentals including database persistence with `ATTACH DATABASE`, working with various file formats, and extensions for cloud storage access\n- [DuckDB Python Quickstart (Part 1)](https://motherduck.com/learn-more/duckdb-python-quickstart-part1) - Details on connecting to persistent databases, the `to_table()` method for converting relations to tables, reading files with `read_csv()` and `read_parquet()`, and MotherDuck integration via Python\n",
  [2m  â”‚[0m   "structuredPatch": [
  [2m  â”‚[0m     {
  [2m  â”‚[0m       "oldStart": 1,
  [2m  â”‚[0m       "oldLines": 164,
  [2m  â”‚[0m       "newStart": 1,
  [2m  â”‚[0m       "newLines": 185,
  [2m  â”‚[0m       "lines": [
  [2m  â”‚[0m         " # How to Migrate Data from a Local DuckDB Database to MotherDuck",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-Migrating data from a local DuckDB database to MotherDuck is straightforward thanks to DuckDB's unified architecture. MotherDuck uses the same DuckDB engine locally and in the cloud, ensuring seamless data transfer and query compatibility.",
  [2m  â”‚[0m         "+There are several methods to migrate data from a local DuckDB database to MotherDuck, ranging from simple one-command uploads to more granular table-by-table approaches.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Quick Migration Steps",
  [2m  â”‚[0m         "+## Method 1: Upload Entire Local Database (Simplest)",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### 1. Connect to Both Databases",
  [2m  â”‚[0m         "+The easiest way to migrate your local DuckDB database to MotherDuck is using a single command:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-The key is to use the `ATTACH` command to connect both your local DuckDB database and your MotherDuck cloud database simultaneously:",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         " ```sql",
  [2m  â”‚[0m         "--- Attach your local DuckDB database",
  [2m  â”‚[0m         "-ATTACH 'local_database.ddb';",
  [2m  â”‚[0m         "+CREATE DATABASE remote_database_name FROM CURRENT_DATABASE();",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "--- Connect to MotherDuck (requires MOTHERDUCK_TOKEN environment variable)",
  [2m  â”‚[0m         "-ATTACH 'md:';",
  [2m  â”‚[0m         "+-- or more generally",
  [2m  â”‚[0m         "+CREATE DATABASE remote_database_name FROM '<local database name>';",
  [2m  â”‚[0m         " ```",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### 2. Copy Tables to MotherDuck",
  [2m  â”‚[0m         "+**Important considerations:**",
  [2m  â”‚[0m         "+- The local and remote database names **must be different**, otherwise you'll get an error: `Catalog Error: error while importing share: Schema with name <local-database-name> does not exist!`",
  [2m  â”‚[0m         "+- For large databases, this can take considerable time (e.g., ~1 hour for an 11GB database, transferring 15GB of data)",
  [2m  â”‚[0m         "+- This method uploads all tables and data from your local database to MotherDuck",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-Once both databases are attached, you can copy tables using standard SQL:",
  [2m  â”‚[0m         "+## Method 2: Connect and Attach Local Database",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-**Option A: Create new tables**",
  [2m  â”‚[0m         "-```sql",
  [2m  â”‚[0m         "-CREATE TABLE md:cloud_database.table_name AS",
  [2m  â”‚[0m         "-SELECT * FROM local_database.table_name;",
  [2m  â”‚[0m         "-```",
  [2m  â”‚[0m         "+First, connect to MotherDuck by attaching it to your local DuckDB session:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-**Option B: Use CREATE OR REPLACE for full refreshes**",
  [2m  â”‚[0m         " ```sql",
  [2m  â”‚[0m         "-CREATE OR REPLACE TABLE md:cloud_database.table_name AS",
  [2m  â”‚[0m         "-SELECT * FROM local_database.table_name;",
  [2m  â”‚[0m         "+-- From DuckDB CLI",
  [2m  â”‚[0m         "+.open md:",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+-- Or attach MotherDuck",
  [2m  â”‚[0m         "+ATTACH 'md:';",
  [2m  â”‚[0m         " ```",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-**Option C: Insert into existing tables**",
  [2m  â”‚[0m         "-```sql",
  [2m  â”‚[0m         "-INSERT INTO md:cloud_database.table_name",
  [2m  â”‚[0m         "-SELECT * FROM local_database.table_name;",
  [2m  â”‚[0m         "+When you first connect, you'll be prompted to authenticate via SSO. The system will provide a motherduck_token that you should save as an environment variable:",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+```bash",
  [2m  â”‚[0m         "+export motherduck_token='eyJhbGciOiJI..._Jfo'",
  [2m  â”‚[0m         " ```",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### 3. Migrate an Entire Database",
  [2m  â”‚[0m         "+## Method 3: Create Database and Copy Tables from Files",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-The fastest way to migrate an entire local database to MotherDuck is using the `CREATE DATABASE ... FROM` command:",
  [2m  â”‚[0m         "+If you have your data exported to files (like Parquet or CSV), you can create tables directly in MotherDuck from those files:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         " ```sql",
  [2m  â”‚[0m         "--- Attach local database",
  [2m  â”‚[0m         "-ATTACH 'local_database.ddb';",
  [2m  â”‚[0m         "+-- Create the database in MotherDuck",
  [2m  â”‚[0m         "+CREATE DATABASE so;",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "--- Connect to MotherDuck",
  [2m  â”‚[0m         "-ATTACH 'md:';",
  [2m  â”‚[0m         "+-- Create tables from Parquet files on S3",
  [2m  â”‚[0m         "+CREATE TABLE users AS",
  [2m  â”‚[0m         "+FROM 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/users.parquet';",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "--- Upload the entire database to MotherDuck",
  [2m  â”‚[0m         "-CREATE DATABASE cloud_database FROM local_database;",
  [2m  â”‚[0m         "+CREATE TABLE posts AS",
  [2m  â”‚[0m         "+FROM 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/posts.parquet';",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+-- Or from local files",
  [2m  â”‚[0m         "+CREATE TABLE population AS",
  [2m  â”‚[0m         "+FROM 'local_file.parquet';",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+-- Repeat for other tables",
  [2m  â”‚[0m         " ```",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-This single command copies all tables and data from your local database to MotherDuck.",
  [2m  â”‚[0m         "+This approach works well when:",
  [2m  â”‚[0m         "+- You have data already exported to cloud storage (S3, GCS, etc.)",
  [2m  â”‚[0m         "+- You want to selectively migrate specific tables",
  [2m  â”‚[0m         "+- You want to transform data during migration",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Complete Python Example",
  [2m  â”‚[0m         "+## Method 4: Copy Data Table-by-Table Using ATTACH",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-Here's a complete Python script for migrating data:",
  [2m  â”‚[0m         "+You can also use the standard SQL `CREATE TABLE AS SELECT` (CTAS) pattern to copy tables:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-```python",
  [2m  â”‚[0m         "-import duckdb",
  [2m  â”‚[0m         "+```sql",
  [2m  â”‚[0m         "+-- Attach your local DuckDB database",
  [2m  â”‚[0m         "+ATTACH 'my_local_data.duckdb' AS local_db;",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-# Create a local DuckDB connection",
  [2m  â”‚[0m         "-duck_con = duckdb.connect()",
  [2m  â”‚[0m         "+-- Connect to MotherDuck",
  [2m  â”‚[0m         "+ATTACH 'md:' AS motherduck;",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-# Attach your local database",
  [2m  â”‚[0m         "-duck_con.sql(\"ATTACH 'my_local_data.ddb' AS local_db;\")",
  [2m  â”‚[0m         "+-- Create database in MotherDuck (if needed)",
  [2m  â”‚[0m         "+CREATE DATABASE my_remote_db;",
  [2m  â”‚[0m         "+USE my_remote_db;",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-# Connect to MotherDuck (token from environment variable)",
  [2m  â”‚[0m         "-duck_con.sql(\"ATTACH 'md:my_cloud_db';\")",
  [2m  â”‚[0m         "+-- Copy tables from local to MotherDuck",
  [2m  â”‚[0m         "+CREATE TABLE population AS SELECT * FROM local_db.population;",
  [2m  â”‚[0m         "+CREATE TABLE sales AS SELECT * FROM local_db.sales;",
  [2m  â”‚[0m         "+```",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-# Use the cloud database",
  [2m  â”‚[0m         "-duck_con.sql(\"USE my_cloud_db\")",
  [2m  â”‚[0m         "+## Understanding DuckDB's Dual Execution Model",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-# Copy tables one by one",
  [2m  â”‚[0m         "-tables_to_migrate = ['customers', 'orders', 'products']",
  [2m  â”‚[0m         "+MotherDuck uses a \"dual execution\" model that intelligently decides whether to run queries locally or in the cloud:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-for table in tables_to_migrate:",
  [2m  â”‚[0m         "-    print(f\"Migrating {table}...\")",
  [2m  â”‚[0m         "-    duck_con.sql(f\"CREATE OR REPLACE TABLE {table} AS SELECT * FROM local_db.{table};\")",
  [2m  â”‚[0m         "-    print(f\"\u2713 {table} migrated successfully\")",
  [2m  â”‚[0m         "+- When both tables reside in MotherDuck, queries run entirely in the cloud",
  [2m  â”‚[0m         "+- When joining local files with cloud tables, DuckDB processes data where it makes most sense",
  [2m  â”‚[0m         "+- The system minimizes data movement by transferring only necessary intermediate data",
  [2m  â”‚[0m         "+- You can manually control execution location using the `md_run` parameter:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-# Verify the migration",
  [2m  â”‚[0m         "-for table in tables_to_migrate:",
  [2m  â”‚[0m         "-    count = duck_con.sql(f\"SELECT COUNT(*) FROM {table}\").fetchone()[0]",
  [2m  â”‚[0m         "-    print(f\"{table}: {count} rows\")",
  [2m  â”‚[0m         "+```sql",
  [2m  â”‚[0m         "+-- Force local execution",
  [2m  â”‚[0m         "+SELECT * FROM read_parquet('s3://bucket/file.parquet', md_run='local');",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+-- Force remote execution in MotherDuck",
  [2m  â”‚[0m         "+SELECT * FROM read_parquet('s3://bucket/file.parquet', md_run='remote');",
  [2m  â”‚[0m         " ```",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Understanding Dual Execution",
  [2m  â”‚[0m         "+**Performance example:** Querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to local execution that had to download the data first (36 seconds).",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-MotherDuck's **dual execution model** is what makes this migration seamless. When you run a query that involves both local and cloud data:",
  [2m  â”‚[0m         "+## Data Persistence Options",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-1. **Local scan**: The local data is read from your machine",
  [2m  â”‚[0m         "-2. **Minimal data transfer**: Only necessary data is sent to the cloud",
  [2m  â”‚[0m         "-3. **Cloud execution**: The final operations run in MotherDuck",
  [2m  â”‚[0m         "+DuckDB offers two modes:",
  [2m  â”‚[0m         "+1. **In-memory** (default): Data disappears when session ends - use `duckdb.connect(database=':memory:')`",
  [2m  â”‚[0m         "+2. **Persistent**: Data saved to a database file",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-This means you're not downloading or uploading entire tables unnecessarily\u2014the system intelligently optimizes data movement.",
  [2m  â”‚[0m         "+To create a persistent local database before uploading:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Migration Methods Comparison",
  [2m  â”‚[0m         "+```bash",
  [2m  â”‚[0m         "+# Start DuckDB with a database file",
  [2m  â”‚[0m         "+duckdb my_local_data.duckdb",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-| Method | Use Case | Command |",
  [2m  â”‚[0m         "-|--------|----------|---------|",
  [2m  â”‚[0m         "-| **Whole Database** | Migrate everything at once | `CREATE DATABASE cloud_db FROM local_db` |",
  [2m  â”‚[0m         "-| **Individual Tables** | Selective migration | `CREATE TABLE md:db.table AS SELECT * FROM local_db.table` |",
  [2m  â”‚[0m         "-| **Full Refresh** | Replace existing data | `CREATE OR REPLACE TABLE ...` |",
  [2m  â”‚[0m         "-| **Incremental Load** | Append new data | `INSERT INTO ... SELECT * FROM ...` |",
  [2m  â”‚[0m         "+# Or from Python",
  [2m  â”‚[0m         "+import duckdb",
  [2m  â”‚[0m         "+con = duckdb.connect(database='my_local_data.duckdb')",
  [2m  â”‚[0m         "+```",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Authentication Setup",
  [2m  â”‚[0m         "+Or attach an existing database if DuckDB is already running:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-Before migrating, ensure you have a MotherDuck token:",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-```bash",
  [2m  â”‚[0m         "-# Set your MotherDuck token as an environment variable",
  [2m  â”‚[0m         "-export MOTHERDUCK_TOKEN='your_token_here'",
  [2m  â”‚[0m         "+```sql",
  [2m  â”‚[0m         "+ATTACH DATABASE '/path/to/your/database.db' AS mydb;",
  [2m  â”‚[0m         " ```",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-You can get your token from the [MotherDuck web interface](https://app.motherduck.com/).",
  [2m  â”‚[0m         "+## Python Example",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Best Practices",
  [2m  â”‚[0m         "+Here's a complete Python example for migrating data using the relational API:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-1. **Start with a test migration**: Try migrating a small table first to verify your setup",
  [2m  â”‚[0m         "-2. **Use CREATE OR REPLACE**: This ensures clean, repeatable migrations",
  [2m  â”‚[0m         "-3. **Verify row counts**: Always check that source and destination counts match",
  [2m  â”‚[0m         "-4. **Consider data size**: This approach works well for datasets up to hundreds of GB. For very large tables (multi-TB), consider using incremental approaches with timestamps",
  [2m  â”‚[0m         "-5. **Track sync times**: Create a metadata table to record when migrations occurred",
  [2m  â”‚[0m         "+```python",
  [2m  â”‚[0m         "+import duckdb",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Real-World Performance",
  [2m  â”‚[0m         "+# Connect to persistent local database",
  [2m  â”‚[0m         "+con = duckdb.connect(database='my_local_data.duckdb')",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-According to MotherDuck's internal usage, this approach can replicate approximately **150GB of data in about 10 minutes**, making it practical for regular sync operations.",
  [2m  â”‚[0m         "+# Load data into local database",
  [2m  â”‚[0m         "+population_relation = con.read_csv(\"https://bit.ly/3KoiZR0\")",
  [2m  â”‚[0m         "+population_relation.to_table(\"population\")  # Persist as table",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## When to Use This Approach",
  [2m  â”‚[0m         "+# Connect to MotherDuck",
  [2m  â”‚[0m         "+con.sql(\"ATTACH 'md:my_cloud_db'\")",
  [2m  â”‚[0m         "+con.sql(\"USE my_cloud_db\")",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-\u2705 **Good for:**",
  [2m  â”‚[0m         "-- Migrating local development databases to production",
  [2m  â”‚[0m         "-- Regular scheduled syncs (every few hours)",
  [2m  â”‚[0m         "-- Complete table refreshes",
  [2m  â”‚[0m         "-- Datasets from a few MB to hundreds of GB",
  [2m  â”‚[0m         "+# Copy table to MotherDuck",
  [2m  â”‚[0m         "+con.sql(\"CREATE TABLE population AS SELECT * FROM my_local_data.population\")",
  [2m  â”‚[0m         "+```",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-\u274c **Not ideal for:**",
  [2m  â”‚[0m         "-- Real-time, continuous replication",
  [2m  â”‚[0m         "-- Extremely large tables (10+ TB)",
  [2m  â”‚[0m         "-- High-frequency syncs (every few seconds)",
  [2m  â”‚[0m         "-- When you only need to sync changes (use CDC tools instead)",
  [2m  â”‚[0m         "+## Best Practices",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Sharing Your Migrated Data",
  [2m  â”‚[0m         "+1. **For small to medium databases**: Use Method 1 (single command upload) for simplicity",
  [2m  â”‚[0m         "+2. **For large databases**: Consider exporting to Parquet files on cloud storage first, then use Method 3 to leverage MotherDuck's high-bandwidth cloud connections",
  [2m  â”‚[0m         "+3. **For selective migration**: Use Method 4 to copy only specific tables",
  [2m  â”‚[0m         "+4. **Set environment variable**: Always export your `motherduck_token` to avoid repeated authentication",
  [2m  â”‚[0m         "+5. **Test connection first**: Verify you can connect to MotherDuck before starting large migrations",
  [2m  â”‚[0m         "+6. **Monitor progress**: Large uploads can take time; plan accordingly",
  [2m  â”‚[0m         "+7. **Leverage cloud proximity**: MotherDuck's cloud infrastructure provides high-bandwidth connections to cloud storage, making it faster to load from S3/GCS than from local files",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-Once your data is in MotherDuck, you can easily share it with your team:",
  [2m  â”‚[0m         "+## Working with Extensions",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "+If you need to read from cloud storage, make sure to install and load the necessary extensions:",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         " ```sql",
  [2m  â”‚[0m         "--- Create a shareable snapshot",
  [2m  â”‚[0m         "-CREATE SHARE FROM cloud_database;",
  [2m  â”‚[0m         "-```",
  [2m  â”‚[0m         "+-- Install httpfs extension for S3 access",
  [2m  â”‚[0m         "+INSTALL httpfs;",
  [2m  â”‚[0m         "+LOAD httpfs;",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-This returns a share URL that colleagues can attach to query your data without duplicating it.",
  [2m  â”‚[0m         "+-- Configure for public S3 bucket",
  [2m  â”‚[0m         "+SET s3_region='us-east-1';",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "+-- For private buckets, use CREATE SECRET",
  [2m  â”‚[0m         "+CREATE SECRET (",
  [2m  â”‚[0m         "+    TYPE S3,",
  [2m  â”‚[0m         "+    PROVIDER credential_chain",
  [2m  â”‚[0m         "+);",
  [2m  â”‚[0m         "+```",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         " ## Sources",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-- [DuckDB & MotherDuck for Beginners: Your Ultimate Guide](https://motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide) - Overview of DuckDB architecture and MotherDuck connection basics",
  [2m  â”‚[0m         "-- [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained) - Technical deep-dive into MotherDuck's dual execution architecture",
  [2m  â”‚[0m         "-- [Hybrid Analytics: Query Local & Cloud Data Instantly](https://motherduck.com/learn-more/hybrid-analytics-guide) - Guide to joining local and cloud data in hybrid queries",
  [2m  â”‚[0m         "-- [Using MotherDuck at MotherDuck: Loading Data from Postgres with DuckDB](https://motherduck.com/blog/pg%20to%20motherduck%20at%20motherduck) - Real-world example of database migration patterns",
  [2m  â”‚[0m         "-- [CSV Files Persist: DuckDB Solution](https://motherduck.com/blog/csv-files-persist-duckdb-solution) - Demonstrates the `CREATE DATABASE FROM` command for uploading local databases to MotherDuck",
  [2m  â”‚[0m         "+- [DuckDB & MotherDuck for Beginners: Your Ultimate Guide](https://motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide) - Comprehensive guide covering DuckDB basics and MotherDuck integration, including how to connect with `ATTACH 'md:'`, data persistence with `.to_table()`, and the dual execution model",
  [2m  â”‚[0m         "+- [Exploring StackOverflow with DuckDB on MotherDuck (Part 2)](https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2) - Tutorial demonstrating the `CREATE DATABASE FROM CURRENT_DATABASE()` method and table creation from Parquet files on S3, with real-world timing examples",
  [2m  â”‚[0m         "+- [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained) - Deep dive into MotherDuck's architecture and dual execution model for hybrid local/cloud queries, with performance comparisons and manual execution control",
  [2m  â”‚[0m         "+- [DuckDB Tutorial For Beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners) - Covers DuckDB fundamentals including database persistence with `ATTACH DATABASE`, working with various file formats, and extensions for cloud storage access",
  [2m  â”‚[0m         "+- [DuckDB Python Quickstart (Part 1)](https://motherduck.com/learn-more/duckdb-python-quickstart-part1) - Details on connecting to persistent databases, the `to_table()` method for converting relations to tables, reading files with `read_csv()` and `read_parquet()`, and MotherDuck integration via Python"
  [2m  â”‚[0m       ]
  [2m  â”‚[0m     }
  [2m  â”‚[0m   ]
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! I've successfully completed the task. Here's what I did:
  [94mâ”‚[0m 
  [94mâ”‚[0m ## Summary
  [94mâ”‚[0m 
  [94mâ”‚[0m I searched through the `.kurt/sources/` directory to find relevant information about migrating data from a local DuckDB database to MotherDuck. I found and read several key documents:
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. **DuckDB & MotherDuck for Beginners: Your Ultimate Guide** - Covered connection basics and the dual execution model
  [94mâ”‚[0m 2. **Exploring StackOverflow with DuckDB on MotherDuck (Part 2)** - Provided the crucial `CREATE DATABASE FROM CURRENT_DATABASE()` command
  [94mâ”‚[0m 3. **Bringing DuckDB to the Cloud: Dual Execution Explained** - Explained the architecture and performance considerations
  [94mâ”‚[0m 4. **DuckDB Tutorial For Beginners** - Covered database persistence and file operations
  [94mâ”‚[0m 5. **DuckDB Python Quickstart (Part 1)** - Detailed Python integration and data loading methods
  [94mâ”‚[0m 
  [94mâ”‚[0m ## Answer Written
  [94mâ”‚[0m 
  [94mâ”‚[0m I synthesized the information into a comprehensive markdown document at `/tmp/answer_without_kg_6.md` that covers:
  [94mâ”‚[0m 
  [94mâ”‚[0m - **4 different migration methods** (from simplest to most granular)
  [94mâ”‚[0m - **Step-by-step instructions** with code examples
  [94mâ”‚[0m - **Understanding of dual execution** and how it optimizes data transfer
  [94mâ”‚[0m - **Python examples** for programmatic migration
  [94mâ”‚[0m - **Best practices** for different scenarios
  [94mâ”‚[0m - **Performance insights** from real-world usage
  [94mâ”‚[0m - **Proper source citations** with descriptions of what each source contributed
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer is complete, accurate, and grounded in the source material from the MotherDuck documentation.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 6,714 | Cost: $0.4670
     Cumulative: 6,714 tokens | $0.4670
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent's message provides a comprehensive summary of the completed task and does not pose any questions or requests for user input.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 22 | Duration: 147.1s
â•‘    Tokens: 6,714 | Cost: $0.4670
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  LLM Judge score: 1.00

======================================================================
â“ Question 7/10
If I have a CSV on my laptop and a table in S3, what's the most efficient way to join them using MotherDuck?
======================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 7
â”‚ Search through the .kurt/sources/ directory to answer this question:

If I have a CSV on my laptop and a table in S3, what's the most efficient way to join them using MotherDuck?

Steps:
1. Search for the most relevant markdown files in .kurt/sources/
2. Read those files and synthesize the answer
3. Write a markdown answer (with a ## Sources section) to: /tmp/answer_without_kg_7.md
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I'll help you find the answer by searching through the MotherDuck documentation in .kurt/sources/.
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ CSV.*S3|S3.*CSV|join.*CSV|join.*S3

  [96mğŸ”§ TOOL:[0m Grep â†’ local.*file.*S3|S3.*local.*file|attach.*S3|read_csv

  [96mğŸ”§ TOOL:[0m Grep â†’ efficient.*join|join.*performance|join.*optimize
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/postgres-integration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/quacking-around-the-web.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/product-analytics-motherduck-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/index.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/community-and-open-source.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-summary-chapter2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/six-reasons-duckdb-slaps.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-duckdb-release.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/cultivating-growth-how-gardyn-scaled-its-data-operations-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/eu-region.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Pandas DataFrames.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vibe-coding-sql-cursor.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streaming-oracle-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-motherduck-for-business-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/geospatial-for-beginner-duckdb-spatial-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eleven.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ai-powered-bi-can-llms-really-generate-your-dashboards-ft-michael-driscoll.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/pandas-dataframes-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-lake-vs-data-warehouse-vs-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data pipeline.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/dataset.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/auto inference.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/ETL.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/CSV.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-summary-chapter2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me search more specifically for hybrid execution and attach concepts:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ hybrid.*execution|dual.*execution|attach.*database

  [96mğŸ”§ TOOL:[0m Grep â†’ local.*cloud|cloud.*local|laptop.*cloud
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-bootstrap-a-data-warehouse-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/faster-data-pipelines-development-with-mcp-and-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/build-bigger-with-small-ai-running-small-models-locally.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-duck-in-the-hand-is-worth-two-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-new-paradigm-for-data-visualization-with-just-sql-markdown.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/quacking-around-the-web.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/react-components-testing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/sql-duckdb-book-form.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-ga-produck-tour.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/product-analytics-motherduck-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me read the most relevant files about hybrid execution, S3 querying, and CSV handling:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     "content": "---\ntitle: hybrid-analytics-guide\ncontent_type: tutorial\nsource_url: https://motherduck.com/learn-more/hybrid-analytics-guide\nindexed_at: '2025-11-25T09:57:23.352209'\ncontent_hash: 91344eaa7ee54322\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO LEARN](https://motherduck.com/learn-more/)\n\n# Hybrid Analytics: Query Local & Cloud Data Instantly\n\n12 min readBY\n\n[Aditya Somani](https://motherduck.com/authors/aditya-aomani/)\n\n![Hybrid Analytics: Query Local & Cloud Data Instantly](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FZero_latency_e8da68f055.png&w=3840&q=75)\n\nAre your analytics queries timing out in Postgres? Does your laptop grind to a halt when you try to analyze a large CSV file? If you feel caught between the memory limits of local tools and the spiraling costs of cloud data warehouses, you're not alone. The traditional divide between local development and cloud-scale analytics creates friction, slows down projects, and drains budgets.\n\nBut there is a modern, more efficient way. Imagine running complex SQL queries on a 20 GB Parquet file directly on your laptop, without a flicker of memory strain. Picture joining a local spreadsheet with a massive table in Amazon S3 using a single, elegant command. This is the reality of the **hybrid analytics workflow**, a powerful approach that unifies your local machine and the cloud into a single, unified data environment.\n\nThis guide is your definitive resource for understanding and implementing this new workflow. We'll break down the core concepts, provide practical examples, and show you how to solve the most common data bottlenecks for good. You will learn why traditional databases fail for analytics, how to analyze datasets larger than your RAM, and how the power of \"Dual Execution\" lets you instantly join local files with cloud data, all without the cost and complexity of managing a server cluster.\n\n## Why Are My Analytics Queries on Postgres & MySQL Timing Out?\n\nIf your analytical queries on a database like PostgreSQL or MySQL are consistently slow or timing out, you've hit a fundamental architectural limit. These databases are masterpieces of engineering for **[Online Transaction Processing (OLTP)](https://estuary.dev/blog/postgres-to-motherduck/)**. These are the small, fast operations that power applications, like creating a user or updating an order. However, this same design becomes a major bottleneck for **[Online Analytical Processing (OLAP)](https://en.wikipedia.org/wiki/Online_analytical_processing)**, which involves complex queries that scan and aggregate vast amounts of data.\n\nThe root of the problem is their **[row-oriented](https://en.wikipedia.org/wiki/Data_orientation)** storage model. A row-store keeps all data for a single record together on disk, which is efficient for retrieving an entire user profile. But for an analytical query that only needs to sum the `sale_amount` column across millions of rows, a row-store is forced to read every single column for every single row. This wastes an enormous amount of I/O and CPU cycles on data it doesn't need, which is the primary reason your dashboards are slow and your queries fail.\n\nThe next logical step is to move analytical workloads to a system built on a **[columnar architecture](https://motherduck.com/learn-more/columnar-storage-guide/)**. A columnar database organizes data by column, storing all values for `sale_amount` together. When your query asks for the sum of `sale_amount`, the database reads _only_ that column, greatly reducing I/O and speeding up performance by orders of magnitude. Modern analytical engines like DuckDB couple this with **[vectorized query execution](https://15721.courses.cs.cmu.edu/spring2024/notes/06-vectorization.pdf)**, a technique that processes data in large batches or \"vectors\" instead of row-by-row. This method fully utilizes modern CPU capabilities to perform a single instruction on multiple data points simultaneously, leading to large gains in computational efficiency.\n\n| Architecture | Row-Oriented (OLTP) | Column-Oriented (OLAP) |\n| --- | --- | --- |\n| **Primary Use Case** | Application backends, frequent small updates. | Business intelligence, data analytics, large scans. |\n| **Data Storage** | All data for a single record is stored together. | All data for a single column is stored together. |\n| **Query Example** | `SELECT * FROM users WHERE user_id = 123;` (Fast) | `SELECT SUM(sale_amount) FROM sales;` (Slow) |\n| **Best For** | Fast writes and single-record lookups. | Fast aggregations and filtering on specific columns. |\n\nBy offloading analytics to a purpose-built columnar engine, you let your transactional database continue to excel at what it does best while your analytical queries run in a [high-speed, optimized environment](https://motherduck.com/blog/postgres-duckdb-options/).\n\n## How Can I Analyze a 20 GB Parquet File on My Laptop Without It Crashing?\n\nThe second major bottleneck for data professionals is local memory. Trying to load a 20 GB file into a traditional in-memory library like Pandas on a 16 GB laptop will almost certainly result in a `MemoryError`. This happens because these tools must load the entire dataset into your computer's RAM before they can begin processing.\n\nThe modern solution is **out-of-core processing**, a strategy where data is processed in manageable chunks directly from disk without ever being fully loaded into RAM. This is the key feature of engines like DuckDB, which uses a [streaming execution model to handle datasets far larger than your available memory](https://duckdb.org/2024/07/09/memory-management.html). If a complex operation requires more memory than is available (by default, 80% of your system's RAM), DuckDB can \"spill\" intermediate results to a temporary file on disk, ensuring your query completes without crashing.\n\nThis capability turns your laptop into a surprisingly powerful analytics workstation. With an out-of-core engine, analyzing that 20 GB file becomes straightforward. You can simply use SQL to query it directly from a Python script or Jupyter notebook.\n\n```python\nCopy code\n\nimport duckdb\n\n# This query runs out-of-core, never loading the full 20GB file into RAM.\n# DuckDB's streaming engine processes the file in chunks.\nresult = duckdb.sql(\"\"\"\n  SELECT\n      product_category,\n      AVG(sale_price) as avg_price\n  FROM read_parquet('large_sales_dataset.parquet')\n  WHERE region = 'North America'\n  GROUP BY product_category\n  ORDER BY avg_price DESC;\n\"\"\").arrow()\n\nprint(result)\n```\n\nThis simple, powerful approach allows you to perform heavy-duty data analysis on your local machine, iterating quickly without waiting for a cloud cluster to spin up or worrying about memory crashes.\n\n## How Can I Join Local CSVs with Cloud Data in a Single Query?\n\nThis is where the hybrid workflow really shows its strength. Joining a local CSV of fresh sales data with a historical customer table in an S3 bucket has traditionally been a major challenge. The problem is \"data gravity,\" since it's [slow and expensive to move large amounts of data](https://motherduck.com/learn-more/no-etl-query-raw-files/). A naive query would have to either upload your entire local file to the cloud or download the entire cloud table to your laptop, both of which are very inefficient.\n\nMotherDuck solves this with an innovative query planner called **Dual Execution**. It treats your laptop and the MotherDuck cloud as two nodes in a single distributed system. When you run a hybrid query, the planner intelligently breaks it down and pushes computation to where the data lives, minimizing data transfer.\n\n> \"The most innovative feature of MotherDuck's architecture is its dual execution model. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\" - [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained/)\n\nImagine you want to join a local CSV of new product pricing with a massive sales table in MotherDuck, but only for a specific product. Instead of moving entire tables, the Dual Execution planner does the following:\n\n1. **Local Scan:** It scans the small pricing CSV on your laptop.\n2. **Filter Locally:** It applies the filter for the specific product on your machine.\n3. **Transfer Minimal Data:** It sends only the single, filtered pricing row (a few bytes of data) to the cloud.\n4. **Join in the Cloud:** It performs the final join against the massive sales table in the MotherDuck cloud.\n\nThis process can reduce network traffic by orders of magnitude compared to traditional methods, turning a difficult query into an interactive one. You can see this in action with a simple `EXPLAIN` statement, which [shows which parts of the query run locally `(L)` and which run remotely `(R)`](https://motherduck.com/docs/key-tasks/running-hybrid-queries/).\n\n## Why Are Data Engineers So Excited About Dual-Execution Engines?\n\nData engineers are excited because dual-execution engines solve one of their biggest challenges: the \"it worked on my machine\" problem. In traditional setups, the tools and data used for local development are often completely different from the production cloud environment, leading to bugs and deployment failures.\n\nMotherDuck eliminates this by using the **exact same DuckDB engine** both locally and in the cloud. A query that works on your laptop is guaranteed to work in the cloud, creating a direct path from development to production. This greatly improves developer experience (DX) and accelerates iteration cycles.\n\nThis architecture provides three key benefits:\n\n1. **Faster Development:** Engineers can build and test pipelines with the zero-latency feedback of local development before scaling to the cloud.\n2. **Lower Costs:** By using the free, powerful compute on users' laptops and minimizing data transfer, this model significantly reduces cloud bills.\n3. **Better Collaboration:** It transforms the traditionally \"single-player\" DuckDB into a [\"multiplayer\" platform](https://motherduck.com/videos/339/bringing-duckdb-to-the-cloud-dual-execution-explained/) where teams can share databases and work from a single source of truth.\n\nThis powerful combination of local speed and cloud scale is backed by real-world success.\n\n> **[Case Study: Finqore's 60x Pipeline Acceleration](https://motherduck.com/case-studies/)**\n> Finqore, a financial technology company, was struggling with an 8-hour data pipeline built on Postgres. By migrating to a hybrid workflow with MotherDuck, they were able to use DuckDB's performance and MotherDuck's serverless scale to transform that pipeline into an **8-minute workflow**, a 60x improvement that unlocked real-time data exploration for their team.\n\n## How Can I Run SQL Analytics Without Managing Clusters?\n\nThe operational overhead of provisioning, scaling, and maintaining server clusters is a major drain on data teams. **[Serverless SQL analytics](https://motherduck.com/docs/concepts/architecture-and-capabilities/)** removes this burden entirely. Instead of managing infrastructure, you simply write and run your queries, and the platform handles the rest.\n\nThis model is not only simpler but also much more cost-effective for the \"spiky\" workloads typical of analytics. You pay only for the compute you actually use, avoiding the massive costs of idle clusters.\n\n| Feature / Scenario | Traditional Cloud Warehouse (e.g., Snowflake/BigQuery) | Hybrid Approach (Local + Serverless) |\n| --- | --- | --- |\n| **Development & Prototyping** | Billed for active compute time, even for small test queries. | **Free.** Uses local machine resources with no cloud costs. |\n| **Pricing Model** | Complex credits or per-TB scanned, which is hard to predict. | Simple, usage-based billing; pay only for queries you run. |\n| **Idle Compute** | Billed for provisioned clusters, even when idle, which can create a costly [idle tax on short-running queries](https://motherduck.com/learn-more/reduce-snowflake-costs-duckdb). | **No cost.** Serverless architecture has no idle compute. |\n| **Hardware Requirement** | Entirely reliant on expensive, provisioned cloud hardware. | Leverages powerful, existing local hardware (laptops, servers). |\n\nBy adopting a serverless, hybrid approach, you can reduce your cloud data warehouse costs while empowering your team with a faster, more flexible workflow.\n\n## What Lightweight SQL Solutions Can Be Integrated with Jupyter Notebooks?\n\nFor data scientists who work primarily in Jupyter, several excellent tools bring the power of SQL directly into the notebook environment. The most powerful option for serious analytics is the **DuckDB engine** itself. It can query Pandas DataFrames, Arrow tables, and large local files directly with full SQL support. Its performance on analytical queries is excellent in the embedded space.\n\nFor comparison, while Python's standard library includes **SQLite**, it's a row-based OLTP engine and is significantly slower than DuckDB for analytical queries. An independent benchmark found DuckDB to be **[12-35 times faster](https://motherduck.com/learn-more/duckdb-vs-sqlite-databases/)** for aggregations on a 100-million-row dataset. For any analytical task in a notebook, DuckDB is the clear choice for performance and scalability.\n\nAnother popular tool is **[JupySQL](https://jupysql.ploomber.io/)**, which provides convenient SQL \"magics\" (`%sql`, `%%sql`) that turn a notebook cell into a SQL editor. It connects to various backends, including DuckDB, making it great for quick, ad-hoc exploration.\n\n## How Can I Keep Queries on My Laptop in Sync With the Cloud?\n\nThe key to keeping local and cloud queries synchronized is **single-engine semantics**. Because MotherDuck uses the same DuckDB engine on your laptop and in the cloud, a query validated locally is guaranteed to behave identically in production. This eliminates the most common source of dev-prod drift.\n\nA modern dev-to-prod workflow combines this with tools like dbt (data build tool). An analyst can develop and test a dbt model on their laptop against local data for rapid iteration. Once validated, they can promote the model to run in the cloud against the full production dataset with a [single configuration change](https://motherduck.com/blog/dual-execution-dbt/), ensuring complete consistency. This workflow combines the speed of local development with the scale and reliability of a production cloud environment.\n\n## When Should I Not Use This Hybrid Approach?\n\nWhile the hybrid, in-process model is very useful for analytics, it's not a solution for every problem. It's crucial to understand its limitations to know when a different architecture is needed.\n\n1. **High-Concurrency Applications:** An in-process engine like DuckDB is not designed to be a client-server database serving hundreds of simultaneous application users. For powering a public-facing web application, a traditional OLTP database like PostgreSQL or a distributed SQL database is the right choice.\n2. **Real-Time, High-Volume Ingestion:** If your use case involves ingesting thousands of events per second in real-time, a specialized streaming platform or real-time OLAP database would be more suitable.\n3. **Truly Massive, Multi-Terabyte Workloads:** While DuckDB can handle surprisingly large datasets on a single node, if your active working set is in the tens or hundreds of terabytes, you've reached the scale where a distributed cloud data warehouse becomes necessary to parallelize work across a large cluster.\n\nThe beauty of the MotherDuck ecosystem is that it provides a smooth way to scale up. You can start with a local-first, hybrid workflow and, as your concurrency or data scale needs grow, easily push more of the workload to the MotherDuck cloud without changing your core tools or SQL logic.\n\n### TABLE OF CONTENTS\n\n[Why Are My Analytics Queries on Postgres & MySQL Timing Out?](https://motherduck.com/learn-more/hybrid-analytics-guide/#why-are-my-analytics-queries-on-postgres-mysql-timing-out)\n\n[How Can I Analyze a 20 GB Parquet File on My Laptop Without It Crashing?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-analyze-a-20-gb-parquet-file-on-my-laptop-without-it-crashing)\n\n[How Can I Join Local CSVs with Cloud Data in a Single Query?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-join-local-csvs-with-cloud-data-in-a-single-query)\n\n[Why Are Data Engineers So Excited About Dual-Execution Engines?](https://motherduck.com/learn-more/hybrid-analytics-guide/#why-are-data-engineers-so-excited-about-dual-execution-engines)\n\n[How Can I Run SQL Analytics Without Managing Clusters?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-run-sql-analytics-without-managing-clusters)\n\n[What Lightweight SQL Solutions Can Be Integrated with Jupyter Notebooks?](https://motherduck.com/learn-more/hybrid-analytics-guide/#what-lightweight-sql-solutions-can-be-integrated-with-jupyter-notebooks)\n\n[How Can I Keep Queries on My Laptop in Sync With the Cloud?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-keep-queries-on-my-laptop-in-sync-with-the-cloud)\n\n[When Should I Not Use This Hybrid Approach?](https://motherduck.com/learn-more/hybrid-analytics-guide/#when-should-i-not-use-this-hybrid-approach)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\n## FAQS\n\n### My analytics queries on Postgres / MySQL are timing out. What\u2019s the next step up?\n\nThe next step is to move your analytical workloads from a row-oriented (OLTP) database like Postgres to a purpose-built columnar (OLAP) engine like DuckDB. Columnar engines are designed for analytics and are orders of magnitude faster for large scans and aggregations because they only read the specific columns needed for a query.\n\n### How can I analyze a 20 GB Parquet file on my laptop without it crashing?\n\nUse a query engine that supports \"out-of-core\" processing, like DuckDB. It processes data in manageable chunks directly from your disk instead of loading the entire file into RAM. This allows you to analyze datasets that are significantly larger than your computer's available memory.\n\n### Why is it so hard to join local CSVs with cloud data?\n\nThe main challenge is \"data gravity\"\u2014it's slow and expensive to move large datasets across a network. Traditional tools force you to either upload your entire local file or download the entire cloud table. A modern hybrid platform with a dual-execution engine solves this by intelligently minimizing data transfer.\n\n### Can I mix local datasets and S3 data in a single query?\n\nYes. With a hybrid analytics platform like MotherDuck, you can write a single SQL query that joins local files (e.g., CSVs) with data in cloud storage (e.g., Parquet files in S3). The system treats them as if they exist in one unified environment, abstracting away their physical location.\n\n### How can I run SQL analytics without managing clusters?\n\nAdopt a serverless SQL analytics platform. These services handle all infrastructure provisioning, scaling, and maintenance for you. You simply run your queries and pay only for the compute you use, which eliminates the high cost and operational overhead of managing idle clusters.\n\n### Why are data engineers excited about dual-execution engines?\n\nDual-execution engines solve the \"it worked on my machine\" problem by using the exact same query engine on the developer's laptop and in the cloud. This guarantees consistency, speeds up development cycles, lowers costs by leveraging local compute, and enables better team collaboration on a single source of truth.\n\n### What lightweight SQL solutions can be integrated with Jupyter notebooks?\n\nDuckDB is the leading lightweight SQL solution for Jupyter. It can be installed via pip and can directly query Pandas DataFrames, Arrow tables, and large local files with full SQL support. It is significantly faster for analytical queries than other embedded options like SQLite.\n\n### How can I keep queries on my laptop in sync with the cloud?\n\nUse a platform that offers single-engine semantics, like MotherDuck. Because it runs the same DuckDB engine locally and in the cloud, a query validated on your laptop is guaranteed to behave identically in production. This eliminates dev-prod drift and ensures consistency.\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 212,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 212
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     "content": "---\ntitle: bringing-duckdb-to-the-cloud-dual-execution-explained\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained\nindexed_at: '2025-11-25T20:45:20.295187'\ncontent_hash: ab4030806c409492\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nBringing DuckDB to the Cloud: Dual Execution Explained - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Bringing DuckDB to the Cloud: Dual Execution Explained](https://www.youtube.com/watch?v=2eVNm4A8sLw)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=2eVNm4A8sLw&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 1:08:11\n\n\u2022Live\n\n\u2022\n\nYouTubeQuack & CodeMotherDuck Features\n\n# Bringing DuckDB to the Cloud: Dual Execution Explained\n\n2024/06/28\n\nBringing DuckDB's analytical power to the cloud requires more than just running it on a server. While DuckDB excels at processing data on a local machine, the dynamics of data analysis change when dealing with cloud-scale data, collaboration, and shared resources. This shift introduces challenges around [security](https://motherduck.com/learn-more/secure-startup-data-warehouse/), concurrent access, and performance, which is where a purpose-built cloud architecture becomes essential.\n\nMotherDuck, a modern cloud data warehouse, is built on DuckDB to solve these challenges. It extends DuckDB\u2019s capabilities without forking the open source project, creating a hybrid system that intelligently balances local and cloud computing. In a conversation with MotherDuck founding engineer Stephanie, she explained the architecture and the innovative query model, known as **dual execution**, that makes this possible.\n\n* * *\n\n## The MotherDuck Architecture: Beyond Hosted DuckDB\n\nA common misconception is that MotherDuck is simply DuckDB hosted in the cloud. The reality is a more sophisticated system designed to overcome the limitations of using a locally-optimized engine in a distributed environment. MotherDuck\u2019s architecture is built on three key components and a core philosophy of not forking DuckDB.\n\nInstead of maintaining a separate version, MotherDuck leverages DuckDB\u2019s powerful **extension system**. This allows MotherDuck to add new capabilities at multiple layers, including the SQL parser, optimizer, and storage interface, while staying current with the latest open source DuckDB releases. This tight integration means users benefit from the rapid innovation of the DuckDB community almost immediately.\n\nThe architecture can be broken down into three main layers:\n\n1. **The Client Extension**: This is how MotherDuck integrates with the DuckDB ecosystem. Whether a user is working in the CLI, a Python script, or a JDBC connection, a simple `ATTACH` command connects their local DuckDB instance to the MotherDuck cloud. This extension is even used to run DuckDB in the browser via WASM for the MotherDuck UI, enabling client-side processing to reduce latency and cloud compute costs.\n2. **The Compute Layer**: In the cloud, queries are processed by containerized DuckDB instances, fondly called \"ducklings.\" These compute resources are scaled based on user needs, providing the necessary CPU and memory to handle complex analytical queries on large datasets.\n3. **The Storage Layer**: DuckDB's native file format is optimized for a single writer on a local file system. This model is not well-suited for the cloud, where multiple users need to read and write to the same database concurrently. To solve this, MotherDuck implemented a differential storage system that maps a logical database file to append-only snapshot layers in cloud object storage. This design is cloud-friendly, enabling efficient in-place updates and forming the foundation for features like database sharing and time travel.\n\n* * *\n\n## Enabling Collaboration with Database Sharing and Secure Credentials\n\nThis unique architecture transforms DuckDB from a \"single-player\" tool into a collaborative \"multiplayer\" platform. One of the most significant advantages is **database sharing**. Instead of emailing SQL scripts or passing around large database files, team members within the same organization can grant query access to their databases. This streamlines collaboration, ensuring everyone works from a consistent and up-to-date version of the data.\n\nSecurity is another critical aspect of any cloud data platform. MotherDuck provides a **centralized Secret Manager**, co-designed with the DuckDB team. Users can create persistent, encrypted secrets for accessing external data sources like AWS S3, Google Cloud Storage, or Delta Lake. Once a secret is created in MotherDuck, it can be reused across different clients and sessions without needing to expose or reconfigure credentials on each local machine.\n\n* * *\n\n## Understanding Dual Execution\n\nThe most innovative feature of MotherDuck's architecture is its **dual execution model**. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\n\nThe process was demonstrated with a query joining two tables. When both tables reside in a MotherDuck database, the query is executed entirely in the cloud. An `EXPLAIN` plan for this query reveals that all operators, from the table scans to the final join, are marked as **remote**. The only local operation is downloading the final result set to the client.\n\nThe real power of dual execution becomes apparent when a query involves both local and cloud data. In the demonstration, one of the remote tables was copied to a local Parquet file. The query was then modified to join this local file with the table that remained in the cloud.\n\nThe `EXPLAIN` plan for this new query showed a mix of local and remote operations. The scan of the Parquet file and its associated processing happened **locally**, leveraging the user's machine. The scan of the cloud table happened **remotely** on MotherDuck's compute. The system then efficiently transferred only the necessary intermediate data to complete the join. This hybrid approach avoids needlessly uploading the local file to the cloud or downloading the entire remote table, resulting in faster and more efficient queries.\n\n* * *\n\n## Taking Control with Manual Execution Overrides\n\nWhile MotherDuck's optimizer is designed to make the most efficient choice automatically, some situations require manual control. For this, users can use the `md_run` parameter within scan functions like `read_parquet()` or `read_csv()`.\n\nBy setting `md_run = 'local'`, a user can force the scan of a remote S3 file to be executed on their local client. This involves downloading the data, which might be desirable if the data needs to be processed near the user's location or if they prefer using local credentials. Conversely, setting `md_run = 'remote'` forces the operation to run in the MotherDuck cloud.\n\nThis control is especially useful for performance tuning. A direct comparison showed that querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds). By pushing the computation to the data's location, MotherDuck minimizes network I/O and delivers results more quickly.\n\n* * *\n\n## A Symbiotic Future for Local and Cloud Analytics\n\nMotherDuck's architecture is a thoughtful extension of DuckDB's core principles, adapting its local-first power for the demands of the cloud. The dual execution model provides a flexible and efficient bridge between a user's laptop and cloud data, optimizing workloads in a way that pure-cloud or pure-local systems cannot.\n\nThis relationship is symbiotic. As the largest production user of DuckDB, MotherDuck continuously pushes the engine to its limits, uncovering opportunities for improvement and contributing enhancements back to the open source project. This collaboration ensures that both DuckDB and MotherDuck will continue to evolve, offering a powerful and seamless analytical experience that spans from local exploration to cloud-scale production.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Ficeberg_video_thumbnail_adfdabe3d6.jpg&w=3840&q=75)\\\\\n\\\\\n46:26](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n[2025-06-12](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n### [Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse)\n\nBuilding a data stack means choosing between easy SaaS and complex open-source. Apache Iceberg is a middle ground, but its catalog is a hurdle. New tools now simplify using Iceberg with DuckDB to create a flexible, local data lakehouse.\n\nMotherDuck Features\n\n[![\"Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FG_Se_B_Sox_AW_Fg_HD_f4fddaa9ab.jpg&w=3840&q=75)](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n[2025-04-23](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n### [Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly)\n\nHamilton Ulmer shares insights from MotherDuck's Instant SQL Mode, exploring how real-time query result previews eliminate the traditional write-run-debug cycle through client-side parsing and DuckDB-WASM caching.\n\nSQL\n\nTalk\n\nMotherDuck Features\n\n[![\"DuckDB: Run dbt build with sub-second execution times\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_03_26_at_3_40_16_PM_c6e3a8096f.png&w=3840&q=75)\\\\\n\\\\\n24:06](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n[2025-03-13](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n### [DuckDB: Run dbt build with sub-second execution times](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times)\n\nWhether you're new to DuckDB or looking to optimize your workflows, this session will provide practical insights to help you leverage its full potential.\n\nYouTube\n\nData Pipelines\n\ndbt\n\nMotherDuck Features\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 184,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 184
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md",
  [2m  â”‚[0m     "content": "---\ntitle: querying-data-from-s3-with-3-lines-in-your-terminal\ncontent_type: event\nsource_url: https://motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal\nindexed_at: '2025-11-25T20:44:10.612560'\ncontent_hash: 86949cc4a5628597\nhas_code_examples: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nQuerying Data From S3 With 3 Lines In Your Terminal - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Querying Data From S3 With 3 Lines In Your Terminal](https://www.youtube.com/watch?v=2Ug5HpWY4qs)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=2Ug5HpWY4qs&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 0:56\n\n\u2022Live\n\n\u2022\n\nYouTubeShort\n\n# Querying Data From S3 With 3 Lines In Your Terminal\n\n2024/01/10\n\nThis short from Mehdi Ouazza shows you how to use DuckDB to query data from S3 data lakes with 3 lines in your terminal.\n\n## Related Videos\n\n[![\"One data tool with all its dependencies: DuckDB and extensions\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fvideo_we_RMT_Aj_Udic_9e9f5c6d41.jpg&w=3840&q=75)\\\\\n\\\\\n0:00:55](https://motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions/)\n\n[2024-06-21](https://motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions/)\n\n### [One data tool with all its dependencies: DuckDB and extensions](https://motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions)\n\nLearn about DuckDB extensions, including the ability to query data in your AWS S3-powered data lake.\n\nYouTube\n\nShort\n\n[![\"No SQL standard for 7 years ?!\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fvideo_Joro_Ss_Keq_Sk_9a6deced1a.jpg&w=3840&q=75)\\\\\n\\\\\n0:00:47](https://motherduck.com/videos/no-sql-standard-for-7-years/)\n\n[2024-01-22](https://motherduck.com/videos/no-sql-standard-for-7-years/)\n\n### [No SQL standard for 7 years ?!](https://motherduck.com/videos/no-sql-standard-for-7-years)\n\nThis short of Mehdi with Alex Monahan discusses the lack of evolution of the SQL standard. \u201c(SQL) stands for Structured Query Language, not Standard Query Language\u201d\n\nYouTube\n\nShort\n\nSQL\n\n[![\"Simple way to convert CSV and Parquet files\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fvideo_Y_G_Xdbet9_Gk_6fc9d7bdd0.jpg&w=3840&q=75)\\\\\n\\\\\n0:01:00](https://motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files/)\n\n[2024-01-17](https://motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files/)\n\n### [Simple way to convert CSV and Parquet files](https://motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files)\n\nWant convert CSV to Parquet or vice versa? In this YouTube short, Mehdi shows you how to convert using DuckDB as a Swiss Army Knife to convert data formats.\n\nYouTube\n\nShort\n\nTutorial\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 130,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 130
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md",
  [2m  â”‚[0m     "content": "---\ntitle: taming-wild-csvs-with-duckdb-data-engineering\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering\nindexed_at: '2025-11-25T19:57:41.542448'\ncontent_hash: 6594cfafde3c18de\nhas_code_examples: true\nhas_step_by_step: true\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Taming Wild CSVs: Advanced DuckDB Techniques for Data Engineers\n\n2025/05/17 - 12 min read\n\nBY\n\n[Ryan Boyd](https://motherduck.com/authors/ryan-boyd/)\n\nIt's 2:30 AM. The office is empty. Your coffee's gone cold, and you're staring blankly at your screen as it mockingly displays: `Error: Could not convert string 'N/A' to INTEGER` on line 56,789. All you wanted was to import a \"simple\" CSV export from that upstream system. Sound familiar?\n\nWe've all been in CSV purgatory. That moment when what should be a five-minute task turns into a multi-hour ordeal because somewhere, buried deep in that innocent-looking file, lurks an inconsistent delimiter, a rogue quote, or my personal favorite\u2014columns that mysteriously appear and disappear like fish fry diving underwater to avoid being eaten by our duck friends.\n\nI've spent countless hours wrestling with problematic CSVs, but after discovering some of DuckDB's lesser-known features, those late-night CSV battles have become far less common. While DuckDB's automatic CSV reader is already impressively smart, knowing a few advanced techniques can save you from writing custom preprocessing scripts when things get messy.\n\nIn this guide, I'll share the DuckDB techniques that have repeatedly saved me from CSV hell:\n\n- How to diagnose what DuckDB actually thinks your [**CSV looks like**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#peeking-under-the-hood-sniffcsv)\n- Deep dive into the [**CSV sniffer**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#how-the-sniffer-works) and how it works under the hood\n- Ways to [**handle problematic rows**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#wrangling-the-errors) without aborting your entire import\n- Strategies for [**dealing with inconsistent schemas**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#handling-shifting-schemas-unionbyname) across files\n- [**Fine-tuning options**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#fine-tuning-detection-and-overrides) when auto-detection needs a helping hand\n- Real-world robustness: how DuckDB performs on a [**benchmark of messy CSVs**](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#duckdbs-csv-parser-champion-of-the-pollock-benchmark)\n\nLet's dive in and see if we can make your next CSV import session a little less...quackers.\n\n## **Peeking Under the Hood: sniff\\_csv**\n\nBefore attempting to load the data using DuckDB\u2019s auto-detection capabilities, it's incredibly useful to understand what DuckDB _thinks_ it's dealing with. Is it guessing the delimiter correctly? Did it detect the header? What types is it inferring? The [sniff\\_csv()](https://duckdb.org/docs/stable/data/csv/auto_detection.html) function is your reconnaissance tool here.\n\nInstead of blindly running `read_csv` and potentially hitting errors, run `sniff_csv` first. It analyzes a sample of the file and reports back the detected dialect, types, header presence, and more.\n\n**Let\u2019s imagine a file [events.csv](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/events.csv):**\n\n```txt\nCopy code\n\nEventDate|Organizer|City|Venue\n2022-03-01|OpenTech|San Francisco, CA|Moscone Center, Hall A\n2022-03-02|OpenTech|San Francisco, CA|Moscone Center, Hall B\n2022-03-03|OpenTech|San Francisco, CA|Moscone Center, Hall C\n```\n\n**Let's see what DuckDB makes of this:**\n\n```sql\nCopy code\n\nFROM sniff_csv('events.csv');\n```\n\nINFO: Using full http URLs\nFor brevity, we'll just include the filename in each of the example commands below. You can download the example files using the links (eg, how events.csv is linked above). However, you can also specify the full http URL instead because DuckDB supports the http protocol for retrieving CSV files.\n\n```sql\nCopy code\n\nFROM sniff_csv('http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/events.csv');\n```\n\n**You can also control how much of the file it samples:**\n\n```sql\nCopy code\n\nFROM sniff_csv('events.csv', sample_size=10000); -- Sample 10k rows\n```\n\n**Or sample the whole file (careful with huge files!):**\n\n```sql\nCopy code\n\nFROM sniff_csv('events.csv', sample_size=-1);\n```\n\n**The output provides a wealth of information in a single row table:**\n\n- `Delimiter`, `Quote`, `Escape`, `NewLineDelimiter`: The detected structural characters.\n- `SkipRows`: Number of rows it thinks should be skipped at the start.\n- `HasHeader`: Boolean flag if a header is detected.\n- `Columns`: A list of structs showing detected column names and types (e.g., `{'name': 'VARCHAR', 'age': 'BIGINT'}`).\n- `DateFormat`, `TimestampFormat`: Any special date/time formats it detected.\n- `Prompt`: This is extremely useful! It gives you a `read_csv` command _with_ all the detected options explicitly set. You can copy, paste, and modify this as needed.\n\nRunning `sniff_csv` first can save you significant guesswork when an import fails. If the detected `Delimiter` is wrong, or it thinks `HasHeader` is true when it isn't, you know exactly which options to override in your `read_csv` call.\n\n### **How the Sniffer Works**\n\nDuckDB's CSV sniffer actually operates through multiple phases to determine the best way to read your file:\n\n1. **Dialect Detection**: At the time of writing, sniffer tests 24 different combinations of dialect configurations (delimiters, quotes, escapes, newlines) to see which one creates the most consistent number of columns across rows.\n\n2. **Type Detection**: After determining the dialect, the sniffer analyzes the first chunk of data (2048 rows by default) to detect column types, trying to cast values from most to least specific types (SQLNULL \u2192 BOOLEAN \u2192 BIGINT \u2192 DOUBLE \u2192 TIME \u2192 DATE \u2192 TIMESTAMP \u2192 VARCHAR).\n\n3. **Header Detection**: The sniffer checks if the first valid line can be cast to the detected column types. If it can't, that line is considered a header.\n\n4. **Type Replacement**: If you specified column types, they override the sniffer's detected types.\n\n5. **Type Refinement**: The sniffer validates the detected types on more data using vectorized casting.\n\n\n**Here's a code example showing how to inspect what the sniffer sees in a more complex scenario:**\n\n```sql\nCopy code\n\n-- Examine what the sniffer detects with a larger sample size\nSELECT\n    Delimiter, Quote, Escape, SkipRows, HasHeader, DateFormat, TimestampFormat\nFROM sniff_csv('events.csv',  sample_size=50000);\n\n-- To see the detected column types\nSELECT unnest(Columns)\nFROM sniff_csv('events.csv');\n```\n\nWhen I was working with a dataset containing 20+ columns of mixed types, the `unnest(Columns)` trick was particularly helpful to see exactly which columns were being detected as which types, saving a ton of back-and-forth troubleshooting.\n\n## **Wrangling the Errors: ignore\\_errors, store\\_rejects, and More**\n\nSo `sniff_csv` looks good, but your file _still_ has issues\u2014maybe just a few problematic rows scattered throughout millions of good ones. By default, DuckDB will halt the import on the first error. But often, you just want the valid data and to deal with the bad rows separately.\n\n### **Option 1: Just Skip 'Em (`ignore_errors`)**\n\nThe simplest approach is to tell DuckDB to skip rows that cause parsing or casting errors using `ignore_errors = true`.\n\nLet's imagine a file [`faulty_folks.csv`](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/faulty_folks.csv):\n\n```txt\nCopy code\n\nName,Age\nAlice,30\nBob,forty-two\nCharlie,35\n```\n\nTrying to read this normally with explicit types will fail on Bob's age:\n\n```sql\nCopy code\n\n-- This will error out!\nSELECT * FROM read_csv('faulty_folks.csv', header=true, columns={'Name':'VARCHAR', 'Age':'INTEGER'});\n```\n\nBut if we just want Alice and Charlie:\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('faulty_folks.csv',\n    header = true,\n    -- Specify expected types\n    columns = {'Name': 'VARCHAR', 'Age': 'INTEGER'},\n    ignore_errors = true  -- The key part!\n    );\n```\n\n**Explanation:**\n\n- We define the `columns` we expect, including the `INTEGER` type for `Age`.\n- `ignore_errors = true` tells the reader: if you hit a row where 'Age' can't become an `INTEGER` (like \"forty-two\"), just drop that row and keep going.\n\n**Output:**\n\n| Name | Age |\n| --- | --- |\n| Alice | 30 |\n| Charlie | 35 |\n\nBob gets left behind, but the import succeeds with the valid rows. This approach skips rows with various issues: casting errors, wrong number of columns, unescaped quotes, etc.\n\n### **Option 2: Skip and Store (store\\_rejects)**\n\nIgnoring errors is okay, but generally, you need to know _what_ went wrong and _which_ rows were rejected. Maybe you need to fix the source data or report the issues. This is where `store_rejects = true` becomes invaluable.\n\nWhen you use [`store_rejects`](https://duckdb.org/docs/stable/data/csv/overview.html), DuckDB still skips the bad rows (like `ignore_errors`), but it also logs detailed information about each rejected row and the error encountered into two temporary tables: `reject_scans` and `reject_errors`.\n\n```sql\nCopy code\n\n-- Read the file, storing rejected rows\nSELECT * FROM read_csv(\n    'faulty_folks.csv',\n    header = true,\n    columns = {'Name': 'VARCHAR', 'Age': 'INTEGER'},\n    store_rejects = true -- Store info about errors\n    -- Optional: Customize table names and limit\n    -- rejects_scan = 'my_scan_info',\n    -- rejects_table = 'my_rejected_rows',\n    -- rejects_limit = 100 -- Store max 100 errors per file\n);\n\n-- Now, let's see what was rejected\nFROM reject_errors;\n-- And details about the scan itself\nFROM reject_scans;\n```\n\n**Explanation:**\n\n1. The `read_csv` call runs, skips Bob's row, and returns Alice and Charlie just like before.\n2. The key difference: `store_rejects = true` populates the temporary tables.\n3. `FROM reject_errors;` shows details about the failed rows:\n\n   - `scan_id`, `file_id`: Link back to the specific scan/file.\n   - `line`: The original line number in the CSV.\n   - `column_idx`, `column_name`: Which column had the issue (if applicable).\n   - `error_type`: The category of error (e.g., `CAST`, `TOO_MANY_COLUMNS`).\n   - `csv_line`: The actual content of the rejected line.\n   - `error_message`: The specific error message DuckDB generated.\n4. `FROM reject_scans;` gives metadata about the `read_csv` operation itself (delimiter, quote rule, schema used, file path, etc.).\n\nI've found this incredibly useful for debugging dirty data. You get the clean data loaded _and_ a detailed report on the rejects, all within DuckDB. No more grep-ing through massive files trying to find that one problematic line!\n\n### **Option 3: Relaxing the Rules (strict\\_mode=false and null\\_padding=true)**\n\nSometimes, you just want to _get the data in_, even if it\u2019s a little messy. That\u2019s where DuckDB's more forgiving CSV parsing options can help you out. **`strict_mode = false`** option tells DuckDB to loosen up its parsing expectations. It will _try_ to read rows even if they contain typical formatting problems like:\n\n- Unescaped quote characters in fields (e.g., `\"15\" Laptop\"`).\n- Rows with _more_ columns than defined (DuckDB just drops the extras).\n- Mixed newline formats (like mixing `\\n` and `\\r\\n`).\n\nWARNING: fast, but not precise\nWhen you set `strict_mode=false`, you\u2019re trusting DuckDB to make its best guess. That works great when you want results fast\u2014but double-check the output if data precision matters!\n\nAnother commonly used option is **`null_padding = true,`** which handles rows that come up _short_, meaning they have fewer columns than expected. Instead of throwing an error, DuckDB just fills in the blanks with `NULL`.\n\nLet\u2019s look at an example. Here's a messy CSV file named [`inventory.csv`](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/inventory.csv):\n\n```txt\nCopy code\n\nItemID,Description,Price\n101,\"15\" Laptop\",999.99\n102,\"Wireless Mouse\"\n103,\"Mechanical Keyboard\",129.99,ExtraField\n```\n\nThis file includes:\n\n- An unescaped quote in the first row\u2019s description\n- A missing price in the second row\n- An extra column in the third row\n\nTry reading it normally:\n\n```sql\nCopy code\n\nFROM read_csv('inventory.csv');\n```\n\nDuckDB will skip all lines except the last.\n\n![DuckDB CLI image](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimage1_7ef2565c8e.png&w=3840&q=75)\n\nBut with relaxed settings:\n\n```sql\nCopy code\n\n-- Parsing a messy CSV while gracefully handling missing and extra fields\nFROM read_csv('inventory.csv',\n    strict_mode = false,   -- Forgive formatting quirks\n    null_padding = true   -- Fill missing columns with NULLs\n);\n```\n\n**Resulting Table:**\n\n| ItemID | Description | Price | column3 |\n| --- | --- | --- | --- |\n| 101 | 15 Laptop | 999.99 | NULL |\n| 102 | Wireless Mouse | NULL | NULL |\n| 103 | Mechanical Keyboard | 129.99 | ExtraField |\n\n## **Handling Shifting Schemas: union\\_by\\_name**\n\nHere's another common headache: you have multiple CSV files to load (e.g., monthly reports), but the columns aren't _quite_ the same across files. Maybe a column was added in March, or the order changed in June. Trying to read them together with a simple `read_csv('monthly_report_*.csv')` might fail or produce misaligned data.\n\nDuckDB's `union_by_name = true` option handles this elegantly. When reading multiple files (using globs or lists), it aligns columns based on their _header names_ rather than their position. If a file is missing a column found in other files, it fills it with `NULL`.\n\nImagine [`report_jan.csv`](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/report_jan.csv):\n\n```txt\nCopy code\n\nUserID,MetricA,MetricB\n1,10,100\n2,15,110\n```\n\nAnd [`report_feb.csv`](http://duckdb-example-files.s3-website-us-east-1.amazonaws.com/2025-blog-post-taming-wild-csvs/report_feb.csv):\n\n```txt\nCopy code\n\nUserID,MetricB,MetricC,MetricA\n3,120,xyz,20\n4,125,abc,25\n```\n\nNotice the different order and the new `MetricC` in February.\n\n```sql\nCopy code\n\nSELECT *\nFROM read_csv(\n    ['report_jan.csv', 'report_feb.csv'], -- List of files (or glob)\n    union_by_name = true -- The magic!\n);\n```\n\n**Explanation:**\n\n- DuckDB reads the headers from all files involved.\n- It creates a combined schema containing _all_ unique column names (`UserID`, `MetricA`, `MetricB`, `MetricC`).\n- For each file, it matches the data to the combined schema based on the header names found _in that specific file_.\n- Where a column doesn't exist in a file (like `MetricC` in `report_jan.csv`), it inserts `NULL`.\n\n**Output:**\n\n| UserID | MetricA | MetricB | MetricC |\n| --- | --- | --- | --- |\n| 1 | 10 | 100 | NULL |\n| 2 | 15 | 110 | NULL |\n| 3 | 20 | 120 | xyz |\n| 4 | 25 | 125 | Abc |\n\n## **Fine-Tuning Detection and Overrides**\n\nWhile auto-detection is great, sometimes you know better, or the sample DuckDB takes isn't quite representative. Here are some ways to fine-tune the process:\n\n**Bigger Sample:** If type detection seems off (e.g., a column that's mostly integers but has a few floats later gets detected as `BIGINT`), try increasing the sample size:\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('file.csv', sample_size = 50000);\n\n-- Or scan the whole file (can be slow for huge files)\nSELECT * FROM read_csv('file.csv', sample_size = -1);\n```\n\n**Manual Types:** Override specific column types if detection gets it wrong or if you want a different type:\n\n```sql\nCopy code\n\n-- Override by name\nSELECT * FROM read_csv('file.csv',\n  types = {'UserID': 'VARCHAR', 'TransactionAmount': 'DOUBLE'});\n\n-- Or by position if no headers\nSELECT * FROM read_csv('file.csv', header = false,\n  types = ['VARCHAR', 'DOUBLE', 'DATE']);\n```\n\n**Force Header/No Header:** If header detection fails (common if all columns look like strings):\n\n```sql\nCopy code\n\n-- Force header presence\nSELECT * FROM read_csv('file.csv', header = true);\n\n-- Or no header with custom names\nSELECT * FROM read_csv('file.csv',\n header = false,\n names = ['colA', 'colB', 'colC']);\n```\n\n**Date/Timestamp Formats:** If dates aren't ISO 8601 (`YYYY-MM-DD`) or times aren't standard:\n\n```sql\nCopy code\n\n SELECT * FROM read_csv('file.csv',\n  dateformat = '%m/%d/%Y',\n  timestampformat = '%Y-%m-%dT%H:%M:%S.%f');\n```\n\n**Everything is a String:** If you want to load _everything_ as `VARCHAR` and deal with types later:\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('file.csv', all_varchar = true);\n```\n\n**Which Columns Can Be NULL?:** By default, an empty field is treated as `NULL`. If empty strings should be valid values:\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('file.csv',\n  force_not_null = ['column_name1', 'column_name2']);\n```\n\n**Clean Up Names:** Got headers with spaces or weird characters?\n\n```sql\nCopy code\n\nSELECT * FROM read_csv('file.csv', normalize_names = true);\n```\n\nThis will [automatically clean them up](https://duckdb.org/docs/stable/data/csv/overview.html#parameters) (replacing non-alphanumeric with `_`, etc.) during import.\n\n## DuckDB's CSV Parser: Champion of the Pollock Benchmark\n\nFor those really interested in CSV robustness, there's an intriguing benchmark called [Pollock](https://hpi.de/naumann/projects/data-preparation/pollock.html) that evaluates how well different systems handle non-standard CSV files. The creators studied over 245,000 public CSV datasets to identify common violations of the RFC-4180 standard, then created test files with these issues.\n\nIn recent testing, DuckDB [ranked #1](https://github.com/HPI-Information-Systems/Pollock) in the benchmark when configured to handle problematic files, correctly reading 99.61% of the data across all test files. Even in auto-detect mode with minimal configuration, DuckDB still managed to read about 90.75% of the data correctly.\n\nThis is practical validation that the approaches we've covered in this article can handle the vast majority of real-world CSV issues you'll encounter.\n\n| System under test | Pollock score<br>(simple) | Pollock score<br>(weighted) |\n| --- | --- | --- |\n| DuckDB 1.2 | 9.961 | 9.599 |\n| SQLite 3.39.0 | 9.955 | 9.375 |\n| UniVocity 2.9.1 | 9.939 | 7.936 |\n| LibreOffice Calc 7.3.6 | 9.925 | 7.833 |\n| SpreadDesktop | 9.929 | 9.597 |\n| SpreadWeb | 9.721 | 9.431 |\n| Python native csv 3.10.5 | 9.721 | 9.436 |\n| Pandas 1.4.3 | 9.895 | 9.431 |\n| MySQL 8.0.31 | 9.587 | 7.484 |\n| Mariadb 10.9.3 | 9.585 | 7.483 |\n\n## **Taking Flight Beyond the Basics**\n\nWe've covered quite a bit in our journey through DuckDB's CSV capabilities\u2014from diagnosing issues with `sniff_csv` to handling errors with `ignore_errors` and `store_rejects`, merging inconsistent schemas with `union_by_name`, and fine-tuning the whole process with various overrides.\n\nWhat I've come to appreciate about DuckDB is that its CSV reader isn't just a basic loader\u2014it's a sophisticated tool designed to handle real-world data messiness directly within SQL. Most data tools can handle the perfect CSV file, but it's how they deal with the imperfect ones that really matters in day-to-day work.\n\nBy understanding these slightly more advanced options, you can often avoid external preprocessing steps, keeping your data loading logic right within your SQL workflow. The result is cleaner pipelines that are less likely to waddle when faced with unexpected CSV quirks.\n\nThe next time a tricky CSV lands on your desk, remember these techniques. They might just save you some time and frustration, letting you get back to the more interesting parts of data analysis sooner. Happy querying!\n\n### TABLE OF CONTENTS\n\n[Peeking Under the Hood: sniff\\_csv](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#peeking-under-the-hood-sniffcsv)\n\n[Wrangling the Errors: ignore\\_errors, store\\_rejects, and More](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#wrangling-the-errors-ignoreerrors-storerejects-and-more)\n\n[Handling Shifting Schemas: union\\_by\\_name](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#handling-shifting-schemas-unionbyname)\n\n[Fine-Tuning Detection and Overrides](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#fine-tuning-detection-and-overrides)\n\n[DuckDB's CSV Parser: Champion of the Pollock Benchmark](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#duckdbs-csv-parser-champion-of-the-pollock-benchmark)\n\n[Taking Flight Beyond the Basics](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/#taking-flight-beyond-the-basics)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![MotherDuck lands on Tableau Cloud: Live, Fast Analytics Unleashed](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FTableau_Cloud_52bd53b821.png&w=3840&q=75)](https://motherduck.com/blog/tableau-cloud-motherduck/)\n\n[2025/05/06 - Jacob Matson](https://motherduck.com/blog/tableau-cloud-motherduck/)\n\n### [MotherDuck lands on Tableau Cloud: Live, Fast Analytics Unleashed](https://motherduck.com/blog/tableau-cloud-motherduck)\n\nUse MotherDuck to power your Tableau Cloud, Server, and Desktop dashboards.\n\n[![DuckDB Ecosystem: May 2025](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fduckdbeco_may_ca294a4d7f.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025/)\n\n[2025/05/08 - Simon Sp\u00e4ti](https://motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025/)\n\n### [DuckDB Ecosystem: May 2025](https://motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025)\n\nDuckDB Monthly #29: From Metabase to Doom, DuckDB powers everything.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 513,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 513
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me search for more specific information about attaching databases and working with S3:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ ATTACH|attach database|read_csv.*S3|read_parquet.*S3
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/privacy-policy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/S3 bucket.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dominik-moritz.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-future-of-bi-bi-as-code-duckdb-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vibe-coding-sql-cursor.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/spark-ducklake-getting-started.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-keyboard-shortcuts-for-joyful-querying.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me read one more file about querying raw files and check for more specific documentation:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/glossary/S3 bucket.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     "content": "---\ntitle: 'No-ETL: Query Raw CSV & JSON Files Directly with SQL'\ncontent_type: guide\ndescription: Skip complex ETL. Learn the No-ETL method for startups to query multiple\n  raw CSV, JSON, & Parquet files directly with SQL. Get insights in minutes, not months.\npublished_date: '2025-10-17T00:00:00'\nsource_url: https://motherduck.com/learn-more/no-etl-query-raw-files\nindexed_at: '2025-11-25T20:37:09.260086'\ncontent_hash: 317312307e45ea1c\nhas_step_by_step: true\nhas_narrative: true\n---\n\n# The No-ETL Playbook: How to Query Raw CSV & JSON Files Directly with SQL\n\nFor startups and small companies, your most valuable data, like customer sign-ups, sales transactions, and product usage logs, is likely scattered across hundreds of local files like CSVs, Excel spreadsheets, and JSON logs. The conventional wisdom says you need to build a complex and expensive **Extract, Transform, Load (ETL)** pipeline before you can even begin to analyze it. But for a lean team that needs to move fast, this approach is a trap. It\u2019s slow, rigid, and drains precious engineering resources before you\u2019ve asked your first question.\n\nWhat if you could skip the pipeline and go straight to the insights?\n\nA modern, **\"No-ETL\"** approach allows you to do just that. Instead of spending months building a complex data infrastructure, you can use simple SQL to query your raw data files directly where they live, on your laptop or in cloud storage. This guide will show you how this lean, serverless method turns fragmented files into a powerful, queryable database in minutes, not months. You will learn why traditional ETL is a bottleneck, how to get immediate answers from your raw files, and how to consolidate thousands of fragmented files into a single, high-performance database, all while unifying data across your local machine and the cloud.\n\n## Why Are Traditional Data Pipelines a Trap for Startups?\n\nFor decades, the path from data to decision was a one-way street paved with ETL. This process involves extracting data from various sources, transforming it into a rigid, predefined schema, and loading it into a central data warehouse. While this model can work for large enterprises with stable processes, it creates a \"pipeline city\" that demands constant maintenance and becomes a major bottleneck for agile startups that need to pivot quickly.\n\nThe \"No-ETL\" philosophy flips this script entirely. Instead of moving all your data to a central processing location, you bring the processing power directly to your data. By pointing SQL at the files you already have, you can eliminate 80-90% of the traditional pipeline steps. This approach, powered by MotherDuck's serverless engine, allows teams to [consolidate data from dozens of sources in days, not months](https://motherduck.com/learn-more/modern-data-warehouse-playbook/).\n\nThis shift dramatically accelerates your time-to-insight and slashes operational costs. Processing a 100MB CSV file on your laptop is instantaneous and costs nothing, whereas using a traditional cloud warehouse incurs charges for compute time and network data transfer. Companies that adopt this lean model report [ 70-90% cost reductions](https://motherduck.com/learn-more/modern-data-warehouse-playbook/) compared to traditional cloud data warehouses.\n\n### How Did Five Startups Slash Data Prep Time from Hours to Minutes?\n\nThe benefits of abandoning premature ETL aren't just theoretical. Startups across various industries have replaced slow, brittle pipelines and overloaded databases with MotherDuck\u2019s query-in-place model, leading to dramatic improvements in speed, cost, and agility. These changes often lead to huge improvements, not just small percentage gains.\n\n| Company & Industry | Previous State & Pain Point | Transformation with MotherDuck | Quantified Outcome |\n|---|---|---|---|\nFinqore (FinTech) | 8-hour data pipelines using Postgres for complex financial data and a heavy reliance on manual Excel processes. | Replaced Postgres to process and unify data directly, enabling a real-time metrics explorer and AI agents. |\n8 hours to 8 minutes |\n\n**Gardyn**(IoT / AgTech)[Pipeline time cut from](https://motherduck.com/case-studies/gardyn/)at 10x lower cost than other data warehouses.**over 24 hours to under 1 hour****UDisc**(Sports Tech)[dbt job time reduced from](https://motherduck.com/case-studies/udisc-motherduck-sports-management/), and typical queries dropped from minutes to**6 hours to 30 minutes****5 seconds**.**Dexibit**(Analytics)[Analytical query times reduced from](https://motherduck.com/case-studies/dexibit/), eliminating traditional data warehouse costs.**minutes to a few seconds****Layers**(SaaS)[dashboards loading in](https://motherduck.com/case-studies/layers-multi-tenant-data-warehouse/).**110 ms**## How Can You Get Instant Answers from Excel & CSV Files?\n\nFor many teams, valuable data is trapped in local spreadsheets. The traditional path to analyzing this data, which includes manual imports, database setup, and schema definition, is slow and frustrating. MotherDuck eliminates this friction by allowing you to query CSV and Excel files directly with SQL, just as if they were database tables.\n\nThis is powered by DuckDB's [ read_csv_auto](https://duckdb.org/docs/stable/data/csv/auto_detection.html) and\n\n[functions, which automatically infer column names, data types, and file dialects (like delimiters) by sampling the file. This](https://duckdb.org/docs/stable/guides/file_formats/excel_import.html)\n\n`read_xlsx`\n\n**schema-on-read**capability means a non-technical user can go from a local file to a powerful SQL query in seconds, without writing any\n\n`CREATE TABLE`\n\nstatements or managing a database server.For a local CSV file, a query is as simple as this:\n\nCopy code\n\n```\nSELECT\nproduct_category,\nSUM(sale_amount) AS total_sales\nFROM 'transactions.csv'\nGROUP BY ALL\nORDER BY total_sales DESC;\n```\n\n\nMotherDuck [automatically detects the columns and their types](https://duckdb.org/docs/stable/data/csv/overview.html), allowing you to filter, aggregate, and sort on the fly. This direct-query capability extends to files stored in cloud object storage and [even Google Sheets](https://motherduck.com/docs/key-tasks/data-warehousing/Replication/spreadsheets/), providing a unified way to access spreadsheet data wherever it lives.\n\n### What About Messy Spreadsheets? How to Handle Common Pitfalls in SQL\n\n\"Wild\" CSV and Excel files often have inconsistencies. Instead of spending time on manual cleanup, you can handle these common issues directly in your SQL query using optional parameters.\n\n| Pitfall | Problem Description | SQL Solution with MotherDuck |\n|---|---|---|\nIncorrect Delimiter | A CSV file uses a pipe (`|` ) or semicolon (`;` ) instead of a comma, causing columns to be misread. | Use the\n`delim` parameter |\n\n`read_csv('data.csv', delim = '|')`\n\n.**Inconsistent Date Formats**`MM/DD/YYYY`\n\nor `DD-Mon-YY`\n\n.[to provide the exact format string:](https://duckdb.org/docs/stable/data/csv/overview.html)`dateformat`\n\nparameter`read_csv('data.csv', dateformat = '%m/%d/%Y')`\n\n.**Header and Footer Rows**[to select only the data cells:](https://duckdb.org/docs/stable/guides/file_formats/excel_import.html)`range`\n\nparameter`range = 'A5:Z100'`\n\n.**Mixed Data Types**[, then use](https://duckdb.org/docs/stable/guides/file_formats/excel_import.html)`all_varchar = true`\n\n`TRY_CAST()`\n\nin your `SELECT`\n\nstatement to safely convert types.## How Do I Turn Thousands of Fragmented Files into a Single Database?\n\nA common challenge for growing companies is data fragmentation, where analytics data is spread across thousands of individual files in cloud storage like Amazon S3. MotherDuck sidesteps complex ingestion jobs by [treating an entire folder of files as a single, queryable database table](https://duckdb.org/docs/stable/data/multiple_files/overview.html).\n\nBy using SQL with glob patterns, you can instantly query a whole collection of Parquet, CSV, or JSON files directly in S3. For example, the following query will scan all Parquet files for the year 2025, no matter how many subdirectories they are in, and treat them as one large table:\n\nCopy code\n\n```\nSELECT\nevent_type,\nCOUNT(*) AS event_count\nFROM read_parquet('s3://my-bucket/logs/2025/**/*.parquet')\nGROUP BY event_type;\n```\n\n\nThis is a high-performance feature, not just a convenience. MotherDuck\u2019s query engine [pushes down filters and projections to the file level](https://duckdb.org/docs/stable/data/parquet/overview.html), minimizing the amount of data read from cloud storage and reducing costs. For even greater efficiency, organizing files using [ Hive-style partitioning](https://duckdb.org/docs/stable/guides/performance/how_to_tune_workloads.html) (e.g.,\n\n`/year=2025/month=10/`\n\n) allows the engine to skip entire folders that don\u2019t match a query\u2019s `WHERE`\n\nclause, dramatically reducing scan time and cost.## How Can I Join Local Files with Data in the Cloud?\n\nStartups rarely have their data in one neat location. You might have recent sales data in a CSV on your laptop, historical logs in an S3 bucket, and user profiles in a managed database. MotherDuck\u2019s [ Dual Execution feature](https://motherduck.com/docs/concepts/architecture-and-capabilities/) unifies these fragmented datasets into a single analytical layer without requiring you to move the data first.\n\nWhen your local DuckDB client is connected to MotherDuck, they form a distributed system where the query optimizer [intelligently routes parts of your query to where the data lives](https://motherduck.com/videos/339/bringing-duckdb-to-the-cloud-dual-execution-explained/). A query on a local file runs entirely on your machine's resources. A query on a large S3 file runs in the MotherDuck cloud to use its scale. Most powerfully, a join between the two is automatically optimized to minimize data transfer.\n\nAn analyst can prototype a query by joining a local spreadsheet with a massive cloud table. MotherDuck is smart enough to automatically push filters down to the local file, send only the small, filtered result to the cloud, and perform the final join there. This [avoids the slow and costly process of uploading entire local files](https://motherduck.com/learn-more/fix-slow-bi-dashboards/) just to perform a join. The entire process is smooth, and you simply write standard SQL.\n\n## How Can You Explore Data Before Building a Formal Pipeline?\n\nThe schema-on-read approach is powerful because it lets you [explore and understand your data before committing](https://motherduck.com/duckdb-book-summary-chapter5/) to a rigid transformation pipeline. This \"explore-then-model\" workflow de-risks data projects by allowing you to assess data quality, discover hidden patterns, and validate business assumptions upfront. With MotherDuck, you can use a suite of simple SQL commands to profile your raw CSV, JSON, and Parquet files directly.\n\n### How Can You See the Structure of Your JSON and Parquet Files Automatically?\n\nMotherDuck provides powerful functions to look inside the structure of your semi-structured and columnar files without manually parsing them.\n\nFor **Parquet files**, you can [query the file\u2019s internal metadata directly](https://duckdb.org/docs/stable/data/parquet/metadata.html) to see column names, types, and nullability. This is very helpful for understanding the data you've received from a partner or another system.\n\nCopy code\n\n```\nSELECT * FROM parquet_schema('your_file.parquet');\n```\n\n\nFor **JSON files**, the [ read_json_auto function automatically infers a schema](https://duckdb.org/docs/stable/data/json/loading_json.html), representing nested objects as\n\n`STRUCT`\n\ns and arrays as `LIST`\n\ns. You can see this inferred schema by creating a temporary table and describing it:Copy code\n\n```\nCREATE TABLE temp_json AS SELECT * FROM read_json_auto('api_response.json');\nDESCRIBE temp_json;\n```\n\n\n### How Can You Use SQL to Explore JSON Data Without Knowing Its Structure?\n\nJSON's nested, schema-less nature makes it notoriously difficult to analyze with traditional SQL. MotherDuck lets you [explore and query deeply nested JSON files immediately](https://motherduck.com/blog/analyze-json-data-using-sql/), even with zero prior knowledge of their structure.\n\nYou can instantly navigate the nested structure using simple **dot notation** (e.g., `SELECT user.name.first FROM 'users.json'`\n\n) and flatten complex arrays into rows using the ** UNNEST** function. This turns what was once a painful data preparation task into a simple, interactive exploration process.\n\nCopy code\n\n```\n-- Explore nested JSON and flatten an array of items into separate rows\nSELECT\norder_id,\ncustomer.id AS customer_id,\nitem.product_id::INTEGER,\nitem.quantity::INTEGER\nFROM read_json_auto('orders.json'), UNNEST(line_items) AS t(item);\n```\n\n\n## Is the \"No-ETL\" Approach a Permanent Solution?\n\nFor a lean team, building a full-blown ETL pipeline too early is a strategic error. It locks you into a rigid structure before you fully understand your data's value or how your business questions will evolve. The \"No-ETL\" approach is a better starting point for most startups.\n\nHowever, \"No-ETL\" does not mean \"Never-ETL.\" As your company matures and your data processes become more standardized, certain triggers justify introducing a more formalized, lightweight EL(T) process, where raw data is loaded into cloud storage and then transformed within the warehouse. You should consider this change when you need things like **repeatability for audits**, such as for financial reporting or compliance. It also becomes valuable when you need to improve **performance on complex joins** for frequently-run dashboards, or when business operations depend on [ strict data freshness SLAs](https://motherduck.com/learn-more/modern-data-warehouse-use-cases/) of minutes, not hours. Finally, as your team grows, a formalized model is essential for\n\n[, allowing you to systematically clean data or mask PII before exposing it to a wider audience.](https://motherduck.com/blog/motherduck-kestra-etl-pipelines/)\n\n**data governance at scale**This **progressive modeling pattern**, which involves starting with raw files, creating semantic views, and only materializing tables when necessary, allows your data architecture to evolve with your business, not against it.\n\n## Your Path Forward: From Files to Insights\n\nThe message for startups is clear: stop building pipelines and start asking questions. The modern, No-ETL approach used by MotherDuck removes the friction between your data and your decisions. By letting your entire team query raw files directly with the SQL they already know, you unlock a level of speed and agility that traditional data stacks simply cannot match. Start by exploring your local files, scale to the cloud, and let your data architecture grow with your business needs. The power to be data-driven is no longer locked behind complex engineering projects. It's right there in your files, waiting for a query.\n\nStart using MotherDuck now!\n\n## FAQS\n\n### Do I really need an ETL pipeline if I\u2019m just getting started?\n\nFor most startups, no. A \"No-ETL\" approach lets you query raw CSV, JSON, and Excel files directly with SQL. This is faster and cheaper, allowing you to get insights immediately without building complex infrastructure.\n\n### How can I analyze data in Excel and CSV files without hassle?\n\nYou can use SQL to query local Excel and CSV files as if they were database tables. Modern tools like DuckDB automatically detect columns and data types, so you can go from a spreadsheet to analysis in seconds without any setup.\n\n### How do I query thousands of files in a folder as a single database?\n\nUse SQL with glob patterns (e.g., `FROM 's3://bucket/logs/**/*.parquet'`\n\n). This treats an entire folder of files in cloud storage as one large, queryable table, eliminating the need for complex data ingestion jobs.\n\n### Can I explore my data before building an ETL pipeline?\n\nYes, this is a key benefit of the No-ETL approach. By querying raw files directly, you can profile data quality, discover patterns, and validate assumptions with SQL before committing to a rigid transformation model.\n\n### How can I see the structure of my JSON and Parquet files automatically?\n\nUse simple SQL commands. For Parquet, `parquet_schema('file.parquet')`\n\nreveals the schema. For JSON, `read_json_auto()`\n\ninfers the structure, which you can view with a `DESCRIBE`\n\ncommand on the query result.\n\n### Why is schema design so difficult for startups?\n\nStartups evolve rapidly, causing data sources and business needs to change constantly. A rigid, upfront schema (schema-on-write) becomes a bottleneck. A flexible, schema-on-read approach is better as it applies structure at query time.",
  [2m  â”‚[0m     "numLines": 229,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 229
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/glossary/S3 bucket.md",
  [2m  â”‚[0m     "content": "---\ntitle: S3 bucket\ncontent_type: tutorial\nsource_url: https://motherduck.com/glossary/S3 bucket\nindexed_at: '2025-11-25T20:02:55.683911'\ncontent_hash: 245482b768c285f5\nhas_code_examples: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n# S3 bucket\n\n_[Back to DuckDB Data Engineering Glossary](https://motherduck.com/glossary/)_\n\nAn [S3 bucket](https://aws.amazon.com/s3/) is a fundamental storage container in Amazon Web Services' Simple Storage Service (S3). It functions as a cloud-based folder for storing and organizing data objects, such as files, images, and documents. S3 buckets are globally unique, scalable, and designed to provide high durability and availability for data storage. They support various access control mechanisms and can be configured for different storage classes based on data access patterns and cost considerations. Data engineers often use S3 buckets as a central repository for raw data, processed datasets, or as part of data lakes. When working with DuckDB, youcan directly query data stored in S3 buckets using syntax like:\n\n```sql\nCopy code\n\nSELECT * FROM read_parquet('s3://your-bucket-name/path/to/file.parquet');\n```\n\nThis seamless integration allows for efficient data processing without the need to download files locally.\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 30,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 30
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! Now I have enough information to write a comprehensive answer. Let me create the markdown file with the answer.
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_7.md

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me check if the file exists first:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ ls -la /tmp/answer_without_kg_7.md 2>&1 || echo "File does not exist"
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m -rw-------  1 julien  wheel  4778 Nov 27 12:42 /tmp/answer_without_kg_7.md
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m The file already exists, so I need to read it first before writing:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /tmp/answer_without_kg_7.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/tmp/answer_without_kg_7.md",
  [2m  â”‚[0m     "content": "# Joining a Local CSV with an S3 Table Using MotherDuck\n\nIf you have a CSV file on your laptop and a table in S3, MotherDuck provides an extremely efficient way to join them using its **Dual Execution** architecture. This approach minimizes data movement and leverages compute resources intelligently.\n\n## The Most Efficient Approach\n\nSimply write a standard SQL join query that references both your local CSV and your S3 data:\n\n```sql\nSELECT\n    local.customer_id,\n    local.recent_purchase,\n    cloud.historical_data\nFROM 'local_sales.csv' AS local\nJOIN read_parquet('s3://my-bucket/customers.parquet') AS cloud\n    ON local.customer_id = cloud.customer_id\nWHERE local.purchase_date >= '2025-01-01';\n```\n\n## How Dual Execution Makes This Efficient\n\nMotherDuck's **Dual Execution** model treats your laptop and the MotherDuck cloud as two nodes in a single distributed system. When you run a hybrid query like this, the query planner intelligently optimizes execution:\n\n1. **Local Processing**: The CSV file is scanned and filtered on your laptop\n2. **Minimal Data Transfer**: Only the filtered results (potentially just a few rows) are sent to the cloud\n3. **Cloud Processing**: The S3 table is scanned in the MotherDuck cloud where the data lives\n4. **Optimized Join**: The final join happens in the cloud, avoiding the need to upload your entire CSV or download the entire S3 table\n\n### Example: What the Optimizer Does\n\nWhen joining a local CSV with a cloud table filtered by product:\n\n1. **Local Scan**: Scans the pricing CSV on your laptop\n2. **Filter Locally**: Applies the filter for the specific product\n3. **Transfer Minimal Data**: Sends only the single filtered row (a few bytes) to the cloud\n4. **Join in Cloud**: Performs the join against the massive sales table in MotherDuck\n\nThis can reduce network traffic by **orders of magnitude** compared to traditional approaches.\n\n## Key Benefits\n\n- **No ETL Required**: No need to upload the CSV to S3 or download the S3 data locally\n- **Minimal Data Movement**: Only necessary data crosses the network boundary\n- **Single SQL Query**: Write standard SQL as if all data were in one location\n- **Cost Efficient**: Leverages free local compute and minimizes cloud data transfer costs\n- **Fast Performance**: Computation happens where the data lives\n\n## Verifying the Execution Plan\n\nYou can use `EXPLAIN` to see how MotherDuck will execute your query:\n\n```sql\nEXPLAIN\nSELECT *\nFROM 'local_data.csv' AS local\nJOIN read_parquet('s3://bucket/table.parquet') AS cloud\n    ON local.id = cloud.id;\n```\n\nThe explain plan will show:\n- Operations marked `(L)` run locally on your laptop\n- Operations marked `(R)` run remotely in MotherDuck cloud\n\n## Manual Control (When Needed)\n\nWhile the optimizer usually makes the best choice, you can manually control execution location using the `md_run` parameter:\n\n```sql\n-- Force S3 scan to run locally (downloads data first)\nFROM read_parquet('s3://bucket/file.parquet', md_run = 'local')\n\n-- Force S3 scan to run remotely in MotherDuck cloud\nFROM read_parquet('s3://bucket/file.parquet', md_run = 'remote')\n```\n\nFor a 2GB S3 Parquet file, remote execution in MotherDuck (25 seconds) is typically faster than local execution that requires downloading first (36 seconds).\n\n## Connecting Local DuckDB to MotherDuck\n\nTo enable this hybrid execution, simply attach your local DuckDB instance to MotherDuck:\n\n```sql\nATTACH 'md:my_database';\n```\n\nAfter this, any query can seamlessly mix local files and cloud data.\n\n## Additional Tips for CSV Files\n\nMotherDuck automatically detects CSV structure, but you can override if needed:\n\n```sql\n-- Let MotherDuck auto-detect everything\nFROM 'local_sales.csv'\n\n-- Or specify details for messy CSVs\nFROM read_csv('local_sales.csv',\n    delim = '|',\n    header = true,\n    dateformat = '%m/%d/%Y'\n)\n```\n\n## Sources\n\n- [Hybrid Analytics: Query Local & Cloud Data Instantly](https://motherduck.com/learn-more/hybrid-analytics-guide) - Comprehensive guide explaining dual execution and how to join local CSVs with cloud data\n- [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained) - Detailed explanation of MotherDuck's dual execution architecture\n- [The No-ETL Playbook: Query Raw CSV & JSON Files Directly with SQL](https://motherduck.com/learn-more/no-etl-query-raw-files) - How to query raw files without building ETL pipelines\n- [Taming Wild CSVs: Advanced DuckDB Techniques](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering) - Advanced CSV handling techniques\n- [Querying Data From S3 With 3 Lines In Your Terminal](https://motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal) - Quick demonstration of S3 querying\n",
  [2m  â”‚[0m     "numLines": 111,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 111
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Good! The file already exists with a comprehensive answer. Now I'll update it with my synthesized answer that includes all the key information I found:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_7.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "update",
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_7.md",
  [2m  â”‚[0m   "content": "# How to Join a Local CSV with an S3 Table Using MotherDuck\n\nIf you have a CSV file on your laptop and a table stored in S3, the most efficient way to join them using MotherDuck is to leverage **Dual Execution** \u2014 MotherDuck's innovative hybrid query execution model that intelligently minimizes data movement.\n\n## The Most Efficient Approach\n\nSimply write a standard SQL join query that references both your local CSV and your S3 data:\n\n```sql\nSELECT\n    local.customer_id,\n    local.recent_purchase,\n    cloud.historical_data\nFROM read_csv('local_sales.csv') AS local\nJOIN read_parquet('s3://my-bucket/customer_history.parquet') AS cloud\n    ON local.customer_id = cloud.customer_id\nWHERE local.purchase_date >= '2025-01-01';\n```\n\n## How Dual Execution Makes This Efficient\n\nMotherDuck's **Dual Execution** model is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on your laptop or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\n\nWhen you run a hybrid query joining local and cloud data, the planner automatically:\n\n1. **Local Scan & Filter**: Scans your local CSV on your laptop and applies any filters there\n2. **Minimal Transfer**: Sends only the filtered results (potentially just a few kilobytes) to the cloud\n3. **Cloud Processing**: Scans the S3 table in the MotherDuck cloud where the data already lives\n4. **Optimized Join**: Performs the final join in the cloud\n\n### Example Optimization in Action\n\nImagine you want to join a local CSV of new product pricing with a massive sales table in MotherDuck, filtered for a specific product. Instead of moving entire tables, the Dual Execution planner:\n\n1. **Local Scan**: Scans the small pricing CSV on your laptop\n2. **Filter Locally**: Applies the filter for the specific product on your machine\n3. **Transfer Minimal Data**: Sends only the single, filtered pricing row (a few bytes of data) to the cloud\n4. **Join in the Cloud**: Performs the final join against the massive sales table in the MotherDuck cloud\n\nThis process can reduce network traffic by **orders of magnitude** compared to traditional methods, turning a difficult query into an interactive one.\n\n## Key Benefits\n\n- **No ETL Required**: No need to upload your entire CSV to S3 or download the entire S3 table locally\n- **Minimizes Data Movement**: Only the minimum necessary data crosses the network boundary\n- **Single Query**: Write standard SQL as if all data exists in one unified environment\n- **Cost Efficient**: Leverages your laptop's free compute resources and minimizes cloud data transfer costs\n- **Faster Development**: Zero-latency feedback for local development before scaling to cloud\n- **Better Collaboration**: Transforms DuckDB from \"single-player\" to \"multiplayer\" platform\n\n## Setup: Connecting to MotherDuck\n\nTo enable hybrid execution, connect your local DuckDB instance to MotherDuck:\n\n```sql\nATTACH 'md:my_database';\n```\n\nAfter this simple `ATTACH` command, any query can seamlessly mix local files and cloud data. MotherDuck uses the exact same DuckDB engine both locally and in the cloud, ensuring that a query validated locally is guaranteed to behave identically in production.\n\n## Verifying the Execution Plan\n\nYou can use `EXPLAIN` to see which parts of your query run locally versus remotely:\n\n```sql\nEXPLAIN\nSELECT *\nFROM 'local_data.csv' AS local\nJOIN read_parquet('s3://bucket/table.parquet') AS cloud\n    ON local.id = cloud.id;\n```\n\nThe explain plan will show:\n- Operations marked `(L)` run locally on your laptop\n- Operations marked `(R)` run remotely in MotherDuck cloud\n\nThis transparency helps you understand and optimize the execution strategy.\n\n## Manual Control (Optional)\n\nWhile the automatic optimization usually makes the best choice, you can manually control execution location using the `md_run` parameter:\n\n```sql\n-- Force S3 scan to run locally (downloads data first)\nFROM read_parquet('s3://bucket/file.parquet', md_run = 'local')\n\n-- Force S3 scan to run remotely in MotherDuck cloud\nFROM read_parquet('s3://bucket/file.parquet', md_run = 'remote')\n```\n\n**Performance comparison**: In testing, querying a 2GB Parquet file on S3 remotely in MotherDuck (25 seconds) was significantly faster than downloading and processing locally (36 seconds). By pushing computation to where the data lives, MotherDuck minimizes network I/O and delivers results more quickly.\n\n## Advanced CSV Handling\n\nMotherDuck's CSV reader automatically detects structure (headers, delimiters, data types), but you can override for messy files:\n\n```sql\n-- Let MotherDuck auto-detect (recommended)\nFROM 'local_sales.csv'\n\n-- Or specify details for problematic CSVs\nFROM read_csv('local_sales.csv',\n    delim = '|',                    -- Custom delimiter\n    header = true,                  -- Force header presence\n    dateformat = '%m/%d/%Y',        -- Custom date format\n    ignore_errors = true            -- Skip problematic rows\n)\n```\n\nFor more advanced CSV techniques, see the \"Taming Wild CSVs\" guide in the sources below.\n\n## Querying Multiple Files in S3\n\nYou can also query entire folders of files in S3 as a single table using glob patterns:\n\n```sql\nSELECT\n    event_type,\n    COUNT(*) AS event_count\nFROM read_parquet('s3://my-bucket/logs/2025/**/*.parquet')\nGROUP BY event_type;\n```\n\nThis treats all matching Parquet files as one unified dataset, with intelligent filter pushdown to minimize data scanned.\n\n## Why Data Engineers Are Excited\n\nData engineers love dual-execution engines because they solve the \"it worked on my machine\" problem. Since MotherDuck uses the exact same DuckDB engine locally and in the cloud:\n\n- **Faster Development**: Build and test pipelines with zero-latency local feedback before scaling to cloud\n- **Lower Costs**: Use free laptop compute and minimize data transfer\n- **Consistency Guaranteed**: A query that works on your laptop is guaranteed to work in the cloud\n- **Smooth Scale-Up**: Start local-first and seamlessly push more workload to cloud as needs grow\n\n## Real-World Performance\n\nCompanies have achieved dramatic improvements with this approach:\n- **Finqore**: Reduced 8-hour data pipelines to 8 minutes (60x improvement)\n- **Gardyn**: Cut pipeline time from 24+ hours to under 1 hour\n- **UDisc**: Reduced dbt jobs from 6 hours to 30 minutes, with queries dropping from minutes to 5 seconds\n\n## Sources\n\n- [Hybrid Analytics: Query Local & Cloud Data Instantly](https://motherduck.com/learn-more/hybrid-analytics-guide/) - Comprehensive guide on joining local CSVs with cloud data, including detailed explanation of Dual Execution\n- [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained/) - Technical deep dive into MotherDuck's dual execution architecture and query optimization from MotherDuck founding engineer\n- [The No-ETL Playbook: How to Query Raw CSV & JSON Files Directly with SQL](https://motherduck.com/learn-more/no-etl-query-raw-files/) - Guide on querying raw files in S3 and joining local files with cloud data\n- [Taming Wild CSVs: Advanced DuckDB Techniques for Data Engineers](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/) - Advanced CSV handling techniques including error handling and schema detection\n- [S3 bucket - DuckDB Data Engineering Glossary](https://motherduck.com/glossary/S3 bucket/) - How to query data stored in S3 buckets with DuckDB syntax\n- [Querying Data From S3 With 3 Lines In Your Terminal](https://motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal/) - Quick demonstration of S3 querying\n",
  [2m  â”‚[0m   "structuredPatch": [
  [2m  â”‚[0m     {
  [2m  â”‚[0m       "oldStart": 1,
  [2m  â”‚[0m       "oldLines": 6,
  [2m  â”‚[0m       "newStart": 1,
  [2m  â”‚[0m       "newLines": 6,
  [2m  â”‚[0m       "lines": [
  [2m  â”‚[0m         "-# Joining a Local CSV with an S3 Table Using MotherDuck",
  [2m  â”‚[0m         "+# How to Join a Local CSV with an S3 Table Using MotherDuck",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-If you have a CSV file on your laptop and a table in S3, MotherDuck provides an extremely efficient way to join them using its **Dual Execution** architecture. This approach minimizes data movement and leverages compute resources intelligently.",
  [2m  â”‚[0m         "+If you have a CSV file on your laptop and a table stored in S3, the most efficient way to join them using MotherDuck is to leverage **Dual Execution** \u2014 MotherDuck's innovative hybrid query execution model that intelligently minimizes data movement.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         " ## The Most Efficient Approach",
  [2m  â”‚[0m         " "
  [2m  â”‚[0m       ]
  [2m  â”‚[0m     },
  [2m  â”‚[0m     {
  [2m  â”‚[0m       "oldStart": 11,
  [2m  â”‚[0m       "oldLines": 43,
  [2m  â”‚[0m       "newStart": 11,
  [2m  â”‚[0m       "newLines": 56,
  [2m  â”‚[0m       "lines": [
  [2m  â”‚[0m         "     local.customer_id,",
  [2m  â”‚[0m         "     local.recent_purchase,",
  [2m  â”‚[0m         "     cloud.historical_data",
  [2m  â”‚[0m         "-FROM 'local_sales.csv' AS local",
  [2m  â”‚[0m         "-JOIN read_parquet('s3://my-bucket/customers.parquet') AS cloud",
  [2m  â”‚[0m         "+FROM read_csv('local_sales.csv') AS local",
  [2m  â”‚[0m         "+JOIN read_parquet('s3://my-bucket/customer_history.parquet') AS cloud",
  [2m  â”‚[0m         "     ON local.customer_id = cloud.customer_id",
  [2m  â”‚[0m         " WHERE local.purchase_date >= '2025-01-01';",
  [2m  â”‚[0m         " ```",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         " ## How Dual Execution Makes This Efficient",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-MotherDuck's **Dual Execution** model treats your laptop and the MotherDuck cloud as two nodes in a single distributed system. When you run a hybrid query like this, the query planner intelligently optimizes execution:",
  [2m  â”‚[0m         "+MotherDuck's **Dual Execution** model is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on your laptop or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-1. **Local Processing**: The CSV file is scanned and filtered on your laptop",
  [2m  â”‚[0m         "-2. **Minimal Data Transfer**: Only the filtered results (potentially just a few rows) are sent to the cloud",
  [2m  â”‚[0m         "-3. **Cloud Processing**: The S3 table is scanned in the MotherDuck cloud where the data lives",
  [2m  â”‚[0m         "-4. **Optimized Join**: The final join happens in the cloud, avoiding the need to upload your entire CSV or download the entire S3 table",
  [2m  â”‚[0m         "+When you run a hybrid query joining local and cloud data, the planner automatically:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-### Example: What the Optimizer Does",
  [2m  â”‚[0m         "+1. **Local Scan & Filter**: Scans your local CSV on your laptop and applies any filters there",
  [2m  â”‚[0m         "+2. **Minimal Transfer**: Sends only the filtered results (potentially just a few kilobytes) to the cloud",
  [2m  â”‚[0m         "+3. **Cloud Processing**: Scans the S3 table in the MotherDuck cloud where the data already lives",
  [2m  â”‚[0m         "+4. **Optimized Join**: Performs the final join in the cloud",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-When joining a local CSV with a cloud table filtered by product:",
  [2m  â”‚[0m         "+### Example Optimization in Action",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-1. **Local Scan**: Scans the pricing CSV on your laptop",
  [2m  â”‚[0m         "-2. **Filter Locally**: Applies the filter for the specific product",
  [2m  â”‚[0m         "-3. **Transfer Minimal Data**: Sends only the single filtered row (a few bytes) to the cloud",
  [2m  â”‚[0m         "-4. **Join in Cloud**: Performs the join against the massive sales table in MotherDuck",
  [2m  â”‚[0m         "+Imagine you want to join a local CSV of new product pricing with a massive sales table in MotherDuck, filtered for a specific product. Instead of moving entire tables, the Dual Execution planner:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-This can reduce network traffic by **orders of magnitude** compared to traditional approaches.",
  [2m  â”‚[0m         "+1. **Local Scan**: Scans the small pricing CSV on your laptop",
  [2m  â”‚[0m         "+2. **Filter Locally**: Applies the filter for the specific product on your machine",
  [2m  â”‚[0m         "+3. **Transfer Minimal Data**: Sends only the single, filtered pricing row (a few bytes of data) to the cloud",
  [2m  â”‚[0m         "+4. **Join in the Cloud**: Performs the final join against the massive sales table in the MotherDuck cloud",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "+This process can reduce network traffic by **orders of magnitude** compared to traditional methods, turning a difficult query into an interactive one.",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         " ## Key Benefits",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-- **No ETL Required**: No need to upload the CSV to S3 or download the S3 data locally",
  [2m  â”‚[0m         "-- **Minimal Data Movement**: Only necessary data crosses the network boundary",
  [2m  â”‚[0m         "-- **Single SQL Query**: Write standard SQL as if all data were in one location",
  [2m  â”‚[0m         "-- **Cost Efficient**: Leverages free local compute and minimizes cloud data transfer costs",
  [2m  â”‚[0m         "-- **Fast Performance**: Computation happens where the data lives",
  [2m  â”‚[0m         "+- **No ETL Required**: No need to upload your entire CSV to S3 or download the entire S3 table locally",
  [2m  â”‚[0m         "+- **Minimizes Data Movement**: Only the minimum necessary data crosses the network boundary",
  [2m  â”‚[0m         "+- **Single Query**: Write standard SQL as if all data exists in one unified environment",
  [2m  â”‚[0m         "+- **Cost Efficient**: Leverages your laptop's free compute resources and minimizes cloud data transfer costs",
  [2m  â”‚[0m         "+- **Faster Development**: Zero-latency feedback for local development before scaling to cloud",
  [2m  â”‚[0m         "+- **Better Collaboration**: Transforms DuckDB from \"single-player\" to \"multiplayer\" platform",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "+## Setup: Connecting to MotherDuck",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+To enable hybrid execution, connect your local DuckDB instance to MotherDuck:",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+```sql",
  [2m  â”‚[0m         "+ATTACH 'md:my_database';",
  [2m  â”‚[0m         "+```",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+After this simple `ATTACH` command, any query can seamlessly mix local files and cloud data. MotherDuck uses the exact same DuckDB engine both locally and in the cloud, ensuring that a query validated locally is guaranteed to behave identically in production.",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         " ## Verifying the Execution Plan",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-You can use `EXPLAIN` to see how MotherDuck will execute your query:",
  [2m  â”‚[0m         "+You can use `EXPLAIN` to see which parts of your query run locally versus remotely:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         " ```sql",
  [2m  â”‚[0m         " EXPLAIN"
  [2m  â”‚[0m       ]
  [2m  â”‚[0m     },
  [2m  â”‚[0m     {
  [2m  â”‚[0m       "oldStart": 61,
  [2m  â”‚[0m       "oldLines": 10,
  [2m  â”‚[0m       "newStart": 74,
  [2m  â”‚[0m       "newLines": 12,
  [2m  â”‚[0m       "lines": [
  [2m  â”‚[0m         " - Operations marked `(L)` run locally on your laptop",
  [2m  â”‚[0m         " - Operations marked `(R)` run remotely in MotherDuck cloud",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Manual Control (When Needed)",
  [2m  â”‚[0m         "+This transparency helps you understand and optimize the execution strategy.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-While the optimizer usually makes the best choice, you can manually control execution location using the `md_run` parameter:",
  [2m  â”‚[0m         "+## Manual Control (Optional)",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "+While the automatic optimization usually makes the best choice, you can manually control execution location using the `md_run` parameter:",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         " ```sql",
  [2m  â”‚[0m         " -- Force S3 scan to run locally (downloads data first)",
  [2m  â”‚[0m         " FROM read_parquet('s3://bucket/file.parquet', md_run = 'local')"
  [2m  â”‚[0m       ]
  [2m  â”‚[0m     },
  [2m  â”‚[0m     {
  [2m  â”‚[0m       "oldStart": 73,
  [2m  â”‚[0m       "oldLines": 38,
  [2m  â”‚[0m       "newStart": 88,
  [2m  â”‚[0m       "newLines": 62,
  [2m  â”‚[0m       "lines": [
  [2m  â”‚[0m         " FROM read_parquet('s3://bucket/file.parquet', md_run = 'remote')",
  [2m  â”‚[0m         " ```",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-For a 2GB S3 Parquet file, remote execution in MotherDuck (25 seconds) is typically faster than local execution that requires downloading first (36 seconds).",
  [2m  â”‚[0m         "+**Performance comparison**: In testing, querying a 2GB Parquet file on S3 remotely in MotherDuck (25 seconds) was significantly faster than downloading and processing locally (36 seconds). By pushing computation to where the data lives, MotherDuck minimizes network I/O and delivers results more quickly.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Connecting Local DuckDB to MotherDuck",
  [2m  â”‚[0m         "+## Advanced CSV Handling",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-To enable this hybrid execution, simply attach your local DuckDB instance to MotherDuck:",
  [2m  â”‚[0m         "+MotherDuck's CSV reader automatically detects structure (headers, delimiters, data types), but you can override for messy files:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         " ```sql",
  [2m  â”‚[0m         "-ATTACH 'md:my_database';",
  [2m  â”‚[0m         "+-- Let MotherDuck auto-detect (recommended)",
  [2m  â”‚[0m         "+FROM 'local_sales.csv'",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+-- Or specify details for problematic CSVs",
  [2m  â”‚[0m         "+FROM read_csv('local_sales.csv',",
  [2m  â”‚[0m         "+    delim = '|',                    -- Custom delimiter",
  [2m  â”‚[0m         "+    header = true,                  -- Force header presence",
  [2m  â”‚[0m         "+    dateformat = '%m/%d/%Y',        -- Custom date format",
  [2m  â”‚[0m         "+    ignore_errors = true            -- Skip problematic rows",
  [2m  â”‚[0m         "+)",
  [2m  â”‚[0m         " ```",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-After this, any query can seamlessly mix local files and cloud data.",
  [2m  â”‚[0m         "+For more advanced CSV techniques, see the \"Taming Wild CSVs\" guide in the sources below.",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Additional Tips for CSV Files",
  [2m  â”‚[0m         "+## Querying Multiple Files in S3",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-MotherDuck automatically detects CSV structure, but you can override if needed:",
  [2m  â”‚[0m         "+You can also query entire folders of files in S3 as a single table using glob patterns:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         " ```sql",
  [2m  â”‚[0m         "--- Let MotherDuck auto-detect everything",
  [2m  â”‚[0m         "-FROM 'local_sales.csv'",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "--- Or specify details for messy CSVs",
  [2m  â”‚[0m         "-FROM read_csv('local_sales.csv',",
  [2m  â”‚[0m         "-    delim = '|',",
  [2m  â”‚[0m         "-    header = true,",
  [2m  â”‚[0m         "-    dateformat = '%m/%d/%Y'",
  [2m  â”‚[0m         "-)",
  [2m  â”‚[0m         "+SELECT",
  [2m  â”‚[0m         "+    event_type,",
  [2m  â”‚[0m         "+    COUNT(*) AS event_count",
  [2m  â”‚[0m         "+FROM read_parquet('s3://my-bucket/logs/2025/**/*.parquet')",
  [2m  â”‚[0m         "+GROUP BY event_type;",
  [2m  â”‚[0m         " ```",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "+This treats all matching Parquet files as one unified dataset, with intelligent filter pushdown to minimize data scanned.",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+## Why Data Engineers Are Excited",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+Data engineers love dual-execution engines because they solve the \"it worked on my machine\" problem. Since MotherDuck uses the exact same DuckDB engine locally and in the cloud:",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+- **Faster Development**: Build and test pipelines with zero-latency local feedback before scaling to cloud",
  [2m  â”‚[0m         "+- **Lower Costs**: Use free laptop compute and minimize data transfer",
  [2m  â”‚[0m         "+- **Consistency Guaranteed**: A query that works on your laptop is guaranteed to work in the cloud",
  [2m  â”‚[0m         "+- **Smooth Scale-Up**: Start local-first and seamlessly push more workload to cloud as needs grow",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+## Real-World Performance",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+Companies have achieved dramatic improvements with this approach:",
  [2m  â”‚[0m         "+- **Finqore**: Reduced 8-hour data pipelines to 8 minutes (60x improvement)",
  [2m  â”‚[0m         "+- **Gardyn**: Cut pipeline time from 24+ hours to under 1 hour",
  [2m  â”‚[0m         "+- **UDisc**: Reduced dbt jobs from 6 hours to 30 minutes, with queries dropping from minutes to 5 seconds",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         " ## Sources",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-- [Hybrid Analytics: Query Local & Cloud Data Instantly](https://motherduck.com/learn-more/hybrid-analytics-guide) - Comprehensive guide explaining dual execution and how to join local CSVs with cloud data",
  [2m  â”‚[0m         "-- [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained) - Detailed explanation of MotherDuck's dual execution architecture",
  [2m  â”‚[0m         "-- [The No-ETL Playbook: Query Raw CSV & JSON Files Directly with SQL](https://motherduck.com/learn-more/no-etl-query-raw-files) - How to query raw files without building ETL pipelines",
  [2m  â”‚[0m         "-- [Taming Wild CSVs: Advanced DuckDB Techniques](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering) - Advanced CSV handling techniques",
  [2m  â”‚[0m         "-- [Querying Data From S3 With 3 Lines In Your Terminal](https://motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal) - Quick demonstration of S3 querying",
  [2m  â”‚[0m         "+- [Hybrid Analytics: Query Local & Cloud Data Instantly](https://motherduck.com/learn-more/hybrid-analytics-guide/) - Comprehensive guide on joining local CSVs with cloud data, including detailed explanation of Dual Execution",
  [2m  â”‚[0m         "+- [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained/) - Technical deep dive into MotherDuck's dual execution architecture and query optimization from MotherDuck founding engineer",
  [2m  â”‚[0m         "+- [The No-ETL Playbook: How to Query Raw CSV & JSON Files Directly with SQL](https://motherduck.com/learn-more/no-etl-query-raw-files/) - Guide on querying raw files in S3 and joining local files with cloud data",
  [2m  â”‚[0m         "+- [Taming Wild CSVs: Advanced DuckDB Techniques for Data Engineers](https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/) - Advanced CSV handling techniques including error handling and schema detection",
  [2m  â”‚[0m         "+- [S3 bucket - DuckDB Data Engineering Glossary](https://motherduck.com/glossary/S3 bucket/) - How to query data stored in S3 buckets with DuckDB syntax",
  [2m  â”‚[0m         "+- [Querying Data From S3 With 3 Lines In Your Terminal](https://motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal/) - Quick demonstration of S3 querying"
  [2m  â”‚[0m       ]
  [2m  â”‚[0m     }
  [2m  â”‚[0m   ]
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! I've successfully completed the task. Here's what I did:
  [94mâ”‚[0m 
  [94mâ”‚[0m ## Summary
  [94mâ”‚[0m 
  [94mâ”‚[0m I searched through the `.kurt/sources/` directory containing 874 MotherDuck documentation files and found the most relevant sources to answer your question about efficiently joining a local CSV with an S3 table using MotherDuck.
  [94mâ”‚[0m 
  [94mâ”‚[0m ### Key Findings:
  [94mâ”‚[0m 
  [94mâ”‚[0m **The most efficient approach is to use MotherDuck's Dual Execution model**, which:
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. **Treats your laptop and cloud as one unified system** - Simply write a standard SQL join query referencing both local and cloud data
  [94mâ”‚[0m 2. **Intelligently minimizes data movement** - The query planner automatically:
  [94mâ”‚[0m    - Scans and filters the local CSV on your laptop
  [94mâ”‚[0m    - Sends only the filtered results (potentially just kilobytes) to the cloud
  [94mâ”‚[0m    - Performs the join against the S3 table in the cloud where it lives
  [94mâ”‚[0m 3. **Reduces network traffic by orders of magnitude** - Instead of uploading entire CSVs or downloading entire S3 tables
  [94mâ”‚[0m 
  [94mâ”‚[0m ### The answer includes:
  [94mâ”‚[0m - **Practical SQL examples** showing how to join local and cloud data
  [94mâ”‚[0m - **Performance metrics** (e.g., 25 seconds for remote S3 queries vs 36 seconds for local download+processing)
  [94mâ”‚[0m - **Real-world case studies** (Finqore: 60x speedup, Gardyn: 24+ hours to <1 hour)
  [94mâ”‚[0m - **Advanced techniques** for CSV handling, manual execution control, and querying multiple S3 files
  [94mâ”‚[0m - **Six comprehensive sources** from the MotherDuck documentation
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer has been written to `/tmp/answer_without_kg_7.md` with a complete "Sources" section linking back to the relevant documentation.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 6,487 | Cost: $0.3559
     Cumulative: 6,487 tokens | $0.3559
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent has clearly summarized the task completion and provided information without asking for any user input.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 16 | Duration: 133.4s
â•‘    Tokens: 6,487 | Cost: $0.3559
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  LLM Judge score: 0.81

======================================================================
â“ Question 8/10
What's the difference between a MotherDuck database and a share?
======================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 8
â”‚ Search through the .kurt/sources/ directory to answer this question:

What's the difference between a MotherDuck database and a share?

Steps:
1. Search for the most relevant markdown files in .kurt/sources/
2. Read those files and synthesize the answer
3. Write a markdown answer (with a ## Sources section) to: /tmp/answer_without_kg_8.md
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I'll help you find the answer to this question by searching through the Kurt sources and synthesizing the information.
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ find .kurt/sources -name "*.md" -type f | head -20

  [96mğŸ”§ TOOL:[0m Grep â†’ database.*share|share.*database

  [96mğŸ”§ TOOL:[0m Grep â†’ MotherDuck database

  [96mğŸ”§ TOOL:[0m Grep â†’ share
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m .kurt/sources/motherduck.com/research/execution-guided-sql-generation.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/research/ssdtrain-framework-paper.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/research/accelerating-machine-learning-lifecycle.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/customer-support.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/datalab.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/secoda.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/cube.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/airbyte.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/marimo.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/zenlytic.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/bruin.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/ascend.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/hex.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/OneSignal.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/datawisp.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/Shearwater.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/astronomer.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/streamkap.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/puppygraph.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/ecosystem/codecentric.md
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-power-of-wasm-for-analytics-duckdb-in-the-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-surprising-birth-of-duckdb-ft-co-creator-hannes-muhleisen.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-if-sql-queries-returned-results-instantly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/spatial-data-management-with-duckdb-ft-mattforrest.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/speed-querying-stackoverflow-data-with-duckdb-ft-michael-hunger.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/sql-ide-safari-harlequin-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/take-flight-with-dbt-and-duckdb-dropping-dev-warehouse-costs-to-zero.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-bad-the-good-and-the-ugly-navigating-the-new-waves-of-machine-learning-ft-adi-polak.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-future-of-bi-exploring-the-impact-of-bi-as-code-tools-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 30,
  [2m  â”‚[0m   "appliedLimit": 30
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/hack-night.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/API.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dexibit.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-simple-joys-of-scaling-up.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/read-scaling-preview.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/redshift-files-hunt-for-big-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/scaling-duckdb-with-ducklings.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-brief.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-future-of-bi-bi-as-code-duckdb-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/postgres-duckdb-options.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/local-duckdb-ui-visual-data-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/log-processing-savings-bacalhau-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/estuary-streaming-cdc-replication.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-on-edge-raspberry-pi.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 18,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search more specifically for content about shares and databases:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ \bshares?\b

  [96mğŸ”§ TOOL:[0m Bash â†’ find .kurt/sources -name "*.md" -type f -exec grep -l "share" {} \; | grep -E "(doc|guide|concept|tutorial)" | head -20
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-power-of-wasm-for-analytics-duckdb-in-the-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-surprising-birth-of-duckdb-ft-co-creator-hannes-muhleisen.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-if-sql-queries-returned-results-instantly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/spatial-data-management-with-duckdb-ft-mattforrest.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/speed-querying-stackoverflow-data-with-duckdb-ft-michael-hunger.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/sql-ide-safari-harlequin-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/take-flight-with-dbt-and-duckdb-dropping-dev-warehouse-costs-to-zero.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-bad-the-good-and-the-ugly-navigating-the-new-waves-of-machine-learning-ft-adi-polak.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-future-of-bi-exploring-the-impact-of-bi-as-code-tools-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-in-100-seconds-by-a-duck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/moving-forward-from-ad-hoc-reports-with-duckdb-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/no-sql-standard-for-7-years.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/panel-fixing-the-data-engineering-lifecycle-coalesce-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/is-duckdb-the-secret-to-unlocking-your-gis-potential.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/leveraging-duckdb-and-delta-lake-together.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/lies-damn-lies-and-benchmarks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/machine-learning-and-ai-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-an-impact-with-a-research-project-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/monte-carlo-simulations-with-duckdb-ft-james-mcneill.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/more-than-a-vibe-ai-driven-sql-that-actually-works.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-ga-produck-tour.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 50,
  [2m  â”‚[0m   "appliedLimit": 50
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m .kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/semantic-layer-duckdb-tutorial.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me look for documentation files specifically about shares:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ find .kurt/sources -name "*.md" -type f | xargs grep -l "MotherDuck share" | head -10

  [96mğŸ”§ TOOL:[0m Bash â†’ ls .kurt/sources/motherduck.com/docs/ 2>/dev/null | head -20
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m 
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m .kurt/sources/motherduck.com/authors/ryan-boyd.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/redshift-files-hunt-for-big-data.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/motherduck-headquarters-seattle-opening.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/videos/a-new-paradigm-for-data-visualization-with-just-sql-markdown.md
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/blog/pg: No such file or directory
  [2m  â”‚[0m grep: to: No such file or directory
  [2m  â”‚[0m grep: motherduck: No such file or directory
  [2m  â”‚[0m grep: at: No such file or directory
  [2m  â”‚[0m grep: motherduck.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/DB-API: No such file or directory
  [2m  â”‚[0m grep: 2.0.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/command: No such file or directory
  [2m  â”‚[0m grep: line.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/DuckDB: No such file or directory
  [2m  â”‚[0m grep: CLI.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Apache: No such file or directory
  [2m  â”‚[0m grep: Superset.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/MotherDuck: No such file or directory
  [2m  â”‚[0m grep: extension.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Java: No such file or directory
  [2m  â”‚[0m grep: programming: No such file or directory
  [2m  â”‚[0m grep: language.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/data: No such file or directory
  [2m  â”‚[0m grep: type.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/data: No such file or directory
  [2m  â”‚[0m grep: load: No such file or directory
  [2m  â”‚[0m grep: tool: No such file or directory
  [2m  â”‚[0m grep: (dlt).md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Common: No such file or directory
  [2m  â”‚[0m grep: Table: No such file or directory
  [2m  â”‚[0m grep: Expressions: No such file or directory
  [2m  â”‚[0m grep: (CTEs).md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/ALTER: No such file or directory
  [2m  â”‚[0m grep: TABLE: No such file or directory
  [2m  â”‚[0m grep: statement.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/storage: No such file or directory
  [2m  â”‚[0m grep: layer.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/foreign: No such file or directory
  [2m  â”‚[0m grep: key.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Go: No such file or directory
  [2m  â”‚[0m grep: programming: No such file or directory
  [2m  â”‚[0m grep: language.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Jupyter: No such file or directory
  [2m  â”‚[0m grep: notebooks.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/data: No such file or directory
  [2m  â”‚[0m grep: model.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/data: No such file or directory
  [2m  â”‚[0m grep: app.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/INSERT: No such file or directory
  [2m  â”‚[0m grep: statement.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/data: No such file or directory
  [2m  â”‚[0m grep: pipeline.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/relational: No such file or directory
  [2m  â”‚[0m grep: object.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/data: No such file or directory
  [2m  â”‚[0m grep: build: No such file or directory
  [2m  â”‚[0m grep: tool: No such file or directory
  [2m  â”‚[0m grep: (dbt).md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/relational: No such file or directory
  [2m  â”‚[0m grep: database.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Online: No such file or directory
  [2m  â”‚[0m grep: Analytical: No such file or directory
  [2m  â”‚[0m grep: Processing: No such file or directory
  [2m  â”‚[0m grep: (OLAP).md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Apache: No such file or directory
  [2m  â”‚[0m grep: Arrow.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/window: No such file or directory
  [2m  â”‚[0m grep: functions.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/SELECT: No such file or directory
  [2m  â”‚[0m grep: statement.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Amazon: No such file or directory
  [2m  â”‚[0m grep: S3.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/EXPLAIN: No such file or directory
  [2m  â”‚[0m grep: clause.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Cloud: No such file or directory
  [2m  â”‚[0m grep: Storage.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/auto: No such file or directory
  [2m  â”‚[0m grep: inference.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/SQL: No such file or directory
  [2m  â”‚[0m grep: analytics.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/data: No such file or directory
  [2m  â”‚[0m grep: visualization.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Business: No such file or directory
  [2m  â”‚[0m grep: Intelligence: No such file or directory
  [2m  â”‚[0m grep: (BI).md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/PIVOT: No such file or directory
  [2m  â”‚[0m grep: clause.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/S3: No such file or directory
  [2m  â”‚[0m grep: bucket.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Julia: No such file or directory
  [2m  â”‚[0m grep: programming: No such file or directory
  [2m  â”‚[0m grep: language.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/JavaScript: No such file or directory
  [2m  â”‚[0m grep: programming: No such file or directory
  [2m  â”‚[0m grep: language.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Pandas: No such file or directory
  [2m  â”‚[0m grep: DataFrames.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/primary: No such file or directory
  [2m  â”‚[0m grep: key.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/data: No such file or directory
  [2m  â”‚[0m grep: engineering.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/relational: No such file or directory
  [2m  â”‚[0m grep: API.md: No such file or directory
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me try a different approach and look for the docs directory structure:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ find .kurt/sources/motherduck.com -type d | grep -i doc | head -10

  [96mğŸ”§ TOOL:[0m Bash â†’ ls -la .kurt/sources/motherduck.com/ | head -30
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m 
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m total 1176
  [2m  â”‚[0m drwxr-xr-x   51 julien  staff   1632 Nov 27 13:57 .
  [2m  â”‚[0m drwxr-xr-x    3 julien  staff     96 Nov 27 13:57 ..
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   4118 Nov 26 10:30 about-us.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   2154 Nov 26 10:30 acceptable-use-policy.md
  [2m  â”‚[0m drwxr-xr-x   55 julien  staff   1760 Nov 27 13:57 authors
  [2m  â”‚[0m drwxr-xr-x  178 julien  staff   5696 Nov 27 13:57 blog
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  12059 Nov 26 10:30 blog.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  15483 Nov 26 10:30 candidate-privacy.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  27511 Nov 26 10:30 careers.md
  [2m  â”‚[0m drwxr-xr-x   13 julien  staff    416 Nov 27 13:57 case-studies
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  15955 Nov 26 10:30 case-studies.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  20608 Nov 26 10:30 community-and-open-source.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   1734 Nov 26 10:30 community-code.md
  [2m  â”‚[0m drwxr-xr-x    4 julien  staff    128 Nov 27 13:57 contact-us
  [2m  â”‚[0m -rw-r--r--    1 julien  staff    901 Nov 26 10:30 contact-us.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   2380 Nov 26 10:30 customer-support.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   6132 Nov 26 10:30 duckdb-book-brief.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   4571 Nov 26 10:30 duckdb-book-summary-chapter2.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   4014 Nov 26 10:30 duckdb-book-summary-chapter3.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   4370 Nov 26 10:30 duckdb-book-summary-chapter4.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  11557 Nov 26 10:30 duckdb-news.md
  [2m  â”‚[0m drwxr-xr-x   68 julien  staff   2176 Nov 27 13:57 ecosystem
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  66511 Nov 26 10:30 ecosystem.md
  [2m  â”‚[0m drwxr-xr-x    3 julien  staff     96 Nov 27 13:57 events
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   2525 Nov 26 10:30 events.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   7249 Nov 26 10:30 fees-addendum.md
  [2m  â”‚[0m drwxr-xr-x    3 julien  staff     96 Nov 27 13:57 forms
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   1392 Nov 26 10:30 getting-started-with-motherduck.md
  [2m  â”‚[0m drwxr-xr-x   71 julien  staff   2272 Nov 27 13:57 glossary
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ ls -la .kurt/sources/motherduck.com/ | tail -30
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m drwxr-xr-x   68 julien  staff   2176 Nov 27 13:57 ecosystem
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  66511 Nov 26 10:30 ecosystem.md
  [2m  â”‚[0m drwxr-xr-x    3 julien  staff     96 Nov 27 13:57 events
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   2525 Nov 26 10:30 events.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   7249 Nov 26 10:30 fees-addendum.md
  [2m  â”‚[0m drwxr-xr-x    3 julien  staff     96 Nov 27 13:57 forms
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   1392 Nov 26 10:30 getting-started-with-motherduck.md
  [2m  â”‚[0m drwxr-xr-x   71 julien  staff   2272 Nov 27 13:57 glossary
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   5549 Nov 26 10:30 hack-night.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  49247 Nov 26 10:30 index.md
  [2m  â”‚[0m drwxr-xr-x   37 julien  staff   1184 Nov 27 13:57 learn-more
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  20455 Nov 26 10:30 learn-more.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   1720 Nov 26 10:30 local-dev-cloud-prod-dagster-motherduck.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff    524 Nov 26 10:30 onboarding-sessions.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  17998 Nov 26 10:30 privacy-policy.md
  [2m  â”‚[0m drwxr-xr-x    8 julien  staff    256 Nov 27 13:57 product
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  26113 Nov 26 10:30 product.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  16979 Nov 26 10:30 quacking-around-the-web.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   4924 Nov 26 10:30 react-components-testing.md
  [2m  â”‚[0m drwxr-xr-x    5 julien  staff    160 Nov 27 13:57 research
  [2m  â”‚[0m -rw-r--r--    1 julien  staff    550 Nov 26 10:30 research.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   2381 Nov 26 10:30 sql-duckdb-book-form.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  12172 Nov 26 10:30 startups.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff    406 Nov 26 10:30 terms-of-service-app.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  39072 Nov 26 10:30 terms-of-service.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  22162 Nov 26 10:30 terms-of-use.md
  [2m  â”‚[0m -rw-r--r--    1 julien  staff   2528 Nov 26 10:30 trust-and-security.md
  [2m  â”‚[0m drwxr-xr-x   99 julien  staff   3168 Nov 27 13:57 videos
  [2m  â”‚[0m -rw-r--r--    1 julien  staff  86237 Nov 26 10:30 videos.md
  [2m  â”‚[0m drwxr-xr-x    6 julien  staff    192 Nov 27 13:57 webinar
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me search through the learn-more and blog directories for content about databases and shares:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ database.*share|share.*database|creating a share|share data|shared database
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/hack-night.md-94--- Join both datasets, and create a table in MotherDuck\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/hack-night.md-95-CREATE OR REPLACE TABLE animals_wiki AS (SELECT * FROM animals LEFT JOIN wiki ON wiki.title = animals.scientific_name);\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/hack-night.md-96-\n.kurt/sources/motherduck.com/hack-night.md:97:-- Create a SHARE of your database, to share it with others in MotherDuck\n.kurt/sources/motherduck.com/hack-night.md-98-(Learn more about shares here: https://motherduck.com/docs/key-tasks/sharing-data/sharing-overview)\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/hack-night.md-99-CREATE SHARE hacknight FROM my_db (ACCESS UNRESTRICTED);\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/hack-night.md-100-```\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/product/duckdb-users.md-28-\n.kurt/sources/motherduck.com/product/duckdb-users.md-29-![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fshare-data-not-files.db8baefd.png&w=3840&q=75)\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/product/duckdb-users.md-30-\n.kurt/sources/motherduck.com/product/duckdb-users.md:31:### Share Data, Not Files\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/product/duckdb-users.md-32-\n.kurt/sources/motherduck.com/product/duckdb-users.md:33:No more exporting files or managing S3 buckets to share your work. Create database shares with specific team members while maintaining the familiar DuckDB experience.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/product/duckdb-users.md-34-\n.kurt/sources/motherduck.com/product/duckdb-users.md-35-![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmultiple-users-one-database.0b9d7dc7.png&w=3840&q=75)\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/product/duckdb-users.md-36-\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/product/duckdb-users.md-122-\n.kurt/sources/motherduck.com/product/duckdb-users.md-123-[![Notebook-like UI feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-ui.c08b01e4.png&w=1080&q=75)\\\\\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/product/duckdb-users.md-124-**Notebook-like UI** \\\\\n.kurt/sources/motherduck.com/product/duckdb-users.md:125:Use the web to browse the data catalog, write SQL, filter and sort results and share data.\\\\\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/product/duckdb-users.md-126-\\\\\n.kurt/sources/motherduck.com/product/duckdb-users.md-127-More Details](https://motherduck.com/product/duckdb-users/notebook-like-ui/)\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/product/duckdb-users.md-128-\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/index.md-95-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/index.md-96-### Data Warehousing\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/index.md-97-\n.kurt/sources/motherduck.com/index.md:98:Is your data all over the place? Start making sense of your business by bringing it together for internal business intelligence and analytics. Build pure SQL pipelines, share data and quickly collaborate with your team.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/index.md-99-\n.kurt/sources/motherduck.com/index.md-100-[How it Works](https://motherduck.com/docs/getting-started/data-warehouse/)\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/index.md-101-\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/case-studies/dexibit.md-37-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/case-studies/dexibit.md-38-Our customers need an interactive dashboard experience, so we really love the performance we get with MotherDuck. It is super fast, and our customers love it. And when we're talking super fast, we're talking about analytical load times being just a few seconds when running many ad hoc queries at once. And that is great when you compare that to alternative solutions.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/case-studies/dexibit.md-39-\n.kurt/sources/motherduck.com/case-studies/dexibit.md:40:[Omitted long matching line]\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/case-studies/dexibit.md-41-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/case-studies/dexibit.md-42-We are confident we have chosen an innovative partner in the data space who influences our growth with these emergent use cases for our product. If we went with Snowflake, we wouldn't have the option of thinking about getting data into our client's browsers. If we stayed with Postgres, we wouldn't have the option of just connecting to a share.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/case-studies/dexibit.md-43-\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/blog/the-simple-joys-of-scaling-up.md-105-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/blog/the-simple-joys-of-scaling-up.md-106-The actual protocol here is not as important as the fact that this is the simplest case of one of the more basic algorithms for distributed consensus. Engineering millenia have gone into implementing these algorithms. On a single node system, these algorithms are generally unnecessary.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/blog/the-simple-joys-of-scaling-up.md-107-\n.kurt/sources/motherduck.com/blog/the-simple-joys-of-scaling-up.md:108:Building software for distributed systems is just harder than building on a single node. Distributed databases need to worry about shuffling data between nodes for joins, and aligning data to particular nodes. Single-node systems are dramatically simpler; to do a join you just create a hash table and share pointers. There are no independent failures that you have to recover from.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/blog/the-simple-joys-of-scaling-up.md-109-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/blog/the-simple-joys-of-scaling-up.md-110-The downsides of complexity aren\u2019t just felt by the programmers building the systems themselves. Abstractions leak, so things like eventual consistency, storage partitioning, and failure domains need to be handled by developers and end users. The CAP theorem is real, so users of distributed systems will need to make active tradeoffs between consistency, availability, and what to do when you get network failures.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/blog/the-simple-joys-of-scaling-up.md-111-\n--",
  [2m  â”‚[0m   "numLines": 50,
  [2m  â”‚[0m   "appliedLimit": 50
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Good! I found a reference to shares. Let me search more specifically for documentation about shares:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ CREATE SHARE|sharing-overview|shares|ATTACH.*share
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/sql-ide-safari-harlequin-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-in-100-seconds-by-a-duck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/no-sql-standard-for-7-years.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/more-than-a-vibe-ai-driven-sql-that-actually-works.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-ga-produck-tour.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-bootstrap-a-data-warehouse-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/fixing-sql-spaghetti-effective-refactoring-techniques.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/friendly-sql-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/building-data-apps-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/quacking-around-the-web.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/hack-night.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/careers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dexibit.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 30,
  [2m  â”‚[0m   "appliedLimit": 30
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me read some of the most relevant files that mention shares:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/hack-night.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/product/duckdb-users.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/hack-night.md",
  [2m  â”‚[0m     "content": "---\ntitle: hack-night\ncontent_type: tutorial\nsource_url: https://motherduck.com/hack-night\nindexed_at: '2025-11-25T20:37:01.144440'\ncontent_hash: b7c50bbf0ba2814e\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n# Quickstart Challenge\n\n![Header](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGroup_48096402_b6ae83825f.png&w=3840&q=75)\n\n## Use **Hugging Face** datasets and **MotherDuck** to enrich and prepare your dataset for your project.\n\nCheck out the example we've included below to see how you could explore endangered species, and a couple downstream ideas. Feel free to explore your own ideas and be creative!\n\n- While the examples below cover Python and SQL, MotherDuck supports multiple clients, such as Node.JS, Golang, Java, and Rust. More information on these clients is available\u00a0[here](https://duckdb.org/docs/api/overview).\n- You can also use the [MotherDuck Web UI](https://app.motherduck.com/) to explore data, visualize your tables with\u00a0[Column Explorer](https://motherduck.com/blog/introducing-column-explorer/),\u00a0and take advantage of MotherDuck\u2019s AI SQL error fixer,\u00a0[FixIt](https://motherduck.com/blog/introducing-fixit-ai-sql-error-fixer/).\n\n## Getting Started with MotherDuck in Python\n\n### Head to [https://motherduck.com](https://motherduck.com/), and create an account.\n\n- Every new account receives a 30-day free trial of the MotherDuck Standard Plan, with no credit card required.\n- After the end of your Standard Plan free trial, your account will automatically move to the MotherDuck Free Plan, no action needed on your part.\n\n### How to get started with MotherDuck in Python:\n\n- In the MotherDuck UI, grab an access token to connect with Python.\n- Next, run the following in Python:\n\n```ini\nCopy code\n\n!pip install duckdb==1.0.0\nimport duckdb\n\n# Connect to MotherDuck\u2028con = duckdb.connect('md:?motherduck_token=<your_motherduck_token>')\n\n# Run a sample query using MotherDuck\nres = con.execute(\"\"\"\nSELECT\n\u00a0\u00a0created_date, agency_name, complaint_type,\n\u00a0\u00a0descriptor, incident_address, resolution_description\nFROM\n\u00a0\u00a0sample_data.nyc.service_requests\nWHERE\n\u00a0\u00a0created_date >= '2022-03-27' AND\n\u00a0\u00a0created_date <= '2022-03-31';\n\"\"\")\n\n# Fetch MotherDuck query results to pandas df\ndf = res.df()\n```\n\n### Reading a dataset from Hugging Face using Python:\n\n```ini\nCopy code\n\n# Run a query on Hugging Face data, using MotherDuck\nhf_query = con.execute(\"\"\"\nSELECT *\nFROM read_parquet('hf://datasets/<user>/<dataset-name>/data/*.parquet'));\n\"\"\")\n```\n\nRead more about using Hugging Face with DuckDB and MotherDuck [in the documentation here](https://duckdb.org/docs/extensions/httpfs/hugging_face.html).\n\n## Example Using SQL\n\n### Start with [this dataset](https://huggingface.co/datasets/datonic/threatened_animal_species) of 150k endangered species:\n\n- The dataset has very little information about the species. So it would be ideal to enrich with data from Wikipedia to enable downstream tasks: https://huggingface.co/datasets/wikimedia/wikipedia\n\n```sql\nCopy code\n\n-- Load endangered animal species dataset from Hugging Face (hf)\nCREATE OR REPLACE TABLE animals AS (SELECT * FROM read_parquet(\n'hf://datasets/datonic/threatened_animal_species/data/threatened_animal_species.parquet'));\n\n-- Load wiki en dataset from hf (this may take a few minutes)\nCREATE OR REPLACE TABLE wiki AS (SELECT * FROM read_parquet('hf://datasets/wikimedia/wikipedia/20231101.en/*'));\n\n-- Join both datasets, and create a table in MotherDuck\nCREATE OR REPLACE TABLE animals_wiki AS (SELECT * FROM animals LEFT JOIN wiki ON wiki.title = animals.scientific_name);\n\n-- Create a SHARE of your database, to share it with others in MotherDuck\n(Learn more about shares here: https://motherduck.com/docs/key-tasks/sharing-data/sharing-overview)\nCREATE SHARE hacknight FROM my_db (ACCESS UNRESTRICTED);\n```\n\n## Sample Analysis Ideas with SQL\n\n```sql\nCopy code\n\n-- Show a sample of endangered animal species, including their wikipedia info\nSELECT * FROM animals_wiki LIMIT 100;\n\n-- Check how many animal species have a wikipedia entry\nSELECT count(*) FROM animals_wiki WHERE text IS NOT NULL;\n\n-- Check the distribution of endangerment categories across all species\nSELECT category, count(*) AS cnt FROM animals_wiki\nGROUP BY category\nORDER BY cnt DESC;\n\n-- Check how many wikipedia articles contain the word \u201cendangered\u201d and which endangerment category those animals are in\nSELECT category, count(*) AS cnt FROM animals_wiki\nWHERE text IS NOT NULL AND text LIKE '%endangered%'\nGROUP BY category\nORDER BY cnt DESC;\n```\n\n## Downstream Task Ideas\n\n- How many endangered duck species are there?\n- DuckDB versions are named after duck species. Which DuckDB version has the most endangered duck as its namesake?\n- Help Wikipedia editors keep endangerment information in articles up-to-date.\n- Try to find most relevant Wikipedia article for animals that didn\u2019t have an exact match based on the scientific\\_name.\n- Enrich endangered species data with structured data extracted from Wikipedia articles.\n\n![Footer](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGroup_48096401_99dbfb925f.png&w=3840&q=75)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 135,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 135
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     "content": "---\ntitle: duckdb-users\ncontent_type: product_page\nsource_url: https://motherduck.com/product/duckdb-users/\nindexed_at: '2025-11-25T20:15:52.565114'\ncontent_hash: dfb4ed9cd3383b9d\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n# local + Cloud analytics,  backed by the same database\n\nA duckdb-powered data warehouse for production grade analytics\n\n[Get Started](https://motherduck.com/get-started/)\n\nLEARN MORE\n\nWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuckWhy MotherDuck\n\n## Transform Solo DuckDB Into a Collaborative Powerhouse\n\nDuckDB's in-process nature means your data stays trapped on your machine. MotherDuck eliminates this limitation with a simple \"md:\" prefix.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fshare-data-not-files.db8baefd.png&w=3840&q=75)\n\n### Share Data, Not Files\n\nNo more exporting files or managing S3 buckets to share your work. Create database shares with specific team members while maintaining the familiar DuckDB experience.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmultiple-users-one-database.0b9d7dc7.png&w=3840&q=75)\n\n### Multiple Users, One Database\n\nEnable simultaneous access for Data Engineers, Analytics teams, and Scientists to the same datasets. Each user gets their own dedicated compute instance \u201cduckling\u201d for concurrent queries without conflicts.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-bring-the-flock-together.be57c6e9.png&w=2048&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-bring-the-flock-together.be57c6e9.png&w=2048&q=75)\n\n## Local Speed with Cloud-Scale Production Power\n\nEnjoy the best of both worlds with a seamless dev/prod workflow for your data. Develop locally for immediate feedback, then deploy identical workloads to the cloud with zero code changes.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Frapid-iteration-zero-cost.ff94f539.png&w=3840&q=75)\n\n### Rapid Iteration at Zero Cost\n\nExperiment locally with lightning-fast feedback cycles without spending a dime. When your solution is ready, your exact same queries work in the cloud by simply adding \"md:\" to your connection.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftest-safely-deploy-confidently.ed329fbf.png&w=3840&q=75)\n\n### Test Safely, Deploy Confidently\n\nPrototype complex data transformations without risking expensive cloud resources. This local dev, cloud prod model ensures what works locally will perform at scale in production.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fship-data-solutions-faster.bdfdd521.png&w=3840&q=75)\n\n### Ship Data Solutions Faster\n\nEliminate the friction between development and deployment environments. Your team can innovate with quick local tests and push proven solutions to cloud production seamlessly.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-scalable-node-architecture.91c200dd.png&w=1920&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-scalable-node-architecture.91c200dd.png&w=1920&q=75)\n\n## Familiar Interface with Intelligent Superpowers\n\nMotherDuck maintains the DuckDB interface you love while adding thoughtful, AI-backed intelligent assistance. Experience enhanced productivity with features designed for SQL experts and business users.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fvisual-insights-at-a-glance.60ec6358.png&w=3840&q=75)\n\n### Visual Insights at a Glance\n\nBeautiful visualizations help you understand complex data patterns immediately. MotherDuck's design affordances make exploration intuitive while preserving the power of DuckDB's analytical capabilities.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fautomatic-error-correction.fe59afe0.png&w=3840&q=75)\n\n### Automatic Error Correction\n\n\"Fix It\" repairs common syntax errors with a single click, eliminating frustration from missing commas or quotes. This intelligent assistance ensures everyone on your team can work productively with SQL, regardless of expertise level.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fnatural-language-to-sql.72331bc3.png&w=3840&q=75)\n\n### Natural Language to SQL\n\nNever written SQL before? Non-technical team members can contribute with \"PROMPT QUERY\" turning plain English into correct SQL. This feature democratizes data access by removing the barrier of SQL syntax knowledge.\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-ergonomic-and-effortless.b3f51a46.png&w=3840&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsticky-ergonomic-and-effortless.b3f51a46.png&w=3840&q=75)\n\n## Features\n\n[![Cloud database storage feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-cloud.3e9c0a0a.png&w=640&q=75)\\\\\n**Cloud database storage** \\\\\nManage your DuckDB database catalog in the cloud for easy scaling and collaboration.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/cloud-database-storage/)\n\n[![SQL analytics engine feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-analytics.f04308e0.png&w=640&q=75)\\\\\n**SQL analytics engine** \\\\\nEfficiently run the same SQL queries on data stored locally, in MotherDuck or in your data lake.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/sql-analytics-engine/)\n\n[![Simplified database sharing feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-db.11e966ef.png&w=640&q=75)\\\\\n**Simplified database sharing** \\\\\nCreate a shareable snapshot of your data, which your colleagues can easily attach and query in MotherDuck.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/simplified-database-sharing/)\n\n[![Dual query execution feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-query.28419698.png&w=640&q=75)\\\\\n**Dual query execution** \\\\\nUse idle compute on your laptop in concert with the cloud to increase speed and lower cost.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/dual-query-execution/)\n\n[![Notebook-like UI feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-ui.c08b01e4.png&w=1080&q=75)\\\\\n**Notebook-like UI** \\\\\nUse the web to browse the data catalog, write SQL, filter and sort results and share data.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/notebook-like-ui/)\n\n[![Strong DuckDB ecosystem feature](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffeature-ecosystem.0ab95f7c.png&w=640&q=75)\\\\\n**Strong DuckDB ecosystem** \\\\\nUse with 25+ tools in the modern data stack for import, orchestration and business intelligence.\\\\\n\\\\\nMore Details](https://motherduck.com/product/duckdb-users/strong-duckdb-ecosystem/)\n\n## Architecture\n\nManaged DuckDB-in-the-cloud\n\n## Ecosystem\n\nModern Duck Stack\n\n### CLOUD DATA WAREHOUSE\n\n### Sources\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpostgres-sql.d737f4f5.png&w=750&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Famazon-s3.72386dfc.png&w=640&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsalesforce.35d52b31.png&w=384&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fstripe.2160e881.png&w=384&q=75)\n\n[**Business Intelligence**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftableau.0ec523e2.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpower-bi.f3563826.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fomni.cb7aa381.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmetabase.5f8fe44e.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Business+Intelligence) [**Ingestion**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ffivetran.26f3817d.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fairbyte.8371d2f0.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Festuary.babad369.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlthub.e9892b97.png&w=384&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Ingestion) [**Data Science & AI**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhex.275dad7d.png&w=256&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fjupyter.2a6af3de.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fcolab.30ab10af.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmarimo.b699c73d.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Data+Science+%26+AI) [**Reverse ETL**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fcensus.352f1d69.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Reverse+ETL) [**Transformation**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdbt.fd2184d1.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Ftobiko.d0e3d1e5.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsql-mesh.6fceb012.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Transformation) [**Dev Tools**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdatagrip.f48eba23.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpuppy-graph.848400c6.png&w=750&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdagster.e1970a7c.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Dev+Tools)\n\n[**Orchestration**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fairflow.7f70081a.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdagster.e1970a7c.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fprefect.fd6371b3.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fkestra.aa10acfc.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Orchestration)\n\n[**Data Quality**\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fmonte-carlo.2143f962.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsecoda.9b7e86fb.png&w=640&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsoda.59e5aa02.png&w=384&q=75)\\\\\n\\\\\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fgreat-expectations.c544f1ef.png&w=640&q=75)\\\\\n\\\\\nMORE INFO](https://motherduck.com/ecosystem/?category=Data+Quality)\n\n## Case Studies\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nOur data pipelines used to take eight hours. Now they're taking eight minutes, and I see a world where they take eight seconds. This is why we made the big bet on DuckDB and MotherDuck. It's only possible with DuckDB and MotherDuck.\n\n[READ MORE](https://motherduck.com/case-studies/finqore/)\n\n![Jim O\u2019Neill's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fjim_oneil_photo_930c637f49.jpg&w=3840&q=75)\n\nJim O\u2019Neill\n\nCo-founder and CTO at FinQore\n\nOur data pipelines used to take eight hours. Now they're taking eight minutes, and I see a world where they take eight seconds. This is why we made the big bet on DuckDB and MotherDuck. It's only possible with DuckDB and MotherDuck.\n\n[READ MORE](https://motherduck.com/case-studies/finqore/)\n\nWith MotherDuck, it only took an hour to get an app up and running, which was pretty cool. It uses 18 million rows of data, but once it\u2019s loaded, we have all this interactivity right on the website. And since it's all coming from MotherDuck, we can create a shareable snapshot of our data that others can attach to and query.\n\n[READ MORE](https://motherduck.com/case-studies/dominik-moritz/)\n\n![Dominik Moritz's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdominik_berlin_f8a31170af.jpg&w=3840&q=75)\n\nDominik Moritz\n\nProfessor at Carnegie Mellon University\n\nWith MotherDuck, it only took an hour to get an app up and running, which was pretty cool. It uses 18 million rows of data, but once it\u2019s loaded, we have all this interactivity right on the website. And since it's all coming from MotherDuck, we can create a shareable snapshot of our data that others can attach to and query.\n\n[READ MORE](https://motherduck.com/case-studies/dominik-moritz/)\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nOur data pipelines used to take eight hours. Now they're taking eight minutes, and I see a world where they take eight seconds. This is why we made the big bet on DuckDB and MotherDuck. It's only possible with DuckDB and MotherDuck.\n\n[READ MORE](https://motherduck.com/case-studies/finqore/)\n\n![Jim O\u2019Neill's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fjim_oneil_photo_930c637f49.jpg&w=3840&q=75)\n\nJim O\u2019Neill\n\nCo-founder and CTO at FinQore\n\nOur data pipelines used to take eight hours. Now they're taking eight minutes, and I see a world where they take eight seconds. This is why we made the big bet on DuckDB and MotherDuck. It's only possible with DuckDB and MotherDuck.\n\n[READ MORE](https://motherduck.com/case-studies/finqore/)\n\nWith MotherDuck, it only took an hour to get an app up and running, which was pretty cool. It uses 18 million rows of data, but once it\u2019s loaded, we have all this interactivity right on the website. And since it's all coming from MotherDuck, we can create a shareable snapshot of our data that others can attach to and query.\n\n[READ MORE](https://motherduck.com/case-studies/dominik-moritz/)\n\n![Dominik Moritz's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdominik_berlin_f8a31170af.jpg&w=3840&q=75)\n\nDominik Moritz\n\nProfessor at Carnegie Mellon University\n\nWith MotherDuck, it only took an hour to get an app up and running, which was pretty cool. It uses 18 million rows of data, but once it\u2019s loaded, we have all this interactivity right on the website. And since it's all coming from MotherDuck, we can create a shareable snapshot of our data that others can attach to and query.\n\n[READ MORE](https://motherduck.com/case-studies/dominik-moritz/)\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nMotherDuck\u2019s innovative approach to managing data and providing easy-to-use tooling has allowed us to expand our application use cases (e.g., getting data into the client's browser) and offer more value to our customers.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n![Ravi Chandra's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fravi_chandra_photo_7db3a4c457.jpg&w=3840&q=75)\n\nRavi Chandra\n\nCTO at Dexibit\n\nWe can create an instance per customer easily as opposed to Postgres, where it\u2019s a hassle to create and manage that many instances. We've now got these new levers for performance scaling because we can split and store the data and query efficiently as needed. If we need to handle a load spike or a huge amount of queries, we can spin up more ducklings on demand.\n\n[READ MORE](https://motherduck.com/case-studies/dexibit/)\n\n## Get in Touch\n\nWe\u2019d love to learn more about what you\u2019re building. A Product Expert will fly right back to you with resources to help you take flight.\n\nFirst Name\n\nLast Name\n\nE-mail\n\nCompany\n\nLeave us a message.\n\nThis site is protected by reCAPTCHA and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply.\n\nSubmit\n\n![Duck looking up](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fduck-looking-up.1c763865.png&w=3840&q=75)\n\nAuthorization Response\n\nreCAPTCHA\n\nRecaptcha requires verification.\n\n[Privacy](https://www.google.com/intl/en/policies/privacy/) \\- [Terms](https://www.google.com/intl/en/policies/terms/)\n\nprotected by **reCAPTCHA**\n\n[Privacy](https://www.google.com/intl/en/policies/privacy/) \\- [Terms](https://www.google.com/intl/en/policies/terms/)",
  [2m  â”‚[0m     "numLines": 403,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 403
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     "content": "---\ntitle: duckdb-motherduck-for-beginners-your-ultimate-guide\ncontent_type: event\nsource_url: https://motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide\nindexed_at: '2025-11-25T20:44:57.276294'\ncontent_hash: 9ac8939ea24395a7\nhas_code_examples: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nDuckDB & MotherDuck for Beginners: Your Ultimate Guide - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[DuckDB & MotherDuck for Beginners: Your Ultimate Guide](https://www.youtube.com/watch?v=WYV8hvJOAQE)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Why am I seeing this?](https://support.google.com/youtube/answer/9004474?hl=en)\n\n[Watch on](https://www.youtube.com/watch?v=WYV8hvJOAQE&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 36:27\n\n\u2022Live\n\n\u2022\n\nYouTube\n\n# DuckDB & MotherDuck for Beginners: Your Ultimate Guide\n\n2025/02/21\n\n## Why DuckDB is Revolutionizing Data Analytics\n\nDuckDB has experienced explosive growth in popularity, with download statistics showing remarkable adoption rates, particularly in the Python ecosystem. This open-source analytical database is designed to handle everything from quick data exploration to complex data pipelines, and it's already trusted by multiple companies in production environments.\n\n## Understanding the Small and Medium Data Revolution\n\nThe tech industry has long operated under the assumption that analytics requires big data infrastructure. However, this paradigm is being challenged by three key insights:\n\n1. **Most workloads aren't big data**: According to data from AWS Redshift and other cloud analytical databases, approximately 83% of users work with query sizes up to 1TB, and 94% work with data below 10TB.\n\n2. **Modern hardware is incredibly powerful**: Today's single machines can handle up to 24TB of memory on AWS, making distributed systems unnecessary for most use cases.\n\n3. **Distributed systems are expensive**: They require significant IO network traffic for coordination, and the development experience is considerably more complex.\n\n\nDuckDB capitalizes on these realities by enabling users to work with large datasets on a single machine, whether locally on a laptop or in the cloud via a scale-up strategy.\n\n## How DuckDB Works: The In-Process Advantage\n\nDuckDB is an open-source, in-process analytical database written in C++, designed as a single self-contained binary with all dependencies included. This architecture sets it apart from traditional databases.\n\n### Traditional Database Architecture\n\nDatabases typically fall into two categories:\n\n- **OLTP (Online Transaction Processing)**: Databases like PostgreSQL and MySQL, optimized for handling transactions with small datasets. Query times typically range from 1-10 milliseconds.\n\n- **OLAP (Online Analytical Processing)**: Databases like BigQuery and Snowflake, built for analytical queries processing large datasets. Query times can range from 100 milliseconds to several minutes.\n\n\nMost traditional databases use a client-server architecture where the database runs as a separate process, and applications connect to it through SQL queries.\n\n### The In-Process Revolution\n\nIn-process databases run directly within the application process itself. While SQLite pioneered this approach for OLTP workloads, DuckDB introduces something new: an in-process OLAP database optimized for analytical workloads.\n\nThis design enables DuckDB to:\n\n- Run on virtually any platform (laptops to cloud workflows)\n- Integrate seamlessly with any programming language\n- Execute in web browsers via WebAssembly\n- Eliminate network overhead for local operations\n\n## Getting Started with DuckDB\n\n### Installation and Basic Usage\n\nDuckDB can be installed through various methods:\n\n- Direct binary download for CLI usage\n- Package managers (Homebrew for macOS)\n- Language-specific packages (Python, R, Java, etc.)\n\nThe CLI provides a powerful interface for data exploration:\n\n```sql\nCopy code\n\n-- Simple query reading from S3\nFROM 's3://bucket/path/to/file.parquet' LIMIT 5;\n```\n\n### Key Features in Action\n\n**Friendly SQL Dialect**: DuckDB extends standard SQL with productivity enhancements, such as the FROM-first syntax shown above.\n\n**Automatic File Format Detection**: DuckDB automatically detects and handles various file formats including Parquet, CSV, JSON, Iceberg, and Delta Lake.\n\n**Extension System**: DuckDB's functionality is modular through extensions. Core extensions (like HTTPFS for S3 access) are auto-loaded when needed, while community extensions can be installed manually.\n\n## Data Persistence and the DuckDB File Format\n\nBy default, DuckDB operates in-memory, but it offers powerful persistence options:\n\n### Creating and Managing Databases\n\n```sql\nCopy code\n\n-- Attach or create a database\nATTACH 'mydatabase.ddb';\n\n-- Create a table from a query\nCREATE TABLE mytable AS SELECT * FROM source_data;\n```\n\n### The DuckDB File Format\n\nDuckDB's native file format (.ddb or .db) is:\n\n- Self-contained (all tables and metadata in one file)\n- ACID-compliant\n- Highly compressed\n- Optimized for analytical workloads\n\n### Exporting Data\n\nDuckDB supports seamless data export:\n\n```sql\nCopy code\n\n-- Export to CSV\nCOPY (SELECT * FROM mytable) TO 'output.csv';\n```\n\n## Managing Secrets and Authentication\n\nDuckDB includes a comprehensive secret management system for secure cloud access:\n\n```sql\nCopy code\n\n-- Create temporary secret using AWS credential chain\nCREATE SECRET (\n    TYPE S3,\n    PROVIDER credential_chain\n);\n```\n\nThis approach supports:\n\n- AWS SSO authentication\n- Temporary and persistent secrets\n- Multiple cloud providers\n- Secure credential storage\n\n## Scaling to the Cloud with MotherDuck\n\nMotherDuck supercharges DuckDB by transforming it from a single-player to a multiplayer analytics experience. The integration is remarkably simple:\n\n### Connecting to MotherDuck\n\n```sql\nCopy code\n\n-- Connect to MotherDuck with one command\nATTACH 'md:';\n```\n\nAuthentication requires only a MotherDuck token, which can be set as an environment variable.\n\n### Key MotherDuck Features\n\n**Dual Execution**: MotherDuck enables intelligent query execution, automatically determining whether to run computations locally or in the cloud based on data location and query requirements.\n\n**Database Sharing**: Create and share cloud databases with simple commands:\n\n```sql\nCopy code\n\n-- Create a share\nCREATE SHARE myshare FROM mydatabase;\n```\n\n**Performance Benefits**: Leveraging cloud infrastructure provides:\n\n- High-bandwidth connections to cloud storage\n- Elimination of local network bottlenecks\n- Seamless collaboration features\n\n### Cloud Storage Integration\n\nMotherDuck dramatically improves performance when querying cloud storage. In benchmarks, queries that take 11 seconds locally can complete in just 2 seconds when leveraging MotherDuck's cloud infrastructure and network proximity to storage.\n\n## The Complete Analytics Toolkit\n\nDuckDB and MotherDuck together provide:\n\n- **Simplicity**: One-command installation and cloud connection\n- **Flexibility**: Run anywhere from browsers to cloud environments\n- **Performance**: Lightning-fast analytical queries on small to medium datasets\n- **Integration**: Native support for numerous file formats and cloud providers\n- **Collaboration**: Easy data sharing and team workflows\n\nWhether you're analyzing data on your laptop, scaling computations in the cloud, or building production data pipelines, DuckDB offers a modern approach to analytics that challenges traditional big data assumptions while delivering exceptional performance and developer experience.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Data-based: Going Beyond the Dataframe\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FData_based_f32745b461.png&w=3840&q=75)](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n[2025-11-20](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n### [Data-based: Going Beyond the Dataframe](https://motherduck.com/videos/going-beyond-the-dataframe)\n\nLearn how to turbocharge your Python data work using DuckDB and MotherDuck with Pandas. We walk through performance comparisons, exploratory data analysis on bigger datasets, and an end-to-end ML feature engineering pipeline.\n\nWebinar\n\nPython\n\nAI, ML and LLMs\n\n[![\"Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FHex_Webinar_778e3959e4.png&w=3840&q=75)](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n[2025-11-14](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n### [Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck)\n\nAI isn't here to replace data work, it's here to make it better. Watch this webinar to see how Hex and MotherDuck build AI workflows that prioritize context, iteration, and real-world impact.\n\nWebinar\n\nAI, ML and LLMs\n\n[![\"Lies, Damn Lies, and Benchmarks\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FLies_Damn_Lies_and_Benchmarks_Thumbnail_404db1bf46.png&w=3840&q=75)](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n[2025-10-31](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n### [Lies, Damn Lies, and Benchmarks](https://motherduck.com/videos/lies-damn-lies-and-benchmarks)\n\nWhy do database benchmarks so often mislead? MotherDuck CEO Jordan Tigani discusses the pitfalls of performance benchmarking, lessons from BigQuery, and why your own workload is the only benchmark that truly matters.\n\nStream\n\nInterview\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 301,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 301
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Good! Now let me search for more specific documentation about databases vs shares:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ ATTACH.*share|shareable snapshot|create.*share.*database
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/case-studies/dominik-moritz.md-15-\n.kurt/sources/motherduck.com/case-studies/dominik-moritz.md-16-[GO BACK TO CASE STUDIES](https://motherduck.com/case-studies/)\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/case-studies/dominik-moritz.md-17-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/case-studies/dominik-moritz.md-18-# How Mosaic by Dominik Moritz Achieved Interactive Browser Visualization of 18 Million Data Points\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/case-studies/dominik-moritz.md-19-\n.kurt/sources/motherduck.com/case-studies/dominik-moritz.md:20:With MotherDuck, it only took an hour to get an app up and running, which was pretty cool. It uses 18 million rows of data, but once it\u2019s loaded, we have all this interactivity right on the website. And since it's all coming from MotherDuck, we can create a shareable snapshot of our data that others can attach to and query.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/case-studies/dominik-moritz.md-21-\n.kurt/sources/motherduck.com/case-studies/dominik-moritz.md-22-![Dominik Moritz's photo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdominik_berlin_f8a31170af.jpg&w=3840&q=75)\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/case-studies/dominik-moritz.md-23-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/case-studies/dominik-moritz.md-24-Dominik Moritz\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/case-studies/dominik-moritz.md-25-\n--\n.kurt/sources/motherduck.com/hack-night.md-92-CREATE OR REPLACE TABLE wiki AS (SELECT * FROM read_parquet('hf://datasets/wikimedia/wikipedia/20231101.en/*'));\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/hack-night.md-93-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/hack-night.md-94--- Join both datasets, and create a table in MotherDuck\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/hack-night.md-95-CREATE OR REPLACE TABLE animals_wiki AS (SELECT * FROM animals LEFT JOIN wiki ON wiki.title = animals.scientific_name);\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/hack-night.md-96-\n.kurt/sources/motherduck.com/hack-night.md:97:-- Create a SHARE of your database, to share it with others in MotherDuck\n.kurt/sources/motherduck.com/hack-night.md-98-(Learn more about shares here: https://motherduck.com/docs/key-tasks/sharing-data/sharing-overview)\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/hack-night.md-99-CREATE SHARE hacknight FROM my_db (ACCESS UNRESTRICTED);\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/hack-night.md-100-```\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/hack-night.md-101-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/hack-night.md-102-## Sample Analysis Ideas with SQL\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md-221-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md-222-### Key MotherDuck Features\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md-223-\n.kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md-224-**Dual Execution**: MotherDuck enables intelligent query execution, automatically determining whether to run computations locally or in the cloud based on data location and query requirements.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md-225-\n.kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md:226:**Database Sharing**: Create and share cloud databases with simple commands:",
  [2m  â”‚[0m   "numLines": 30,
  [2m  â”‚[0m   "appliedLimit": 30
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me search for more specific documentation about how shares work vs databases:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ read-only|snapshot|share.*read|share.*write|database.*writable
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md-115-While powerful, the catalog is also what holds many teams back from adopting Iceberg. According to Julien, the main barriers are:\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md-116-\n.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md-117-1. **Poor User Experience:** The APIs and tooling can be complex, especially for developers outside the JVM ecosystem (e.g., Python and Node.js users).\n.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md:118:2. **Table Maintenance:** Suddenly, tasks like compaction, cleaning up old snapshots, and optimizing file layouts become _your_ responsibility, not the warehouse's.\n.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md-119-3. **The Catalog Itself:** It's another critical piece of infrastructure you have to choose, deploy, and manage. This is often the biggest source of complexity and frustration\u2014what we call **\"catalog hell.\"**\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md-120-\n.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md-121-### The Iceberg Catalog Landscape: REST, Serverless & More\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md-177-ice commit trips --source yellow_tripdata.parquet\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md-178-```\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md-179-\n.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md:180:That's it! You've just created a new Iceberg table and committed your first snapshot. The workflow is intentionally `git`-like. You can even view the history of your table.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md-181-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md-182-```bash\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md-183-Copy code\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md-279-\n.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md-280-### [Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly)\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md-281-\n.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md:282:Hamilton Ulmer shares insights from MotherDuck's Instant SQL Mode, exploring how real-time query result previews eliminate the traditional write-run-debug cycle through client-side parsing and DuckDB-WASM caching.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md-283-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md-284-SQL\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md-285-\n--\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md-251-\n.kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md-252-### [Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly)\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md-253-\n.kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md:254:Hamilton Ulmer shares insights from MotherDuck's Instant SQL Mode, exploring how real-time query result previews eliminate the traditional write-run-debug cycle through client-side parsing and DuckDB-WASM caching.\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md-255-\n/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_n9u62akk/.kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md-256-SQL",
  [2m  â”‚[0m   "numLines": 30,
  [2m  â”‚[0m   "appliedLimit": 30
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search for files that specifically talk about "simplified database sharing":
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ find .kurt/sources/motherduck.com -name "*.md" -type f | xargs grep -l "simplified.*database.*sharing\|sharing.*database" | head -10
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m .kurt/sources/motherduck.com/authors/ryan-boyd.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/product/data-teams.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/product/duckdb-users.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-nov-2024.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/faster-ducks.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-oct-2024.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2025.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/building-motherduck-partner-ecosystem.md
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/blog/pg: No such file or directory
  [2m  â”‚[0m grep: to: No such file or directory
  [2m  â”‚[0m grep: motherduck: No such file or directory
  [2m  â”‚[0m grep: at: No such file or directory
  [2m  â”‚[0m grep: motherduck.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/DB-API: No such file or directory
  [2m  â”‚[0m grep: 2.0.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/command: No such file or directory
  [2m  â”‚[0m grep: line.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/DuckDB: No such file or directory
  [2m  â”‚[0m grep: CLI.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Apache: No such file or directory
  [2m  â”‚[0m grep: Superset.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/MotherDuck: No such file or directory
  [2m  â”‚[0m grep: extension.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Java: No such file or directory
  [2m  â”‚[0m grep: programming: No such file or directory
  [2m  â”‚[0m grep: language.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/data: No such file or directory
  [2m  â”‚[0m grep: type.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/data: No such file or directory
  [2m  â”‚[0m grep: load: No such file or directory
  [2m  â”‚[0m grep: tool: No such file or directory
  [2m  â”‚[0m grep: (dlt).md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Common: No such file or directory
  [2m  â”‚[0m grep: Table: No such file or directory
  [2m  â”‚[0m grep: Expressions: No such file or directory
  [2m  â”‚[0m grep: (CTEs).md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/ALTER: No such file or directory
  [2m  â”‚[0m grep: TABLE: No such file or directory
  [2m  â”‚[0m grep: statement.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/storage: No such file or directory
  [2m  â”‚[0m grep: layer.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/foreign: No such file or directory
  [2m  â”‚[0m grep: key.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Go: No such file or directory
  [2m  â”‚[0m grep: programming: No such file or directory
  [2m  â”‚[0m grep: language.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Jupyter: No such file or directory
  [2m  â”‚[0m grep: notebooks.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/data: No such file or directory
  [2m  â”‚[0m grep: model.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/data: No such file or directory
  [2m  â”‚[0m grep: app.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/INSERT: No such file or directory
  [2m  â”‚[0m grep: statement.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/data: No such file or directory
  [2m  â”‚[0m grep: pipeline.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/relational: No such file or directory
  [2m  â”‚[0m grep: object.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/data: No such file or directory
  [2m  â”‚[0m grep: build: No such file or directory
  [2m  â”‚[0m grep: tool: No such file or directory
  [2m  â”‚[0m grep: (dbt).md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/relational: No such file or directory
  [2m  â”‚[0m grep: database.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Online: No such file or directory
  [2m  â”‚[0m grep: Analytical: No such file or directory
  [2m  â”‚[0m grep: Processing: No such file or directory
  [2m  â”‚[0m grep: (OLAP).md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Apache: No such file or directory
  [2m  â”‚[0m grep: Arrow.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/window: No such file or directory
  [2m  â”‚[0m grep: functions.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/SELECT: No such file or directory
  [2m  â”‚[0m grep: statement.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Amazon: No such file or directory
  [2m  â”‚[0m grep: S3.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/EXPLAIN: No such file or directory
  [2m  â”‚[0m grep: clause.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Cloud: No such file or directory
  [2m  â”‚[0m grep: Storage.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/auto: No such file or directory
  [2m  â”‚[0m grep: inference.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/SQL: No such file or directory
  [2m  â”‚[0m grep: analytics.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/data: No such file or directory
  [2m  â”‚[0m grep: visualization.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Business: No such file or directory
  [2m  â”‚[0m grep: Intelligence: No such file or directory
  [2m  â”‚[0m grep: (BI).md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/PIVOT: No such file or directory
  [2m  â”‚[0m grep: clause.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/S3: No such file or directory
  [2m  â”‚[0m grep: bucket.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Julia: No such file or directory
  [2m  â”‚[0m grep: programming: No such file or directory
  [2m  â”‚[0m grep: language.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/JavaScript: No such file or directory
  [2m  â”‚[0m grep: programming: No such file or directory
  [2m  â”‚[0m grep: language.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/Pandas: No such file or directory
  [2m  â”‚[0m grep: DataFrames.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/primary: No such file or directory
  [2m  â”‚[0m grep: key.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/data: No such file or directory
  [2m  â”‚[0m grep: engineering.md: No such file or directory
  [2m  â”‚[0m grep: .kurt/sources/motherduck.com/glossary/relational: No such file or directory
  [2m  â”‚[0m grep: API.md: No such file or directory
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me read a blog post that might have more details:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-nov-2024.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-nov-2024.md",
  [2m  â”‚[0m     "content": "---\ntitle: data-warehouse-feature-roundup-nov-2024\ncontent_type: blog\nsource_url: https://motherduck.com/blog/data-warehouse-feature-roundup-nov-2024\nindexed_at: '2025-11-25T19:57:10.556721'\ncontent_hash: 4f829b09e8008753\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Improved Control and Ergonomics on MotherDuck\n\n2024/11/25 - 3 min read\n\nBY\n\n[Sheila Sitaram](https://motherduck.com/authors/sheila-sitaram/)\n\nAt MotherDuck, we\u2019ve been hard at work on new features to give you better tools for managing your accounts, scaling your applications, and handling individual queries. This month's Feature Roundup highlights recent updates designed to empower you with more control over your data and queries for a seamless, efficient experience.\n\nLet\u2019s dive in.\n\n## Query Monitoring and Management Functions\n\nMotherDuck now provides the ability to [monitor](https://motherduck.com/docs/sql-reference/motherduck-sql-reference/connection-management/monitor-connections/) and [interrupt](https://motherduck.com/docs/sql-reference/motherduck-sql-reference/connection-management/interrupt-connections/) active server connections with two new functions in Preview.\n\nDatabase activity monitoring gives users a real-time view of their active connections to understand their current load and database usage. `md_active_server_connections` is a table function that lists all server-side connections with active transactions.\n\nQuickly identify long-running queries and problematic connections to support resource optimization and monitor active transactions to prevent disruptions during schema changes or database maintenance.\nFurthermore, users can now interrupt active transactions on a server-side connection with the `md_interrupt_server_connection` scalar function. Doing so will fail / rollback the active transaction while allowing the connection to be used for future transactions and queries.\n\nTogether, these functions support a complete workflow for understanding query performance and interrupting ad-hoc or erroneous queries without requiring a fresh connection setup. In a multi-user context, [Org Admins](https://motherduck.com/docs/key-tasks/managing-organizations/#roles) can identify problematic queries from one user and use `client_connection_id` from the active server connections returned with `md_active_server_connections` to interrupt the stalled connection using `md_interrupt_server_connection`, all without impacting other users or services that rely on that same connection.\n\n## Specify Attach Mode for Streamlined Connections to MotherDuck\n\nMotherDuck now saves you time when you only need to connect to a single database by allowing you to specify the attach mode when connecting.\n\nMotherDuck\u2019s data warehouse [sharing model](https://motherduck.com/docs/key-tasks/sharing-data/sharing-overview/) operates at the database level. Shares are read-only databases that are purpose-built for data collaboration and ad-hoc analytics. These zero-copy clones help savvy data leaders and small teams derive insights without directly accessing the production dataset. Shares can be [attached](https://motherduck.com/docs/sql-reference/motherduck-sql-reference/attach-share/) and [updated](https://motherduck.com/docs/sql-reference/motherduck-sql-reference/update-share/) manually or automatically by the Share\u2019s creator.\n\nSpecifying `attach_mode={single|workspace}` lets you tailor your connection to your needs. Single database attach mode simplifies the connection process when you are only working with a single database by streamlining your workflow and removing unnecessary setup steps.\n\nUse `attach_mode=single` in scenarios where you only need to query a single database. It simplifies the connection by ensuring no additional workspace context or databases are involved.\n\nTo access multiple databases as part of cross-database workflows, use `attach_mode=workspace` instead.\n\nThe value of specifying attach mode ultimately comes down to intent. Being explicit ensures MotherDuck can optimize the connection behavior for your use case to streamline operations.\n\n## In-Memory Queries are (even more!) Efficient and Powerful\n\nAs part of our commitment to continuous improvement, our Platform team is constantly tuning our infrastructure to give you the best experience possible. MotherDuck\u2019s [architecture](https://motherduck.com/docs/architecture-and-capabilities/) is built around the power of scaling up with highly efficient and scalable single nodes.\n\nMotherDuck now enables you to run larger queries in-memory so you can handle more complex workloads and data-intensive queries with ease.\n\n## Take Flight\n\nLet us know how you\u2019re using MotherDuck: Share your success stories and feedback with us on [Slack](https://join.slack.com/t/motherduckcommunity/shared_invite/zt-2hh1g7kec-Z9q8wLd_~alry9~VbMiVqA). If you\u2019d like to discuss your use case in more detail, please [connect with us](https://motherduck.com/contact-us/sales/) \\- we\u2019d love to learn more about what you\u2019re building and how we can make your MotherDuck experience even better.\n\nHappy querying!\n\n### TABLE OF CONTENTS\n\n[Query Monitoring and Management Functions](https://motherduck.com/blog/data-warehouse-feature-roundup-nov-2024/#query-monitoring-and-management-functions)\n\n[Specify Attach Mode for Streamlined Connections to MotherDuck](https://motherduck.com/blog/data-warehouse-feature-roundup-nov-2024/#specify-attach-mode-for-streamlined-connections-to-motherduck)\n\n[In-Memory Queries are Efficient and Powerful](https://motherduck.com/blog/data-warehouse-feature-roundup-nov-2024/#in-memory-queries-are-efficient-and-powerful)\n\n[Take Flight](https://motherduck.com/blog/data-warehouse-feature-roundup-nov-2024/#take-flight)\n\nSubscribe to DuckDB Newsletter\n\nE-mail\n\nSubscribe to other MotherDuck news\n\nSubmit\n\nSubscribe\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![From Data Lake to Lakehouse: Can DuckDB be the best portable data catalog?](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_catalog_51d1dc1d0c.png&w=3840&q=75)](https://motherduck.com/blog/from-data-lake-to-lakehouse-duckdb-portable-catalog/)\n\n[2024/11/14 - Mehdi Ouazza](https://motherduck.com/blog/from-data-lake-to-lakehouse-duckdb-portable-catalog/)\n\n### [From Data Lake to Lakehouse: Can DuckDB be the best portable data catalog?](https://motherduck.com/blog/from-data-lake-to-lakehouse-duckdb-portable-catalog)\n\nDiscover how catalog became crucial for Lakehouse and how DuckDB can help as a catalog\n\n[![How to Extract Analytics from Bluesky, the New Open Social Network](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fthumbnail_bsky2_239933cbc2.png&w=3840&q=75)](https://motherduck.com/blog/how-to-extract-analytics-from-bluesky/)\n\n[2024/11/20 - Simon Sp\u00e4ti, Mehdi Ouazza](https://motherduck.com/blog/how-to-extract-analytics-from-bluesky/)\n\n### [How to Extract Analytics from Bluesky, the New Open Social Network](https://motherduck.com/blog/how-to-extract-analytics-from-bluesky)\n\nDiscover how to build data pipelines to get insights from Bluesky\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 116,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 116
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md",
  [2m  â”‚[0m     "content": "---\ntitle: exploring-stackoverflow-with-duckdb-on-motherduck-2\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2\nindexed_at: '2025-11-25T19:57:26.723171'\ncontent_hash: e4f1c15b5c200e6d\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Exploring StackOverflow with DuckDB on MotherDuck (Part 2)\n\n2023/10/02 - 11 min read\n\nBY\n\n[Michael Hunger](https://motherduck.com/authors/michael-hunger/)\n\n## From Local to Cloud - Loading our Database into MotherDuck and Querying it with AI Prompts\n\nIn the\u00a0[first part of the series](https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-1/)\u00a0we looked at the full StackOverflow dump as an interesting dataset to explore with DuckDB. We downloaded the data, converted it to CSV and loaded it into DuckDB and explored tags, users and posts a bit before exporting the database to Parquet. Today we want to move from our local evironment to MotherDuck, where we want to look at importing these parquet files into a database, sharing that database with you and exploring the data with the new AI prompt features.\n\n## Getting started with MotherDuck\n\nDuckDB itself is focusing on local, and in-process execution of the analytical database engine. While you can access remote data, it\u2019s downloaded to your machine every time you access the remote files, so you really might want to move your DuckDB execution to where the data lives.\n\nTo make it easier to query data that resides in other, remote locations, MotherDuck offers a managed service that allows you to run DuckDB in the cloud.\n\nWith MotherDuck you can query the data on your cloud storage transparently as if it was local. But what\u2019s even better, is you can join and combine local tables transparently with data in tables residing in the cloud. The MotherDuck UI runs a build of DuckDB WASM in your browser, so the operations in the database that can be executed and rendered locally, are executed inside your web-browser.\n\nHere is a picture of the architecture from the\u00a0[documentation](https://motherduck.com/docs/architecture-and-capabilities/):\n\n![motherduck_hld](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmotherduck_hld_081bc8a023.png%3Fupdated_at%3D2023-10-02T11%3A52%3A29.138Z&w=3840&q=75)\n\nMotherduck also integrates with Python and all the other access libraries and integrations for DuckDB.\n\nIf you already signed up, you can just log-in to MotherDuck, otherwise you can create an account on the\u00a0[homepage](https://motherduck.com/)\u00a0(via Google, GitHub or email auth).\n\nAnywhere you can run DuckDB you can use MotherDuck as it connects through an official DuckDB extension which is downloaded & loaded as soon as you connect to a motherduck database through\u00a0`.open md:`\u00a0or similar commands.\n\n```bash\nCopy code\n\n.open md:\nAttempting to automatically open the SSO authorization page\n   in your default browser.\n1. Please open this link to login into your account:\n    https://auth.motherduck.com/activate\n2. Enter the following code: XXXX-XXXX\n\nToken successfully retrieved \u2705\nYou can store it as an environment variable to avoid having to log in again:\n  $ export motherduck_token='eyJhbGciOiJI..._Jfo'\n```\n\nOnce you have an account you get a\u00a0**motherduck\\_token**, which you need to connect to MotherDuck. Best to set the token as an environment variable, instead of a database variable, because opening a new database wipes the settings in DuckDB (trust me, I tried).\n\nIf you want to explore the MotherDuck UI first, feel free to do so, you can create new databases, upload files and create tables from those. You can run queries and get a nice pivotable, sortable output table with inline frequency charts in the header.\n\n![motherduck-ui1](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmotherduck_ui_1964bb8d8b.png%3Fupdated_at%3D2023-10-02T11%3A52%3A30.426Z&w=3840&q=75)\n\n## Loading our StackOverflow Data into MotherDuck\n\nYou have the option of uploading your local database with single command, which is really neat.\n\n```bash\nCopy code\n\nCREATE DATABASE remote_database_name FROM CURRENT_DATABASE();\n\n-- or more generally\nCREATE DATABASE remote_database_name FROM '<local database name>';\n```\n\nThere are only two caveats,\u00a0**the local and remote name must be different**, otherwise you might get the error below.\n\n`Catalog Error: error while importing share: Schema with name <local-database-name> does not exist!`\n\nAnd for the size of our StackOverflow database and the it took quite some time to finish the upload, around 1 hour, sending 15GB of data for our 11GB database.\n\nSo we can either create the database on the MotherDuck UI and import our tables from our Parquet files on S3, or upload the database from our local system.\n\nFor creating the database and tables from Parquest, we use the web interface or DuckDB on the local machine, connected to MotherDuck. Here are the SQL commands you need to run.\n\n```bash\nCopy code\n\ncreate database so;\n\ncreate table users as\nfrom 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/users.parquet';\n-- Run Time (s): real 10.401 user 0.006417 sys 0.003527\n\ndescribe users;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  column_name   \u2502 column_type \u2502\n\u2502    varchar     \u2502   varchar   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Id             \u2502 BIGINT      \u2502\n\u2502 Reputation     \u2502 BIGINT      \u2502\n\u2502 CreationDate   \u2502 TIMESTAMP   \u2502\n\u2502 DisplayName    \u2502 VARCHAR     \u2502\n\u2502 LastAccessDate \u2502 TIMESTAMP   \u2502\n\u2502 AboutMe        \u2502 VARCHAR     \u2502\n\u2502 Views          \u2502 BIGINT      \u2502\n\u2502 UpVotes        \u2502 BIGINT      \u2502\n\u2502 DownVotes      \u2502 BIGINT      \u2502\n\u2502 Id             \u2502 BIGINT      \u2502\n\u2502 Reputation     \u2502 BIGINT      \u2502\n\u2502 CreationDate   \u2502 TIMESTAMP   \u2502\n\u2502 DisplayName    \u2502 VARCHAR     \u2502\n\u2502 LastAccessDate \u2502 TIMESTAMP   \u2502\n\u2502 AboutMe        \u2502 VARCHAR     \u2502\n\u2502 Views          \u2502 BIGINT      \u2502\n\u2502 UpVotes        \u2502 BIGINT      \u2502\n\u2502 DownVotes      \u2502 BIGINT      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 18 rows                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nRun Time (s): real 0.032 user 0.026184 sys 0.002383\n\n-- do the same for the other tables\n\ncreate table comments as\nfrom 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/comments.parquet';\ncreate table posts as\nfrom 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/posts.parquet';\ncreate table votes as\nfrom 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/votes.parquet';\ncreate table badges as\nfrom 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/badges.parquet';\ncreate table post_links as\nfrom 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/post_links.parquet';\ncreate table tags as\nfrom 's3://us-prd-motherduck-open-datasets/stackoverflow/parquet/2023-05/tags.parquet';\n```\n\nIn the left sidebar of the web interface, now the database\u00a0`so`\u00a0and the tables should show up, if not, refresh the page.\n\n![motherduck-ui](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmotherduck_ui_2_3a7e8b67f8.png%3Fupdated_at%3D2023-10-02T12%3A24%3A30.413Z&w=3840&q=75)\n\n## Querying the Data with AI \ud83e\udd16\n\nA while ago MotherDuck released a new\u00a0[generative AI feature](https://motherduck.com/docs/key-tasks/writing-sql-with-ai/)\u00a0that allows you to\n\n- query your data using natural language\n- generate and fix SQL statements and\n- describe your data.\n\nAs LLMs, GPT and foundational models are\u00a0[close to my heart](https://medium.com/@mesirii), I was really excited to try these out.\n\nIt works actually already quite well, let\u2019s see how it does on this dataset.\n\nThe schema description is a bit uninspiring, I could have seen the same by just looking at the table list. As expected from probabilistic models it returns different results on each run.\n\n```bash\nCopy code\n\nCALL prompt_schema();\n\nsummary = The database contains information related to posts, comments, votes, badges, tags, post links, and users for a platform.\n\nRun Time (s): real 1.476 user 0.001069 sys 0.000778\n\nsummary = The database schema represents a collection of data about various aspects of a community platform, including users, posts, comments, tags, badges, votes, and post links.\n```\n\nOk, let\u2019s try a simple question:\u00a0`What are the most popular tags?`\n\n```bash\nCopy code\n\n.mode duckbox\npragma prompt_query('What are the most popular tags?');\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  TagName   \u2502  Count  \u2502\n\u2502  varchar   \u2502  int64  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 javascript \u2502 2479947 \u2502\n\u2502 python     \u2502 2113196 \u2502\n\u2502 java       \u2502 1889767 \u2502\n\u2502 c#         \u2502 1583879 \u2502\n\u2502 php        \u2502 1456271 \u2502\n\u2502 android    \u2502 1400026 \u2502\n\u2502 html       \u2502 1167742 \u2502\n\u2502 jquery     \u2502 1033113 \u2502\n\u2502 c++        \u2502  789699 \u2502\n\u2502 css        \u2502  787138 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 10 rows    2 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n-- Run Time (s): real 3.763 user 0.124567 sys 0.001716\n```\n\nNice, what is the SQL it might have used for that (probabilistically it could have been slightly different)?\n\n```bash\nCopy code\n\n.mode line\ncall prompt_sql('What are the most popular tags?');\n\n-- query = SELECT TagName, Count FROM tags ORDER BY Count DESC LIMIT 5;\n-- Run Time (s): real 2.813 user 2.808042 sys 0.005866\n```\n\nLooks good to me, it\u2019s even smart enough to use the attribute and ordering and limit to get \"most popular\" tags. The runtime for these AI prompts is between 2 and 10 seconds almost exclusively depending on the processing time of the LLM.\n\nThat was pretty easy, so let\u2019s see how it deals a few more involved questions.\n\n- What question has the highest score and what are it\u2019s other attributes?\n- Which 5 questions have the most comments, what is the post title and comment count\n\n```bash\nCopy code\n\npragma prompt_query(\"What question has the highest score and what are it's other attributes?\");\n\n                   Id = 11227809\n           PostTypeId = 1\n     AcceptedAnswerId = 11227902\n         CreationDate = 2012-06-27 13:51:36.16\n                Score = 26903\n            ViewCount = 1796363\n                 Body =\n          OwnerUserId = 87234\n     LastEditorUserId = 87234\nLastEditorDisplayName =\n         LastEditDate = 2022-10-12 18:56:47.68\n     LastActivityDate = 2023-01-10 04:40:07.12\n                Title = Why is processing a sorted array faster than processing an unsorted array?\n                 Tags = <java><c++><performance><cpu-architecture><branch-prediction>\n          AnswerCount = 26\n         CommentCount = 9\n        FavoriteCount = 0\n   CommunityOwnedDate =\n       ContentLicense = CC BY-SA 4.0\n\ncall prompt_sql(\"What question has the highest score and what are it's other attributes?\");\nquery = SELECT *\nFROM posts\nWHERE PostTypeId = 1\nORDER BY Score DESC\nLIMIT 1;\nRun Time (s): real 3.683 user 0.001970 sys 0.000994\n```\n\nOk, not bad, it\u2019s nice that it detects that\u00a0`PostTypeId = 1`\u00a0are questions (or known that from its training data on Stackoverflow), now lets go for the next one.\n\n```bash\nCopy code\n\n.mode duckbox\npragma prompt_query(\"Which 5 questions have the most comments, what is the post title and comment count\");\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                          Title                            \u2502 comment_count \u2502\n\u2502                                         varchar                           \u2502     int64     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 UIImageView Frame Doesnt Reflect Constraints                              \u2502           108 \u2502\n\u2502 Is it possible to use adb commands to click on a view by finding its ID?  \u2502           102 \u2502\n\u2502 How to create a new web character symbol recognizable by html/javascript? \u2502           100 \u2502\n\u2502 Why isnt my CSS3 animation smooth in Google Chrome (but very smooth on ot \u2502            89 \u2502\n\u2502 Heap Gives Page Fault                                                     \u2502            89 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nRun Time (s): real 19.695 user 2.406446 sys 0.018353\n\n.mode line\ncall prompt_sql(\"Which 5 questions have the most comments, what is the post title and comment count\");\n\nquery = SELECT p.Title, COUNT(c.Id) AS comment_count\nFROM posts p\nJOIN comments c ON p.Id = c.PostId AND p.PostTypeId = 1\nGROUP BY p.Title\nORDER BY comment_count DESC\nLIMIT 5;\nRun Time (s): real 4.795 user 0.002301 sys 0.001346\n```\n\nThis is what it looks like in the MotherDuck UI:\n\n![motherduck-ui-3](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmd_query_ai_5afa2fb313.png%3Fupdated_at%3D2023-10-02T11%3A52%3A30.745Z&w=3840&q=75)\n\nActually the comment count is a column on the posts table, so it could have used that, let\u2019s see if we can make it use only the one table.\n\n```sql\nCopy code\n\ncall prompt_sql(\"System: No joins! User: Which 5 questions have the most comments, what is the post title and comment count\");\n\nquery = SELECT Title, CommentCount\nFROM posts\nWHERE PostTypeId = 1\nORDER BY CommentCount DESC\nLIMIT 5;\nRun Time (s): real 3.587 user 0.001733 sys 0.000865\n```\n\nNice, that worked!\n\nYou can also use\u00a0`prompt_fixup`\u00a0to fix the SQL for a query, e.g. the infamous, \"I forgot GROUP BY\".\n\n```sql\nCopy code\n\ncall prompt_fixup(\"select postTypeId, count(*) from posts\");\n\nquery = SELECT postTypeId, COUNT(*) FROM posts GROUP BY postTypeId\nRun Time (s): real 12.006 user 0.004266 sys 0.002980\n```\n\nOr fixing a wrong join column name, or two.\n\n```sql\nCopy code\n\ncall prompt_fixup(\"select count(*) from posts join users on posts.userId = users.userId\");\n\nquery = SELECT COUNT(*) FROM posts JOIN users ON posts.OwnerUserId = users.Id\nRun Time (s): real 2.378 user 0.001770 sys 0.001067\n```\n\nThat\u2019s a really neat feature, hope they use it in their UI when your query would encounter an error with an explain in the background.\n\n### Data Sharing\n\nTo\u00a0[make this data available to others](https://motherduck.com/docs/key-tasks/sharing-data/sharing-overview/), we can use the\u00a0`CREATE SHARE`\u00a0command.\n\nIf we run it, we will get a shareable link, that others can use with\u00a0`ATTACH`\u00a0to\u00a0[attach our database](https://motherduck.com/docs/key-tasks/sharing-data/sharing-overview/). Currently it takes about a minute to create the share, but in the future it will be a zero-copy operation.\n\n```sql\nCopy code\n\n-- CREATE SHARE <share name> [FROM <database name>];\nCREATE SHARE so_2023_05 FROM so;\n-- share_url = md:_share/so/373594a2-06f7-4c33-814e-cf59028482ca\n-- Run Time (s): real 63.335 user 0.014849 sys 0.013110\n\n-- ATTACH '<share URL>' [AS <database name>];\nATTACH 'md:_share/so/373594a2-06f7-4c33-814e-cf59028482ca' AS so;\n\n-- show the contents of the share\nDESCRIBE SHARE \"so_2023_05\";\n\nLIST SHARES;\n\n-- After making changes to the shared database, you need to update the share\nUPDATE SHARE \"so_2023_05\";\n```\n\nToday we explored the MotherDuck interface, created a database and populated it with tables using Parquet data on S3. That worked really well and you should be able to do this with your own data easily.\n\nThen we tried the new AI prompts on MotherDuck, which work quite well, of course not 100% but often good enough to get a starting point or learn something new. Given the amount of SQL information that was used to the train the LLMs plus the additional schema information, that is not surprising. SQL (derived from structured english query language SEQUEL) is just another langauge for the LLM to translate into, much like Korean or Klingon.\n\nSo while you\u2019re waiting for the third part of the blog series, you can attach our share (which is public) and run your own queries on it.\n\nIn the third part we want to connect to our StackOverflow database on MotherDuck using Python and explore some more ways accessing, querying and visualizing our data.\n\nPlease share any interesting queries or issues on the\u00a0[MotherDuck Slack channel](https://slack.motherduck.com/).\n\n### TABLE OF CONTENTS\n\n[From Local to Cloud - Loading our Database into MotherDuck and Querying it with AI Prompts](https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2/#from-local-to-cloud-loading-our-database-into-motherduck-and-querying-it-with-ai-prompts)\n\n[Getting started with MotherDuck](https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2/#getting-started-with-motherduck)\n\n[Loading our StackOverflow Data into MotherDuck](https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2/#loading-our-stackoverflow-data-into-motherduck)\n\n[Querying the Data with AI \ud83e\udd16](https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2/#querying-the-data-with-ai)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![This Month in the DuckDB Ecosystem: September 2023](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fthumbnail_duckdb_newsletter_september_9b90e1e119.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-ten/)\n\n[2023/09/30 - Marcos Ortiz](https://motherduck.com/blog/duckdb-ecosystem-newsletter-ten/)\n\n### [This Month in the DuckDB Ecosystem: September 2023](https://motherduck.com/blog/duckdb-ecosystem-newsletter-ten)\n\nThis Month in the DuckDB Ecosystem: September 2023\n\n[![DuckDB Tutorial For Beginners](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgetting_started_duckdb_thumbnail_70b197b1ab.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n[2024/10/31 - Mehdi Ouazza, Ryan Boyd](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n### [DuckDB Tutorial For Beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners)\n\nGet up to speed quickly with DuckDB, including installation, VSCode workflow integration and your first SQL analytics project.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 415,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 415
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! Now I have good information. Let me write the answer based on what I've learned from these sources:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_8.md

  [96mğŸ”§ TOOL:[0m Bash â†’ touch /tmp/answer_without_kg_8.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m 
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /tmp/answer_without_kg_8.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/tmp/answer_without_kg_8.md",
  [2m  â”‚[0m     "content": "# The Difference Between a MotherDuck Database and a Share\n\nIn MotherDuck, **databases** and **shares** serve different purposes in how you store and collaborate on data.\n\n## MotherDuck Database\n\nA **MotherDuck database** is a live, mutable database stored in the cloud that serves as your active workspace for data analysis and collaboration.\n\nKey characteristics:\n- **Cloud storage**: Your DuckDB database catalog is managed in the cloud for easy scaling and collaboration\n- **Writable**: You can create, update, and delete tables and data\n- **Multiple concurrent users**: Multiple users can access the same database simultaneously, with each user getting their own dedicated compute instance (called a \"duckling\") for concurrent queries without conflicts\n- **Hybrid execution**: Databases support MotherDuck's dual execution model, where queries can intelligently run locally on your machine or in the cloud\n- **Created with standard SQL**: Use `CREATE DATABASE` to create a new database or `ATTACH 'md:'` to connect to MotherDuck\n\nExample:\n```sql\n-- Create a new database in MotherDuck\nCREATE DATABASE my_database;\n\n-- Upload a local database to MotherDuck\nCREATE DATABASE remote_db FROM CURRENT_DATABASE();\n```\n\n## Share\n\nA **share** is a read-only snapshot of a database that can be easily distributed to others for querying without giving them write access or requiring them to copy the data.\n\nKey characteristics:\n- **Read-only snapshot**: A share captures the state of a database at a point in time\n- **Easy distribution**: Creates a shareable URL that others can attach to and query\n- **No file exports needed**: No need to export files or manage S3 buckets to share your work\n- **Attach and query**: Recipients use `ATTACH` with the share URL to access the data\n- **Created from a database**: Use `CREATE SHARE` to create a snapshot from an existing database\n\nExample:\n```sql\n-- Create a share from a database\nCREATE SHARE my_share FROM my_database;\n-- Returns: share_url = md:_share/my_database/373594a2-06f7-4c33-814e-cf59028482ca\n\n-- Others can then attach it\nATTACH 'md:_share/my_database/373594a2-06f7-4c33-814e-cf59028482ca' AS shared_data;\n\n-- Update the share after making changes to the source database\nUPDATE SHARE my_share;\n```\n\n## Summary\n\n| Aspect | Database | Share |\n|--------|----------|-------|\n| **Purpose** | Active workspace for data analysis | Distribution of read-only data snapshots |\n| **Mutability** | Read-write | Read-only |\n| **Creation** | `CREATE DATABASE` or `ATTACH 'md:'` | `CREATE SHARE <name> FROM <database>` |\n| **Access** | Direct database connection | `ATTACH '<share_url>'` |\n| **Use Case** | Collaborative data development and analytics | Sharing data with team members or externally |\n| **Updates** | Real-time | Requires `UPDATE SHARE` to reflect source changes |\n\nIn essence, a **database** is where you actively work with and modify your data in MotherDuck, while a **share** is how you distribute a read-only copy of that data to others without requiring file exports or data duplication.\n\n## Sources\n\n- [MotherDuck for DuckDB Users - Product Page](https://motherduck.com/product/duckdb-users/)\n  - Description of database sharing: \"Create a shareable snapshot of your data, which your colleagues can easily attach and query in MotherDuck\"\n  - Cloud database storage: \"Manage your DuckDB database catalog in the cloud for easy scaling and collaboration\"\n  - Multiple users feature: \"Enable simultaneous access for Data Engineers, Analytics teams, and Scientists to the same datasets. Each user gets their own dedicated compute instance 'duckling' for concurrent queries without conflicts\"\n\n- [DuckDB & MotherDuck for Beginners Video Guide](https://motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide/)\n  - Database sharing example: \"CREATE SHARE myshare FROM mydatabase\"\n  - Describes database attachment and connection\n\n- [Exploring StackOverflow with DuckDB on MotherDuck (Part 2)](https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2/)\n  - Detailed share creation workflow with SQL examples\n  - `CREATE SHARE so_2023_05 FROM so;` producing a shareable URL\n  - `ATTACH 'md:_share/so/373594a2-06f7-4c33-814e-cf59028482ca' AS so;`\n  - `UPDATE SHARE` command for refreshing shares after database changes\n\n- [Dominik Moritz Case Study](https://motherduck.com/case-studies/dominik-moritz)\n  - Quote: \"We can create a shareable snapshot of our data that others can attach to and query\"\n  - Real-world application of shares for data distribution\n",
  [2m  â”‚[0m     "numLines": 82,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 82
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! I see there's already content there. Let me update it with my more comprehensive answer:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_8.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "update",
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_8.md",
  [2m  â”‚[0m   "content": "# The Difference Between a MotherDuck Database and a Share\n\n## MotherDuck Database\n\nA **MotherDuck database** is a fully functional, writable database hosted in the cloud using MotherDuck's managed DuckDB service. Key characteristics include:\n\n- **Writable**: You can create, modify, and delete tables and data\n- **Full Control**: Database owners have complete control over the schema and content\n- **Collaborative**: Multiple users can simultaneously access with dedicated compute instances (\"ducklings\") for concurrent queries\n- **Cloud Storage**: Database catalog is managed in the cloud for easy scaling and collaboration\n- **Local + Cloud**: Can be queried both locally and in the cloud with seamless integration\n\n## MotherDuck Share\n\nA **share** is a **read-only, shareable snapshot** of a MotherDuck database, purpose-built for data collaboration and ad-hoc analytics. Key characteristics include:\n\n- **Read-Only**: Recipients can query the data but cannot modify it\n- **Snapshot-Based**: Represents a point-in-time copy of the database\n- **Zero-Copy**: Shares are designed as zero-copy clones, avoiding data duplication\n- **Controlled Access**: Share creators control who can access the data and when updates are pushed\n- **Easy Distribution**: Shares can be attached by others using a simple SQL command\n\n## Key Differences\n\n| Aspect | Database | Share |\n|--------|----------|-------|\n| **Permissions** | Read and write | Read-only |\n| **Purpose** | Primary data storage and processing | Data distribution and collaboration |\n| **Updates** | Direct modification allowed | Must be updated by creator |\n| **Access** | Requires database ownership or permissions | Anyone with the share URL can attach |\n| **Data Protection** | Direct access to production data | Isolated snapshot protects production dataset |\n\n## How Shares Work\n\nCreating and using shares is straightforward:\n\n```sql\n-- Create a share from a database\nCREATE SHARE share_name FROM database_name (ACCESS UNRESTRICTED);\n\n-- Share URL is returned, e.g.:\n-- md:_share/database/373594a2-06f7-4c33-814e-cf59028482ca\n\n-- Others can attach the share:\nATTACH 'md:_share/database/373594a2-06f7-4c33-814e-cf59028482ca';\n\n-- When the database owner updates data, they update the share:\nUPDATE SHARE share_name;\n```\n\n## Use Cases\n\n**Use a Database when:**\n- You need to create, update, or delete data\n- Building data pipelines or transformations\n- Working as part of a team that needs write access\n- Managing production datasets\n\n**Use a Share when:**\n- Distributing data to external stakeholders or partners\n- Providing read-only access for analytics and reporting\n- Protecting production data from accidental modifications\n- Enabling ad-hoc data exploration without risking data integrity\n- Sharing datasets publicly or with specific users\n\n## Sources\n\n- [Hack Night Quickstart Challenge](https://motherduck.com/hack-night) - Example of creating a share from a database\n- [MotherDuck Product Page: DuckDB Users](https://motherduck.com/product/duckdb-users/) - \"Simplified database sharing\" feature description\n- [DuckDB & MotherDuck for Beginners Guide](https://motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide) - Database sharing explanation\n- [Exploring StackOverflow with DuckDB on MotherDuck (Part 2)](https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2) - Detailed data sharing workflow\n- [Data Warehouse Feature Roundup: November 2024](https://motherduck.com/blog/data-warehouse-feature-roundup-nov-2024) - MotherDuck's sharing model description\n- [Case Study: Dexibit](https://motherduck.com/case-studies/dexibit) - Real-world usage of shares\n- [Case Study: Dominik Moritz](https://motherduck.com/case-studies/dominik-moritz) - Creating shareable snapshots for interactive visualizations\n",
  [2m  â”‚[0m   "structuredPatch": [
  [2m  â”‚[0m     {
  [2m  â”‚[0m       "oldStart": 1,
  [2m  â”‚[0m       "oldLines": 81,
  [2m  â”‚[0m       "newStart": 1,
  [2m  â”‚[0m       "newLines": 74,
  [2m  â”‚[0m       "lines": [
  [2m  â”‚[0m         " # The Difference Between a MotherDuck Database and a Share",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-In MotherDuck, **databases** and **shares** serve different purposes in how you store and collaborate on data.",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         " ## MotherDuck Database",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-A **MotherDuck database** is a live, mutable database stored in the cloud that serves as your active workspace for data analysis and collaboration.",
  [2m  â”‚[0m         "+A **MotherDuck database** is a fully functional, writable database hosted in the cloud using MotherDuck's managed DuckDB service. Key characteristics include:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-Key characteristics:",
  [2m  â”‚[0m         "-- **Cloud storage**: Your DuckDB database catalog is managed in the cloud for easy scaling and collaboration",
  [2m  â”‚[0m         "-- **Writable**: You can create, update, and delete tables and data",
  [2m  â”‚[0m         "-- **Multiple concurrent users**: Multiple users can access the same database simultaneously, with each user getting their own dedicated compute instance (called a \"duckling\") for concurrent queries without conflicts",
  [2m  â”‚[0m         "-- **Hybrid execution**: Databases support MotherDuck's dual execution model, where queries can intelligently run locally on your machine or in the cloud",
  [2m  â”‚[0m         "-- **Created with standard SQL**: Use `CREATE DATABASE` to create a new database or `ATTACH 'md:'` to connect to MotherDuck",
  [2m  â”‚[0m         "+- **Writable**: You can create, modify, and delete tables and data",
  [2m  â”‚[0m         "+- **Full Control**: Database owners have complete control over the schema and content",
  [2m  â”‚[0m         "+- **Collaborative**: Multiple users can simultaneously access with dedicated compute instances (\"ducklings\") for concurrent queries",
  [2m  â”‚[0m         "+- **Cloud Storage**: Database catalog is managed in the cloud for easy scaling and collaboration",
  [2m  â”‚[0m         "+- **Local + Cloud**: Can be queried both locally and in the cloud with seamless integration",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-Example:",
  [2m  â”‚[0m         "-```sql",
  [2m  â”‚[0m         "--- Create a new database in MotherDuck",
  [2m  â”‚[0m         "-CREATE DATABASE my_database;",
  [2m  â”‚[0m         "+## MotherDuck Share",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "--- Upload a local database to MotherDuck",
  [2m  â”‚[0m         "-CREATE DATABASE remote_db FROM CURRENT_DATABASE();",
  [2m  â”‚[0m         "-```",
  [2m  â”‚[0m         "+A **share** is a **read-only, shareable snapshot** of a MotherDuck database, purpose-built for data collaboration and ad-hoc analytics. Key characteristics include:",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Share",
  [2m  â”‚[0m         "+- **Read-Only**: Recipients can query the data but cannot modify it",
  [2m  â”‚[0m         "+- **Snapshot-Based**: Represents a point-in-time copy of the database",
  [2m  â”‚[0m         "+- **Zero-Copy**: Shares are designed as zero-copy clones, avoiding data duplication",
  [2m  â”‚[0m         "+- **Controlled Access**: Share creators control who can access the data and when updates are pushed",
  [2m  â”‚[0m         "+- **Easy Distribution**: Shares can be attached by others using a simple SQL command",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-A **share** is a read-only snapshot of a database that can be easily distributed to others for querying without giving them write access or requiring them to copy the data.",
  [2m  â”‚[0m         "+## Key Differences",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-Key characteristics:",
  [2m  â”‚[0m         "-- **Read-only snapshot**: A share captures the state of a database at a point in time",
  [2m  â”‚[0m         "-- **Easy distribution**: Creates a shareable URL that others can attach to and query",
  [2m  â”‚[0m         "-- **No file exports needed**: No need to export files or manage S3 buckets to share your work",
  [2m  â”‚[0m         "-- **Attach and query**: Recipients use `ATTACH` with the share URL to access the data",
  [2m  â”‚[0m         "-- **Created from a database**: Use `CREATE SHARE` to create a snapshot from an existing database",
  [2m  â”‚[0m         "+| Aspect | Database | Share |",
  [2m  â”‚[0m         "+|--------|----------|-------|",
  [2m  â”‚[0m         "+| **Permissions** | Read and write | Read-only |",
  [2m  â”‚[0m         "+| **Purpose** | Primary data storage and processing | Data distribution and collaboration |",
  [2m  â”‚[0m         "+| **Updates** | Direct modification allowed | Must be updated by creator |",
  [2m  â”‚[0m         "+| **Access** | Requires database ownership or permissions | Anyone with the share URL can attach |",
  [2m  â”‚[0m         "+| **Data Protection** | Direct access to production data | Isolated snapshot protects production dataset |",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-Example:",
  [2m  â”‚[0m         "+## How Shares Work",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+Creating and using shares is straightforward:",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         " ```sql",
  [2m  â”‚[0m         " -- Create a share from a database",
  [2m  â”‚[0m         "-CREATE SHARE my_share FROM my_database;",
  [2m  â”‚[0m         "--- Returns: share_url = md:_share/my_database/373594a2-06f7-4c33-814e-cf59028482ca",
  [2m  â”‚[0m         "+CREATE SHARE share_name FROM database_name (ACCESS UNRESTRICTED);",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "--- Others can then attach it",
  [2m  â”‚[0m         "-ATTACH 'md:_share/my_database/373594a2-06f7-4c33-814e-cf59028482ca' AS shared_data;",
  [2m  â”‚[0m         "+-- Share URL is returned, e.g.:",
  [2m  â”‚[0m         "+-- md:_share/database/373594a2-06f7-4c33-814e-cf59028482ca",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "--- Update the share after making changes to the source database",
  [2m  â”‚[0m         "-UPDATE SHARE my_share;",
  [2m  â”‚[0m         "+-- Others can attach the share:",
  [2m  â”‚[0m         "+ATTACH 'md:_share/database/373594a2-06f7-4c33-814e-cf59028482ca';",
  [2m  â”‚[0m         "+",
  [2m  â”‚[0m         "+-- When the database owner updates data, they update the share:",
  [2m  â”‚[0m         "+UPDATE SHARE share_name;",
  [2m  â”‚[0m         " ```",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-## Summary",
  [2m  â”‚[0m         "+## Use Cases",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-| Aspect | Database | Share |",
  [2m  â”‚[0m         "-|--------|----------|-------|",
  [2m  â”‚[0m         "-| **Purpose** | Active workspace for data analysis | Distribution of read-only data snapshots |",
  [2m  â”‚[0m         "-| **Mutability** | Read-write | Read-only |",
  [2m  â”‚[0m         "-| **Creation** | `CREATE DATABASE` or `ATTACH 'md:'` | `CREATE SHARE <name> FROM <database>` |",
  [2m  â”‚[0m         "-| **Access** | Direct database connection | `ATTACH '<share_url>'` |",
  [2m  â”‚[0m         "-| **Use Case** | Collaborative data development and analytics | Sharing data with team members or externally |",
  [2m  â”‚[0m         "-| **Updates** | Real-time | Requires `UPDATE SHARE` to reflect source changes |",
  [2m  â”‚[0m         "+**Use a Database when:**",
  [2m  â”‚[0m         "+- You need to create, update, or delete data",
  [2m  â”‚[0m         "+- Building data pipelines or transformations",
  [2m  â”‚[0m         "+- Working as part of a team that needs write access",
  [2m  â”‚[0m         "+- Managing production datasets",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-In essence, a **database** is where you actively work with and modify your data in MotherDuck, while a **share** is how you distribute a read-only copy of that data to others without requiring file exports or data duplication.",
  [2m  â”‚[0m         "+**Use a Share when:**",
  [2m  â”‚[0m         "+- Distributing data to external stakeholders or partners",
  [2m  â”‚[0m         "+- Providing read-only access for analytics and reporting",
  [2m  â”‚[0m         "+- Protecting production data from accidental modifications",
  [2m  â”‚[0m         "+- Enabling ad-hoc data exploration without risking data integrity",
  [2m  â”‚[0m         "+- Sharing datasets publicly or with specific users",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         " ## Sources",
  [2m  â”‚[0m         " ",
  [2m  â”‚[0m         "-- [MotherDuck for DuckDB Users - Product Page](https://motherduck.com/product/duckdb-users/)",
  [2m  â”‚[0m         "-  - Description of database sharing: \"Create a shareable snapshot of your data, which your colleagues can easily attach and query in MotherDuck\"",
  [2m  â”‚[0m         "-  - Cloud database storage: \"Manage your DuckDB database catalog in the cloud for easy scaling and collaboration\"",
  [2m  â”‚[0m         "-  - Multiple users feature: \"Enable simultaneous access for Data Engineers, Analytics teams, and Scientists to the same datasets. Each user gets their own dedicated compute instance 'duckling' for concurrent queries without conflicts\"",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-- [DuckDB & MotherDuck for Beginners Video Guide](https://motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide/)",
  [2m  â”‚[0m         "-  - Database sharing example: \"CREATE SHARE myshare FROM mydatabase\"",
  [2m  â”‚[0m         "-  - Describes database attachment and connection",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-- [Exploring StackOverflow with DuckDB on MotherDuck (Part 2)](https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2/)",
  [2m  â”‚[0m         "-  - Detailed share creation workflow with SQL examples",
  [2m  â”‚[0m         "-  - `CREATE SHARE so_2023_05 FROM so;` producing a shareable URL",
  [2m  â”‚[0m         "-  - `ATTACH 'md:_share/so/373594a2-06f7-4c33-814e-cf59028482ca' AS so;`",
  [2m  â”‚[0m         "-  - `UPDATE SHARE` command for refreshing shares after database changes",
  [2m  â”‚[0m         "-",
  [2m  â”‚[0m         "-- [Dominik Moritz Case Study](https://motherduck.com/case-studies/dominik-moritz)",
  [2m  â”‚[0m         "-  - Quote: \"We can create a shareable snapshot of our data that others can attach to and query\"",
  [2m  â”‚[0m         "-  - Real-world application of shares for data distribution",
  [2m  â”‚[0m         "+- [Hack Night Quickstart Challenge](https://motherduck.com/hack-night) - Example of creating a share from a database",
  [2m  â”‚[0m         "+- [MotherDuck Product Page: DuckDB Users](https://motherduck.com/product/duckdb-users/) - \"Simplified database sharing\" feature description",
  [2m  â”‚[0m         "+- [DuckDB & MotherDuck for Beginners Guide](https://motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide) - Database sharing explanation",
  [2m  â”‚[0m         "+- [Exploring StackOverflow with DuckDB on MotherDuck (Part 2)](https://motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2) - Detailed data sharing workflow",
  [2m  â”‚[0m         "+- [Data Warehouse Feature Roundup: November 2024](https://motherduck.com/blog/data-warehouse-feature-roundup-nov-2024) - MotherDuck's sharing model description",
  [2m  â”‚[0m         "+- [Case Study: Dexibit](https://motherduck.com/case-studies/dexibit) - Real-world usage of shares",
  [2m  â”‚[0m         "+- [Case Study: Dominik Moritz](https://motherduck.com/case-studies/dominik-moritz) - Creating shareable snapshots for interactive visualizations"
  [2m  â”‚[0m       ]
  [2m  â”‚[0m     }
  [2m  â”‚[0m   ]
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! I've successfully completed the task. Here's a summary of what I did:
  [94mâ”‚[0m 
  [94mâ”‚[0m ## Summary
  [94mâ”‚[0m 
  [94mâ”‚[0m I searched through the `.kurt/sources/` directory to find information about the difference between a MotherDuck database and a share, then synthesized a comprehensive answer.
  [94mâ”‚[0m 
  [94mâ”‚[0m **Key findings:**
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. **MotherDuck Database**: A fully functional, writable cloud database where you can create, modify, and delete data. Multiple users can collaborate simultaneously with dedicated compute instances.
  [94mâ”‚[0m 
  [94mâ”‚[0m 2. **MotherDuck Share**: A read-only snapshot of a database designed for data distribution and collaboration. Shares are zero-copy clones that protect production data while enabling easy access for analytics.
  [94mâ”‚[0m 
  [94mâ”‚[0m **Main differences:**
  [94mâ”‚[0m - Databases are read-write; shares are read-only
  [94mâ”‚[0m - Databases are for active work; shares are for distribution
  [94mâ”‚[0m - Shares require `UPDATE SHARE` to reflect changes from the source database
  [94mâ”‚[0m - Shares use a simple `ATTACH` command with a shareable URL
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer has been written to `/tmp/answer_without_kg_8.md` with a complete Sources section citing 7 different MotherDuck resources including product pages, tutorials, blog posts, and case studies.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 6,366 | Cost: $0.5173
     Cumulative: 6,366 tokens | $0.5173
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent clearly states that it has successfully completed the task and provides a summary, indicating that it is not seeking further input but rather concluding the interaction.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 25 | Duration: 140.4s
â•‘    Tokens: 6,366 | Cost: $0.5173
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  LLM Judge score: 0.96

======================================================================
â“ Question 9/10
What compute instance sizes does MotherDuck offer?
======================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 9
â”‚ Search through the .kurt/sources/ directory to answer this question:

What compute instance sizes does MotherDuck offer?

Steps:
1. Search for the most relevant markdown files in .kurt/sources/
2. Read those files and synthesize the answer
3. Write a markdown answer (with a ## Sources section) to: /tmp/answer_without_kg_9.md
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I'll search through the .kurt/sources/ directory to find information about MotherDuck's compute instance sizes.
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ instance.*size|compute.*size|warehouse.*size|memory.*size|CPU.*size

  [96mğŸ”§ TOOL:[0m Grep â†’ small|medium|large|x-large|instance type

  [96mğŸ”§ TOOL:[0m Grep â†’ pricing|tier|plan|compute
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/sql-group-by-examples-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-use-cases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-lake-vs-data-warehouse-vs-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/index.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/reflex-careers-gig-economy-retail.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-simple-joys-of-scaling-up.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-recap-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-recap.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-power-of-wasm-for-analytics-duckdb-in-the-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-surprising-birth-of-duckdb-ft-co-creator-hannes-muhleisen.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-if-sql-queries-returned-results-instantly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-duckdb-motherduck.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/scaling-duckdb-panel-ondemand.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-surprising-birth-of-duckdb-ft-co-creator-hannes-muhleisen.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-future-of-bi-exploring-the-impact-of-bi-as-code-tools-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/moving-forward-from-ad-hoc-reports-with-duckdb-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/panel-fixing-the-data-engineering-lifecycle-coalesce-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search more specifically for warehouse, pricing, and instance information:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ warehouse|WH_|serverless

  [96mğŸ”§ TOOL:[0m Bash â†’ find .kurt/sources -name "*pricing*" -o -name "*plan*" -o -name "*warehouse*" 2>/dev/null | head -20
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m .kurt/sources/motherduck.com/learn-more/data-warehouse-tco.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/learn-more/data-lake-vs-data-warehouse-vs-lakehouse.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/learn-more/modern-data-warehouse-use-cases.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/product/pricing.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-nov-2024.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-oct-2024.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-dec-2024.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/new-collaboration-sharing-motherduck-data-warehouse-organization-auto-join.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/dosomething-motherduck-data-warehouse-ROI.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/differential-storage-building-block-for-data-warehouse.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/blog/motherduck-data-warehouse.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/case-studies/layers-multi-tenant-data-warehouse.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/videos/take-flight-with-dbt-and-duckdb-dropping-dev-warehouse-costs-to-zero.md
  [2m  â”‚[0m .kurt/sources/motherduck.com/videos/how-to-bootstrap-a-data-warehouse-with-duckdb.md
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/scaling-duckdb-panel-ondemand.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/data-discoverability-secoda-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/take-flight-with-dbt-and-duckdb-dropping-dev-warehouse-costs-to-zero.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/panel-fixing-the-data-engineering-lifecycle-coalesce-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/monte-carlo-simulations-with-duckdb-ft-james-mcneill.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-ga-produck-tour.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-bootstrap-a-data-warehouse-with-duckdb.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Great! Let me read the pricing page which should have information about compute instance sizes:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/product/pricing.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     "content": "---\ntitle: pricing\ncontent_type: product_page\nsource_url: https://motherduck.com/product/pricing/\nindexed_at: '2025-11-25T20:16:03.158148'\ncontent_hash: 62844ce7682cbcb5\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n# Simple, straightforward cloud data warehouse pricing\n\nDitch your overengineered cloud data warehouse for blazing fast analytics!\n\n![Free plan's illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpricing-plan-free.35076c86.png&w=3840&q=75)\n\nFree\n\n$0NO CREDIT CARD\nREQUIRED\n\nA soft landing for dabbling and experimenting with MotherDuck\n\n[Get Started](https://app.motherduck.com/?auth_flow=signup)\n\n- Up to 5 Members\n- Up to 10GB of Storage\n- Pragmatic, AI-Backed UI\n\nSQL \u2018FixIt\u2019\n\nResolve common SQL errors with thoughtful, in-line suggestions\n[Learn more](https://motherduck.com/docs/key-tasks/writing-sql-with-ai/#automatically-fix-sql-errors-in-the-webui)\n\n\n\nto keep you in the flow\n\n- Community SupportSelf-serve via Slack\n\n- Up to 10 Compute Unit Hours\n\n![Business plan's illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpricing-plan-business.f2750d2a.png&w=3840&q=75)\n\nBusiness\n\n$100PER ORG\n/ MONTH + USAGE\n\nProduction analytics and BI workloads without the maintenance overhead\n\n[Try 21 days FREE](https://app.motherduck.com/?auth_flow=signup)\n\n- Unlimited Members\n- Unlimited Storage\n\nPay as you go for unlimited storage\n\n- Pragmatic, AI-Backed UI +\n\nAI Functions\n\nSingle call and bulk AI functions, including prompt() and embedding()\n[Learn more](https://motherduck.com/docs/about-motherduck/billing/)\n\n- Priority Support\n\nFaster access to MotherDuck experts with an [expedited response SLA](https://motherduck.com/support-policy/)\n\n\n\nFor production-grade workloads\n\n- 5 Compute\n\nInstance Types\n\nBalance speed and performance for serverless scale and per-user tenancy\n[Learn more](https://motherduck.com/product/pricing/#instance-types)\n\n\n\n+\n\nRead Scaling\n\nEnable up to 16 read scaling replicas, each backed by its own instance\n[Learn more](https://motherduck.com/docs/key-tasks/authenticating-and-connecting-to-motherduck/read-scaling/)\n\n\n[SEE INSTANCE TYPES](https://motherduck.com/product/pricing/#instance-types)\n\n![Lite plan's illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpricing-plan-lite.f629c933.png&w=3840&q=75)\n\nLite\n\n$25PER ORG\n/ MONTH + USAGE\n\nPerfect for individuals and small teams looking for their first data warehouse\n\n[Get Started](https://app.motherduck.com/?auth_flow=signup)\n\n- Up to 5 Members\n- Unlimited Storage\n\nPay as you go for unlimited storage\n\n- Pragmatic, AI-Backed UI +\n\nAI Functions\n\nSingle call and bulk AI functions, including prompt() and embedding()\n[Learn more](https://motherduck.com/docs/about-motherduck/billing/)\n\n- Standard SupportPerfect for getting started\n\n- 2 Compute\n\nInstance Types\n\nBalance speed and performance for serverless scale and per-user tenancy\n[Learn more](https://motherduck.com/product/pricing/#instance-types)\n\n\n\nPay as you go for Unlimited Compute\n\n\n[SEE INSTANCE TYPES](https://motherduck.com/product/pricing/#instance-types)\n\nLooking for an annual plan?\n[Get in touch](https://motherduck.com/contact-us/product-expert/)\n\n![Annual plan illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpricing-annual-plan.aef751c8.png&w=3840&q=75)\n\nMoving to MotherDuck, a billed by-the-second cloud offering is a no-brainer for us, considering the elegance and efficiency of a single node system compared to traditional OLAP solutions.\n\nNico Ritschel\n\nDirector of Engineering @ atm.com\n\n![Nico Ritschel](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fatm-logo.e3612038.png&w=828&q=75)\n\nMotherDuck is insanely performant, and there\u2019s no infrastructure to manage. The cost of experimentation is incredibly low \u2014 which means we can afford to ask more questions and build more features, faster.\n\nKristov Paulus\n\nFounder, CIO @ Kultura Capital\n\n![Kristov Paulus](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fkultura-logo.77e389a5.png&w=640&q=75)\n\nMoving to MotherDuck, a billed by-the-second cloud offering is a no-brainer for us, considering the elegance and efficiency of a single node system compared to traditional OLAP solutions.\n\nNico Ritschel\n\nDirector of Engineering @ atm.com\n\n![Nico Ritschel](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fatm-logo.e3612038.png&w=828&q=75)\n\nMotherDuck is insanely performant, and there\u2019s no infrastructure to manage. The cost of experimentation is incredibly low \u2014 which means we can afford to ask more questions and build more features, faster.\n\nKristov Paulus\n\nFounder, CIO @ Kultura Capital\n\n![Kristov Paulus](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fkultura-logo.77e389a5.png&w=640&q=75)\n\nMoving to MotherDuck, a billed by-the-second cloud offering is a no-brainer for us, considering the elegance and efficiency of a single node system compared to traditional OLAP solutions.\n\nNico Ritschel\n\nDirector of Engineering @ atm.com\n\n![Nico Ritschel](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fatm-logo.e3612038.png&w=828&q=75)\n\nMotherDuck is insanely performant, and there\u2019s no infrastructure to manage. The cost of experimentation is incredibly low \u2014 which means we can afford to ask more questions and build more features, faster.\n\nKristov Paulus\n\nFounder, CIO @ Kultura Capital\n\n![Kristov Paulus](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fkultura-logo.77e389a5.png&w=640&q=75)\n\n## Compare Plans\n\nJust four elements: Storage, Compute, AI, and Platform Access.\n\nRegion\n\nUSA (N. Virginia): us-east-1\n\nPLANS\n\nFree\n\n**$0** No credit card required\n\n[Get Started](https://app.motherduck.com/?auth_flow=signup)\n\nBusiness\n\n**$100** per org / month + Usage\n\n[Try 21 days free](https://app.motherduck.com/?auth_flow=signup)\n\nLite\n\n**$25** per org / month + Usage\n\n[Get Started](https://app.motherduck.com/?auth_flow=signup)\n\n[Get Started for Free](https://app.motherduck.com/?auth_flow=signup)\n\nMotherDuck Data WarehouseData\n\nWarehouse\n\nMotherDuck Data WarehouseData\n\nWarehouse\n\nOrg. Members\n\nCollaborate and share data securely with org members in your organization, including users and service accounts\n\nUp to 5\n\nUnlimited\n\nUp to 5\n\nPlatform Access\n\nPlatform access covers MotherDuck support and AI-backed UI functions for members and service accounts in your Org\n[Learn more](https://motherduck.com/docs/about-motherduck/billing/pricing/#ai-function-pricing)\n\n$0\n\n$100 per org\n\n$25 per org\n\nAI Functions\n\nFor single call and bulk AI functions, including prompt() and embedding()\n[Learn more](https://motherduck.com/docs/about-motherduck/billing/)\n\nUI functions only\n\n$1.00 per AI Unit\n\n$1.00 per AI Unit\n\nStorage\n\nCost effective storage\n\nUp to 10GB\n\n$0.08 per GB per month\n\n$0.08 per GB per month\n\nCompute\n\nA selection of instances for all your analytics needs\n[Learn more](https://motherduck.com/product/pricing/#instance-types)\n\n1 Instance type:\n\nOptimize your workload's underlying compute resources\n\n5 Instance types:\n\nOptimize your workload's underlying compute resources\n\n2 Instance types:\n\nOptimize your workload's underlying compute resources\n\nPulse\n\nFor lightweight, fully serverless analytics\nBilled per Compute Unit (CU) second, 1 second minimum\n[Learn more](https://motherduck.com/product/pricing/#instance-types)\n\n(Pay on-demand)\n\nUp to 10 CU hours\n\n$0.40 per CU hour\n\n$0.25 per CU hour\n\nStandard\n\nFor everyday tasks and balanced performance\nBilled per second, 60 second minimum\n[Learn more](https://motherduck.com/product/pricing/#instance-types)\n\n(Pay per instance)\n\n$1.80 per hour\n\n$1.20 per hour\n\nJumbo\n\nFor predictable performance on heavy workloads\nBilled per second, 60 second minimum\n[Learn more](https://motherduck.com/product/pricing/#instance-types)\n\n(Pay per instance)\n\n$3.60 per hour\n\nMega\n\nFor demanding jobs at an even larger scale\nBilled per second, 5 minute minimum\n[Learn more](https://motherduck.com/product/pricing/#large-instance-types)\n\n(Pay per instance)\n\n$10.80 per hour\n\nGiga\n\nFor the largest and toughest workloads\nBilled per second, 10 minute minimum\n[Learn more](https://motherduck.com/product/pricing/#large-instance-types)\n\n(Pay per instance)\n\n[Contact Us](https://motherduck.com/contact-us/product-expert/)\n\nRead Scaling\n\nEnable up to 16 read-scaling replicas, each backed by its own instance, for BI dashboards and read-only use cases\n[Learn more](https://motherduck.com/docs/key-tasks/authenticating-and-connecting-to-motherduck/read-scaling/)\n\nQuery History\n\nA consolidated view of all queries run across your organization.\n[Learn more](https://motherduck.com/docs/sql-reference/motherduck-sql-reference/md_information_schema/query_history/)\n\nDual Execution\n\nIncredibly efficient queries, shared across local and cloud compute\n[Learn more](https://motherduck.com/docs/concepts/architecture-and-capabilities/#dual-execution)\n\nWorkflow and CollaborationWorkflow\n\nWorkflow and CollaborationWorkflow\n\nInstant SQL\n\nBuild and debug faster with\nzero-latency query results\n[Learn more](https://motherduck.com/blog/introducing-instant-sql/)\n\nFixIt\n\nResolve SQL errors with in-line fixes\n[Learn more](https://motherduck.com/docs/getting-started/interfaces/motherduck-quick-tour/#writing-sql-with-confidence-using-fixit-and-edit)\n\nColumn Explorer\n\nInstantly visualize the columns\nof your tables and result sets.\n[Learn more](https://motherduck.com/docs/getting-started/interfaces/motherduck-quick-tour/#diving-into-your-data-with-column-explorer)\n\nDatabase Sharing\n\nShare zero-copy snapshots\nfor easier collaboration\n[Learn more](https://motherduck.com/docs/key-tasks/sharing-data/sharing-overview/)\n\nSecurity And ComplianceSecurity &\n\nCompliance\n\nSecurity And ComplianceSecurity &\n\nCompliance\n\nSOC2 Type II\n\nFull compliance with SOC 2 Type II\n[Learn more](https://motherduck.com/trust-and-security/#Compliance)\n\nGDPR Compliance\n\nGeneral Data Protection Regulation (GDPR) compliant\n[Learn more](https://motherduck.com/trust-and-security/#Compliance)\n\nPrivate Link\n\nSecure, private connectivity through the AWS network\n\n\\+ Paid Add On\n\nHIPAA BAA\n\nAvailable on request\n[Contact us](https://motherduck.com/contact-us/product-expert/) for more details\n\nAvailable on request\n\nSupport\n\nSupport\n\nHuman-First SupportSupport\n\nReal people who are ready to help you in a dedicated space for your MotherDuck questions\n\nCommunity\n\nVia the [Slack Community](https://slack.motherduck.com/)\n\nPriority\n\nFaster access to MotherDuck experts with an [expedited response SLA](https://motherduck.com/support-policy/)\n\nStandard\n\nFull access to MotherDuck experts with a [standard response SLA](https://motherduck.com/support-policy/)\n\nIn-App Expert ChatIn-App\n\nExpert Chat\n\nChat with product experts directly\nfrom the MotherDuck UI\n\n## Serverless Instances\n\nMotherDuck\u2019s [per-user tenancy model](https://motherduck.com/product/data-teams/) gives each Org member an isolated instance in one of five types to enable individual, user-level configuration.Multiple read scaling replicas can also be added to handle extra load from concurrent users.\n\n![Pulse Instance type illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Finstance-type-illus-pulse.8c39b9cc.png&w=3840&q=75)\n\n### Pulse\n\nOur smallest instance, perfect for ad-hoc analytics tasks\n\nCommonly used when...\n\n- Doing ad-hoc analytics tasks with datasets in MotherDuck\n- Building read-only workloads with high volumes of concurrent users like customer-facing analytics applications\n- A larger, dedicated instance is overkill for the task at hand\n\nIncluded Plans:\n\nFreeLiteBusiness\n\n![Standard Instance type illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Finstance-type-illus-standard.3d0ae9e9.png&w=3840&q=75)\n\n### Standard\n\nBuilt to handle common data warehouse workloads, including loads and transforms\n\nCommonly used when...\n\n- Performing typical data engineering tasks like data ingest and dbt transformations\n- Running dbt jobs that have multiple transformations in parallel\n- Building out standard data pipelines before moving to production and scaling up\n\nIncluded Plans:\n\nFreeLiteBusiness\n\n![Jumbo Instance type illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Finstance-type-illus-jumbo.b77c7bca.png&w=3840&q=75)\n\n### Jumbo\n\nFor larger data warehouse workloads with many transformations or complex aggregations\n\nCommonly used when...\n\n- Query complexity, data volume, and the number of transformations are too high for the Standard instance\n- Performing complex joins and aggregations on growing datasets using event stream data or nested JSON with changing schemas\n- Running faster with your workloads\n\nIncluded Plans:\n\nFreeLiteBusiness\n\nAnd for even heavier workloads...\n\n![Mega Instance type illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Finstance-type-illus-mega.cd4cd44f.png&w=3840&q=75)\n\n### Mega\n\nAn extremely large instance for when you need complex transformations done quickly\n\nCommonly used when...\n\n- Queries are too complex or data volumes too high for Jumbo instances to handle in crunch time\n- Running a weekly job that rebuilds all of your tables and it has to run in minutes, not hours\n- One customer has 10x the amount of data as others and you want to deliver same speed\n\nIncluded Plans:\n\nFreeLiteBusiness\n\n![Giga Instance type illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Finstance-type-illus-giga.3228dda6.png&w=3840&q=75)\n\n### Giga\n\nLargest instances enable the toughest transformations to run faster\n\nCommonly used when...\n\n- Your data workload is so complex that nothing else will work\n- Running a one-time job to go back and restate revenue for the last 10 years at a large company\n- You need a growth path beyond Mega for when your data volume and complexity grows 10x\n\n[Contact Us](https://motherduck.com/contact-us/product-expert/)\n\n## flock together with 40+ integration partners\n\n![Hex](https://motherduck.com/_next/static/media/hex-logo.f94f1afa.png)\n\n![Dbt](https://motherduck.com/_next/static/media/dbt-logo.5fd68952.png)\n\n![Omni](https://motherduck.com/_next/static/media/omni-logo.28efaf09.png)\n\n![Airflow](https://motherduck.com/_next/static/media/airflow-logo.aa9b0b7a.png)\n\n![Fivetran](https://motherduck.com/_next/static/media/fivetran-logo.e1839559.png)\n\n![Superset](https://motherduck.com/_next/static/media/superset-logo.ba9ddb48.png)\n\n![Tableau](https://motherduck.com/_next/static/media/tableau-logo.5d1e2634.png)\n\n![Airbyte](https://motherduck.com/_next/static/media/airbyte-logo.8ec3c7fc.png)\n\n![PowerBI](https://motherduck.com/_next/static/media/power-bi-logo.ca8fb520.png)\n\n![Dagster](https://motherduck.com/_next/static/media/dagster-logo.c48c5136.png)\n\n[EXPLORE ECOSYSTEM](https://motherduck.com/ecosystem/)\n\n![Extensive Ecosystem illustration](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fextensive-ecosystem-illus.9113fad6.png&w=3840&q=75)\n\n## Built for humans, powered by ducks\n\nMotherDuck\u2019s strategic partnership with DuckDB Labs brings the fast improving efficiencies of DuckDB\u2019s query engine from local machines to the cloud as a managed service. Working closely with the DuckDB Labs team has helped us co-create a shared vision for the future of blazing fast analytics. Together, we are committed to cutting-edge innovation in service of ergonomic, radically streamlined analytics workflows.\n\n[LEARN MORE](https://motherduck.com/about-us/)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpricing-info-built-for-humans.ad49af55.png&w=3840&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpricing-info-built-for-humans.ad49af55.png&w=3840&q=75)\n\n## Support with a heart\n\nGreat technology deserves a remarkable customer experience to match. We believe a thoughtful, human-first approach to delivering great support is as important as building great products. Our Customer team genuinely cares about your success and strives to act as an extension of your own team.\n\n[LEARN MORE](https://motherduck.com/customer-support/)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpricing-info-support-with-hearth.bdef3120.png&w=3840&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpricing-info-support-with-hearth.bdef3120.png&w=3840&q=75)\n\n## Trusted security standards\n\nKeeping your data safe and secure in the MotherDuck nest is our top priority. Our robust security and privacy program uses a defense in-depth strategy to protect your most valuable business assets and fortify trust. MotherDuck is SOC 2 Type II and GDPR compliant, with full reports and HIPAA BAAs available on request for [Business Plan](https://motherduck.com/product/pricing/#info-table) customers.\n\n[LEARN MORE](https://motherduck.com/trust-and-security/#Compliance)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpricing-info-gold-standard.a6afcacd.png&w=1920&q=75)\n\n![](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpricing-info-gold-standard.a6afcacd.png&w=1920&q=75)\n\n## FAQS\n\n### What is a MotherDuck Organization?\n\nIn MotherDuck, an Organization (Org for short) is a team of users. Each Org has member limits aligned to your pricing plan. Accounts for team members and service accounts will count towards the Org members included in your pricing plan.\n\n### How does MotherDuck calculate and meter compute hours?\n\nDepending on the Instance Type, MotherDuck meters compute on demand or per-instance.\n\nThe Pulse instance is an on-demand, auto-scaling instance that is metered on a per-query basis. We measure the amount of CPU and memory usage over time as a Compute Unit (CU). Pulse has a minimum of 1 CU second per query.\n\nStandard, Jumbo, Mega, and Giga instances are metered per second an instance is running, with variable compute costs based on the pricing plan and selected Instance Type. They start up quickly, within ~200 ms, and continue running for 60 seconds once the last query completes.\n\nLarger Mega and Giga instances take up to a few minutes to start up and have a cooldown period of 5 and 10 minutes, respectively.\n\n### What is MotherDuck\u2019s storage minimum, and how is it calculated and metered?\n\nMotherDuck uses cloud object storage. Usage is metered on the size of compressed data stored on-disk.\n\nTo provide data recoverability, MotherDuck charges for 7 days\u2019 worth of fail-safe data stored and meters on a per-day basis. Any data that is mutated or deleted will be retained for 7 days.\n\nMotherDuck does not charge you for: Data managed by you in your own object storage bucket, data stored on your laptop, or MotherDuck-managed data used to provide zero-copy clones called [shares](https://motherduck.com/docs/key-tasks/sharing-data/sharing-overview/) and associated data sharing features.\n\n### How are AI Units measured and metered?\n\n1 AI Unit = $1.00\n\nPlease refer to the [Documentation](https://motherduck.com/docs/about-motherduck/billing/pricing/) for a detailed overview of how AI units burn down against [SQL Assistant](https://motherduck.com/docs/category/sql-assistant/) and [Bulk AI Functions](https://motherduck.com/docs/about-motherduck/billing/pricing/).\n\nPaid Plan customers have full access to [FixIt](https://motherduck.com/docs/key-tasks/writing-sql-with-ai/#how-fixit-works), MotherDuck\u2019s in-line suggestions to help resolve common SQL errors, and usage-based pricing for [SQL Assistant](https://motherduck.com/docs/category/sql-assistant/) and [Bulk AI Functions](https://motherduck.com/docs/about-motherduck/billing/pricing/).\n\nFree Plan users have full access to [FixIt](https://motherduck.com/docs/key-tasks/writing-sql-with-ai/#how-fixit-works) and limited access to our [SQL Assistant Functions](https://motherduck.com/docs/category/sql-assistant/).\n\n### In which cloud regions is MotherDuck available?\n\nMotherDuck is currently available in the [AWS us-east-1 and AWS eu-central-1 regions](https://motherduck.com/docs/concepts/architecture-and-capabilities/). We have plans to expand to additional regions as part of our roadmap and encourage you to upvote ( [US-West](https://motherduck.canny.io/feature-requests/p/us-west-region)) or add other desired regions to [Canny](https://motherduck.canny.io/feature-requests).\n\n### Can I use MotherDuck in two cloud regions simultaneously?\n\nEach MotherDuck Organization is currently scoped to a single cloud region that must be chosen at Org creation when signing up.\n\n### Is there an on-premises version of MotherDuck?\n\nNo - MotherDuck is a managed cloud service.\n\n### Does MotherDuck have a free trial?\n\nEvery new account gets a free 21-day trial with full access to all of MotherDuck\u2019s features. We also offer a Free Plan with 10 GB of storage and 10 Compute Unit (CU) hours per month.\n\n### Does MotherDuck have an annual plan?\n\nYes - teams who are interested in an annual contract have the option to pre-commit to a level of MotherDuck usage. To learn more, please connect with our [Sales team](https://motherduck.com/contact-us/product-expert/).\n\n### How does MotherDuck protect my data?\n\nMotherDuck is committed to protecting customer data, honoring your global privacy rights, and securing the data you provide to us through the use of our products. We build our platform using best practices for highly available, scalable, and secure cloud applications, and we regularly monitor and assess our program to ensure that it meets or exceeds compliance and regulatory requirements. For more information and details about our privacy and security programs, please visit our [Trust & Security](https://motherduck.com/trust-and-security/#Compliance) page.\n\n### Does MotherDuck have academic discounts for schools or students?\n\nWe do not currently have any discounts for schools or students. Anyone is welcome to use MotherDuck\u2019s Free Plan.\n\n### Does MotherDuck offer a discount for non-profits?\n\nWe do not currently have any discounts for non-profits. Anyone is welcome to use MotherDuck\u2019s Free Plan.\n\n### What is your refund policy?\n\nPlease refer to MotherDuck\u2019s [Terms of Service](https://motherduck.com/terms-of-service/) for more information. If you have additional questions, please reach out to our [Customer team](https://motherduck.com/contact-us/support/) for assistance.\n\n![MotherDuck logo](https://motherduck.com/_next/static/media/md-simple-logo.00372b77.png)\n\n### Try Motherduck now\n\nGet started for free,\nshare data with the whole flock as you grow.\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 661,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 661
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! I found the information about MotherDuck's compute instance sizes. Let me also search for any additional documentation that might have more technical details:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ Pulse|Standard|Jumbo|Mega|Giga
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/no-sql-standard-for-7-years.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/faster-data-pipelines-development-with-mcp-and-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ai-powered-bi-can-llms-really-generate-your-dashboards-ft-michael-driscoll.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/privacy-policy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-warehouse-tco.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/hack-night.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/index.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/ETL.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me check a few more files that might have additional technical details:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/learn-more/data-warehouse-tco.md

  [96mğŸ”§ TOOL:[0m Grep â†’ vCPU|memory|RAM|GB|cores
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/learn-more/data-warehouse-tco.md",
  [2m  â”‚[0m     "content": "---\ntitle: data-warehouse-tco\ncontent_type: guide\nsource_url: https://motherduck.com/learn-more/data-warehouse-tco\nindexed_at: '2025-11-25T09:57:20.488375'\ncontent_hash: 9cd66b6dc23bf342\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO LEARN](https://motherduck.com/learn-more/)\n\n# The Data Warehouse TCO: A Guide to the True Costs of Snowflake, BigQuery, and Redshift\n\n11 min readBY\n\n[Manveer Chawla](https://motherduck.com/authors/manveer-chawla/)\n\n![The Data Warehouse TCO: A Guide to the True Costs of Snowflake, BigQuery, and Redshift](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fduck_with_data_547d5d7d19_b45b87719d.png&w=3840&q=75)\n\nYour dashboards are fast, your data is centralized, and your team is finally making [data-driven decisions](https://motherduck.com/learn-more/modern-data-warehouse-playbook/). Then, a five-figure surprise bill arrives from your cloud data warehouse.\n\nThis is a common story. Teams choose one of the three leading cloud data warehouses (Snowflake, Google BigQuery, or Amazon Redshift) for their promise of consumption-based pricing and massive scalability. The model seems simple: pay only for what you use. However, the reality is that advertised rates for compute and storage are just one part of a much larger cost structure.\n\nA simple comparison of list prices is misleading because each of these platforms has a unique architecture and a distinct economic model. Your true Total Cost of Ownership (TCO) is determined by how well your specific workloads align with the architectural trade-offs inherent in each system. To budget accurately, you must understand these underlying models:\n\n- **[Snowflake's TCO](https://www.snowflake.com/en/pricing/)** is driven by its credit-based model, where you pay for compute time consumed by virtual warehouses.\n- **[BigQuery's TCO](https://cloud.google.com/bigquery/pricing)** is most commonly driven by its on-demand model, where you pay for the terabytes of data your queries scan.\n- **[Redshift's TCO](https://aws.amazon.com/redshift/pricing/)** is often rooted in its provisioned cluster model, where you pay per hour for a fixed set of resources, frequently with significant long-term discounts.\n\nThis guide provides a framework to calculate your true TCO by examining three key cost drivers across these platforms: nuanced compute billing models, standard cloud egress fees, and the necessary investment in [personnel and administrative overhead](https://motherduck.com/learn-more/self-service-analytics-startups/).\n\n| Cost Driver | Snowflake | Google BigQuery (On-Demand) | Amazon Redshift |\n| --- | --- | --- | --- |\n| **Primary Compute Model** | Credit-based (per-second after 60s min) | Scan-based ($/TB scanned) | Provisioned Cluster (per-hour) or Serverless (per-second after 60s min) |\n| **Key Financial Risk** | \"Death by a thousand cuts\" from small queries with 60s minimum billing | A single \"runaway query\" scanning a large table can cost thousands | Paying for idle provisioned capacity or the 60s minimum on serverless |\n| **Admin Overhead** | Medium: Focus on warehouse sizing, cost monitoring, and permissions | Low: Focus on query discipline and cost-control quotas | High (Provisioned): Requires deep expertise in cluster tuning, distribution keys, and `VACUUM` |\n| **Best Fit Workload** | Predictable, long-running batch jobs; less ideal for high-concurrency, short queries | Ad-hoc exploratory analysis where infrastructure management is not desired | Stable, well-understood workloads where Reserved Instances can provide discounts |\n\n## Compute: The \\#1 Source of Unpredictable Data Warehouse Costs\n\nThe largest variable in your monthly bill is compute, and each platform bills for it differently. Understanding these models is the key to avoiding unexpected costs, whether from idle-but-billed resources or from a single, poorly written query.\n\n### The Cost of \"Warm\" Compute: Minimum Billing Increments\n\nBoth Snowflake and Amazon Redshift Serverless employ a 60-second minimum billing window for compute activation. For Snowflake, this occurs when a virtual warehouse resumes from a suspended state. For Redshift Serverless, it is when a workgroup becomes active. While billing becomes per-second after that minute, the initial 60-second charge is unavoidable for each activation.\n\nThis has a significant financial impact on [common startup workloads](https://motherduck.com/learn-more/cloud-data-warehouse-startup-guide/) like interactive business intelligence.\n\nConsider a realistic scenario: Your Head of Sales loads a Tableau dashboard that runs 10 separate, highly optimized queries to render its visuals. Each query takes only 3 seconds to complete.\n\nLet's do the math:\n\n- **Actual Compute Time Used:** 10 queries x 3 seconds/query = 30 seconds\n- **Billed Compute Time (Snowflake/Redshift Serverless):** 10 queries x 60-second minimum charge/query = 600 seconds, or 10 minutes\n\nIn this scenario, you paid for 20 times the compute you actually used. This inefficiency is a core component of the ['big data tax'](https://motherduck.com/learn-more/modern-data-warehouse-playbook/) that traditional warehouses impose on interactive workloads.\n\n**The Architect's View:** This 60-second minimum is not arbitrary. It represents a trade-off to keep compute resources \"warm\" and instantly available, avoiding the latency of a \"cold start\" that could take much longer. The cost is in service of a better, more responsive interactive user experience.\n\nHowever, this cost can be actively managed. Best practices include implementing aggressive auto-suspend policies (e.g., 60 seconds) and using BI tools that can batch dashboard queries into a single request. For Snowflake, [Resource Monitors](https://docs.snowflake.com/en/user-guide/resource-monitors) can also be configured to automatically suspend warehouses when credit consumption exceeds a set budget, providing a financial safeguard.\n\n### BigQuery TCO and the \"Runaway Query\" Risk\n\nBigQuery's on-demand pricing model presents a different kind of financial risk: the \"scan-based pricing trap.\" In this model, you pay for the amount of data a query scans, typically around $6.25 per terabyte. This is incredibly powerful for ad-hoc analysis, but it can lead to large, unexpected bills from a single inefficient query.\n\nFor example, a new analyst running a `SELECT *` query on a multi-terabyte table without a `WHERE` clause can accidentally generate a bill for thousands of dollars in a matter of seconds.\n\n**The Architect's View:** This model's trade-off is one of ultimate simplicity for power. It completely abstracts away all infrastructure management. There are no clusters to size or tune. The responsibility for cost control, therefore, shifts from infrastructure governance to query discipline.\n\nThis risk is highly manageable. Costs can be controlled by enforcing query best practices, such as always using a `WHERE` clause and leveraging BigQuery's partitioning and clustering features to reduce the amount of data scanned. Critically, organizations can implement hard financial guardrails by setting [project-level and user-level quotas](https://cloud.google.com/bigquery/quotas) that cap the amount of data that can be processed daily.\n\n## The Cost of Data Movement: Standard Cloud Egress Fees\n\nThe second major cost driver is for moving data out of your cloud provider's network. These [standard cloud egress fees](https://holori.com/egress-costs-comparison) are a reality of every major cloud platform. While loading data into the warehouse is almost always free, taking it out incurs costs that typically range from $90 to over $150 per terabyte.\n\nFor a growing company, this is not an edge case. It is a direct cost on everyday workflows:\n\n- **Multi-Cloud Tooling:** If your BI tool is hosted in a different cloud or region than your data warehouse, every query pulling data to a dashboard is subject to egress fees.\n- **Data Science and Machine Learning:** When a data scientist pulls a 100 GB dataset to their local machine to train a model, you could pay a $9 to $15 \"data export\" cost.\n- **Embedded Analytics:** In a customer-facing application, every piece of data you send from your warehouse to your users' browsers can incur egress fees. A successful product with high user engagement can directly translate to a higher data warehouse bill.\n\nNetwork egress is a fundamental component of cloud pricing that encourages keeping your data stack within a single vendor's ecosystem to minimize costs. This is an important architectural consideration, as it creates a financial incentive against the flexibility of using best-of-breed tools across different cloud providers.\n\n## The Cost of Human Capital: Personnel and Administrative Overhead\n\nThe cost that never appears on your monthly invoice is the fully-burdened salary of the data engineers required to manage, tune, and govern the warehouse. This personnel and administrative overhead is a necessary investment, and the amount required varies significantly based on the platform's architectural complexity.\n\nFraming this as a hard cost is essential. If a data engineer with a fully-loaded salary of $150,000 per year spends just four hours a week on warehouse administration, you are paying $15,000 annually in personnel costs just for platform maintenance.\n\nThis \"administration\" involves different tasks on each platform:\n\n- **Amazon Redshift** traditionally requires the most hands-on management. Engineers must become experts in [physical data layout](https://motherduck.com/learn-more/columnar-storage-guide/), selecting distribution keys and sort keys, and performing manual maintenance like `VACUUM` and `ANALYZE` to prevent performance degradation. Predictable workloads can see compute costs cut by up to 75% with **Reserved Instances**, but this requires upfront capacity planning and management.\n- **Snowflake** shifts the focus from physical tuning to logical governance. The primary administrative tasks involve \"right-sizing\" virtual warehouses for different workloads, managing a complex hierarchy of roles and permissions, and actively monitoring consumption to control costs.\n- **Google BigQuery**, as a fully serverless platform, demands the least infrastructure administration. The focus is almost entirely on logical governance: managing IAM permissions, monitoring query patterns for inefficiency, and setting cost-control quotas.\n\nThis engineering time is your most valuable resource. The administrative burden of your chosen platform is a direct factor in your TCO and a tax on your team's ability to deliver new data products and business insights.\n\n## How to Calculate Your Data Warehouse TCO: A 4-Part Checklist\n\nTo avoid surprise bills, you need a financial model that accounts for these architectural nuances. Use this checklist to calculate a more realistic TCO:\n\n**1\\. Calculate Base Costs:** What is your estimated monthly bill for compute credits or provisioned nodes and storage?\n\n**2\\. Model Architectural Costs:** Based on your workload (e.g., spiky BI vs. steady ETL), what is the likely cost from minimum billing increments or accidental large scans? (e.g., `(Avg. BI Sessions/Day * 60 seconds) * Cost/Second`)\n\n**3\\. Estimate Egress Fees:** How much data will you move to other clouds, regions, or local machines monthly? (`GBs moved * $/GB Egress Rate`)\n\n**4\\. Quantify Personnel Overhead:** How many hours per week will your team spend on platform management? (`(Engineer Salary / 2080 hours) * Hours/Week * 52`)\n\nLet's apply this to a hypothetical \"SaaSCo,\" a 100-person company with a high-concurrency BI workload powered by 10 active users.\n\n- **Workload:** Frequent, short-running queries on 500 GB of data.\n- **Personnel:** One data engineer ($150k/yr) spends ~4 hours/week on management.\n- **Base Bill:** Their estimated monthly bill for compute and storage is **$2,000**.\n\n### Calculating SaaSCo's True TCO\n\n**Scenario A: A Platform with Minimum Billing Increments (e.g., Snowflake)**\n\n- **Direct Bill:** $2,000\n- **Architectural Cost (Minimum Billing):** The thousands of small BI queries incur the 60-second minimum charge, adding **$1,500/month** in billed-but-unused compute time.\n- **Egress Fees:** Negligible for this internal workload, assume **$20/month**.\n- **Personnel Overhead:** The engineer's time costs **$1,250/month**.\n- **Total True TCO:** $2,000 + $1,500 + $20 + $1,250 = **$4,770 per month**\n\n**Scenario B: A Platform with Scan-Based Pricing (e.g., BigQuery On-Demand)**\n\n- **Direct Bill:** $2,000\n- **Architectural Cost (Scan Risk):** The BI queries are efficient, but one unoptimized ad-hoc query from an analyst scans several terabytes, adding a one-time **$500** charge for the month.\n- **Egress Fees:** Negligible, **$20/month**.\n- **Personnel Overhead:** Lower admin burden reduces this cost to **$800/month**.\n- **Total True TCO:** $2,000 + $500 + $20 + $800 = **$3,320 per month**\n\nSaaSCo's true cost can be more than double their initial budget, and the primary driver of that overage depends entirely on the platform's architecture.\n\n## Conclusion: Aligning Architecture with Your Workload\n\nThe most expensive part of a cloud data warehouse is not the data, it is the architecture. The leading platforms were designed for a different era, prioritizing massive scale and centralization for large enterprises. For [startups and lean teams](https://motherduck.com/learn-more/cloud-data-warehouse-startup-guide/), this approach can introduce unnecessary complexity and overhead, forcing engineers to spend valuable time managing costs and tuning infrastructure rather than building products.\n\nA modern, lean architectural approach eliminates these hidden costs by design. By embracing principles like true per-second serverless execution with no billing minimums and zero infrastructure administration, it is possible to achieve cost predictability. This new model is ideal for smaller, agile teams that need to iterate quickly without accumulating financial or performance debt. While this approach comes with its own set of trade-offs, it is fundamentally designed to align with the goals of a lean business.\n\nSee how a lean architecture provides predictable, transparent pricing. Read our [Modern Data Warehouse Playbook to learn more](https://motherduck.com/learn-more/modern-data-warehouse-playbook/).\n\n### TABLE OF CONTENTS\n\n[Compute: The Source of Unpredictable Data Warehouse Costs](https://motherduck.com/learn-more/data-warehouse-tco/#compute-the-source-of-unpredictable-data-warehouse-costs)\n\n[The Cost of Data Movement: Standard Cloud Egress Fees](https://motherduck.com/learn-more/data-warehouse-tco/#the-cost-of-data-movement-standard-cloud-egress-fees)\n\n[The Cost of Human Capital: Personnel and Administrative Overhead](https://motherduck.com/learn-more/data-warehouse-tco/#the-cost-of-human-capital-personnel-and-administrative-overhead)\n\n[How to Calculate Your Data Warehouse TCO: A 4-Part Checklist](https://motherduck.com/learn-more/data-warehouse-tco/#how-to-calculate-your-data-warehouse-tco-a-4-part-checklist)\n\n[Conclusion: Aligning Architecture with Your Workload](https://motherduck.com/learn-more/data-warehouse-tco/#conclusion-aligning-architecture-with-your-workload)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\n## FAQS\n\n### What does data warehouse TCO mean?\n\nTCO stands for **Total Cost of Ownership**. It represents the complete cost of using a data warehouse, including the direct costs for compute and storage, plus all indirect or \"hidden\" costs. These hidden costs include architectural inefficiencies (like paying for unused compute time), network egress fees for moving data, and the cost of engineering salaries for platform administration and governance.\n\n### Why is my Snowflake bill so unpredictable?\n\nSnowflake's costs can be unpredictable due to its credit-based model combined with a [**60-second minimum charge**](https://motherduck.com/learn-more/reduce-snowflake-costs-duckdb) when a virtual warehouse \"wakes up.\" For workloads with many short, frequent queries (common with BI dashboards), you may be billed for 60 seconds of compute for a query that only takes 3 seconds, causing costs to inflate by 20x or more.\n\n### What is the biggest financial risk with BigQuery?\n\nWith BigQuery's on-demand pricing, the biggest risk is the \"runaway query.\" Because you pay for the amount of data a query scans (e.g., ~$6.25/TB), a single inefficient query without proper filters (`WHERE` clause) on a large table can accidentally generate a bill for thousands of dollars in a matter of seconds.\n\n### How can I reduce my cloud data warehouse costs?\n\nYou can reduce costs by enforcing query best practices, setting firm budget alerts and quotas (like Snowflake's Resource Monitors or BigQuery's user-level quotas), and co-locating your BI tools and data warehouse in the same cloud region to minimize egress fees. Most importantly, choose an architecture (like a lean data stack) that is designed to minimize waste and administrative overhead from the start.\n\n### Why is my Redshift bill so high when my data isn\u2019t that big?\n\nA high Redshift bill can be due to its pricing models, even without massive data. With provisioned clusters, you might be paying for significant idle capacity, while the serverless option imposes a 60-second minimum billing charge that inflates costs for short, frequent queries. A truly serverless platform like MotherDuck avoids both of these pitfalls, offering a more direct pay-for-what-you-use model that is better suited for bursty, interactive workloads.\n\n### What are some strategies to manage the cost and scalability of large data queries?\n\nKey strategies include implementing platform-specific controls like setting user-level quotas in BigQuery or using resource monitors in Snowflake to prevent overages. Architecturally, partitioning and clustering tables can significantly reduce the amount of data scanned by large queries. Alternatively, adopting a more efficient platform like MotherDuck can simplify cost management, as its architecture is optimized for fast, scalable analytics without the complex overhead.\n\n### What are some ways small startups can manage data warehousing costs effectively?\n\nStartups can manage costs by choosing a data warehouse whose economic model fits their interactive and often unpredictable workloads. It\u2019s crucial to avoid platforms with 60-second minimum billing increments or high administrative overhead, which can quickly inflate a small budget. A lean, serverless solution like MotherDuck is ideal for startups, as it avoids these cost traps and minimizes administrative burden, allowing the team to focus on building their product.\n\n### Is there a simpler, cheaper alternative to a full-blown data warehouse for a startup or small team?\n\nAbsolutely. For many startups and small teams, a traditional data warehouse is overkill in both cost and complexity. The ideal alternative is a solution that delivers powerful analytics without the high administrative burden and inflexible pricing of larger platforms. MotherDuck is a perfect example, providing a simple, serverless analytics platform that allows teams to run fast queries and scale affordably without the commitment of a full-blown data warehouse.\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 203,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 203
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-if-sql-queries-returned-results-instantly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/speed-querying-stackoverflow-data-with-duckdb-ft-michael-hunger.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-bad-the-good-and-the-ugly-navigating-the-new-waves-of-machine-learning-ft-adi-polak.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/is-duckdb-the-secret-to-unlocking-your-gis-potential.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/machine-learning-and-ai-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/more-than-a-vibe-ai-driven-sql-that-actually-works.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/getting-started-with-motherduck-webinar.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me write the answer to the markdown file:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_9.md

  [96mğŸ”§ TOOL:[0m Bash â†’ cat > /tmp/answer_without_kg_9.md << 'EOF'
# MotherDuck Compute Instance Sizes

MotherDuck offers **5 distinct compute instance types** designed for different workload requirements, following a serverless architecture with per-user tenancy. Each instance type is optimized for specific use cases ranging from lightweight ad-hoc analytics to the most demanding data transformations.

## Available Instance Types

### 1. **Pulse**
The smallest instance designed for lightweight, fully serverless analytics.

**Common use cases:**
- Ad-hoc analytics tasks with datasets in MotherDuck
- Building read-only workloads with high volumes of concurrent users (e.g., customer-facing analytics applications)
- Situations where a larger, dedicated instance is overkill

**Billing:** Per Compute Unit (CU) second, 1 second minimum

**Pricing:**
- Free plan: Up to 10 CU hours
- Lite plan: $0.25 per CU hour
- Business plan: $0.40 per CU hour

### 2. **Standard**
Built for common data warehouse workloads including loads and transforms.

**Common use cases:**
- Typical data engineering tasks like data ingest and dbt transformations
- Running dbt jobs with multiple transformations in parallel
- Building standard data pipelines before moving to production

**Billing:** Per second, 60 second minimum

**Pricing:**
- Lite plan: $1.20 per hour
- Business plan: $1.80 per hour

### 3. **Jumbo**
Designed for larger data warehouse workloads with many transformations or complex aggregations.

**Common use cases:**
- Query complexity, data volume, and transformation counts too high for Standard instances
- Complex joins and aggregations on growing datasets with event stream data or nested JSON
- Workloads requiring faster execution times

**Billing:** Per second, 60 second minimum

**Pricing:**
- Lite plan: Not available
- Business plan: $3.60 per hour

### 4. **Mega**
An extremely large instance for complex transformations that need to be completed quickly.

**Common use cases:**
- Queries too complex or data volumes too high for Jumbo instances
- Weekly jobs that rebuild all tables and must complete in minutes, not hours
- Customers with 10x the data volume of typical users requiring consistent performance

**Billing:** Per second, 5 minute minimum

**Pricing:**
- Business plan only: $10.80 per hour

### 5. **Giga**
The largest instance type for the toughest transformations.

**Common use cases:**
- Extremely complex data workloads that exceed all other instance capabilities
- One-time massive jobs (e.g., restating revenue for 10 years at a large company)
- Growth path beyond Mega for 10x increases in data volume and complexity

**Billing:** Per second, 10 minute minimum

**Pricing:** Contact MotherDuck sales team

## Plan Availability

- **Free Plan:** Pulse only (1 instance type)
- **Lite Plan:** Pulse and Standard (2 instance types)
- **Business Plan:** Pulse, Standard, Jumbo, Mega, and Giga (5 instance types)

## Additional Features

### Read Scaling
Business plan customers can enable up to **16 read-scaling replicas**, each backed by its own instance, for BI dashboards and read-only use cases. This provides additional concurrent query capacity for high-traffic scenarios.

### Per-User Tenancy
MotherDuck's architecture provides each organization member with an isolated instance, enabling individual, user-level configuration and ensuring workload isolation.

## Sources

- [MotherDuck Pricing Page](https://motherduck.com/product/pricing/) - Primary source for instance types, descriptions, and pricing
- [Data Warehouse TCO Guide](https://motherduck.com/learn-more/data-warehouse-tco) - Additional context on compute billing models
EOF
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m 
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! I've completed the task. I searched through the `.kurt/sources/` directory and found comprehensive information about MotherDuck's compute instance sizes from their pricing page. 
  [94mâ”‚[0m 
  [94mâ”‚[0m **Summary of findings:**
  [94mâ”‚[0m 
  [94mâ”‚[0m MotherDuck offers **5 compute instance types**:
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. **Pulse** - Smallest, serverless instance for ad-hoc analytics (1 second minimum billing)
  [94mâ”‚[0m 2. **Standard** - For everyday data engineering tasks (60 second minimum billing)
  [94mâ”‚[0m 3. **Jumbo** - For larger workloads with complex aggregations (60 second minimum billing)
  [94mâ”‚[0m 4. **Mega** - Extremely large for demanding transformations (5 minute minimum billing)
  [94mâ”‚[0m 5. **Giga** - Largest for the toughest workloads (10 minute minimum billing, contact sales)
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer has been written to `/tmp/answer_without_kg_9.md` with detailed information about each instance type including use cases, billing models, pricing, and plan availability, along with proper source citations.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 4,639 | Cost: $0.2273
     Cumulative: 4,639 tokens | $0.2273
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent's message indicates task completion with a summary and does not pose any questions or requests for user input.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 11 | Duration: 93.7s
â•‘    Tokens: 4,639 | Cost: $0.2273
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  LLM Judge score: 0.70

======================================================================
â“ Question 10/10
What DuckDB versions does MotherDuck currently support?
======================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 10
â”‚ Search through the .kurt/sources/ directory to answer this question:

What DuckDB versions does MotherDuck currently support?

Steps:
1. Search for the most relevant markdown files in .kurt/sources/
2. Read those files and synthesize the answer
3. Write a markdown answer (with a ## Sources section) to: /tmp/answer_without_kg_10.md
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I'll help you find information about DuckDB versions supported by MotherDuck by searching through the sources directory.
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ DuckDB version|supported version|version support

  [96mğŸ”§ TOOL:[0m Grep â†’ v\d+\.\d+\.\d+|version \d+\.\d+

  [96mğŸ”§ TOOL:[0m Grep â†’ compatible|compatibility
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/hack-night.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-duckdb-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-ducks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/fix-outdated-llm-documentation-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/galileo-world-geospatial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-cognee-sql-analytics-graph-rag.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-app-generator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/jacob-matson.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/jelte-fennema-nio.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 19,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-duckdb-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/geospatial-for-beginner-duckdb-spatial-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2023.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ai-powered-bi-can-llms-really-generate-your-dashboards-ft-michael-driscoll.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Java programming language.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/DB-API 2.0.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/meltano.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search more specifically for version information and read the most relevant files:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release.md

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/faster-ducks.md

  [96mğŸ”§ TOOL:[0m Grep â†’ supports DuckDB|DuckDB 0\.|DuckDB 1\.|minimum version|required version
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md",
  [2m  â”‚[0m     "content": "---\ntitle: announcing-duckdb-141-motherduck\ncontent_type: blog\nsource_url: https://motherduck.com/blog/announcing-duckdb-141-motherduck\nindexed_at: '2025-11-25T19:56:19.062696'\ncontent_hash: cb38b5b6d8d7bc42\nhas_code_examples: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains\n\n2025/10/09 - 5 min read\n\nBY\n\n[Alex Monahan](https://motherduck.com/authors/alex-monahan/)\n,\n[Garrett O'Brien](https://motherduck.com/authors/garrett-obrien/)\n\nOne of the most exciting things about DuckDB as a technology is just how quickly it improves. It\u2019s hard not to be excited about supporting a major release, but we are _especially_ excited about this one. We\u2019re thrilled to share that MotherDuck now supports DuckDB version 1.4.1 and DuckLake version 0.3.\n\n**DuckDB 1.4.0** introduced landmark features, including the MERGE statement, VARIANT type, and a completely rewritten sorting engine. **DuckDB 1.4.1** builds on that foundation with important bugfixes and additional improvements. MotherDuck now supports the latest 1.4.1 version. While you can continue using your current version of DuckDB, we encourage you to [upgrade your DuckDB clients to 1.4.1](https://duckdb.org/docs/installation/?version=stable&environment=cli&platform=macos&download_method=package_manager) as soon as you can.\n\nOn the DuckLake side, MotherDuck now supports **DuckLake 0.3**. DuckLake 0.3 introduces the DuckLake `CHECKPOINT` function that makes table maintenance automatic, plus interoperability with Iceberg and native support for spatial geometry types.\n\nRead on for our favorite highlights from these releases, and check out the DuckDB blogs on [1.4.0](https://duckdb.org/2025/09/16/announcing-duckdb-140.html) and [1.4.1](https://duckdb.org/2025/10/07/announcing-duckdb-141.html) for all the details.\n\n## DuckLake 0.3: Iceberg Interoperability, Simplified Maintenance, and Spatial Data Support\n\n### Iceberg Interoperability\n\nThanks to the DuckDB `iceberg` extension, migrating your Iceberg data lake to MotherDuck-managed DuckLake just got a lot easier. On the migration path, you\u2019ll find an integrated, cloud-scale lakehouse that maintains support for tools that only speak Iceberg.\n\nYou can now copy directly from Iceberg to DuckLake as part of a migration, or from DuckLake to Iceberg to continue using your favorite Iceberg-only tools.\n\n### DuckLake Checkpoint: Maintenance Made Easy\n\nThe new `CHECKPOINT` statement combines all the maintenance operations you need into a single, simple command. Configure it once, and it automatically runs operations in sequential order:\n\n- Flushes inlined data\n- Compacts small files created by multi-threaded writes\n- Rewrites files with many deletions\n- Cleans up orphaned files\n\nNo more juggling multiple maintenance commands\u2014just call `CHECKPOINT` and DuckLake handles the rest:\n\n```sql\nCopy code\n\nATTACH 'ducklake:my_ducklake.ducklake' AS my_ducklake;\nUSE my_ducklake;\nCHECKPOINT;\n```\n\n### Spatial Geometry Types\n\nDuckLake 0.3 introduces native support for geometry data types, allowing users to take advantage of the DuckDB `spatial` extension\u2019s functionality in DuckLake. This opens up powerful new use cases for geospatial analytics directly on your data lake\u2013see the [DuckLake documentation](https://ducklake.select/docs/stable/specification/data_types#geometry-types) for a list of supported types.\n\n### MERGE INTO: Upserts for Data Lakes\n\nDuckLake 0.3 now fully supports the `MERGE INTO` statement, bringing elegant upsert capabilities to your data lake tables without requiring primary keys or indexes. This is a game-changer for incremental data pipelines and slowly changing dimensions.\n\nAs an example:\n\n```sql\nCopy code\n\n-- Update existing records and insert new ones\nWITH new_stocks(item_id, volume) AS (VALUES (20, 2200), (30, 1900))\nMERGE INTO ducklake_table.Stock\nUSING new_stocks USING (item_id)\nWHEN MATCHED THEN UPDATE SET balance = balance + volume\nWHEN NOT MATCHED THEN INSERT VALUES (new_stocks.item_id, new_stocks.volume)\nRETURNING merge_action, *;\n```\n\n`MERGE` also supports complex conditions and `DELETE` operations, making it perfect for real-world data engineering workflows. `MERGE` operations are efficient and work seamlessly with time travel, versioning, and all other DuckLake features. This gives you OLAP-optimized upsert performance on data lake storage:\n\n```sql\nCopy code\n\nWITH deletes(item_id, delete_threshold) AS (VALUES (10, 3000))\n    MERGE INTO Stock USING deletes USING (item_id)\n    WHEN MATCHED AND balance < delete_threshold THEN DELETE;\nFROM Stock;\n```\n\n### Smarter Write Performance\n\nDuckLake 0.3 speeds up write performance by allowing each thread to write separate files, which can be compacted later using the checkpoint function. This parallelization dramatically improves throughput for bulk inserts while keeping your table organized.\n\n### Additional DuckLake 0.3 Features\n\n- [**Snapshot tracking**](https://github.com/duckdb/ducklake/pull/350): New `current_snapshot()` function for easier snapshot management\n- [**Orphaned file cleanup**](https://github.com/duckdb/ducklake/pull/398): The `ducklake_delete_orphaned_files()` function removes files no longer tracked by DuckLake. Includes a `dry_run` parameter for testing\n- [**Intelligent data file rewriting**](https://github.com/duckdb/ducklake/pull/393) **:** Automatically identifies and rewrites files with many deletions for optimal performance on your current snapshot\n\n## DuckDB 1.4: MERGE Statement, VARIANT Type, and Performance\n\n### MERGE INTO: Upserts Without Primary Keys\n\nDuckDB 1.4.0 adds full support for the `MERGE` statement, giving you a clean, standard SQL way to handle upserts without requiring primary keys or indexes.\n\nHere's a simple example:\n\n```sql\nCopy code\n\nCREATE TABLE Stock(item_id INTEGER, balance INTEGER);\nINSERT INTO Stock VALUES (10, 2200), (20, 1900);\n\nWITH new_stocks(item_id, volume) AS (VALUES (20, 2200), (30, 1900))\n    MERGE INTO Stock\n        USING new_stocks USING (item_id)\n    WHEN MATCHED\n        THEN UPDATE SET balance = balance + volume\n    WHEN NOT MATCHED\n        THEN INSERT VALUES (new_stocks.item_id, new_stocks.volume)\n    RETURNING merge_action, *;\n```\n\n`MERGE` also supports complex conditions and `DELETE` operations, and it works seamlessly with DuckLake 0.3.\n\n### Blazing Fast Sorting: Rewritten from the Ground Up\n\nDuckDB 1.4.0 introduced a completely new sorting implementation that delivers often 2x or better performance improvements while using significantly less memory and scaling better across multiple threads.\n\nThe new k-way merge sort reduces data movement, adapts to pre-sorted data, and powers not just `ORDER BY` clauses but also window functions and list sorting operations. Your most intensive analytical queries just got dramatically faster \u2013 [read the DuckDB blog for more detail](https://www.google.com/url?q=https://duckdb.org/2025/09/24/sorting-again.html&sa=D&source=docs&ust=1759859043223938&usg=AOvVaw1v0Tkh7BSjXrL6K4duBp19).\n\n## Additional SQL Features\n\n### VARIANT type for semi-structured data\n\nThe new `VARIANT` type provides fast processing of JSON and other semi-structured data, with support for reading `VARIANT` types from Parquet files, including shredded encodings.\n\n### FILL window function for interpolation\n\nThe new `FILL()` window function makes it easy to interpolate missing values:\n\n```sql\nCopy code\n\nFROM (VALUES (1, 1), (2, NULL), (3, 42)) t(c1, c2)\nSELECT fill(c2) OVER (ORDER BY c1) f;\n-- Result: 1, 21, 42\n```\n\n## Huge Thanks to the DuckDB Team and Community\n\nIt\u2019s incredibly _fun_ to work with a technology that improves so fast, and we\u2019re so grateful to the entire DuckDB community. [DuckDB 1.4](https://duckdb.org/2025/09/16/announcing-duckdb-140.html) wouldn't be possible without the outstanding work from the DuckDB team and over 90 contributors who made more than 3,500 commits since version 1.3.2.\n\nIf you\u2019re curious about what else shipped in 1.4, head on over to the [DuckDB site](https://duckdb.org/2025/09/16/announcing-duckdb-140.html) and take a gander for yourself. And if you\u2019d like to run DuckDB-powered analytics at cloud scale, spin up a [free trial of MotherDuck](https://app.motherduck.com/?auth_flow=signup&_gl=1*1qteo2d*_gcl_au*MTI1MTE1Nzg3OS4xNzU1MTA4Mjk0*_ga*MTkwNjI1NTM3NS4xNzU1MTA4Mjk0*_ga_L80NDGFJTP*czE3NTk4MTM4MDAkbzE3OCRnMCR0MTc1OTgxMzgwMCRqNjAkbDAkaDE0MjU5MDU5Mzg.) or join our [community Slack](https://slack.motherduck.com/).\n\nLet's get quacking! \ud83e\udd86\n\n### TABLE OF CONTENTS\n\n[DuckLake 0.3: Iceberg Interoperability, Simplified Maintenance, and Spatial Data Support](https://motherduck.com/blog/announcing-duckdb-141-motherduck/#ducklake-03-iceberg-interoperability-simplified-maintenance-and-spatial-data-support)\n\n[DuckDB 1.4: MERGE Statement, VARIANT Type, and Performance](https://motherduck.com/blog/announcing-duckdb-141-motherduck/#duckdb-14-merge-statement-variant-type-and-performance)\n\n[Additional SQL Features](https://motherduck.com/blog/announcing-duckdb-141-motherduck/#additional-sql-features)\n\n[Huge Thanks to the DuckDB Team and Community](https://motherduck.com/blog/announcing-duckdb-141-motherduck/#huge-thanks-to-the-duckdb-team-and-community)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![DuckDB Ecosystem: September 2025](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FThree_items_Duck_DB_Ecosystem_3_72ab709f58.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-september-2025/)\n\n[2025/09/09 - Simon Sp\u00e4ti](https://motherduck.com/blog/duckdb-ecosystem-newsletter-september-2025/)\n\n### [DuckDB Ecosystem: September 2025](https://motherduck.com/blog/duckdb-ecosystem-newsletter-september-2025)\n\nDuckDB Monthly #33: DuckDB 58\u00d7 faster spatial joins, pg\\_duckdb 1.0, and 79% Snowflake cost savings\n\n[![MotherDuck is Landing in Europe! Announcing our EU Region](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Feu_launch_blog_b165ff2751.png&w=3840&q=75)](https://motherduck.com/blog/motherduck-in-europe/)\n\n[2025/09/24 - Garrett O'Brien, Sheila Sitaram](https://motherduck.com/blog/motherduck-in-europe/)\n\n### [MotherDuck is Landing in Europe! Announcing our EU Region](https://motherduck.com/blog/motherduck-in-europe)\n\nServerless analytics built on DuckDB, running entirely in the EU.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 210,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 210
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release.md",
  [2m  â”‚[0m     "content": "---\ntitle: motherduck-congratulates-duckdb-1.0-release\ncontent_type: blog\nsource_url: https://motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release\nindexed_at: '2025-11-25T19:57:46.857822'\ncontent_hash: 5a700d9ff7bfada6\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Congratulations to DuckDB Labs On Reaching 1.0!\n\n2024/06/03 - 4 min read\n\nBY\nMotherDuck team\n\nEarlier today, [DuckDB released version 1.0](https://duckdb.org/2024/06/03/announcing-duckdb-100), marking a key maturity milestone for the nimble yet powerful analytics database quickly taking over the world. MotherDuck would like to _quackgradulate_ DuckDB and extend our gratitude for all their hard work and support (and enabling all the duck puns)!\n\n## Why DuckDB?\n\nFor database nerds, [there\u2019s much to love about DuckDB](https://motherduck.com/duckdb-book-brief/) \u2014 [performance](https://thenewstack.io/duckdb-in-process-python-analytics-for-not-quite-big-data/), [innovation velocity](https://motherduck.com/blog/six-reasons-duckdb-slaps/), [versatility](https://duckdb.org/faq.html#why-call-it-duckdb), [ease of use](https://www.nikolasgoebel.com/2024/05/28/duckdb-doesnt-need-data.html), [rich and user-friendly SQL](https://duckdb.org/2024/03/01/sql-gymnastics.html), and [extreme portability](https://duckdb.org/why_duckdb.html#portable). Thanks to DuckDB, analytics can run virtually anywhere, liberated from the shackles of complex and expensive distributed systems. As an embedded database, it\u2019s the perfect \u2018Lego\u2019 building block that can snap into any process just by linking in a library.\n\nWhen we first learned about DuckDB two years ago, we loved it so much that we decided to start a company to turn it into a serverless cloud data warehouse. While in retrospect, this seems like an obvious duck to bet on, at the time, DuckDB was relatively unknown outside of database enthusiast and academic circles. But you could tell, even then, that they were onto something \u2014 the elegance of the design and the fervent enthusiasm of their growing user base set it apart from other databases. Moreover, their philosophy about what actually matters in data management systems deeply resonated with us at MotherDuck.\n\nThis turned out to be a prophetic choice. In the two years since we started working together, DuckDB has consistently moved up the rankings in the [DB Engines list](https://db-engines.com/en/ranking%5D). They\u2019ve gone from thousands of monthly downloads to millions. And they\u2019ve gone from being the database nobody has heard of to the one everyone is talking about.\n\n![DB Engines growth chart](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimage1_f6daf71258.png&w=3840&q=75)\n\nWith DuckDB as the key building block, MotherDuck is a [complete set of Legos](https://www.lego.com/en-us/product/millennium-falcon-75192?gclid=CjwKCAjwjeuyBhBuEiwAJ3vuoU4Ue7BPvmrfhnovXtGvA-5kp27nHkdJs9LXUCaPZjCUCrewdkOiyRoCnuQQAvD_BwE&ef_id=CjwKCAjwjeuyBhBuEiwAJ3vuoU4Ue7BPvmrfhnovXtGvA-5kp27nHkdJs9LXUCaPZjCUCrewdkOiyRoCnuQQAvD_BwE:G:s&s_kwcid=AL!790!3!!!!x!!!19930801844!&cmp=KAC-INI-GOOGUS-GO-US_GL-EN-RE-SP-BUY-CREATE-MB_ALWAYS_ON-SHOP-BP-PMAX-ALL-CIDNA00000-PMAX-MEDIUM_PRIORITY&gad_source=1), purpose-made for data teams, analytics application developers, and DuckDB users looking to supercharge and extend their favorite database to the cloud.\n\n## DuckDB Labs, Thank You!\n\nWhen we first talked to [Hannes](https://hannes.muehleisen.org/) and [Mark](https://mytherin.github.io/) about bringing DuckDB to the cloud, they were cautiously supportive of the idea. Since then, we\u2019ve built a great working relationship with the DuckDB Labs team to help achieve our shared vision of DuckDB running everywhere. We\u2019ve also created a pioneering model for building a commercial business without stifling open-source independence.\n\nWe at MotherDuck are extending DuckDB beyond its embedded confines by offering [serverless delivery](https://motherduck.com/product/), [secure sharing](https://motherduck.com/docs/key-tasks/managing-shared-motherduck-database/) and [access control](https://motherduck.com/docs/authenticating-to-motherduck/), [durable managed storage](https://motherduck.com/docs/architecture-and-capabilities/), [hybrid/dual query execution](https://motherduck.com/cidr-paper/), [a WebAssembly (Wasm) SDK](https://motherduck.com/blog/building-data-applications-with-motherduck/), and more.\n\nCrucially, thanks to the [extensibility hooks](https://duckdb.org/docs/extensions/overview.html) DuckDB provides, MotherDuck has been able to run standard DuckDB under the hood.\n\nAs DuckDB marched towards its 1.0 release, we saw DuckDB Labs\u2019 hard work firsthand to production-proof DuckDB. We appreciate the hardening, fuzzing, refactoring, and testing that has made for an impressively stable, flexible, and semantically rich data management system. Frankly, many of MotherDuck\u2019s recent improvements, including version independence and multi-statement transactions, were made possible by DuckDB Labs\u2019 collaborative efforts.\n\nWe could not have picked a better database to work with or a better group of passionate database professionals to partner with. To Hannes, Mark, and the rest of DuckDB Labs, we appreciate your continuous support, determination, and excellence.\n\nWe look forward to celebrating 2.0 and beyond with you!\n\n## DuckDB 1.0 and MotherDuck\n\nToday\u2019s release also marks the first simultaneous launch of MotherDuck with a new DuckDB version. MotherDuck already supports DuckDB 1.0; if you run a query via MotherDuck, it will run on the latest DuckDB version. What makes this possible is **Version Independence**, a feature we quietly enabled a few weeks ago that decouples clients from the version of DuckDB that we run on our servers.\n\nWhen DuckDB ships a new version, we can upgrade all the MotherDuck servers to run it in the cloud. Users don\u2019t need to do anything; they\u2019ll get access to improved performance and bug fixes. While users will need to upgrade their clients to access new features, they can now do so at their convenience.\n\n## PS: Something BIG is Coming Soon\n\nAt MotherDuck, we have also been busy, and we have some exciting news to share with you very soon.\n\nStay tuned!\n\nMeanwhile, if you\u2019re in San Francisco tonight, **June 3rd, at 6:00 pm**, [celebrate with us at our party at 111 Minna](https://motherducking-party-snowflake-summit.eventbrite.com/)\u2026 [we\u2019ll run it back on Tuesday, June 11th](https://motherducking-party-data-ai-summit.eventbrite.com/)!\n\n## Take Flight with MotherDuck\n\n### Cloud SQL Analytics Without the Overhead\n\nIf you haven\u2019t tried MotherDuck, [take flight with a 30-day trial of the Standard Plan](https://motherduck.com/) or paddle [Free Forever for small projects](https://motherduck.com/product/pricing/).\n\n### TABLE OF CONTENTS\n\n[Why DuckDB?](https://motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release#why-duckdb)\n\n[DuckDB Labs, Thank You!](https://motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release#duckdb-labs-thank-you)\n\n[DuckDB 1.0 and MotherDuck](https://motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release#duckdb-10-and-motherduck)\n\n[PS: Something BIG is Coming Soon](https://motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release#ps-something-big-is-coming-soon)\n\n[Take Flight with MotherDuck](https://motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release#take-flight-with-motherduck)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![How we Saved 95% on Log Processing with Bacalhau and MotherDuck](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FBacalhau_blog_e9a1602bb1.png&w=3840&q=75)](https://motherduck.com/blog/log-processing-savings-bacalhau-motherduck/)\n\n[2024/05/08 - Sean M. Tracey](https://motherduck.com/blog/log-processing-savings-bacalhau-motherduck/)\n\n### [How we Saved 95% on Log Processing with Bacalhau and MotherDuck](https://motherduck.com/blog/log-processing-savings-bacalhau-motherduck)\n\nWe stopped sifting our log data and started generating speedy logging insights to realize 95% in cost savings by pre-processing logs with Bacalhau and MotherDuck. How is that even possible? Let's walk through a step-by-step overview together.\n\n[![DuckDB Tutorial For Beginners](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgetting_started_duckdb_thumbnail_70b197b1ab.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n[2024/10/31 - Mehdi Ouazza, Ryan Boyd](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n### [DuckDB Tutorial For Beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners)\n\nGet up to speed quickly with DuckDB, including installation, VSCode workflow integration and your first SQL analytics project.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 121,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 121
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/faster-ducks.md",
  [2m  â”‚[0m     "content": "---\ntitle: faster-ducks\ncontent_type: blog\nsource_url: https://motherduck.com/blog/faster-ducks\nindexed_at: '2025-11-25T19:56:38.989923'\ncontent_hash: 228fba6935ede5a6\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Faster Ducks\n\n2025/10/28 - 8 min read\n\nBY\n\n[Jordan Tigani](https://motherduck.com/authors/jordan-tigani/)\n\nYou\u2019ve probably heard the old Henry Ford [quotation](https://hbr.org/2011/08/henry-ford-never-said-the-fast) about customers wanting faster horses. Clearly he was full of horse-shit; what everyone needs is faster ducks. And with the recent DuckDB 1.4 release, we\u2019re seeing an average of nearly 20% improvement in performance; that\u2019s some faster ducks indeed.\n\nWhen we started MotherDuck, we made a huge bet on DuckDB; it was already an amazing analytics engine, but what was even more impressive was how quickly it was getting better. You\u2019d assume that after a while the pace of improvement would slow down, but three and a half years later, if anything they\u2019re moving even faster.\n\nAt MotherDuck, we operate the largest, most complex fleet of DuckDB instances in the world. We push DuckDB hard, know where it reaches its limits, and work very closely with Hannes & Mark (the creators of DuckDB) and the rest of the DuckDB team to pinpoint where people run into problems. Every DuckDB release has gotten harder to break, thanks to improvements from memory management to concurrency.\n\nThere used to be a disclaimer on the DuckDB website about how they didn\u2019t really care about performance; the goal was first to make a database that was correct, and then they\u2019d make it fast. That disclaimer isn\u2019t on the website anymore, because they\u2019ve finally gotten around to working on performance. And, without ruining the surprise, they\u2019ve made DuckDB damn fast.\n\n## Lies, Damn Lies, and Benchmarks\n\nIt is always a good idea to take database benchmarks with a grain of salt, especially when a vendor is sharing the results. Hannes and Mark even wrote a [paper](https://hannes.muehleisen.org/publications/DBTEST2018-performance-testing.pdf) about how fair database benchmarking is difficult to do, which includes this famous satirical graph:\n\n![Satirical image showing benchmark results.](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Ffigure1_53048bde56.png&w=3840&q=75)\n\nOne way to get slightly more valid benchmarks is to look at benchmarks created by someone else. Hannes likes to call these \u201cAway benchmarks\u201d, since it is a lot harder to win when you\u2019re playing on someone else\u2019s home turf rather than your own. When your competitor creates a benchmark, it generally is done to make them look good vs their competition, and when things go well for you using that benchmark, it is probably a very good sign.\n\nOne such \u201caway benchmark\u201d is [ClickBench](https://benchmark.clickhouse.com/). It was created by the folks at ClickHouse and includes a bunch of queries of the type that ClickHouse is good at. That said, for a vendor benchmark, it is pretty good at representing the types of queries that people actually run. It doesn\u2019t use a huge amount of data, but then most people don\u2019t actually use a ton of data in their day-to-day queries (see [this](https://motherduck.com/blog/redshift-files-hunt-for-big-data/) analysis we did of public datasets). Database people tend to favor the TPC-H and TPC-DS benchmarks, but those are pretty [well-known](https://assets.amazon.science/24/3b/04b31ef64c83acf98fe3fdca9107/why-tpc-is-not-enough-an-analysis-of-the-amazon-redshift-fleet.pdf) to be non-representative of real-world workloads. The other nice thing about ClickBench is that anyone can submit results, so dozens of vendors have tried their hands at claiming the top spot.\n\nAs of this morning, the MotherDuck Mega instance is #1 overall in ClickBench. While this is a nice result, there are a handful of systems that are only a few percent slower, and the rankings will almost certainly change over time. We try not to put too much stock in this kind of thing.\n\nWhat is interesting to us, however, is that if you limit the results to the main Cloud Data Warehouses (BigQuery, Snowflake, Redshift, MotherDuck), the results are dramatic, and less likely to be overturned with a clever hack or tweak to the scoring.\n\n![Clickbench results chart.](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Ffigure2_4696ce015a.png&w=3840&q=75)\n\nLet\u2019s take a look at the MotherDuck Standard, at $1.80/hour, and see how it stacks up against the other vendors. The fastest Redshift cluster is the 4x ra3.16xlarge (that really rolls off the tongue), which costs 29 times as much, at $52/hour, and is just a little bit slower than the MotherDuck Standard. MotherDuck Standard is also faster than a Snowflake 3XL at only 1/100 of the price. This last comparison isn\u2019t super fair because Snowflake doesn\u2019t really get much faster after you get to the XL instance. However, a Snowflake XL at $32/hour is still 18 times more expensive than a MotherDuck Standard, while being half the performance.\n\nSay we wanted to compare similarly priced options and how they score in the benchmark. MotherDuck Jumbo instances, at $3.20, are a little bit less expensive than a Snowflake S ($4), but are 6x faster. MotherDuck Mega instances at $10.80 are a little bit more expensive than a Snowflake M ($8), but are 7 times faster. If we\u2019re looking at Redshift, the 4x ra3.xlplus costs $4.34 an hour, not too much more than a MotherDuck Jumbo at $3.20, but with less than 1/7th the performance. The Redshift 2x dc2.8xlarge is $9.60/hour, about 10% less expensive than a MotherDuck Mega, but 1/11th the performance.\n\nHere is another way to look at it; let\u2019s say you want to run the Clickbench workload, how much does it cost you to run it in MotherDuck, Snowflake, and Redshift? Let\u2019s say we want to run it 100 times, and the first time we\u2019ll use the time it took the \u2018cold\u2019 run, and the remaining times we\u2019ll use the time for the hot run. After downloading the raw data from the [results](https://github.com/ClickHouse/ClickBench/tree/main), I\u2019ve summarized the cost to run this workload in the following chart (in dollars, lower is better unless you like spending more money):\n\n![Clickbench benchmark costs by warehouse vendor.](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Ffigure3_b6da1ee098.png&w=3840&q=75)\n\nIn general, database vendors give you the ability to \u201cpay more to make it go faster\u201d. That is, you can run on a larger instance and, in general, your performance will be better. In a perfect world, you could pay 2x more and get 2x the performance, so the actual cost wouldn\u2019t change since it would run in half the time. In that case, the bars in this graph would be flat. The only one of these that looks mostly flat is MotherDuck; not only is it much less expensive to run, but it also scales nearly linearly. So if you pay 2x more, you can run your workload roughly 2x faster.\n\nWhat about BigQuery? I spent a decade of my career working on BigQuery, so it pains me more than a little bit to see it not showing up better in the results. Looking at the code for the benchmark, my guess is that if someone from the BigQuery team updated the method of running the benchmark slightly, the results would look a lot better.\n\nThis goes to show that you don\u2019t want to put too much credence on one benchmark. After all, benchmarks are not the real world. And I think it is always more useful to benchmark against past versions of yourself; if you\u2019re accelerating faster than everyone else, then at the end of the day, you\u2019ll end up in first place, no matter how you measure or where you started. And this is where we can really shine.\n\n## Keep on Flocking in the Real World\n\nAt MotherDuck, we track query statistics across our fleet. Since we rolled out DuckDB 1.4 a few weeks ago, we\u2019ve been looking at the before and after performance to determine, in the real world, how much faster DuckDB 1.4 has gotten. And it is a lot.\n\nWe looked at a sample of around 100 million queries from before and after we released the new DuckDB version on our servers. We compared the performance of successful queries from paying users running in our cloud-hosted DuckDB instances.\n\nThe results are summarized below, with all times in seconds.\n\n|  | average | median | 90%-ile | 99%-ile | 99.9%-ile | 99.99%-ile |\n| --- | --: | --: | --: | --: | --: | --: |\n| **DuckDB 1.4.x** | 0.42 | 0.011 | 0.342 | 5.47 | 43.53 | 283.69 |\n| **DuckDB 1.3.x** | 0.50 | 0.011 | 0.375 | 6.22 | 51.94 | 412.22 |\n| **% change** | 19% | 0% | 10% | 14% | 19% | 45% |\n\n**The average query got 19% faster.** Of course, the average tends to be dominated by slower queries. The median query wasn\u2019t faster but the median queries were already only 11 milliseconds; there wasn\u2019t a whole lot of point in making them faster. Where you really start to see major improvements is when you look at the higher percentiles: the 99th percentile query got 14% faster, and the 99.99 percentile query got 45% faster.\n\nThis is all amazing news for users of DuckDB and MotherDuck, because typically, user experience is driven by the slowest queries. Most people won\u2019t really notice performance improvements when queries are already under 100 milliseconds or so. But if one of your queries takes 4 minutes instead of 7, that\u2019s a big difference.\n\nAnother way of looking at query performance is to ask, \u201cWhat percentage of queries appear to be instantaneous?\u201d Human reaction time is around 200 ms, so queries faster than that appear to be instant. When running DuckDB 1.3 on MotherDuck, 94% of queries were sub-200 ms. With DuckDB 1.4, more than 96% of queries were under 200 ms. This means that there was a 1/3 reduction in the likelihood a user had to wait for a query, and 24/25 of all queries appeared to be instantaneous.\n\n## The Pond Ahead\n\nAt MotherDuck, we strive to increase value for our customers; they get value when they can do more work faster for less money. In the last few weeks, their queries have taken less time to run, and in particular, their slowest ones have been a lot less slow. People have had to do a lot less waiting for queries to complete. This means they can spend more time figuring out what kinds of queries to run, or what to do with the results.\n\nThe exciting thing is that these improvements aren\u2019t a one-time event; every release of DuckDB has both a bunch of new features as well as improved performance. That makes MotherDuck better and faster, too. We estimate that since DuckDB 1.0, MotherDuck performance has doubled. While we still [believe](https://motherduck.com/blog/perf-is-not-enough/) that performance should not be the only criterion you use to choose a database, it certainly helps when your database keeps getting faster.\n\n### TABLE OF CONTENTS\n\n[Lies, Damn Lies, and Benchmarks](https://motherduck.com/blog/faster-ducks/#lies-damn-lies-and-benchmarks)\n\n[Keep on Flocking in the Real World](https://motherduck.com/blog/faster-ducks/#keep-on-flocking-in-the-real-world)\n\n[The Pond Ahead](https://motherduck.com/blog/faster-ducks/#the-pond-ahead)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![Why Python Developers Need DuckDB (And Not Just Another DataFrame Library)](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2Fwhy_pythondev_1_22167e31bf.png&w=3840&q=75)](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/)\n\n[2025/10/08 - Mehdi Ouazza](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries/)\n\n### [Why Python Developers Need DuckDB (And Not Just Another DataFrame Library)](https://motherduck.com/blog/python-duckdb-vs-dataframe-libraries)\n\nUnderstand why a database is much more than just a dataframe library\n\n[![DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_4_1_b6209aca06.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n[2025/10/09 - Alex Monahan, Garrett O'Brien](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n\n### [DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/blog/announcing-duckdb-141-motherduck)\n\nMotherDuck now supports DuckDB 1.4.1 and DuckLake 0.3, with new SQL syntax, faster sorting, Iceberg interoperability, and more. Read on for the highlights from these major releases.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 134,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 134
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": ".kurt/sources/motherduck.com/videos/a-new-paradigm-for-data-visualization-with-just-sql-markdown.md:98:First, Evidence supports DuckDB as a primary server-side data source. Users can connect their projects to local `.duckdb` database files or read directly from collections of Parquet and CSV files. During the build process, Evidence runs queries against this DuckDB source to fetch the data needed for the static site.\n.kurt/sources/motherduck.com/duckdb-news.md:43:DuckDB Monthly #34: DuckDB 1.4.0 LTS, 100\u00d7 Spark benchmarks, official Docker image and more!\n.kurt/sources/motherduck.com/duckdb-news.md:51:DuckDB Monthly #33: DuckDB 58\u00d7 faster spatial joins, pg\\_duckdb 1.0, and 79% Snowflake cost savings\n.kurt/sources/motherduck.com/duckdb-news.md:123:DuckDB Monthly: DuckDB 1.1, community extensions highlights and more!\n.kurt/sources/motherduck.com/duckdb-news.md:155:DuckDB Monthly: DuckDB 1.0.0, Hugging Face data sets, chron expressions, and more!\n.kurt/sources/motherduck.com/duckdb-news.md:179:DuckDB Monthly: Featuring DuckDB 0.10.0, Christophe Oudar, new (free) book chapters, DuckCon videos and more!\n.kurt/sources/motherduck.com/blog.md:93:[![DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_4_1_b6209aca06.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n.kurt/sources/motherduck.com/blog.md:97:## [DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/blog/announcing-duckdb-141-motherduck)\n.kurt/sources/motherduck.com/blog.md:99:MotherDuck now supports DuckDB 1.4.1 and DuckLake 0.3, with new SQL syntax, faster sorting, Iceberg interoperability, and more. Read on for the highlights from these major releases.\n.kurt/sources/motherduck.com/blog.md:115:DuckDB Monthly #34: DuckDB 1.4.0 LTS, 100\u00d7 Spark benchmarks, official Docker image and more!\n.kurt/sources/motherduck.com/blog.md:131:DuckDB Monthly #33: DuckDB 58\u00d7 faster spatial joins, pg\\_duckdb 1.0, and 79% Snowflake cost savings\n.kurt/sources/motherduck.com/quacking-around-the-web.md:135:MotherDuck, builder of a serverless, easy to use data analytics platform based on open source database DuckDB, today announced the general availability of MotherDuck, providing the 99% of users who do not need a complex data infrastructure with the ability to use DuckDB's simplicity at scale. DuckDB 1.0, which was recently announced by DuckDB Foundation and Labs, provides a scale-up approach to analytics that is faster, cheaper and more user-friendly than distributed architectures.\n.kurt/sources/motherduck.com/authors/marcos-ortiz.md:97:Exciting month for DuckDB and the whole ecosystem: DuckdB 0.8.0 is released. The OSS project reached 10k stars on GitHub. Spatial extension. Native Swift API.\n.kurt/sources/motherduck.com/authors/sheila-sitaram.md:35:[![DuckDB 1.3 Lands in MotherDuck: Performance Boosts, Even Faster Parquet, and Smarter SQL](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_3_c312a85df0.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw/)\n.kurt/sources/motherduck.com/authors/sheila-sitaram.md:39:### [DuckDB 1.3 Lands in MotherDuck: Performance Boosts, Even Faster Parquet, and Smarter SQL](https://motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw)\n.kurt/sources/motherduck.com/authors/sheila-sitaram.md:41:DuckDB 1.3 has launched, with performance boosts, faster Parquet reads and writes, and new SQL syntax for ducking awesome analytics with full support in MotherDuck. Read on for highlights from this major release.\n.kurt/sources/motherduck.com/authors/sheila-sitaram.md:51:[![MotherDuck Now Supports DuckDB 1.2: Faster, Friendlier, Better Performance](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_2_ea12029200.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw/)\n.kurt/sources/motherduck.com/authors/sheila-sitaram.md:55:### [MotherDuck Now Supports DuckDB 1.2: Faster, Friendlier, Better Performance](https://motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw)\n.kurt/sources/motherduck.com/authors/sheila-sitaram.md:57:DuckDB 1.2 has launched, with improvements in performance, the SQL experience, CSV handling, and scalability - all fully supported in MotherDuck!\n.kurt/sources/motherduck.com/authors/alex-monahan.md:27:[![DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_4_1_b6209aca06.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n.kurt/sources/motherduck.com/authors/alex-monahan.md:31:### [DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/blog/announcing-duckdb-141-motherduck)\n.kurt/sources/motherduck.com/authors/alex-monahan.md:33:MotherDuck now supports DuckDB 1.4.1 and DuckLake 0.3, with new SQL syntax, faster sorting, Iceberg interoperability, and more. Read on for the highlights from these major releases.\n.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md:28:This month, we've got major updates, including DuckDB 1.4.0 LTS with database encryption and MERGE statements, impressive benchmarks showing 100x performance gains over Spark on modest hardware, and new tooling like an official Docker image and a lightweight geospatial feature server.\n.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md:70:### [Announcing DuckDB 1.4.0 LTS](https://duckdb.org/2025/09/16/announcing-duckdb-140.html)\n.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md:72:**TL;DR**: DuckDB 1.4.0 \"Andium\" brings database encryption with AES-256-GCM, the MERGE statement for upserts, Iceberg writes support, and significant performance improvements through k-way merge sorting and CTE materialization.\n.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md:74:[Omitted long matching line]\n.kurt/sources/motherduck.com/authors/garrett-obrien.md:35:[![DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_4_1_b6209aca06.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-141-motherduck/)\n.kurt/sources/motherduck.com/authors/garrett-obrien.md:39:### [DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck: New SQL Syntax, Iceberg Interoperability, and Performance Gains](https://motherduck.com/blog/announcing-duckdb-141-motherduck)\n.kurt/sources/motherduck.com/authors/garrett-obrien.md:41:MotherDuck now supports DuckDB 1.4.1 and DuckLake 0.3, with new SQL syntax, faster sorting, Iceberg interoperability, and more. Read on for the highlights from these major releases.\n.kurt/sources/motherduck.com/authors/simon-spati.md:58:DuckDB Monthly #34: DuckDB 1.4.0 LTS, 100\u00d7 Spark benchmarks, official Docker image and more!\n.kurt/sources/motherduck.com/authors/simon-spati.md:66:DuckDB Monthly #33: DuckDB 58\u00d7 faster spatial joins, pg\\_duckdb 1.0, and 79% Snowflake cost savings\n.kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md:149:**EDIT:** Since pg\\_duckdb 0.3, indexes are supported.\n.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-six.md:30:[Omitted long matching line]\n.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-six.md:51:### [DuckDB 0.8.0 is out codename \u201cFulvigula\u201d](https://duckdb.org/2023/05/17/announcing-duckdb-080.html)\n.kurt/sources/motherduck.com/authors/ryan-boyd.md:89:DuckDB Monthly: DuckDB 1.0.0, Hugging Face data sets, chron expressions, and more!\n.kurt/sources/motherduck.com/authors/ryan-boyd.md:97:DuckDB Monthly: Featuring DuckDB 0.10.0, Christophe Oudar, new (free) book chapters, DuckCon videos and more!\n.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2025.md:108:### [Announcing DuckDB 1.3.0](https://duckdb.org/2025/05/21/announcing-duckdb-130.html)\n.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2025.md:110:**TL;DR:** DuckDB 1.3.0 \"Ossivalis\" introduces a file cache for remote data, a new spatial join operator, and improved Parquet handling alongside several breaking changes.\n.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md:126:### [New Release DuckDB 1.1.0/1.1.1 is Out](https://duckdb.org/2024/09/09/announcing-duckdb-110)\n.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md:132:Find all changes on [Release DuckDB 1.1.0](https://github.com/duckdb/duckdb/releases/tag/v1.1.0). Besides that release, 1.1.1 has been released with [fixing minor bugs](https://github.com/duckdb/duckdb/releases/tag/v1.1.1) that has been discovered since 1.1.0. MotherDuck also published [a blog to highlight some hidden gems](https://motherduck.com/blog/duckdb-110-hidden-gems/) from 1.1.\n.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md:282:DuckDB Monthly #33: DuckDB 58\u00d7 faster spatial joins, pg\\_duckdb 1.0, and 79% Snowflake cost savings\n.kurt/sources/motherduck.com/blog/getting-started-ducklake-table-format.md:440:[![DuckDB 1.3 Lands in MotherDuck: Performance Boosts, Even Faster Parquet, and Smarter SQL](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_3_c312a85df0.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw/)\n.kurt/sources/motherduck.com/blog/getting-started-ducklake-table-format.md:444:### [DuckDB 1.3 Lands in MotherDuck: Performance Boosts, Even Faster Parquet, and Smarter SQL](https://motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw)\n.kurt/sources/motherduck.com/blog/getting-started-ducklake-table-format.md:446:DuckDB 1.3 has launched, with performance boosts, faster Parquet reads and writes, and new SQL syntax for ducking awesome analytics with full support in MotherDuck. Read on for highlights from this major release.\n.kurt/sources/motherduck.com/blog/fabi-ai-llm-prompt-analysis.md:401:[![MotherDuck Now Supports DuckDB 1.2: Faster, Friendlier, Better Performance](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FDuck_DB_1_2_ea12029200.png&w=3840&q=75)](https://motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw/)\n.kurt/sources/motherduck.com/blog/fabi-ai-llm-prompt-analysis.md:405:### [MotherDuck Now Supports DuckDB 1.2: Faster, Friendlier, Better Performance](https://motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw)\n.kurt/sources/motherduck.com/blog/fabi-ai-llm-prompt-analysis.md:407:DuckDB 1.2 has launched, with improvements in performance, the SQL experience, CSV handling, and scalability - all fully supported in MotherDuck!\n.kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md:17:# DuckDB 1.3 Lands in MotherDuck: Performance Boosts, Even Faster Parquet, and Smarter SQL\n.kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md:25:We\u2019re excited to share that **DuckDB 1.3.0 is now available in MotherDuck**, bringing a wave of performance and usability upgrades to make everyday SQL and analytics faster, friendlier, and more efficient.\n.kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md:27:A major release, [DuckDB 1.3.0](https://github.com/duckdb/duckdb/releases/tag/v1.3.0) improves performance in real-world scenarios with faster queries, updated SQL syntax, and smarter handling for Parquet files.",
  [2m  â”‚[0m   "numLines": 50,
  [2m  â”‚[0m   "appliedLimit": 50
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now let me search for more specific information about version requirements and compatibility:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ client version|version compatibility|upgrade|DuckDB client

  [96mğŸ”§ TOOL:[0m Read â†’ .kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": ".kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md",
  [2m  â”‚[0m     "content": "---\ntitle: announcing-duckdb-13-on-motherduck-cdw\ncontent_type: blog\nsource_url: https://motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw\nindexed_at: '2025-11-25T19:56:52.346338'\ncontent_hash: 0960a8687cbd6e92\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# DuckDB 1.3 Lands in MotherDuck: Performance Boosts, Even Faster Parquet, and Smarter SQL\n\n2025/06/01 - 5 min read\n\nBY\n\n[Sheila Sitaram](https://motherduck.com/authors/sheila-sitaram/)\n\nWe\u2019re excited to share that **DuckDB 1.3.0 is now available in MotherDuck**, bringing a wave of performance and usability upgrades to make everyday SQL and analytics faster, friendlier, and more efficient.\n\nA major release, [DuckDB 1.3.0](https://github.com/duckdb/duckdb/releases/tag/v1.3.0) improves performance in real-world scenarios with faster queries, updated SQL syntax, and smarter handling for Parquet files.\n\nRead on for our favorite highlights from this release.\n\n## Even Better Real-World Query Performance\n\n### A New TRY() expression for safer queries\n\nIf you\u2019re ingesting messy data sources or writing resilient data pipelines, the `TRY ()` [function](https://duckdb.org/2025/05/21/announcing-duckdb-130.html#try-expression) offers **more graceful handling for bad data** by returning `NULL` values instead of errors on problematic rows.\n\n### Pushdown of inequality conditions into joins\n\nA huge win for **incremental dbt models** and other workloads that rely on join conditions, DuckDB and MotherDuck [users can expect much better performance](https://github.com/duckdb/duckdb/pull/17317) when filtering.\n\n### Pushdown of arbitrary expressions into scans\n\nDuckDB can now **push down more types of filter expressions directly into scans**, [reducing the amount of data that needs to be processed downstream](https://github.com/duckdb/duckdb/pull/17213) to deliver up to 30X faster queries in these scenarios.\n\n## Blazing Fast Parquet Reads and Writes\n\nWith DuckDB 1.3.0, Parquet files are more efficient overall. While Parquet reads are even faster thanks to optimizations around caching, materialization, and read performance, Parquet writes are also faster due to a smarter use of multithreaded exports, improved compression mechanisms, and rowgroup merges.\n\n### Late materialization\n\nDuckDB now [defers fetching columns until absolutely necessary](https://github.com/duckdb/duckdb/pull/17325), resulting in **3\u201310x faster reads** for queries with `LIMIT`.\n\n### ~15% average speedup on reads\n\nGeneral **read performance is significantly improved** due to [new efficiency scan and filter improvements](https://github.com/duckdb/duckdb/pull/16315), even without late materialization.\n\n### 30%+ faster write throughput\n\nMajor improvements to **multithreaded Parquet export performance** result in [even faster writes](https://github.com/duckdb/duckdb/pull/16243).\n\n### Better compression for large strings\n\nLarge strings can now be [dictionary-compressed](https://github.com/duckdb/duckdb/pull/17061), resulting in **reduced file sizes** and performance boosts.\n\n### Smarter rowgroup combining\n\n**Smaller rowgroups from multiple threads** are now [merged at the time of write](https://github.com/duckdb/duckdb/pull/17036), resulting in more efficient Parquet files.\n\n## Performance Wins Big and Small\n\nThe release of 1.3.0 isn\u2019t just about headline features: It also includes performance boosts across the stack, from aggregations and string scans to CTEs, smarter algorithms, lower memory usage, and better parallelism.\n\n### Here are 12 performance highlights that caught our attention:\n\n- [2x faster Top-N for large `LIMIT` queries:](https://github.com/duckdb/duckdb/pull/17141) If you\u2019re working with up to 250K rows, Top N is now faster than sorting!\n\n- [3x fewer memory allocations in aggregations:](https://github.com/duckdb/duckdb/pull/16849) Improvements to string hashing and aggregation internals reduce memory pressure and lower contention, leading to more efficient execution of queries like `COUNT(DISTINCT)` at scale.\n\n- [~25% faster performance for large hash table creation:](https://github.com/duckdb/duckdb/pull/16301) The parallelism strategy has been refined to avoid excessive task splitting, leading to better memory access patterns and faster hash table initialization during large joins.\n\n- [20x faster `UNNEST` and `UNPIVOT` for small lists:](https://github.com/duckdb/duckdb/pull/16210) DuckDB now processes multiple lists at once and eliminates unnecessary copying to deliver better performance for common patterns like unpivoting a few columns.\n\n- [30\u201340% faster `RANGE` based window functions:](https://github.com/duckdb/duckdb/pull/16765) Parallelized task processing across hash groups and reduced lock contention during execution now lead to smoother, more efficient performance.\n\n- [7x faster conversion to Python object columns:](https://github.com/duckdb/duckdb/pull/16431) Optimized Python object conversion due to skipping intermediate steps to speed up performance for object columns and scalar UDFs.\n\n- [5\u201325% faster LIKE '%text%' and CONTAINS string scans:](https://github.com/duckdb/duckdb/pull/16484) Unified and optimized DuckDB\u2019s implementation using `memchr` for early match detection to speed up substring searches across the board.\n\n- [Faster list-of-list creation:](https://github.com/duckdb/duckdb/pull/17063) Improved performance when constructing nested lists, boosting speed for transformation pipelines that rely on complex list structures.\n\n- [Reduced memory contention in hash joins:](https://github.com/duckdb/duckdb/pull/16172) Introduced parallel `memset` for initializing large join tables, eliminating single-threaded bottlenecks and improving performance on multi-core systems.\n\n- [Faster recursive CTEs and complex subqueries performance:](https://github.com/duckdb/duckdb/pull/17294) Adopted a new top-down subquery decorrelation strategy, unlocking better optimization for nested queries and improved performance for recursive CTEs.\n\n- [Improved performance and support for JSON-heavy queries:](https://github.com/duckdb/duckdb/pull/16729) More parallelism in `UNION ALL` and resolution of multiple JSON edge cases, for better handling.\n\n- [Faster decoding of short FSST compressed strings:](https://github.com/duckdb/duckdb/pull/16508) Optimized decoding for inlined strings by skipping unnecessary copying, resulting in ~15% speedups without performance regressions on longer strings.\n\n\nAll these optimizations add up to one thing: even faster queries without lifting a finger.\n\n## What This Means for MotherDuck Users\n\nIf you're using MotherDuck, DuckDB 1.3 is already live. Your dbt models, dashboards, and notebooks will feel snappier right away.\n\nWhile you can continue using your current version of DuckDB, we encourage you to [upgrade your DuckDB clients to 1.3.0](https://duckdb.org/docs/installation/?version=stable&environment=cli&platform=macos&download_method=package_manager) as soon as you can to take advantage of the fixes and performance improvements.\n\nCurious what version you\u2019re on? Run this simple query to take a look:\n\n```csharp\nCopy code\n\nSELECT version();\n```\n\n## Huge Thanks to the DuckDB Team\n\nAt MotherDuck, we\u2019re proud to support the best of DuckDB\u2019s powerfully efficient query engine as a managed cloud service so you can easily manage a fleet of DuckDB instances and collaborate with your team. [DuckDB 1.3.0](https://duckdb.org/2025/05/21/announcing-duckdb-130.html) wouldn\u2019t be possible without the incredible engineering work from the DuckDB team and contributors from the broader community and ecosystem.\n\nIf you have feedback or questions, join our [Community Slack](https://slack.motherduck.com/) or reach out directly in the MotherDuck UI or [online](https://motherduck.com/contact-us/product-expert/). We\u2019re eager to hear your feedback so we can help you move faster from question to insight and build a ducking awesome product that best supports your workflow.\n\nHappy querying - let\u2019s get quacking!\n\n### TABLE OF CONTENTS\n\n[Even Better Real-World Query Performance](https://motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw/#even-better-real-world-query-performance)\n\n[Blazing Fast Parquet Reads and Writes](https://motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw/#blazing-fast-parquet-reads-and-writes)\n\n[Performance Wins Big and Small](https://motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw/#performance-wins-big-and-small)\n\n[What This Means for MotherDuck Users](https://motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw/#what-this-means-for-motherduck-users)\n\n[Huge Thanks to the DuckDB Team](https://motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw/#huge-thanks-to-the-duckdb-team)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![Breaking the Excel-SQL Barrier: Leveraging DuckDB's Excel Extension](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FBreaking_Excel_SQL_barrier_d4e2cf549e.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-excel-extension/)\n\n[2025/05/27 - Jacob Matson](https://motherduck.com/blog/duckdb-excel-extension/)\n\n### [Breaking the Excel-SQL Barrier: Leveraging DuckDB's Excel Extension](https://motherduck.com/blog/duckdb-excel-extension)\n\nNow in MotherDuck & DuckDB, its never been easier to join in your data from spreadsheet sources.\n\n[![A Duck Walks into a Lake](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FA_duck_walks_into_a_lake_1_9e9dc6ec88.png&w=3840&q=75)](https://motherduck.com/blog/ducklake-motherduck/)\n\n[2025/05/28 - Jordan Tigani](https://motherduck.com/blog/ducklake-motherduck/)\n\n### [A Duck Walks into a Lake](https://motherduck.com/blog/ducklake-motherduck)\n\nDuckDB introduces a new table format, what does it mean for the future of data lakes ?\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 172,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 172
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": ".kurt/sources/motherduck.com/fees-addendum.md:34:2. Consumption Caps. The Storage Volume and Query Volume available for Free Accounts is limited to the Included Volumes. To receive additional Storage Volume and Query Volume, Customer must upgrade to a Commercial Account.\n.kurt/sources/motherduck.com/fees-addendum.md:52:[Omitted long matching line]\n.kurt/sources/motherduck.com/authors/ryan-boyd.md:113:The DuckDB Snippets site has been upgrade to include sharing sets of code snippets in SQL and Python plus also sharing data with MotherDuck share URLs.\n.kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md:133:- Cross-version compatibility (extensions may work across DuckDB versions)\n.kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md:124:This control is especially useful for performance tuning. A direct comparison showed that querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds). By pushing the computation to the data's location, MotherDuck minimizes network I/O and delivers results more quickly.\n.kurt/sources/motherduck.com/videos/a-new-paradigm-for-data-visualization-with-just-sql-markdown.md:110:[Omitted long matching line]\n.kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md:130:Once attached, your local DuckDB client works as a lightweight interface to your MotherDuck environment. You can execute queries that run within MotherDuck, right next to your data in AWS (assuming your S3 buckets are in the same region as MotherDuck).\n.kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md:226:Only upgrade to paid tiers when clear business need justifies the investment, ideally after achieving product-market fit and revenue growth that supports increased operational expenses.\n.kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md:175:[Omitted long matching line]\n.kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md:125:[Omitted long matching line]\n.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-one.md:89:This [interactive notebook](https://observablehq.com/@observablehq/duckdb) demonstrates how to use the Observable DuckDB client, based on WASM.\n.kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md:25:We\u2019re excited to share that **DuckDB 1.3.0 is now available in MotherDuck**, bringing a wave of performance and usability upgrades to make everyday SQL and analytics faster, friendlier, and more efficient.\n.kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md:106:While you can continue using your current version of DuckDB, we encourage you to [upgrade your DuckDB clients to 1.3.0](https://duckdb.org/docs/installation/?version=stable&environment=cli&platform=macos&download_method=package_manager) as soon as you can to take advantage of the fixes and performance improvements.\n.kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md:73:In this mode, MotherDuck automatically creates the DuckLake catalog database and manages it inside MotherDuck - providing access to the catalog database either in MotherDuck, or for use by local DuckDB clients.\n.kurt/sources/motherduck.com/blog/getting-started-ducklake-table-format.md:203:- Compute engine: any DuckDB client\u2014we'll use the **DuckDB CLI**.\n.kurt/sources/motherduck.com/blog/getting-started-ducklake-table-format.md:331:You can also query these tables directly from the DuckDB client or the [DuckDB UI](https://motherduck.com/blog/local-duckdb-ui-visual-data-analysis/). You can find them through **SELECT database\\_name, table\\_name FROM duckdb\\_tables WHERE schema\\_name='public' and database\\_name LIKE '\\_\\_ducklake\\_metadata\\_%';** , database name will follow the pattern **\\_ducklake\\_metadata\\_<your\\_catalog\\_name>**\n.kurt/sources/motherduck.com/blog/spark-ducklake-getting-started.md:351:One of the most satisfying moments in this entire workflow is connecting to my DuckLake with the DuckDB CLI (or any DuckDB client) and seeing all my Spark-written data sitting there, complete with full lakehouse capabilities.\n.kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2025.md:94:Michael, the creator, also shares a little more in his blog post about [Navigating DuckDB Extension Updates: Lessons from the Field](https://www.databooth.com.au/posts/duckdb-extensions-upgrade/). The code is available on [GitHub](https://github.com/Mjboothaus/duckdb-extensions-analysis).\n.kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md:65:Writing SQL is easier than ever with a more intelligent autocomplete engine that provides context-aware suggestions. Plus, the DuckDB CLI gets a fresh upgrade with **syntax highlighting and thousands-separator support** for better readability.\n.kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md:71:[Omitted long matching line]\n.kurt/sources/motherduck.com/blog/flight-sql-vs-rest-vs-jdbc.md:144:[GizmoSQL](https://github.com/gizmodata/gizmosql) is a full Arrow Flight SQL server with support for both DuckDB and SQLite as pluggable backends. Built in C++ and extended from Voltron Data's sqlflite, it's been battle-tested, hardened, and upgraded for real-world flexibility.\n.kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md:106:For MacOS and Windows users, you can leverage package managers to make the DuckDB CLI directly available in your PATH, simplifying upgrades and installations.\n.kurt/sources/motherduck.com/blog/from-data-lake-to-lakehouse-duckdb-portable-catalog.md:145:Let's start with the DuckDB file `ducky_catalog.ddb`. You can follow along by running the above commands in a DuckDB client, as the link comes from a public bucket.\n.kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md:129:A _share_ is a database that you can easily share, or rather `ATTACH`, to use the appropriate DuckDB term, from any DuckDB client.\n.kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md:143:Of course, you can query the shared database from any DuckDB client, which would be Python, Node.JS, and [many more](https://duckdb.org/docs/api/overview)!\n.kurt/sources/motherduck.com/blog/motherduck-headquarters-seattle-opening.md:74:The DuckDB Snippets site has been upgrade to include sharing sets of code snippets in SQL and Python plus also sharing data with MotherDuck share URLs.\n.kurt/sources/motherduck.com/blog/duckdb-110-hidden-gems.md:26:DuckDB 1.1 was released on September 9, and we recently had a bug fix release, 1.1.1, out on September 23. MotherDuck supported `1.1.1` just two days after its release, and we continue to work closely with the DuckDB Labs team to bring a smooth upgrade experience for all users.\n.kurt/sources/motherduck.com/blog/why-everybody-hates-databases.md:92:Data practitioners were not excited about traditional databases because they\u2019re difficult to install and configure. It\u2019s not a smooth to run a database locally. Plus, the client protocol of databases like JDBC, built in the 90s, hasn\u2019t faced significant upgrades. Hannes wanted to research how he could build a database for these people while removing the hassle of managing one.\n.kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md:30:[Omitted long matching line]\n.kurt/sources/motherduck.com/blog/the-future-of-bi-bi-as-code-duckdb-impact.md:235:upgrade        Upgrade Rill to the latest version",
  [2m  â”‚[0m   "numLines": 30,
  [2m  â”‚[0m   "appliedLimit": 30
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search for more information about version independence and version support:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ version independence|version independent|decouples clients

  [96mğŸ”§ TOOL:[0m Grep â†’ 0\.10|0\.9|0\.8|supports.*version
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": ".kurt/sources/motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release.md:47:As DuckDB marched towards its 1.0 release, we saw DuckDB Labs\u2019 hard work firsthand to production-proof DuckDB. We appreciate the hardening, fuzzing, refactoring, and testing that has made for an impressively stable, flexible, and semantically rich data management system. Frankly, many of MotherDuck\u2019s recent improvements, including version independence and multi-statement transactions, were made possible by DuckDB Labs\u2019 collaborative efforts.\n.kurt/sources/motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release.md:55:Today\u2019s release also marks the first simultaneous launch of MotherDuck with a new DuckDB version. MotherDuck already supports DuckDB 1.0; if you run a query via MotherDuck, it will run on the latest DuckDB version. What makes this possible is **Version Independence**, a feature we quietly enabled a few weeks ago that decouples clients from the version of DuckDB that we run on our servers.",
  [2m  â”‚[0m   "numLines": 2,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-power-of-wasm-for-analytics-duckdb-in-the-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-surprising-birth-of-duckdb-ft-co-creator-hannes-muhleisen.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-if-sql-queries-returned-results-instantly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/spatial-data-management-with-duckdb-ft-mattforrest.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/speed-querying-stackoverflow-data-with-duckdb-ft-michael-hunger.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/sql-ide-safari-harlequin-in-your-terminal.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now I have enough information. Let me write the answer to the file:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_10.md

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m The file doesn't exist yet, so I need to create it. Let me check if the directory exists first:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ ls -la /tmp/ | head -20
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m total 1192
  [2m  â”‚[0m drwxrwxrwt  105 root    wheel    3360 Nov 27 14:19 .
  [2m  â”‚[0m drwxr-xr-x    6 root    wheel     192 Oct 28 22:19 ..
  [2m  â”‚[0m drwxr-xr-x    2 julien  wheel      64 Nov 26 00:02 .kurt
  [2m  â”‚[0m -rw-------    1 julien  wheel    2048 Nov 26 17:09 answer_cc_1.md
  [2m  â”‚[0m -rw-------    1 julien  wheel    2378 Nov 26 17:09 answer_cc_2.md
  [2m  â”‚[0m -rw-r--r--    1 julien  wheel     741 Nov 26 17:09 answer_motherduck_1.md
  [2m  â”‚[0m -rw-r--r--    1 julien  wheel     632 Nov 26 17:10 answer_motherduck_10.md
  [2m  â”‚[0m -rw-r--r--    1 julien  wheel     716 Nov 26 17:09 answer_motherduck_2.md
  [2m  â”‚[0m -rw-r--r--    1 julien  wheel     732 Nov 26 17:09 answer_motherduck_3.md
  [2m  â”‚[0m -rw-r--r--    1 julien  wheel     669 Nov 26 17:09 answer_motherduck_4.md
  [2m  â”‚[0m -rw-r--r--    1 julien  wheel     945 Nov 26 17:09 answer_motherduck_5.md
  [2m  â”‚[0m -rw-r--r--    1 julien  wheel     792 Nov 26 17:09 answer_motherduck_6.md
  [2m  â”‚[0m -rw-r--r--    1 julien  wheel     800 Nov 26 17:10 answer_motherduck_7.md
  [2m  â”‚[0m -rw-r--r--    1 julien  wheel     772 Nov 26 17:10 answer_motherduck_8.md
  [2m  â”‚[0m -rw-r--r--    1 julien  wheel     527 Nov 26 17:10 answer_motherduck_9.md
  [2m  â”‚[0m -rw-r--r--    1 julien  wheel     126 Nov 26 10:01 answer_motherduck_run.log
  [2m  â”‚[0m -rw-r--r--    1 julien  wheel     790 Nov 27 13:58 answer_with_kg_1.md
  [2m  â”‚[0m -rw-r--r--    1 julien  wheel     681 Nov 27 13:59 answer_with_kg_10.md
  [2m  â”‚[0m -rw-r--r--    1 julien  wheel     765 Nov 27 13:58 answer_with_kg_2.md
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Good, /tmp exists. Now let me create the file directly:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ cat > /tmp/answer_without_kg_10.md << 'EOF'
# DuckDB Versions Supported by MotherDuck

Based on the MotherDuck documentation, **MotherDuck currently supports DuckDB version 1.4.1** as of October 2025.

## Current Version Support

- **Server-side**: MotherDuck runs **DuckDB 1.4.1** on their cloud servers
- **Client-side**: Users can connect with any DuckDB client version, but are encouraged to upgrade to 1.4.1 for optimal performance and access to new features

## Version Independence

MotherDuck implemented a feature called **Version Independence** that was first enabled around June 2024 (before the DuckDB 1.0 release). This feature decouples the client version from the server version, meaning:

- When DuckDB ships a new version, MotherDuck can upgrade all servers to run it in the cloud
- Users don't need to immediately upgrade their clients to benefit from performance improvements and bug fixes
- Users can upgrade their clients at their convenience to access new SQL features

## Recent Version History

MotherDuck has supported the following major DuckDB versions in 2024-2025:

- **DuckDB 1.4.1** (October 2025) - Current version with MERGE statements, VARIANT type, improved sorting, and Iceberg support
- **DuckDB 1.3.0** (June 2025) - Performance boosts, faster Parquet operations, file caching
- **DuckDB 1.2** (2025) - Improved SQL experience, CSV handling, autocomplete
- **DuckDB 1.1.0/1.1.1** (September 2024) - Supported within 2 days of the 1.1.1 bugfix release
- **DuckDB 1.0** (June 2024) - First simultaneous launch with MotherDuck support on release day

## Recommendation

While users can continue using older DuckDB client versions, MotherDuck encourages upgrading to the latest client version (1.4.1) to take advantage of:
- New SQL syntax and features (MERGE, VARIANT type, TRY() expressions)
- Performance improvements and bug fixes
- Better compatibility with cloud-side query execution

You can check your current DuckDB version by running:
```sql
SELECT version();
```

## Sources

- [DuckDB 1.4.1 and DuckLake 0.3 Land in MotherDuck](https://motherduck.com/blog/announcing-duckdb-141-motherduck/) - October 2025 announcement
- [DuckDB 1.3 Lands in MotherDuck](https://motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw/) - June 2025 announcement
- [Congratulations to DuckDB Labs On Reaching 1.0](https://motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release/) - June 2024, discussing Version Independence
- [Faster Ducks](https://motherduck.com/blog/faster-ducks/) - October 2025 performance analysis with DuckDB 1.4
- [DuckDB 1.1.0 Hidden Gems](https://motherduck.com/blog/duckdb-110-hidden-gems/) - September 2024, noting MotherDuck supported 1.1.1 within 2 days of release
EOF
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m 
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Perfect! Let me verify the file was created successfully:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Bash â†’ ls -lh /tmp/answer_without_kg_10.md && wc -l /tmp/answer_without_kg_10.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m -rw-------  1 julien  wheel   2.6K Nov 27 14:19 /tmp/answer_without_kg_10.md
  [2m  â”‚[0m       46 /tmp/answer_without_kg_10.md
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Excellent! The answer has been successfully created. Let me provide a summary of what I found:
  [94mâ”‚[0m 
  [94mâ”‚[0m ## Summary
  [94mâ”‚[0m 
  [94mâ”‚[0m I successfully searched through the `.kurt/sources/` directory and found comprehensive information about DuckDB versions supported by MotherDuck. Here's what I discovered:
  [94mâ”‚[0m 
  [94mâ”‚[0m **Key Finding**: MotherDuck currently supports **DuckDB version 1.4.1** as of October 2025.
  [94mâ”‚[0m 
  [94mâ”‚[0m **Important Details**:
  [94mâ”‚[0m 1. **Version Independence**: MotherDuck has a unique feature that decouples client and server versions, allowing users to continue using older client versions while the server runs the latest DuckDB version
  [94mâ”‚[0m 2. **Rapid Updates**: MotherDuck typically supports new DuckDB releases very quickly (within days of release)
  [94mâ”‚[0m 3. **Version History**: Tracked support for DuckDB 1.0 through 1.4.1 across 2024-2025
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer has been written to `/tmp/answer_without_kg_10.md` with:
  [94mâ”‚[0m - Clear explanation of current version support (1.4.1)
  [94mâ”‚[0m - Details about Version Independence feature
  [94mâ”‚[0m - Recent version history from 1.0 to 1.4.1
  [94mâ”‚[0m - Recommendations for users
  [94mâ”‚[0m - Full source citations from 5 different MotherDuck blog posts
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 7,374 | Cost: $0.2986
     Cumulative: 7,374 tokens | $0.2986
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: heuristic: Explicit completion signal detected

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 15 | Duration: 91.3s
â•‘    Tokens: 7,374 | Cost: $0.2986
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  LLM Judge score: 0.82
ğŸ“Š Total tokens collected: 18708.0

ğŸ” Running 0 assertions...
âœ… All assertions passed!
```
