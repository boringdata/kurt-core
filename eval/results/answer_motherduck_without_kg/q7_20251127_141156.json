{
  "question": "If I have a CSV on my laptop and a table in S3, what's the most efficient way to join them using MotherDuck?",
  "answer": "# How to Join a Local CSV with an S3 Table Using MotherDuck\n\nIf you have a CSV file on your laptop and a table stored in S3, the most efficient way to join them using MotherDuck is to leverage **Dual Execution** \u2014 MotherDuck's innovative hybrid query execution model that intelligently minimizes data movement.\n\n## The Most Efficient Approach\n\nSimply write a standard SQL join query that references both your local CSV and your S3 data:\n\n```sql\nSELECT\n    local.customer_id,\n    local.recent_purchase,\n    cloud.historical_data\nFROM read_csv('local_sales.csv') AS local\nJOIN read_parquet('s3://my-bucket/customer_history.parquet') AS cloud\n    ON local.customer_id = cloud.customer_id\nWHERE local.purchase_date >= '2025-01-01';\n```\n\n## How Dual Execution Makes This Efficient\n\nMotherDuck's **Dual Execution** model is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on your laptop or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\n\nWhen you run a hybrid query joining local and cloud data, the planner automatically:\n\n1. **Local Scan & Filter**: Scans your local CSV on your laptop and applies any filters there\n2. **Minimal Transfer**: Sends only the filtered results (potentially just a few kilobytes) to the cloud\n3. **Cloud Processing**: Scans the S3 table in the MotherDuck cloud where the data already lives\n4. **Optimized Join**: Performs the final join in the cloud\n\n### Example Optimization in Action\n\nImagine you want to join a local CSV of new product pricing with a massive sales table in MotherDuck, filtered for a specific product. Instead of moving entire tables, the Dual Execution planner:\n\n1. **Local Scan**: Scans the small pricing CSV on your laptop\n2. **Filter Locally**: Applies the filter for the specific product on your machine\n3. **Transfer Minimal Data**: Sends only the single, filtered pricing row (a few bytes of data) to the cloud\n4. **Join in the Cloud**: Performs the final join against the massive sales table in the MotherDuck cloud\n\nThis process can reduce network traffic by **orders of magnitude** compared to traditional methods, turning a difficult query into an interactive one.\n\n## Key Benefits\n\n- **No ETL Required**: No need to upload your entire CSV to S3 or download the entire S3 table locally\n- **Minimizes Data Movement**: Only the minimum necessary data crosses the network boundary\n- **Single Query**: Write standard SQL as if all data exists in one unified environment\n- **Cost Efficient**: Leverages your laptop's free compute resources and minimizes cloud data transfer costs\n- **Faster Development**: Zero-latency feedback for local development before scaling to cloud\n- **Better Collaboration**: Transforms DuckDB from \"single-player\" to \"multiplayer\" platform\n\n## Setup: Connecting to MotherDuck\n\nTo enable hybrid execution, connect your local DuckDB instance to MotherDuck:\n\n```sql\nATTACH 'md:my_database';\n```\n\nAfter this simple `ATTACH` command, any query can seamlessly mix local files and cloud data. MotherDuck uses the exact same DuckDB engine both locally and in the cloud, ensuring that a query validated locally is guaranteed to behave identically in production.\n\n## Verifying the Execution Plan\n\nYou can use `EXPLAIN` to see which parts of your query run locally versus remotely:\n\n```sql\nEXPLAIN\nSELECT *\nFROM 'local_data.csv' AS local\nJOIN read_parquet('s3://bucket/table.parquet') AS cloud\n    ON local.id = cloud.id;\n```\n\nThe explain plan will show:\n- Operations marked `(L)` run locally on your laptop\n- Operations marked `(R)` run remotely in MotherDuck cloud\n\nThis transparency helps you understand and optimize the execution strategy.\n\n## Manual Control (Optional)\n\nWhile the automatic optimization usually makes the best choice, you can manually control execution location using the `md_run` parameter:\n\n```sql\n-- Force S3 scan to run locally (downloads data first)\nFROM read_parquet('s3://bucket/file.parquet', md_run = 'local')\n\n-- Force S3 scan to run remotely in MotherDuck cloud\nFROM read_parquet('s3://bucket/file.parquet', md_run = 'remote')\n```\n\n**Performance comparison**: In testing, querying a 2GB Parquet file on S3 remotely in MotherDuck (25 seconds) was significantly faster than downloading and processing locally (36 seconds). By pushing computation to where the data lives, MotherDuck minimizes network I/O and delivers results more quickly.\n\n## Advanced CSV Handling\n\nMotherDuck's CSV reader automatically detects structure (headers, delimiters, data types), but you can override for messy files:\n\n```sql\n-- Let MotherDuck auto-detect (recommended)\nFROM 'local_sales.csv'\n\n-- Or specify details for problematic CSVs\nFROM read_csv('local_sales.csv',\n    delim = '|',                    -- Custom delimiter\n    header = true,                  -- Force header presence\n    dateformat = '%m/%d/%Y',        -- Custom date format\n    ignore_errors = true            -- Skip problematic rows\n)\n```\n\nFor more advanced CSV techniques, see the \"Taming Wild CSVs\" guide in the sources below.\n\n## Querying Multiple Files in S3\n\nYou can also query entire folders of files in S3 as a single table using glob patterns:\n\n```sql\nSELECT\n    event_type,\n    COUNT(*) AS event_count\nFROM read_parquet('s3://my-bucket/logs/2025/**/*.parquet')\nGROUP BY event_type;\n```\n\nThis treats all matching Parquet files as one unified dataset, with intelligent filter pushdown to minimize data scanned.\n\n## Why Data Engineers Are Excited\n\nData engineers love dual-execution engines because they solve the \"it worked on my machine\" problem. Since MotherDuck uses the exact same DuckDB engine locally and in the cloud:\n\n- **Faster Development**: Build and test pipelines with zero-latency local feedback before scaling to cloud\n- **Lower Costs**: Use free laptop compute and minimize data transfer\n- **Consistency Guaranteed**: A query that works on your laptop is guaranteed to work in the cloud\n- **Smooth Scale-Up**: Start local-first and seamlessly push more workload to cloud as needs grow\n\n## Real-World Performance\n\nCompanies have achieved dramatic improvements with this approach:\n- **Finqore**: Reduced 8-hour data pipelines to 8 minutes (60x improvement)\n- **Gardyn**: Cut pipeline time from 24+ hours to under 1 hour\n- **UDisc**: Reduced dbt jobs from 6 hours to 30 minutes, with queries dropping from minutes to 5 seconds",
  "answer_file": "/tmp/answer_without_kg_7.md",
  "token_usage": {
    "total_tokens": 0.0,
    "duration_seconds": 133.4381022453308
  },
  "cached_response": true,
  "llm_judge": {
    "overall_score": 0.81,
    "component_scores": {
      "accuracy": 0.8,
      "completeness": 0.9,
      "relevance": 0.7,
      "clarity": 0.8
    },
    "feedback": "The generated answer is mostly accurate and covers the required topics well, but it introduces additional concepts that may not be directly relevant to the question. While it provides a comprehensive overview, the focus on Dual Execution could distract from the simpler solution presented in the canonical answer. Overall, it is clear and well-structured, but slightly less relevant to the specific question asked."
  },
  "command": "question:q7",
  "returncode": 0
}