# How to Set Up MotherDuck to Work with dbt

MotherDuck integrates seamlessly with dbt (data build tool) through the `dbt-duckdb` adapter, allowing you to perform data transformations directly within MotherDuck's cloud data warehouse. Here's a comprehensive guide to getting started.

## Prerequisites

Before you begin, you'll need:
- A MotherDuck account (sign up at [motherduck.com](https://app.motherduck.com/?auth_flow=signup))
- Python 3.x installed
- Basic familiarity with dbt and SQL

## Installation

### 1. Install the dbt-duckdb Adapter

Install the dbt-duckdb package with MotherDuck support:

```bash
pip install dbt-duckdb[md]
# or with Poetry
poetry add dbt-duckdb[md]
```

The `[md]` extra installs the necessary dependencies for MotherDuck integration.

### 2. Get Your MotherDuck Service Token

1. Sign in to your MotherDuck account
2. Click on **Settings** in the upper right corner
3. Copy your **Service Token** to your clipboard
4. Export it as an environment variable:

```bash
export motherduck_token=<your_motherduck_token>
```

## Configuration

### 3. Initialize Your dbt Project

If you don't already have a dbt project:

```bash
dbt init your_project_name
```

### 4. Configure profiles.yml

Create or update your `profiles.yml` file with MotherDuck connection details. The key difference between local DuckDB and MotherDuck is the `path` parameter:

**Basic Configuration:**

```yaml
your_project_name:
  outputs:
    dev:
      type: duckdb
      path: /tmp/dbt.duckdb  # Local development
      threads: 16

    prod:
      type: duckdb
      path: md:your_database_name  # MotherDuck cloud
      threads: 16

  target: dev
```

**Advanced Configuration with AWS S3:**

If you're working with data in S3, you'll need additional configuration:

```yaml
your_project_name:
  target: dev
  outputs:
    dev:
      type: duckdb
      schema: dev_schema
      path: 'md:your_database_name'
      threads: 16
      extensions:
        - httpfs
      settings:
        s3_region: "{{ env_var('S3_REGION', 'us-west-1') }}"
        s3_access_key_id: "{{ env_var('S3_ACCESS_KEY_ID') }}"
        s3_secret_access_key: "{{ env_var('S3_SECRET_ACCESS_KEY') }}"

    prod:
      type: duckdb
      schema: prod_schema
      path: 'md:your_database_name'
      threads: 16
      extensions:
        - httpfs
      settings:
        s3_region: us-west-1
        s3_access_key_id: "{{ env_var('S3_ACCESS_KEY_ID') }}"
        s3_secret_access_key: "{{ env_var('S3_SECRET_ACCESS_KEY') }}"
```

### 5. Set Up Environment Variables

For AWS S3 integration, export your credentials:

```bash
export motherduck_token=<your_motherduck_token>
export S3_REGION=<your_region>
export S3_ACCESS_KEY_ID=<your_access_key_id>
export S3_SECRET_ACCESS_KEY=<your_secret_access_key>
```

## Verify Connection

### 6. Test Your Connection

Run `dbt debug` to verify that dbt can connect to MotherDuck:

```bash
dbt debug
```

You should see confirmation that the connection is successful.

### 7. Run Your First Model

Create a simple dbt model or run an existing one:

```bash
dbt run
# or for a specific target
dbt run --target prod
```

## Key Features and Benefits

### Local Development, Cloud Production

Since both dbt and DuckDB can run locally, you can:
- Develop and test using the same technologies you run in production
- Run models locally for fast iteration
- Seamlessly transition to MotherDuck for production workloads

### Dynamic Execution

MotherDuck intelligently runs queries either in the cloud or locally based on what's most efficient, providing "dynamic execution out of the box."

### Working with S3 Data

MotherDuck can directly query data from S3 without copying it:

```sql
-- Reference S3 data directly in your dbt models
SELECT * FROM 's3://your-bucket/path/to/data.csv'
```

Or use MotherDuck's public datasets:

```sql
SELECT * FROM 's3://us-prd-motherduck-open-datasets/jaffle_shop/csv/raw_customers.csv'
```

### Incremental Models

Configure incremental materializations in your `dbt_project.yml`:

```yaml
models:
  your_project:
    your_model:
      +materialized: "{{ 'incremental' if target.name == 'prod' else 'table' }}"
      +unique_key: load_id
```

## Best Practices

1. **Use Environment Variables**: Keep credentials secure by using environment variables rather than hardcoding them
2. **Separate Dev and Prod**: Use different targets for local development and production deployments
3. **Enable httpfs Extension**: This DuckDB extension is essential for working with S3 and remote files
4. **Fast Iteration**: Take advantage of local DuckDB for rapid development before deploying to MotherDuck
5. **Unit Testing**: Since DuckDB runs in-process, you can write genuine unit tests without cloud dependencies

## Troubleshooting

- **Authentication Issues**: Ensure your `motherduck_token` environment variable is set correctly
- **S3 Access**: Verify your AWS credentials are properly configured if working with S3 data
- **Version Compatibility**: Make sure your DuckDB version matches what MotherDuck supports (check MotherDuck documentation for the current supported version)

## Additional Resources

For more advanced patterns and examples:
- Check out the [jaffle_shop_duckdb](https://github.com/sungchun12/jaffle_shop_duckdb) demo repository
- See the [stocks demo](https://github.com/matsonj/stocks) for performance-optimized pipelines
- Review the [pypi-duck-flow](https://github.com/mehd-io/pypi-duck-flow) end-to-end project

## Sources

- [MotherDuck Ecosystem: dbt Core](https://motherduck.com/ecosystem/dbt)
- [MotherDuck + dbt: Better Together](https://motherduck.com/blog/motherduck-duckdb-dbt)
- [Performant dbt pipelines with MotherDuck](https://motherduck.com/blog/motherduck-dbt-pipelines)
- [DuckDB & dbt | End-To-End Data Engineering Project (2/3)](https://motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2)
