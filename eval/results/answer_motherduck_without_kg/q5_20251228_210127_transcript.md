# Scenario: answer_motherduck_without_kg

**Status**: âœ… PASSED
**Timestamp**: 20251228_210127

**Duration**: 90.21 seconds
**Tokens Used**: 0

---

## Conversation


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ SCENARIO: answer_motherduck_without_kg
   Answer MotherDuck questions by searching sources directly (no KG)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


ğŸ” Verifying .claude installation...
   âœ“ .claude folder exists at /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.claude
   âœ“ Skills: 0 found
   âœ“ Commands: 0 found

ğŸ” Filtering to questions: [5]

ğŸ§ª Running 1 of 10 question(s) defined in questions_motherduck.yaml


======================================================================
â“ Question 5/10
...How do I set up MotherDuck to work with dbt?
======================================================================


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 5
â”‚ Answer this question by searching through the .kurt/sources/ directory:

...How do I set up MotherDuck to work with dbt?

Required Steps:
1. Search for relevant markdown files in .kurt/sources/ using glob and grep
2. Read the most relevant files (aim for 3-5 most relevant sources)
3. As you read each file, keep track of:
   - The file path
   - The file title/name
   - Key information extracted from that file
4. Synthesize a comprehensive answer based on the information found
5. Write your answer to: /tmp/answer_without_kg_5.md

IMPORTANT - Your answer MUST have this structure:
```markdown
# Answer

[Your comprehensive answer here, drawing from the sources you read]

## Sources

List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
[etc.]

DO NOT use URLs or document titles as links. Show the actual file path you read with the Read tool.
```

The Sources section is MANDATORY - you must list every file you read to answer the question.

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [96mğŸ”§ TOOL:[0m Grep â†’ dbt

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/**/*dbt*.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/data-discoverability-secoda-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/sql-ide-safari-harlequin-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/take-flight-with-dbt-and-duckdb-dropping-dev-warehouse-costs-to-zero.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-in-100-seconds-by-a-duck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/no-sql-standard-for-7-years.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/panel-fixing-the-data-engineering-lifecycle-coalesce-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/is-duckdb-the-secret-to-unlocking-your-gis-potential.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/monte-carlo-simulations-with-duckdb-ft-james-mcneill.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-ga-produck-tour.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/faster-data-pipelines-development-with-mcp-and-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/fixing-sql-spaghetti-effective-refactoring-techniques.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/friendly-sql-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-bootstrap-a-data-warehouse-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ducklake-big-data-small-coalesce-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-dbt-accelerating-the-developer-experience-with-local-power.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-dbt-end-to-end-data-engineering-project-23.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-experiments-peeking-into-the-future-of-analytics-ft-christophe-blefari.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/big-data-is-dead-long-live-hot-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/building-data-apps-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/cultivating-growth-how-gardyn-scaled-its-data-operations-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/beyond-data-engineering-ft-joe-reis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/react-components-testing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-use-cases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-lake-vs-data-warehouse-vs-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/index.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data build tool (dbt).md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data model.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data pipeline.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/SQL analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Dagster.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/ELT.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/ETL.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/events.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/orchestra.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/paradime.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/contact-us/product-expert.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-brief.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dosomething-non-profit-tco-cost-savings.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/gardyn.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/reflex-careers-gig-economy-retail.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/trunkrs-same-day-delivery-motherduck-from-redshift.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/udisc-motherduck-sports-management.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/community-and-open-source.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-everybody-hates-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-embeddings-for-semantic-meaning-in-text-and-rag.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/summer-data-engineering-roadmap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-recap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/solving-advent-code-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/spark-ducklake-getting-started.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/semantic-layer-duckdb-tutorial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-kestra-etl-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-open-for-all-with-series-b.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-reflections-sigmod-pods-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/llm-data-pipelines-prompt-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/json-log-analysis-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/guide-to-coalesce-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/geospatial-for-beginner-duckdb-spatial-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/fake-duck-game.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-python-e2e-data-engineering-project-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-three.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-two.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-puppygraph-graph-model-on-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-seven.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-six.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-ten.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-nine.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eight.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eleven.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-five.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-four.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-essential-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-infrastructure-devops.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineers-answer-10-top-reddit-questions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-dec-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/building-motherduck-partner-ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/sung-won-chung.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/jacob-matson.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/marcos-ortiz.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/graham-wetzler.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/adithya-krishnan.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 150
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/blog/llm-data-pipelines-prompt-motherduck-dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/blog/solving-advent-code-duckdb-dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/ecosystem/dbt.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/glossary/data build tool (dbt).md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/videos/duckdb-dbt-accelerating-the-developer-experience-with-local-power.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/videos/duckdb-dbt-end-to-end-data-engineering-project-23.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/videos/take-flight-with-dbt-and-duckdb-dropping-dev-warehouse-costs-to-zero.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "durationMs": 2923,
  [2m  â”‚[0m   "numFiles": 15,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me read the most relevant files - starting with the ecosystem/dbt.md file (likely the main integration page), the glossary entry, and key blog posts about setup:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/ecosystem/dbt.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/glossary/data build tool (dbt).md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/blog/dual-execution-dbt.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/ecosystem/dbt.md",
  [2m  â”‚[0m     "content": "---\ntitle: dbt\ncontent_type: tutorial\nsource_url: https://motherduck.com/ecosystem/dbt\nindexed_at: '2025-11-25T20:37:37.218572'\ncontent_hash: 596a296ccc2dc11d\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO ECOSYSTEM](https://motherduck.com/ecosystem/)\n\n# dbt Core\n\nTRANSFORMATION\n\ndbt (data build tool) is a command-line tool that enables data analysts and engineers to transform data in their warehouse more effectively. It allows users to write modular SQL queries, test data integrity, and document data transformations, streamlining the analytics engineering workflow. dbt supports version control and collaboration, making it easier for teams to manage and deploy data models.\n\n## dbt + MotherDuck\n\ndbt integrates with MotherDuck by allowing users to perform data transformations and analytics directly within the MotherDuck cloud data warehouse. By connecting dbt to MotherDuck, users can leverage DuckDB's efficient query execution engine for transforming and modeling data, ensuring seamless and optimized data workflows. This integration empowers teams to build, test, and document their data pipelines efficiently while utilizing MotherDuck's scalable and performant data infrastructure.\n\nSince both dbt and DuckDB can run locally, users can do development and testing using the same technologies they run in production.\n\nBlog\n\nVideo\n\nDocs\n\n![dbt Core's logo](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdbt_59d5e63eaf.svg&w=3840&q=75)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 38,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 38
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/glossary/data build tool (dbt).md",
  [2m  â”‚[0m     "content": "---\ntitle: data build tool (dbt)\ncontent_type: event\nsource_url: https://motherduck.com/glossary/data build tool (dbt)\nindexed_at: '2025-11-25T20:02:28.271062'\ncontent_hash: 02a47aefb763154f\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n# data build tool (dbt)\n\n_[Back to DuckDB Data Engineering Glossary](https://motherduck.com/glossary/)_\n\n[dbt](https://www.getdbt.com/) is an open-source command-line tool that enables data analysts and engineers to transform data in their warehouses more effectively. It allows users to write modular SQL queries, which can be version-controlled and tested, bringing software engineering best practices to the world of data analytics. dbt works by compiling these SQL models into executable queries and running them against your data warehouse. It supports a wide range of data platforms, including Snowflake, BigQuery, and Redshift. With dbt, you can define dependencies between models, document your data transformations, and even generate data lineage diagrams. This tool is particularly useful for implementing the ELT (Extract, Load, Transform) paradigm, where transformations occur within the data warehouse itself. By using dbt, data teams can collaborate more efficiently, maintain cleaner code, and build more reliable data pipelines.\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 21,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 21
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     "content": "---\ntitle: motherduck-duckdb-dbt\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/motherduck-duckdb-dbt\nindexed_at: '2025-11-25T19:58:31.660396'\ncontent_hash: 4233f76fbdf40552\nhas_code_examples: true\nhas_step_by_step: true\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# MotherDuck + dbt: Better Together\n\n2023/09/07 - 5 min read\n\nBY\n\n[Sung Won Chung](https://motherduck.com/authors/sung-won-chung--/)\n\n## My Personal DuckDB Story\n\nDuckDB has been charming to me ever since I wrote [about it a year ago](https://roundup.getdbt.com/p/dbt-learning-to-love-software-engineers).\n\nIt gave me the glimmers of something I\u2019ve been begging for a long time: fast should be measured in seconds, not minutes.\n\n[I kicked the tires a lot when working at dbt Labs](https://github.com/dbt-labs/jaffle_shop_duckdb).\n\n- [And here](https://www.loom.com/share/ed4a6f59957e43158837eb4ba0c5ed67)\n\n- [And most recently here](https://www.loom.com/share/e213768457094a3187663a6cff76a61d?sid=29d6d696-0581-4b50-af45-7132dfb65f80)\n\n\nAnd in all the tire kicking, it has remained true to the glimmers it gave me and so much more. It\u2019s fast, easy, and cheap. And if it\u2019s running on your local computer, it\u2019s free.\n\nI\u2019ve had incredibly asymmetric expectations of how much money, time, and work it takes to make data fast and easy that I think to myself, \u201cOh, of course you\u2019re supposed to pay lots of dollars to run queries on millions/billions of rows per month.\u201d This has pleasantly disrupted that inner anchoring point. I see something more charming at play. Data teams can be productive with data bigger and work faster and save more money than they could have dreamed of 5 years ago. Heck! Even a year ago. So let\u2019s get into it.\n\n## Why use MotherDuck + dbt?\n\nWell, DuckDB and Motherduck\u2019s primary use case is solving analytical problems fast. Because of its columnar design, it\u2019s able to do just that. Even more so, the creators were smart about making integrations with adjacent data tools a first class experience. We see this with reading S3 files without copying them over and querying postgres directly without needing to extract and load it into DuckDB. And you don\u2019t need to define schemas or tedious configurations to make it work! Motherduck enables the multiplayer experience that having a single file on your machine is too tedious to pass around and synchronize with your teammates. Motherduck runs DuckDB on your behalf AND uses your local computer if the query you\u2019re running makes more sense to run there. You get dynamic execution out of the box. And that\u2019s pretty sweet.\n\nBut more than platitudes, let\u2019s get hands-on with working code so you can taste and see for yourself!\n\n## Get Started\n\nYou can follow along with [this repo](https://github.com/sungchun12/jaffle_shop_duckdb/tree/blog-guide):\n\n1. Signup for a [MotherDuck account!](https://motherduck.com/)![signup](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmotherduck_signup_33d4e9cf54.png%3Fupdated_at%3D2023-09-06T12%3A58%3A42.172Z&w=3840&q=75)\nNote : MotherDuck is still under private beta, but I heard you could get an invite if you join their [community slack](https://slack.motherduck.com/) with a good duck pun.\n\n2. Sign in and your screen should look like this minus some of the stuff you\u2019ll be building in the rest of this guide.\n![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fsignin_14a92fa2b0.png%3Fupdated_at%3D2023-09-06T12%3A58%3A41.340Z&w=3840&q=75)\n\n3. Click on the settings in the upper right hand corner and copy your Service Token to the clipboard.\n![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fservice_token_8fec7f0642.png%3Fupdated_at%3D2023-09-06T12%3A58%3A39.304Z&w=3840&q=75)\n\n4. Clone the repo and change directories into it.\n\n\n```bash\nCopy code\n\ngit clone -b blog-guide https://github.com/sungchun12/jaffle_shop_duckdb.git\ncd jaffle_shop_duckdb\n```\n\n5. Follow the detailed instructions to setup your [free AWS account and use S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/GetStartedWithS3.html):\n\n_Note: Feel free to skip this step if you already have an AWS account with S3 setup! Plus, MotherDuck has these data under their public S3 bucket at s3://us-prd-motherduck-open-datasets/jaffle\\_shop/csv/_\n\n6. Take the csv files stored in the git repo [here](https://github.com/sungchun12/jaffle_shop_duckdb/tree/blog-guide/seeds) and upload them into S3:\n![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fseeds_9b3753fd5d.png%3Fupdated_at%3D2023-09-06T12%3A58%3A41.504Z&w=3840&q=75)![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fs3_seeds_adf0454153.png%3Fupdated_at%3D2023-09-06T12%3A58%3A41.673Z&w=3840&q=75)\n\n7. [Copy the AWS S3 access keys to authenticate](https://docs.aws.amazon.com/powershell/latest/userguide/pstools-appendix-sign-up.html) your dbt project for later.\n\n\n## Configure Your dbt project\n\n_Note: Huge thanks to Josh Wills for creating the dbt-duckdb adapter and it works great with both DuckDB and MotherDuck: [https://github.com/jwills/dbt-duckdb](https://github.com/jwills/dbt-duckdb). This demo only works with DuckDB version 0.8.1: [https://motherduck.com/docs/intro](https://motherduck.com/docs/intro)_\n\n1. Adjust your `profiles.yml` for the naming conventions that make sense to you. Specifically, focus on schema.\n\n```yaml\nCopy code\n\njaffle_shop:\n\n  target: dev\n  outputs:\n    dev:\n      type: duckdb\n      schema: dev_sung\n      path: 'md:jaffle_shop'\n      threads: 16\n      extensions:\n        - httpfs\n      settings:\n        s3_region: \"{{ env_var('S3_REGION', 'us-west-1') }}\"\n        s3_access_key_id: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n        s3_secret_access_key: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n\n    dev_public_s3:\n      type: duckdb\n      schema: dev_sung\n      path: 'md:jaffle_shop'\n      threads: 16\n      extensions:\n        - httpfs\n      settings:\n        s3_region: \"{{ env_var('S3_REGION', 'us-east-1') }}\" # default region to make hello_public_s3.sql work correctly!\n        s3_access_key_id: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n        s3_secret_access_key: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n\n    prod:\n      type: duckdb\n      schema: prod_sung\n      path: 'md:jaffle_shop'\n      threads: 16\n      extensions:\n        - httpfs\n      settings:\n        s3_region: us-west-1\n        s3_access_key_id: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n        s3_secret_access_key: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n```\n\n2. Export your motherduck and S3 credentials to the terminal session, so your dbt project can authenticate to both\n\n```shell\nCopy code\n\n# all examples are fake\nexport motherduck_token=<your motherduck token> # aouiweh98229g193g1rb9u1\nexport S3_REGION=<your region> # us-west-1\nexport S3_ACCESS_KEY_ID=<your access key id> # haoiwehfpoiahpwohf\nexport S3_SECRET_ACCESS_KEY=<your secret access key> # jiaowhefa998333\n```\n\n3. Create a python virtual environment and install the packages to run this dbt project\n\n```shell\nCopy code\n\npython3 -m venv venv\nsource venv/bin/activate\npython3 -m pip install --upgrade pip\npython3 -m pip install -r requirements.txt\n```\n\n4. Run `dbt debug` to verify dbt can connect to motherduck and S3\n\n```shell\nCopy code\n\ndbt debug\n```\n\n5. Run `dbt build` to run and test the project!\n\n```shell\nCopy code\n\ndbt build\n```\n\n6. If you're feeling adventurous, run the below to reference a public s3 bucket provided by MotherDuck!\n\nImpacted dbt model\n\n```sql\nCopy code\n\n--filename: hello_public_s3.sql\n{% if target.name == 'dev_public_s3' %}\n\nSELECT * FROM 's3://us-prd-motherduck-open-datasets/jaffle_shop/csv/raw_customers.csv'\n\n{% else %}\n\nselect 1 as id\n\n{% endif %}\n```\n\n```shell\nCopy code\n\ndbt build --target dev_public_s3\n```\n\n7. Now, you should see everything ran with green font everywhere and you should see this in the UI! Including the S3 data you built a dbt model on top of!\n\n![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgreen_logs_ad3ec33dd1.png%3Fupdated_at%3D2023-09-06T12%3A58%3A39.718Z&w=3840&q=75)![signin](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmotherduck_success_5f718670be.png%3Fupdated_at%3D2023-09-06T12%3A58%3A42.460Z&w=3840&q=75)\n\nThat\u2019s it! Ruffle up those feathers and start quacking and slapping those juicy SQL queries together to solve your analytics problems faster and cheaper than ever before!\n\n## Conclusion\n\nWe\u2019re at a really cool place where all I had to give you was a couple instructions to get you up and running with MotherDuck. I really hope the data industry gets to a place where we brag about the things we do NOT have to do vs. pride ourselves on complexity for its own sake. What matters is that we solve problems and spend time, money, and energy doing it where it\u2019s actually worth it to solve those problems. I\u2019m excited to see you all build MotherDuck guides far superior to mine. That\u2019s why this is so fun. We get to sharpen each other!\n\n_Want to know more about MotherDuck and dbt ? Checkout [MotherDuck & dbt documentation](https://motherduck.com/docs/integrations/transformation/dbt/) and have a look at their YouTube tutorial about DuckDB & dbt \ud83d\udc47_\n\nUnleashing DuckDB & dbt for local analytics triumphs - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Unleashing DuckDB & dbt for local analytics triumphs](https://www.youtube.com/watch?v=asxGh2TrNyI)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nFull screen is unavailable. [Learn More](https://support.google.com/youtube/answer/6276924)\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nMore videos\n\n## More videos\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=asxGh2TrNyI&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 8:54\n\n\u2022Live\n\n\u2022\n\n### TABLE OF CONTENTS\n\n[My Personal DuckDB Story](https://motherduck.com/blog/motherduck-duckdb-dbt/#my-personal-duckdb-story)\n\n[Why use MotherDuck + dbt?](https://motherduck.com/blog/motherduck-duckdb-dbt/#why-use-motherduck-dbt)\n\n[Get Started](https://motherduck.com/blog/motherduck-duckdb-dbt/#get-started)\n\n[Configure Your dbt project](https://motherduck.com/blog/motherduck-duckdb-dbt/#configure-your-dbt-project)\n\n[Conclusion](https://motherduck.com/blog/motherduck-duckdb-dbt/#conclusion)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![This Month in the DuckDB Ecosystem: August 2023](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fthumbnail_duckdb_newsletter_1_feb51165aa.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-nine/)\n\n[2023/08/21 - Marcos Ortiz](https://motherduck.com/blog/duckdb-ecosystem-newsletter-nine/)\n\n### [This Month in the DuckDB Ecosystem: August 2023](https://motherduck.com/blog/duckdb-ecosystem-newsletter-nine)\n\nThis Month in the DuckDB Ecosystem: August 2023\n\n[![DuckDB Tutorial For Beginners](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgetting_started_duckdb_thumbnail_70b197b1ab.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n[2024/10/31 - Mehdi Ouazza, Ryan Boyd](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n### [DuckDB Tutorial For Beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners)\n\nGet up to speed quickly with DuckDB, including installation, VSCode workflow integration and your first SQL analytics project.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 314,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 314
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     "content": "---\ntitle: dual-execution-dbt\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/dual-execution-dbt\nindexed_at: '2025-11-25T19:57:41.215232'\ncontent_hash: 2b6c5ae395798479\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Local dev and cloud prod for faster dbt development\n\n2025/01/16 - 8 min read\n\nBY\n\n[Jacob Matson](https://motherduck.com/authors/jacob-matson/)\n\n## Introducktion\n\nI hate waiting for slow pipelines to run, so I am delighted to share some strategies to iterate on your data problems at maximum speed - MotherDuck even gave a talk on this concept at [dbt Coalesce in 2024](https://www.youtube.com/watch?v=oqwIHvSfOVQ). By harnessing the capabilities of DuckDB locally, backed by MotherDuck in the cloud, we can unlock an incredibly fast and efficient development cycle. We'll explore how to configure your dbt profile for dual execution and share some tips on how much data to bring local. By implementing these techniques, you can significantly accelerate your data pipeline development and iterate even faster to solve business problems.\n\nLooking to following along in the code?\n\nCheck out the [example repo](https://github.com/motherduckdb/motherduck-examples/tree/main/dbt-dual-execution)!\n\n![Instant feedback loop](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FInstant_feedback_loop_b99078a679.png&w=3840&q=75)\n\n## Setting up your Profile\n\nIn order to take advantage of these capabilities, we need to configure our dbt profile to execute in the correct place, as well as define the behavior that we want in our sources. In the example dbt profile below, `prod` runs entirely in the cloud, while `local` runs mostly on local but is also linked to MotherDuck for reading data into your local database.\n\n```yml\nCopy code\n\ndual_execution:\n  outputs:\n    local:\n      type: duckdb\n      path: local.db\n      attach:\n        - path: \"md:\"\t# attaches all MotherDuck databases\n    prod:\n      type: duckdb\n      path: \"md:jdw\"\n  target: local\n```\n\n## Sources & Models\n\nWith your sources, you need to define which ones to replicate entirely, which ones are ok as views, and which ones to sample. Keep in mind for sampling, you need to think about your data model and make sure that related samples are hydrated (i.e. if you only bring in 100 customers, you need to make sure you also bring in their orders too).\n\nIn my example project using TPC-DS as the source data, I am sampling 1% of the data when running locally on the large tables. In general, I am aiming to keep the datasets less than a million rows per table, although there is no hard limit. For the remaining tables, I am replicating the entire data set locally since they are so small.\n\nThe way that we conditionally sample our models is by using the [\u2018target\u2019 variable](https://docs.getdbt.com/reference/dbt-jinja-functions/target). You can add this parameter by checking your `target` and running it conditionally on your model.\n\nAn example sql snippet is below (using jinja).\n\n```sql\nCopy code\n\nfrom {{ source(\"tpc-ds\", \"catalog_sales\") }}\n{% if target.name == 'local' %} using sample 1 % {% endif %}\n```\n\nAs an example of a simple \u201ccreate local table from cloud\u201d, consider the following query plan. The \u201cL\u201d indicates Local and the \u201cR\u201d indicates Remote (i.e. MotherDuck).\n\n```bash\nCopy code\n\n\ud83e\udd86 explain create table\n        \"local\".\"main\".\"call_center\"\n      as (\n        from \"jdw_dev\".\"jdw_tpcds\".\"call_center\"\n      );\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502\u2502       Physical Plan       \u2502\u2502\n\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 BATCH_CREATE_TABLE_AS (L) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    DOWNLOAD_SOURCE (L)    \u2502\n\u2502    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n\u2502        bridge_id: 1       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  BATCH_DOWNLOAD_SINK (R)  \u2502\n\u2502    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n\u2502        bridge_id: 1       \u2502\n\u2502       parallel: true      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       SEQ_SCAN  (R)       \u2502\n\u2502    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n\u2502        call_center        \u2502\n\u2502                           \u2502\n\u2502        Projections:       \u2502\n\u2502     cc_call_center_sk     \u2502\n\u2502     cc_call_center_id     \u2502\n\u2502     cc_rec_start_date     \u2502\n\u2502      cc_rec_end_date      \u2502\n\u2502     cc_closed_date_sk     \u2502\n\u2502      cc_open_date_sk      \u2502\n\u2502          cc_name          \u2502\n\u2502          cc_class         \u2502\n\u2502        cc_employees       \u2502\n\u2502          cc_sq_ft         \u2502\n\u2502          cc_hours         \u2502\n\u2502         cc_manager        \u2502\n\u2502         cc_mkt_id         \u2502\n\u2502        cc_mkt_class       \u2502\n\u2502        cc_mkt_desc        \u2502\n\u2502     cc_market_manager     \u2502\n\u2502        cc_division        \u2502\n\u2502      cc_division_name     \u2502\n\u2502         cc_company        \u2502\n\u2502      cc_company_name      \u2502\n\u2502      cc_street_number     \u2502\n\u2502       cc_street_name      \u2502\n\u2502       cc_street_type      \u2502\n\u2502      cc_suite_number      \u2502\n\u2502          cc_city          \u2502\n\u2502         cc_county         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nThis can also be extended to your `sources.yml` if necessary for testing local datasets (i.e. json or parquet on experimental pipelines that have not yet made it to your data lake). Configuring these is similar:\n\n```ini\nCopy code\n\n{%if- target.name == 'local' -%}\n   meta:\n      external_location:\n        data/tpcds/{name}.parquet\n{%- endif -%}\n```\n\n## Running your pipeline\n\nOnce you have this configuration in place, you can simply run your pipeline as normal, although for ease of use, you may want to add tags to the models that you are working on so you can avoid going back to the cloud data set too often. This can be set simply in the `dbt_project.yml` like this:\n\n```yml\nCopy code\n\nmodels:\n  dual_execution:\n    tpcds:\n      raw:\n        +tags: ['raw']\n        +materialized: table\n      queries:\n        +materialized: view\n        +tags: ['queries']\n```\n\nFrom there, it is as simple as running `dbt build -s tag:raw` to load your raw data and then for subsequent query iteration, run `dbt build -s tag:queries` in the CLI. The subsequent runs can be visualized like this:\n\n![data flow cloud to local](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fdata_flow_cloud_to_local_792a293673.png&w=3840&q=75)\n\n## Shipping dev to the cloud\n\nCertain tables may need to be available in your cloud data warehouse for testing even in the local workflow. This may be something like a BI tool, that is connected to your cloud instance and is difficult to run locally. This can be accomplished by setting the database attribute in your model, so that after the model is run, it is available in the cloud as well.\n\n```yml\nCopy code\n\n{{ config(\n    database=\"jdw_dev\",\n    schema=\"local_to_prod\"\n    materialized=\"table\"\n) }}\n```\n\nIt should be noted that this is a static configuration that is best used for testing. If you don\u2019t want to manually flip models between dev / prod destinations, you can define the database as an attribute of a specific model in your `dbt_project.yml` file.\n\n## Wrapping up\n\nAs you can see from this example, using MotherDuck\u2019s dual execution allows us to leverage the unique value proposition of DuckDB to run an accelerated development cycle on your local machine. With some basic optimization, we can get ~5x faster dbt runs by making the data smaller and using local compute. This is a very powerful combination for rapidly iterating on your pipeline and then pushing a high quality change back into your production environment.\n\nWant to learn more? Join our webinar about Local Dev & Cloud Prod on [February 13th, 2025](https://lu.ma/0die8ual?utm_source=blog).\n\n### TABLE OF CONTENTS\n\n[Introducktion](https://motherduck.com/blog/dual-execution-dbt/#introducktion)\n\n[Setting up your Profile](https://motherduck.com/blog/dual-execution-dbt/#setting-up-your-profile)\n\n[Sources & Models](https://motherduck.com/blog/dual-execution-dbt/#sources-models)\n\n[Running your pipeline](https://motherduck.com/blog/dual-execution-dbt/#running-your-pipeline)\n\n[Shipping dev to the cloud](https://motherduck.com/blog/dual-execution-dbt/#shipping-dev-to-the-cloud)\n\n[Wrapping up](https://motherduck.com/blog/dual-execution-dbt/#wrapping-up)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![What\u2019s New: Streamlined User Management, Metadata, and UI Enhancements](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FMother_Duck_Feature_Roundup_2_47f5d902c0.png&w=3840&q=75)](https://motherduck.com/blog/data-warehouse-feature-roundup-dec-2024/)\n\n[2024/12/21 - Sheila Sitaram](https://motherduck.com/blog/data-warehouse-feature-roundup-dec-2024/)\n\n### [What\u2019s New: Streamlined User Management, Metadata, and UI Enhancements](https://motherduck.com/blog/data-warehouse-feature-roundup-dec-2024)\n\nDecember\u2019s feature roundup is focused on improving the user experience on multiple fronts. Introducing the User Management REST API, the Table Summary, and a read-only MD\\_INFORMATION\\_SCHEMA for metadata.\n\n[![DuckDB Ecosystem: January 2025](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fnewsletter_a65cff5430.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025/)\n\n[2025/01/10 - Simon Sp\u00e4ti](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025/)\n\n### [DuckDB Ecosystem: January 2025](https://motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025)\n\nDuckDB Monthly #25: PyIceberg, 0$ data distribution and more!\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 245,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 245
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me also read one more relevant file about dbt pipelines to get additional setup details:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_l2kkuclr/.kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md",
  [2m  â”‚[0m     "content": "---\ntitle: motherduck-dbt-pipelines\ncontent_type: tutorial\nsource_url: https://motherduck.com/blog/motherduck-dbt-pipelines\nindexed_at: '2025-11-25T19:57:03.385445'\ncontent_hash: 133a50ff003b0fce\nhas_code_examples: true\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[GO BACK TO BLOG](https://motherduck.com/blog/)\n\n# Performant dbt pipelines with MotherDuck\n\n2024/10/07 - 6 min read\n\nBY\n\n[Jacob Matson](https://motherduck.com/authors/jacob-matson/)\n\n_Ed. note: This blog post is a recap of the dbt+MotherDuck workshop at [Small Data SF](https://smalldatasf.com/). For event info and to learn about the next one, hit the website._\n\n## Quick Summary\n\nIn this blog, we will hit on the learnings and unique bits of kit that are a part of DuckDB & MotherDuck to build performant data pipelines in dbt. The final github repository can be [found here](https://github.com/matsonj/stocks). This article is not introductory level, and assumes that you have some experience with dbt.\n\nThe key bits, in order of DAG execution:\n\n- the read\\_blob() function\n- pre\\_hooks & variables + array\\_agg()\n- incremental models & read\\_csv()\n- unnest() + arg\\_max()\n\nThe goal of this exercise is to read a list of files, and then update the dbt models based on this list. The rough data flow looks like this:\n\n![Screenshot 2024-10-07 at 10.47.47\u202fAM.png](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2024_10_07_at_10_47_47_AM_f29c3a552e.png&w=3840&q=75)\n\nIn order to build a pipeline that can run on top of our data lake, we need to understand what file operations are available in MotherDuck and how to utilize them best inside of a dbt pipeline.\n\n## The read\\_blob function\n\n[Read Blob](https://duckdb.org/docs/guides/file_formats/read_file.html#read_blob) is the first function required to make this pipeline work. It takes a path as a parameter and returns a table with filenames, file size, schema, and last modified date. To assure that other files do not randomly get inserted into our pipeline while it is running, we are going to materialize this as a table and use it as the starting point for the pipeline.\n\n```sql\nCopy code\n\nselect\n    \"filename\" as \"file\",\n    regexp_extract(\"filename\", 'data/(.+?)_\\d+\\.csv', 1) as entity,\n    last_modified as modified_ts\nfrom read_blob('data/*.csv')\n```\n\nIn this example, DuckDB is inspecting local data. But DuckDB also includes capabilities to interact with [Object Stores](https://duckdb.org/docs/extensions/httpfs/overview.html), which means this functionality can easily be extended to data lakes.\n\n## Pre-hooks & variables + array\\_agg\n\nThe next set of models will be broken into two parts - the pre-hook and then the incremental mode. First we will discuss the pre-hook, which leverages a new concept in Duckdb 1.1, [variables](https://duckdb.org/docs/sql/statements/set_variable.html). Variables allow us to insert arbitrary values into them with the `set variable` command and then pass arbitrary values into sql queries with `getvariable()`. Variables only support scalar values, but since DuckDB supports structs (that is - custom data structures), those can also be used with variables. DuckDB also contains a sets of functions to handle structs, like `array_agg()` which is used to turn a table column into a list.\n\nThese concepts can be used together like the example below.\n\n```sql\nCopy code\n\n{{\n    config(\n        pre_hook=\"\"\"\n            set variable my_list = (\n                select array_agg(file)\n   from {{ ref('files') }}\n   where entity = 'ticker_info'\n            )\n        \"\"\",\n        materialized=\"incremental\",\n        unique_key=\"id\",\n    )\n}}\n```\n\n## Incremental models & read\\_csv\n\ndbt has the notion of \u201c [Incremental Materializations](https://docs.getdbt.com/docs/build/incremental-models)\u201d \\- models that are handled in a different flow and require more explicit definition, and thus can be built incrementally. These models usually require a unique\\_key, if no key is provided, the model is treated as \u201cappend only\u201d.\n\nFurthermore, incremental models must define which pieces of the model run incrementally.\n\nWhen invoked in normal dbt build or dbt run, incremental models will do the following:\n\n1. Insert new data into a temp table based on the defined increment.\n2. Delete any data from the existing model that matches the unique\\_key defined in the config block.\n3. Insert data from the temp table into the existing model.\n\nThis obviously means that changes to the schema of your model need to be carefully considered - new columns mean that the model must be rebuilt entirely. A rebuild of the model is called a \u201cfull refresh\u201d in dbt can be invoked with the full-refresh flag in the CLI.\n\nAs described in the pre\\_hook, the variable `my_list` contains a list of files to process, and the config block also contains the relevant information for the model type and key.\n\n```sql\nCopy code\n\nselect\n    info.symbol || '-' || info.filename as id,\n    info.*,\n    files.modified_ts,\n    now() at time zone 'UTC' as updated_ts\nfrom read_csv(getvariable('my_list'), filename = true, union_by_name = true) as info\nleft join {{ ref(\"files\") }} as files on info.filename = files.file\n{% if is_incremental() %}\n    where not exists (select 1 from {{ this }} ck where ck.filename = info.filename)\n{% endif %}\n```\n\nThis also introduces the concept of [{{ this }}](https://docs.getdbt.com/reference/dbt-jinja-functions/this), which is a dbt relation and is a reference to the current model.\n\n## Unnest + arg\\_max\n\nIn any data warehouse, the presence of duplicate data is almost inevitable. This can occur due to various reasons, but that doesn\u2019t make it any less painful.\n\n- Data Integration: When combining data from multiple sources, inconsistencies and overlaps can lead to duplicates.\n- REST API sources: Many data sources don\u2019t allow for incremental updates, which means that every time you get new data, it difficult or impossible to handle it with creating duplicates.\n\nIn order to handle de-duplication in the dbt models, we can use `arg_max()` and `unnest()`. `Arg_max()` allows users to pass a table reference and a numeric column (including dates & timestamps) and returns a single row as a struct. Since it returns this data type, `unnest()` is used in order to get a single row from the `arg_max()` function.\n\n```sql\nCopy code\n\nwith\n    cte_all_rows as (\n        select\n            symbol,\n            * exclude(id, symbol),\n            modified_ts as ts\n        from {{ ref(\"company_info\") }}\n    )\nselect unnest(arg_max(cte_all_rows, ts))\nfrom cte_all_rows\ngroup by symbol\n```\n\nAs an aside - why use `arg_max()` instead of a window function? The short answer is that `arg_max()` uses Radix sort, which leverages SQL group by to identify the groups in which to find the max. The time complexity of Radix sort is _O (n k)_, whereas comparison- based sorting algorithms have _O (n log n)_ time complexity.\n\n## Closing Thoughts\n\nIn conclusion, dbt and MotherDuck together offer a powerful framework for efficient data transformations and analysis. By leveraging tools like `read_blob()` for data ingestion, utilizing `pre_hooks` and `variables` to streamline logic with functions like `array_agg()`, and implementing incremental models with `read_csv()` for optimal performance, you can significantly enhance your data workflows. Additionally, advanced techniques like `unnest()` combined with `arg_max()` allow for more sophisticated data manipulation, unlocking even greater efficiency in your analyses. When used effectively, dbt & motherduck can transform your approach to data, enabling both speed and accuracy in your models. A working demo & instruction that can be found in [this github repo](https://github.com/matsonj/stocks). Good luck and happy quacking!\n\n### TABLE OF CONTENTS\n\n[Quick Summary](https://motherduck.com/blog/motherduck-dbt-pipelines/#quick-summary)\n\n[The read\\_blob function](https://motherduck.com/blog/motherduck-dbt-pipelines/#the-readblob-function)\n\n[Pre-hooks & variables + array\\_agg](https://motherduck.com/blog/motherduck-dbt-pipelines/#pre-hooks-variables-arrayagg)\n\n[Incremental models & read\\_csv](https://motherduck.com/blog/motherduck-dbt-pipelines/#incremental-models-readcsv)\n\n[Unnest + arg\\_max](https://motherduck.com/blog/motherduck-dbt-pipelines/#unnest-argmax)\n\n[Closing Thoughts](https://motherduck.com/blog/motherduck-dbt-pipelines/#closing-thoughts)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://motherduck.com/get-started/)\n\nGet Started\n\n![blog subscription icon](https://motherduck.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fblog-subscription-icon.67ccd5ae.png&w=828&q=75)\n\n### Subscribe to motherduck blog\n\nE-mail\n\nSubscribe to other MotherDuck Updates\n\nSubmit\n\n## PREVIOUS POSTS\n\n[![MotherDuck at Coalesce 2024: Your Ultimate Guide to Quack-tastic Fun!](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FCoalesce_3f73bf385e.png&w=3840&q=75)](https://motherduck.com/blog/guide-to-coalesce-2024/)\n\n[2024/10/01 - Jerel Navarrete](https://motherduck.com/blog/guide-to-coalesce-2024/)\n\n### [MotherDuck at Coalesce 2024: Your Ultimate Guide to Quack-tastic Fun!](https://motherduck.com/blog/guide-to-coalesce-2024)\n\nGet ready to make a splash at Coalesce 2024! \ud83e\udd86 MotherDuck is bringing the fun to Las Vegas, and we can't wait to see you there. Whether you're a seasoned Coalesce pro or a first-timer, we've got everything you need to make this year's event unforgettable.\n\n[![DuckDB Tutorial For Beginners](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fgetting_started_duckdb_thumbnail_70b197b1ab.png&w=3840&q=75)](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n[2024/10/31 - Mehdi Ouazza, Ryan Boyd](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)\n\n### [DuckDB Tutorial For Beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners)\n\nGet up to speed quickly with DuckDB, including installation, VSCode workflow integration and your first SQL analytics project.\n\n[View all](https://motherduck.com/blog/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 200,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 200
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now I have comprehensive information. Let me write the answer to the specified file:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_5.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "create",
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_5.md",
  [2m  â”‚[0m   "content": "<3426 chars>",
  [2m  â”‚[0m   "structuredPatch": [],
  [2m  â”‚[0m   "originalFile": null
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I've written a comprehensive answer to `/tmp/answer_without_kg_5.md` that explains how to set up MotherDuck to work with dbt. 
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer covers:
  [94mâ”‚[0m 1. **Prerequisites** - MotherDuck account signup, service token, and dbt-duckdb adapter installation
  [94mâ”‚[0m 2. **Step-by-step setup** - Installing dependencies, configuring profiles.yml with the `md:` prefix, setting environment variables, and verifying the connection
  [94mâ”‚[0m 3. **Dual execution pattern** - An advanced setup for local development with cloud production
  [94mâ”‚[0m 4. **Key benefits** - Speed, cost efficiency, simplicity, and dynamic execution
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer was synthesized from 5 source files found in `.kurt/sources/motherduck.com/`, including the main ecosystem page, glossary entry, and three detailed blog posts with code examples and configuration samples.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 2,018 | Cost: $0.3530
     Cumulative: 2,018 tokens | $0.3530
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent is summarizing the completion of a task and providing information rather than soliciting a response or asking a question.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 8 | Duration: 71.3s
â•‘    Tokens: 2,018 | Cost: $0.3530
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  Running LLM judge evaluation...
   ğŸ§  LLM Judge score: 1.00
