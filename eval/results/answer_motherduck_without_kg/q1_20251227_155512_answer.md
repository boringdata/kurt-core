# Answer

**Parquet is the most efficient file format for loading data into MotherDuck**, significantly outperforming row-based formats like CSV and JSON. However, MotherDuck/DuckDB also provides excellent support for CSV, JSON, and Excel files through its robust auto-detection capabilities.

## Parquet: The Most Efficient Format

Parquet is the recommended format for loading large datasets into MotherDuck for several key reasons:

1. **Columnar Storage**: Parquet stores data by columns rather than rows, enabling DuckDB to read only the columns needed for a query. If your query needs 2 columns out of 100, a columnar system might only need to read 2% of the total data.

2. **Superior Compression**: Values within a column tend to be of the same data type and exhibit similar patterns, allowing Parquet to achieve much higher compression ratios compared to row-based formats. This reduces storage costs and speeds up queries by minimizing disk I/O.

3. **Predicate Pushdown and Column Pruning**: DuckDB can apply WHERE clause filters at the storage level before data is read into memory, and skip reading columns not needed for the query. Parquet files store rich metadata and statistics (min/max values, null counts) for each column chunk, enabling efficient data skipping.

4. **Direct File Access**: DuckDB enables SQL queries directly on Parquet files without requiring data loading steps, allowing immediate data exploration.

5. **Native Integration**: DuckDB's architecture is purpose-built to leverage Parquet's column-oriented design, minimizing overhead and enhancing query performance.

## CSV and JSON: Supported but Less Efficient

While CSV and JSON files are less efficient than Parquet, MotherDuck provides exceptional support for them through DuckDB's advanced CSV parser:

- **Auto-detection**: The `read_csv_auto` and `read_json_auto` functions automatically detect column names, data types, delimiters, and file dialects
- **Schema-on-read**: No need for CREATE TABLE statements; structure is applied at query time
- **Error handling**: Options like `ignore_errors`, `store_rejects`, and `strict_mode` help handle messy data
- **Glob patterns**: Query thousands of files as a single table using patterns like `'s3://bucket/**/*.csv'`

CSV files have advantages for data exchange due to their simplicity and universal support, but they lack compression, typing, and efficient column access that Parquet provides.

## Efficiency Comparison

| Format | Load Speed | Compression | Column Access | Best Use Case |
|--------|------------|-------------|---------------|---------------|
| **Parquet** | Fastest | Excellent | Columnar (efficient) | Large-scale analytics, data lakes |
| **CSV** | Slower | None | Row-based (must read all) | Data exchange, small-medium datasets |
| **JSON** | Slower | None | Nested structure | Semi-structured data, API responses |

## Recommendation

For optimal performance when loading data into MotherDuck:
1. **Use Parquet** for large datasets and regular analytics workloads
2. **Convert CSV to Parquet** if you'll query the data repeatedly
3. **Use CSV/JSON directly** for ad-hoc analysis, data exploration, or when Parquet conversion isn't feasible

DuckDB makes it easy to convert between formats:
```sql
COPY (SELECT * FROM 'data.csv') TO 'data.parquet' (FORMAT PARQUET);
```

## Sources

List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:
- .kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md - Comprehensive guide on Parquet benefits including columnar storage, compression, predicate pushdown, and DuckDB integration
- .kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md - Guide on querying raw CSV, JSON, and Parquet files directly with SQL, including glob patterns and schema-on-read capabilities
- .kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md - Data engineer's guide explaining columnar vs row-oriented storage, compression techniques, and performance advantages
- .kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md - Blog post on why CSVs persist and how DuckDB handles them, including auto-detection and schema challenges
- .kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md - Advanced DuckDB techniques for handling messy CSV files, including sniff_csv, error handling, and the Pollock benchmark results
- .kurt/sources/motherduck.com/glossary/CSV.md - Glossary entry explaining CSV format and DuckDB's read_csv function with auto-detection
