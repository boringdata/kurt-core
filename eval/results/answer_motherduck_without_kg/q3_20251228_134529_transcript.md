# Scenario: answer_motherduck_without_kg

**Status**: âœ… PASSED
**Timestamp**: 20251228_134529

**Duration**: 111.37 seconds
**Tokens Used**: 0

---

## Conversation


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ SCENARIO: answer_motherduck_without_kg
   Answer MotherDuck questions by searching sources directly (no KG)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


ğŸ” Verifying .claude installation...
   âœ“ .claude folder exists at /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_nsdf4vt5/.claude
   âœ“ Skills: 0 found
   âœ“ Commands: 0 found

ğŸ” Filtering to questions: [3]

ğŸ§ª Running 1 of 10 question(s) defined in questions_motherduck.yaml


======================================================================
â“ Question 3/10
...What SQL features from DuckDB are not yet supported in MotherDuck?
======================================================================


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 3
â”‚ Answer this question by searching through the .kurt/sources/ directory:

...What SQL features from DuckDB are not yet supported in MotherDuck?

Required Steps:
1. Search for relevant markdown files in .kurt/sources/ using glob and grep
2. Read the most relevant files (aim for 3-5 most relevant sources)
3. As you read each file, keep track of:
   - The file path
   - The file title/name
   - Key information extracted from that file
4. Synthesize a comprehensive answer based on the information found
5. Write your answer to: /tmp/answer_without_kg_3.md

IMPORTANT - Your answer MUST have this structure:
```markdown
# Answer

[Your comprehensive answer here, drawing from the sources you read]

## Sources

List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
[etc.]

DO NOT use URLs or document titles as links. Show the actual file path you read with the Read tool.
```

The Sources section is MANDATORY - you must list every file you read to answer the question.

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [96mğŸ”§ TOOL:[0m Grep â†’ not.*supported|unsupported|not yet|limitations|DuckDB.*features

  [96mğŸ”§ TOOL:[0m Grep â†’ SQL.*support|feature.*support|compatibility
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ai-powered-bi-can-llms-really-generate-your-dashboards-ft-michael-driscoll.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/react-components-testing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-OLAP.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/web-assembly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/ducklake-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/hack-night.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/acid-transactions-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data build tool (dbt).md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/data model.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/SQLAlchemy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Java programming language.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Pandas DataFrames.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Common Table Expressions (CTEs).md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/DB-API 2.0.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/DuckDB.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Go programming language.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/ALTER TABLE statement.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Apache Superset.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/sqlmesh.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/omni.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/polytomic.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/meltano.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/datagrip.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/cloudquery.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-summary-chapter2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-summary-chapter4.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-future-of-bi-bi-as-code-duckdb-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vector-technologies-ai-data-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-learn-sql-in-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streaming-oracle-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streamkap-mysql-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/six-reasons-duckdb-slaps.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg_duckdb-postgresql-extension-for-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-duckdb-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/preswald-health-data-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/log-processing-savings-bacalhau-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-fixit-ai-sql-error-fixer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-motherduck-for-business-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/getting-started-ducklake-table-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/git-for-data-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/flight-sql-vs-rest-vs-jdbc.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/from-data-lake-to-lakehouse-duckdb-portable-catalog.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/effortless-etl-unstructured-data-unstructuredio-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/fabi-ai-llm-prompt-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-text2sql-llm.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-the-great-federator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-versus-pandas-versus-polars.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-puppygraph-graph-model-on-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-one.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-ten.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-nine.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-series-seed-and-a.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/sheila-sitaram.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/tino-tereshko.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 103
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/lies-damn-lies-and-benchmarks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/machine-learning-and-ai-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-vs-pandas-vs-polars-for-python-devs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/cultivating-growth-how-gardyn-scaled-its-data-operations-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/an-evolving-dag-for-the-llm-world-julia-schottenstein-of-langchain-at-small-data-sf-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-use.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/privacy-policy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-struct-nested-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/dataframes.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/acid-transactions-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/big-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/storage.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/window functions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/MotherDuck extension.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/SELECT statement.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Common Table Expressions (CTEs).md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/C/C++.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/fees-addendum.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/puppygraph.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/census.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-summary-chapter2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/finqore.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/candidate-privacy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/atm-com-analytics-costs-sql-expressibility.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dominik-moritz.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-simple-joys-of-scaling-up.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vector-technologies-ai-data-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-learn-sql-in-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/semantic-layer-duckdb-tutorial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-faker-duckdb-exploration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg to motherduck at motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/postgres-duckdb-options.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/json-log-analysis-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/galileo-world-geospatial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/geospatial-for-beginner-duckdb-spatial-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/getting-started-gis-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-ducks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/fix-outdated-llm-documentation-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/from-data-lake-to-lakehouse-duckdb-portable-catalog.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/estuary-streaming-cdc-replication.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-text2sql-llm.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-the-great-federator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-versus-pandas-versus-polars.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-on-edge-raspberry-pi.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-puppygraph-graph-model-on-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-ten.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-nine.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-five.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-four.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineer-highlights-PyConDE-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-infrastructure-devops.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/differential-storage-building-block-for-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-app-generator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/mehdi-ouazza.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 115
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ not supported|unsupported|limitation|disabled|restriction

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/**/docs/**/*.md

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/**/*sql*features*.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": "<10895 chars>",
  [2m  â”‚[0m   "numLines": 50,
  [2m  â”‚[0m   "appliedLimit": 50
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "durationMs": 572,
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     "/private/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_nsdf4vt5/.kurt/sources/motherduck.com/blog/motherduck-ai-sql-fixit-inline-editing-features.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "durationMs": 714,
  [2m  â”‚[0m   "numFiles": 1,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ MotherDuck.*SQL|DuckDB.*feature|extension|ATTACH|DETACH|transactions

  [96mğŸ”§ TOOL:[0m Glob â†’ .kurt/sources/**/docs*.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "durationMs": 403,
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/working-with-sql-with-your-favorite-editor-check-this.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-if-sql-queries-returned-results-instantly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-and-how-we-integrated-duckdb-motherduck-with-gooddata.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/spatial-data-management-with-duckdb-ft-mattforrest.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/speed-querying-stackoverflow-data-with-duckdb-ft-michael-hunger.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/sql-ide-safari-harlequin-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/take-flight-with-dbt-and-duckdb-dropping-dev-warehouse-costs-to-zero.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-bad-the-good-and-the-ugly-navigating-the-new-waves-of-machine-learning-ft-adi-polak.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-magic-of-duckdb-extensions-for-data-engineering-and-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-in-100-seconds-by-a-duck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/no-sql-standard-for-7-years.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/one-data-tool-with-all-its-dependencies-duckdb-and-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/panel-fixing-the-data-engineering-lifecycle-coalesce-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-ducking-awesome-analytics-in-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/querying-data-from-s3-with-3-lines-in-your-terminal.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/is-duckdb-the-secret-to-unlocking-your-gis-potential.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/leveraging-duckdb-and-delta-lake-together.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/lies-damn-lies-and-benchmarks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/machine-learning-and-ai-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/monte-carlo-simulations-with-duckdb-ft-james-mcneill.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/more-than-a-vibe-ai-driven-sql-that-actually-works.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-ga-produck-tour.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/faster-data-pipelines-development-with-mcp-and-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/fixing-sql-spaghetti-effective-refactoring-techniques.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/friendly-sql-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/getting-started-with-motherduck-webinar.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/going-beyond-the-dataframe.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-bootstrap-a-data-warehouse-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ibis-one-library-to-query-any-backend.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-vs-pandas-vs-polars-for-python-devs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ducklake-big-data-small-coalesce-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-and-recommenders-a-lightning-fast-synergy-ft-khalil-muhammad.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-dbt-accelerating-the-developer-experience-with-local-power.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-dbt-end-to-end-data-engineering-project-23.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-experiments-peeking-into-the-future-of-analytics-ft-christophe-blefari.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-iceberg-the-future-of-lightweight-data-management.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-python-end-to-end-data-engineering-project-13.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/big-data-is-dead-long-live-hot-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/boosting-datasets-discoverability-for-ai-using-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/build-bigger-with-small-ai-running-small-models-locally.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/building-data-apps-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/can-duckdb-replace-your-data-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/cube-motherduck-data-app-monetization-success.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/cultivating-growth-how-gardyn-scaled-its-data-operations-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/4-lightning-talks-on-practical-ai-workflows-from-notion-1password-motherduck-evidence.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-duck-in-the-hand-is-worth-two-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-new-paradigm-for-data-visualization-with-just-sql-markdown.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/ai-powered-bi-can-llms-really-generate-your-dashboards-ft-michael-driscoll.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/an-evolving-dag-for-the-llm-world-julia-schottenstein-of-langchain-at-small-data-sf-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/big-data-is-dead-analytics-is-alive.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/sql-duckdb-book-form.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/postgres-integration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/quacking-around-the-web.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/react-components-testing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/research/execution-guided-sql-generation.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-OLAP.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/why-choose-parquet-table-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/privacy-policy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/product-analytics-motherduck-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/sql-group-by-examples-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/star-schema-data-warehouse-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/web-assembly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-use-cases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/pandas-dataframes-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/partitioned-writes-parquet-ducklake.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-struct-nested-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/duckdb-vs-sqlite-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/ducklake-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-lake-vs-data-warehouse-vs-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/diagnose-fix-slow-queries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/hack-night.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/index.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/acid-transactions-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/big-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/columnar-storage-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/relational database.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/storage.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/window functions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/auto inference.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/dataset.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/SQL analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/SQLAlchemy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/MotherDuck extension.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/SELECT statement.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/Common Table Expressions (CTEs).md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/getting-started-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/glossary/C/C++.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/sqlmesh.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/superset.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/zenlytic.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/preset.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/puppygraph.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/fabi.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/google-colab.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/hevo.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/hex.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/marimo.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/cube.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/dagster.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/evidence.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/airflow.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/artie.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/astronomer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/census.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem/cloudquery.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/customer-support.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-brief.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-summary-chapter2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-book-summary-chapter3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/duckdb-news.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dosomething-non-profit-tco-cost-savings.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/finqore.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/gardyn.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/reflex-careers-gig-economy-retail.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/trunkrs-same-day-delivery-motherduck-from-redshift.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/udisc-motherduck-sports-management.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/community-and-open-source.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/candidate-privacy.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/atm-com-analytics-costs-sql-expressibility.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dexibit.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/dominik-moritz.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/the-future-of-bi-bi-as-code-duckdb-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/unstructured-analysis-tensorlake-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vector-technologies-ai-data-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/vibe-coding-sql-cursor.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-everybody-hates-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/why-learn-sql-in-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-embeddings-for-semantic-meaning-in-text-and-rag.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-keyboard-shortcuts-for-joyful-querying.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/sql-llm-prompt-function-gpt-models.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streaming-data-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streaming-oracle-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/streamkap-mysql-to-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/summer-data-engineering-roadmap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/tableau-cloud-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/simplifying-iot-analytics-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/six-reasons-duckdb-slaps.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-recap-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-recap.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/small-data-sf-workshops-agenda.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/solving-advent-code-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/spark-ducklake-getting-started.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/search-using-duckdb-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/semantic-layer-duckdb-tutorial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/separating-storage-compute-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-duckdb-vs-dataframe-libraries.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/python-faker-duckdb-exploration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/quacktosql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/read-scaling-preview.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/redshift-files-hunt-for-big-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/scaling-duckdb-with-ducklings.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg to motherduck at motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg_duckdb-postgresql-extension-for-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-duckdb-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pg-mooncake-columnstore.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/pgduckdb-beta-release-duckdb-postgres.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/postgres-duckdb-options.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-window-functions-in-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/perf-is-not-enough.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-kestra-etl-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-open-for-all-with-series-b.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-reflections-sigmod-pods-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-union-orchestration.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-ai-sql-fixit-inline-editing-features.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-congratulates-duckdb-1.0-release.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-dbt-pipelines.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/motherduck-duckdb-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/just-enough-sql-for-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/llm-data-pipelines-prompt-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/local-duckdb-ui-visual-data-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/log-processing-savings-bacalhau-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/in-the-news-podcasts-conferences.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-column-explorer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-fixit-ai-sql-error-fixer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/introducing-motherduck-for-business-analytics.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/json-log-analysis-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/how-to-extract-analytics-from-bluesky.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/galileo-world-geospatial.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/geospatial-for-beginner-duckdb-spatial-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/getting-started-ducklake-table-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/getting-started-gis-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/git-for-data-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/faster-ducks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/fix-outdated-llm-documentation-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/flight-sql-vs-rest-vs-jdbc.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/from-data-lake-to-lakehouse-duckdb-portable-catalog.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-wasm-in-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/ducklake-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/effortless-etl-unstructured-data-unstructuredio-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/estuary-streaming-cdc-replication.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/exploring-stackoverflow-with-duckdb-on-motherduck-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/fabi-ai-llm-prompt-analysis.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-python-e2e-data-engineering-project-part-1.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-text2sql-llm.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-the-great-federator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-versus-pandas-versus-polars.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-three.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-two.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-enterprise-5-key-categories.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-excel-extension.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-on-edge-raspberry-pi.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-puppygraph-graph-model-on-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-one.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-september-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-seven.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-six.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-ten.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-may-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-nine.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-november-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-october-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-july-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-june-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-march-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eight.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-eleven.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-february-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-five.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-four.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-january-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-april-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-august-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-ecosystem-newsletter-december-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/dosomething-motherduck-data-warehouse-ROI.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/dual-execution-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-110-hidden-gems.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-cognee-sql-analytics-graph-rag.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dashboard-e2e-data-engineering-project-part-3.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/duckdb-dbt-e2e-data-engineering-project-part-2.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineer-highlights-PyConDE-2023.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-essential-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineering-toolkit-infrastructure-devops.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-engineers-answer-10-top-reddit-questions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-dec-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-nov-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-warehouse-feature-roundup-oct-2024.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/differential-storage-building-block-for-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/building-data-applications-with-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/building-motherduck-partner-ecosystem.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/cidr-paper-hybrid-query-processing-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/csv-files-persist-duckdb-solution.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/data-app-generator.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/big-data-is-dead.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/bigquery-to-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-snippet-sets-with-motherduck-sharing-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-ducklake-support-motherduck-preview.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-series-seed-and-a.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-sqlite-databases-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-12-on-motherduck-cdw.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-13-on-motherduck-cdw.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/announcing-duckdb-141-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/15-companies-duckdb-in-prod.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-data-in-azure-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/analyze-json-data-using-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/blog/MotherDuck-Visualize-Embeddings-Marimo.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/sheila-sitaram.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/simon-spati.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/stephanie-wang.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/thomas-tfmv-mcgeehan.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/till-dohmen.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/vasilije-markovic.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/oli-dinov.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/jacob-matson.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/jordan-tigani.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/marcos-ortiz.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/mehdi-ouazza.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/brett-griffin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/garrett-obrien.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/hamilton-ulmer.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/aditya-aomani.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/authors/alex-monahan.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 350
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Let me search more specifically for unsupported features and SQL compatibility information:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ not support|disabled|can't use|cannot use|doesn't support|does not support

  [96mğŸ”§ TOOL:[0m Grep â†’ extension.*MotherDuck|MotherDuck.*extension
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": "<4702 chars>",
  [2m  â”‚[0m   "numLines": 28,
  [2m  â”‚[0m   "appliedLimit": 60
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": "<36443 chars>",
  [2m  â”‚[0m   "numLines": 136
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_nsdf4vt5/.kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_nsdf4vt5/.kurt/sources/motherduck.com/glossary/MotherDuck extension.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_nsdf4vt5/.kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_nsdf4vt5/.kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     "content": "---\ntitle: 'Fix Slow BI Dashboards: A Guide to Sub-Second Analytics'\ncontent_type: guide\ndescription: Frustrated by slow BI dashboards? Learn the causes of dashboard latency,\n  from architectural bottlenecks to lakehouse issues. See how MotherDuck helps startups\n  scale.\npublished_date: '2025-10-17T00:00:00'\nsource_url: https://motherduck.com/learn-more/fix-slow-bi-dashboards\nindexed_at: '2025-11-25T10:52:12.236559'\ncontent_hash: 0e692003fc4f1f77\nhas_step_by_step: true\nhas_narrative: true\n---\n\nAs a startup, your data is your compass. But as you scale, that compass begins to spin. Business Intelligence (BI) dashboards that were once snappy now take minutes to load, turning quick checks into coffee breaks. Queries time out. Your team hesitates to ask new questions of the data because the feedback loop is agonizingly slow. You're facing a critical growth inflection point: your analytics stack can no longer keep up with your ambition.\n\nYou know you need something more powerful, but the thought of migrating to a traditional cloud data warehouse, with its complex setup, opaque pricing, and multi-year contracts, is daunting. What if there was a better way? A path that gives you the power of a massive data warehouse without the overhead, designed specifically for teams who move fast?\n\nThis guide is for the technical founders, first data hires, and lean analytics teams feeling this pain. We'll provide a clear framework for evaluating if MotherDuck is the right solution to solve your dashboard latency issues, scale your analytics, and empower your team to make faster, data-driven decisions.\n\n### What You'll Learn in This Guide\n\n**What Causes Dashboard Latency?**We'll uncover why your current database struggles with analytical queries and identify the key architectural bottlenecks.**How Do You Scale Beyond a Single Node?**Discover the limitations of single-node engines for large datasets and how MotherDuck's hybrid architecture provides a straightforward path to scale.**How Can You Achieve Real-Time Insights?**Learn how to move from stale, batch-updated data to sub-second freshness without overloading your production systems.**What Does a Migration Look Like?**Follow a practical, week-by-week plan to migrate a critical workload to MotherDuck and start seeing results in under 30 days.**Is MotherDuck the Right Fit for Your Startup?**Get a clear comparison of when MotherDuck excels and what limitations to consider for your specific use case.\n\n## Why Are My Startup's BI Dashboards So Slow?\n\nIf your dashboards are grinding to a halt, the problem usually isn't a single query but an architectural mismatch. Most startups begin by running analytics on a replica of their production database, often a system like PostgreSQL or MySQL. These are [ Online Transaction Processing (OLTP)](https://motherduck.com/learn-more/what-is-OLAP/) databases, brilliant for handling thousands of small, fast transactions like creating a user or processing an order.\n\nHowever, analytical queries are a completely different beast. They are **Online Analytical Processing (OLAP)** workloads, which involve scanning millions or billions of rows across a few columns to calculate aggregations. Asking an OLTP database to perform heavy OLAP queries is like asking a race car to haul lumber. It wasn't built for the job.\n\nThe core issue lies in how the data is stored on disk. OLTP databases are **row-oriented**, meaning they store all the data for a single record together. When your BI tool asks for the total number of users by `plan_type`\n\nfrom a table with 50 columns, a row-oriented database is forced to read all 50 columns for every single row, even though it only needs one. This wastes an enormous amount of I/O.\n\nThis is where [ columnar storage](https://motherduck.com/learn-more/columnar-storage-guide/), the foundation of modern analytical databases like DuckDB, makes a big difference. A columnar database stores all values from a single column together. When you run the same query, it reads\n\n*only*the\n\n`plan_type`\n\ncolumn, dramatically reducing the amount of data scanned. This fundamental difference is often the key to unlocking sub-second query performance.### How Do Row and Columnar Databases Compare for Analytics?\n\n| Feature | Row-Oriented (e.g., PostgreSQL) | Column-Oriented (e.g., DuckDB/MotherDuck) |\n|---|---|---|\nPrimary Use Case | OLTP: Fast reads and writes of individual records. | OLAP: Fast scans and aggregations over large datasets. |\nData Layout | Stores all values for a single record contiguously. | Stores all values for a single column contiguously. |\nQuery Performance | Slow for analytical queries that only need a few columns. | Extremely fast for analytical queries. It only reads the required columns. |\nCompression | Less effective, as it stores mixed data types in each row. | Highly effective, leading to smaller storage footprint and faster scans. |\n\n## What Happens When My Analytics Queries Need to Scan Billions of Rows?\n\nAs your startup succeeds, your data volume explodes. The single-node analytics database that was once a perfect solution starts to show its limits. While an engine like DuckDB is incredibly fast and can even process datasets larger than RAM by [\"spilling\" intermediate results to disk](https://duckdb.org/2024/07/09/memory-management.html), it is ultimately constrained by the resources of a single machine.\n\nWhen you run a complex query with multiple joins and aggregations on billions of rows, the intermediate data generated can overwhelm the system's memory. This can lead to slow queries as the engine constantly writes to and reads from disk, or worse, an [ OutOfMemoryException](https://duckdb.org/docs/stable/guides/troubleshooting/oom_errors.html) that kills the query entirely.\n\nFurthermore, a single node has a finite capacity for **concurrency**. As more team members connect with BI tools, the CPU and I/O resources get saturated, and everyone's queries slow down. This is the practical ceiling of a single-node engine. This is precisely the problem MotherDuck was built to solve. It extends the lightning-fast local experience of DuckDB with a serverless cloud backend, giving you a straightforward path to scale.\n\n## How Does MotherDuck's Hybrid Architecture Deliver Speed and Scale?\n\nMotherDuck introduces a novel architecture that gives you the best of both worlds: the zero-latency feel of local development and the on-demand power of the cloud. This is achieved through a few key concepts.\n\n### What is Dual Execution and Why Does it Matter?\n\nThe magic behind MotherDuck is its [ Dual Execution query planner](https://www.cidrdb.org/cidr2024/papers/p46-atwal.pdf). Instead of forcing you to move all your data to the cloud, it intelligently pushes the computation to where the data lives, minimizing network latency and data transfer costs.\n\n**Local Query:**If you query a CSV file on your laptop, the query runs entirely on your local DuckDB instance. The result is instantaneous.**Cloud Query:**If you query a large table stored in MotherDuck, the work is routed to a dedicated, serverless compute instance (a \"Duckling\") in the cloud.**Hybrid Query:**This is where it gets powerful. If you join a local file with a large cloud table, the planner is smart enough to push filters down to your local machine first. It processes the local file, sends only the small, filtered result to the cloud, and then performs the final join. This makes complex queries incredibly efficient.\n\n### How Do You Handle High Concurrency from BI Tools?\n\nA common and dangerous blind spot for startups is how BI tools handle concurrency. Tools like Tableau or Looker often use a single service account, funneling queries from dozens of users through one connection. This can quickly overwhelm a database.\n\n## Case Study: How Layers Solved Its Concurrency Bottleneck\n\nThe SaaS company\n\n[faced this exact problem. Their analytics, running on PostgreSQL, were overwhelmed when their BI tool masked 73 simultaneous users behind a single service account. This exhausted their connection pool and caused a high rate of query timeouts.]LayersAfter migrating to MotherDuck, they used the\n\nRead Scalingfeature. By connecting their BI tool with a special Read Scaling Token, user queries were automatically distributed across a pool of 16 dedicated, read-only DuckDB replicas. This instantly parallelized the workload, providing each user session with isolated compute. The result was a dramatic improvement in stability, with BI query timeouts dropping to virtually zero.\n\nBy issuing a unique, read-only token for each BI integration and using the `session_hint`\n\nparameter, you can ensure user queries are [intelligently load-balanced](https://motherduck.com/docs/key-tasks/authenticating-and-connecting-to-motherduck/read-scaling/), providing a consistently fast experience for everyone.\n\n## Why Might Lakehouse Architectures Have High Latency Issues?\n\nThe lakehouse architecture, which combines a data lake's flexibility with a data warehouse's performance, is a popular choice. However, startups often find that it comes with a significant \"latency tax,\" especially for interactive queries.\n\nThe primary culprit is the [ \"small files problem.\"](https://dev.to/thedanicafine/the-apache-iceberg-small-file-problem-1k2m) Streaming ingestion jobs often write thousands of tiny files to cloud object storage like Amazon S3. To run a query, the engine must make thousands of separate, high-latency HTTP requests just to read the metadata and find the right files. Each request can take 100-200 milliseconds, adding up to seconds of delay before a single byte of data is even processed.\n\nA benchmark demonstrated this vividly: a query scanning a 5 GB Apache Iceberg table with thousands of small files took **5.093 seconds** to run. The same query on MotherDuck's **DuckLake** format, which uses a database-backed index to avoid scanning S3 for metadata, returned in just **0.146 seconds**, a [ 34x speedup](https://motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats/).\n\n### How Does DuckLake Compare to Apache Iceberg?\n\n| Feature | Apache Iceberg (Standard) | MotherDuck DuckLake |\n|---|---|---|\nMetadata Storage | Stored as files (manifests) in the object store (e.g., S3). | Stored in a fast, transactional database, separate from data files. |\nQuery Planning | Requires multiple sequential S3 reads to discover which data files to scan. | A single SQL query to the catalog database returns the exact list of files. |\nSmall File Handling | Creates many small data and metadata files, requiring costly maintenance jobs (compaction). | Dramatically reduces metadata overhead and the need for frequent compaction. |\nPerformance | High latency for interactive queries due to metadata discovery overhead. |\n34x faster |\n\n## How Can I Get Real-Time Data Without Overloading My Production Database?\n\nIs your sales dashboard always 30 minutes out of date? This frustrating lag is almost always due to a reliance on traditional, batch-based ETL (Extract, Transform, Load) jobs that run on a schedule. For a startup that needs to react instantly to user behavior, this is no longer acceptable.\n\nThe modern solution is a streaming architecture built on **Change Data Capture (CDC)**. Instead of querying your production database tables (which adds heavy load), CDC tools like Estuary or [Streamkap](https://motherduck.com/blog/streamkap-mysql-to-motherduck/) monitor the database's internal transaction log. They capture every insert, update, and delete as it happens and stream these events to MotherDuck in near real-time, often with [ less than 100ms of latency](https://estuary.dev/destination/motherduck/).\n\nThis approach provides two critical benefits:\n\n**Sub-Minute Freshness:**Your analytics are always synchronized with reality.**Zero Production Impact:**It completely isolates your analytical workload, ensuring that heavy queries never slow down your customer-facing application.\n\n## What Are the Signs My Startup Has Outgrown Its Current Analytics Stack?\n\nThe signs that you've outgrown your analytics stack are both technical and cultural. Recognizing them early can save you from months of frustration and slow decision-making.\n\n**Technical Red Flags:**\n\n**High Query Latency:**Your p95 query latency for interactive dashboards consistently exceeds 2-5 seconds. Research shows that[beyond 1 second, users lose their flow of thought, and beyond 10 seconds, you lose their attention entirely](https://www.nngroup.com/articles/powers-of-10-time-scales-in-ux/).**Rising Error Rates:**You see an increase in query timeouts, connection failures, or application errors related to database load.**Stale Data:**Business teams complain that dashboards are always out of date, indicating that nightly batch jobs are no longer sufficient.\n\n**Business and Cultural Red Flags:**\n\n**Dashboard Abandonment:**Analysts and business users stop using the BI tool because it's \"too slow.\"**Rising Costs:**Your infrastructure bill is growing faster than the value you're getting from your data, often due to over-provisioning to compensate for poor performance.**Slow Product Velocity:**Engineers spend more time optimizing the database than building features, and product managers can't get timely data to inform their roadmap.\n\nCompanies that see these signs can achieve significant improvements by migrating. **Finqore** reduced 8-hour financial data pipelines to just 8 minutes, while **uDisc** cut query times from minutes to seconds, leading to [a significant lift in daily active analysts](https://motherduck.com/case-studies/).\n\n## How Do I Plan a Migration to MotherDuck?\n\nMigrating to MotherDuck doesn't have to be a massive, disruptive project. You can see value in under 30 days by following a straightforward, phased approach focused on a single, high-pain workload.\n\n### Your 30-Day Migration Roadmap\n\n| Week | Key Tasks | Success Criteria |\n|---|---|---|\nWeek 1: Connect & Ingest | - Create your MotherDuck account and generate service tokens. - Install the DuckDB SDK and connect via `ATTACH 'md:';` . - Perform an initial bulk load of a target dataset (e.g., one large, slow table) into a new MotherDuck database. | - You can successfully query your data in MotherDuck from your local machine and BI tool. - Historical data for one target workload is fully loaded. |\nWeek 2: Mirror a Pilot Workload | - Select one high-pain, low-risk dashboard to migrate. - Set up an incremental CDC pipeline (e.g., using Estuary) to keep MotherDuck in sync with the source. - Re-create the dashboard's data model to point to MotherDuck. | - The mirrored dashboard in MotherDuck is live and updating in near real-time. - p95 query latency is under 2 seconds. |\nWeek 3: Validate & Optimize | - Share the new dashboard with a small group of business users for feedback. - Monitor query performance and cost. - Use optimizations like Read Scaling tokens for the BI tool connection. | - Users confirm the new dashboard is significantly faster and accurate. - The projected cost for the pilot workload is lower than the legacy system. |\nWeek 4: Cut-Over & Expand | - Officially switch all users to the new MotherDuck-powered dashboard. - Decommission the old data models and pipelines for the migrated workload. - Plan the migration of the next set of analytical workloads. | - 100% of users for the pilot workload are using the new dashboard. - A prioritized backlog for the next migration is created. |\n\n## What Are the Limitations of MotherDuck I Should Consider?\n\nNo tool is perfect for every situation. Being honest about limitations is key to making the right choice. MotherDuck is rapidly evolving, but startups should be aware of a few current constraints:\n\n**Regional Availability:**MotherDuck currently operates in the AWS[us-east-1](https://motherduck.com/docs/concepts/architecture-and-capabilities/)and recently started in[eu-central-1](https://motherduck.com/blog/motherduck-in-europe/), hosted in Frankfurt. Teams with strict data residency requirements outside this region will need to consider this.**Partial DuckDB SQL Coverage:**MotherDuck does not yet support the full range of DuckDB's SQL features. Notably, custom Python/native User-Defined Functions (UDFs), server-side`ATTACH`\n\nto other databases (like Postgres), and custom extensions are not yet available. The common workaround is to perform these specific transformations upstream before loading data into MotherDuck.\n\n## Conclusion: Stop Waiting, Start Analyzing\n\nFor a startup, speed is everything: speed to market, speed to insight, and speed of execution. A slow analytics stack is a direct drag on all three. You don't need the complexity and cost of a legacy data warehouse, nor can you afford the performance bottlenecks of an overloaded production database.\n\nMotherDuck offers a third way, purpose-built for the scale and agility of a modern startup. By combining the raw speed of a local-first engine with the on-demand scale of the cloud, it eliminates the trade-offs that have held analytics teams back for years. If your dashboards are slow and your team is frustrated, it's time to evaluate a new approach.\n\n[Ready to experience sub-second analytics? Start building on MotherDuck's free tier today.](https://motherduck.com/startups/)\n\nStart using MotherDuck now!\n\n## FAQS\n\n### How much does MotherDuck cost for a startup?\n\nMotherDuck's pricing is designed for startups. It includes a generous free tier for smaller projects and two pay-as-you-go plans that charge only for storage used and compute-seconds executed. There are no idle cluster charges or minimums, which can lead to [ 70-90% cost savings](https://motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck/) compared to traditional data warehouses.\n\n### Can MotherDuck handle our data volume?\n\nYes. Analysis shows that [over 95% of startup databases are smaller than 1 TB](https://hemantkgupta.medium.com/insight-from-paper-motherduck-duckdb-in-the-cloud-and-in-the-client-e4a73da9dbec), a size range where MotherDuck excels. The hybrid architecture is designed to scale from megabytes on your laptop to tens of terabytes in the cloud, ensuring you have a growth path.\n\n### How does MotherDuck compare to Snowflake or BigQuery for a startup?\n\nWhile large warehouses are powerful, they often come with significant operational complexity and cost overhead that can be burdensome for a startup. MotherDuck offers [a simpler, more cost-effective path](https://motherduck.com/learn-more/modern-data-warehouse-playbook/). Its key differentiators are the serverless, per-second billing model and the unique \"local-first\" hybrid architecture, which provides an excellent development experience and eliminates network latency for many common analytical tasks.\n\n### How do startups decide if MotherDuck fits their analytics stack?\n\nA startup should consider MotherDuck if they experience slow BI dashboards, rising query timeouts, and stale data. It's a strong fit for teams that have outgrown a single database (like PostgreSQL) but want to avoid the cost and complexity of a traditional data warehouse. MotherDuck excels with data volumes from gigabytes to tens of terabytes and is ideal for building fast, interactive analytics without a large data engineering team.\n\n### Why do dashboards struggle when scanning billions of rows?\n\nWhen scanning billions of rows, single-node engines like DuckDB can be constrained by the memory and I/O of a single machine. Complex queries generate large intermediate results that can exceed available RAM, forcing the engine to \"spill\" to a slower disk, which increases latency. High user concurrency can also saturate the CPU, causing all queries to slow down.\n\n### Why do lakehouse architectures often have high query latency?\n\nLakehouse latency is often caused by the **\"small files problem.\"** Streaming jobs write thousands of tiny files to object storage like S3. To run a query, the engine must make many high-latency network requests just to read the metadata and find the right files, adding seconds of delay before the query even starts processing data.\n\n### What are the main causes of slow BI dashboards?\n\nThe primary cause is an architectural mismatch. Startups often run heavy analytical (OLAP) queries on their production (OLTP) database, like PostgreSQL. These row-oriented databases are inefficient for analytics, as they must read entire records instead of just the needed columns. As data grows, this leads to high I/O, slow queries, and dashboard latency.\n\n### How does data modeling impact dashboard performance?\n\nIt has a massive impact. Using a [ star schema](https://motherduck.com/learn-more/star-schema-data-warehouse-guide/), which organizes data into a central \"fact\" table and surrounding \"dimension\" tables, is a proven technique for accelerating analytical queries in columnar databases.",
  [2m  â”‚[0m     "numLines": 189,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 189
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_nsdf4vt5/.kurt/sources/motherduck.com/glossary/MotherDuck extension.md",
  [2m  â”‚[0m     "content": "---\ntitle: MotherDuck extension\ncontent_type: tutorial\nsource_url: https://motherduck.com/glossary/MotherDuck extension\nindexed_at: '2025-11-25T20:08:29.934950'\ncontent_hash: 5315c271fb5d98b9\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n# MotherDuck extension\n\n_[Back to DuckDB Data Engineering Glossary](https://motherduck.com/glossary/)_\n\nThe [MotherDuck extension](https://github.com/duckdb/duckdb/tree/master/extension/motherduck) is a component of [DuckDB](https://duckdb.org/) that enables seamless integration with the [MotherDuck](https://motherduck.com/) cloud service. This extension allows users to connect their local DuckDB instance to MotherDuck's serverless analytics platform, facilitating hybrid query execution across local and cloud environments. By using the MotherDuck extension, data professionals can leverage the power of cloud computing while maintaining the flexibility of working with data on their local machine. The extension supports features such as remote data access, synchronized schema management, and the ability to push computations to the cloud when needed, enhancing DuckDB's capabilities for handling larger datasets and more complex analytics tasks.\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 22,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 22
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_nsdf4vt5/.kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     "content": "---\ntitle: 'MotherDuck: A Faster, Cost-Effective BigQuery Alternative'\ncontent_type: guide\ndescription: Searching for a BigQuery alternative? See how MotherDuck's serverless\n  platform, built on DuckDB, offers a faster, more cost-effective solution for medium\n  data workloads.\npublished_date: '2025-10-13T00:00:00'\nsource_url: https://motherduck.com/learn-more/bigquery-alternative-motherduck\nindexed_at: '2025-11-25T10:52:10.668191'\ncontent_hash: 76a2a9e273d16956\nhas_step_by_step: true\nhas_narrative: true\n---\n\nYou're running a lean data team with 500GB of data, but your BigQuery bill just hit four figures. You are not processing petabytes, so why does it cost so much? You find yourself spending more time optimizing BigQuery costs than building features. This experience is common for\nteams whose data scale has not yet reached the \"big data\" threshold where a [cloud data warehouse like BigQuery](https://motherduck.com/learn-more/cloud-data-warehouse-startup-guide/) truly shines. For these teams, the cost and complexity can feel disproportionate to the value.\n\nThe [modern data stack](https://motherduck.com/learn-more/modern-data-warehouse-playbook/) offers more than just monolithic data warehouses. A new architectural pattern is emerging for the \"medium data\" world, covering the gigabyte-to-terabyte scale where many businesses operate. Without understanding the architectural and pricing differences between these patterns, teams can overspend, introduce unnecessary complexity, and face slow interactive query performance. Choosing the wrong tool for your scale can burn your budget and kill productivity.\n\n### What You'll Learn\n\nThis guide provides a practical framework for evaluating if your workload is a good fit for a modern, hybrid architecture. You will learn:\n\n- Why BigQuery can be surprisingly expensive for medium-sized data by deconstructing its pricing model.\n- The core architectural difference between MotherDuck's hybrid model and BigQuery's distributed, serverless model, using the concept of \"Compute Locality.\"\n- A direct comparison of pricing, performance, and ideal use cases for MotherDuck and BigQuery.\n- A decision framework, including known trade-offs, to determine if MotherDuck can replace or complement BigQuery for your specific needs.\n- A step-by-step playbook for migrating data from BigQuery to MotherDuck.\n\n## Understanding BigQuery Pricing: Why Your Bill is So High for Medium Data\n\nTo understand BigQuery's cost structure, it is essential to grasp its core design principle. BigQuery is a massively parallel processing (MPP) system built to scan petabytes of data. Its pricing model reflects this architecture, prioritizing throughput for enormous datasets over latency or cost-efficiency for smaller ones.\n\nThe primary driver of [BigQuery's on-demand pricing](https://cloud.google.com/bigquery/pricing) is not query duration or complexity, but the volume of data scanned from disk. When you execute `SELECT COUNT(1) FROM my_table`\n\n, BigQuery may have to read the entire table from storage to give you the answer. If your table is 500GB, you are billed for scanning 500GB, even for a query that returns a single number. This billing model creates several common cost traps for teams working with medium data.\n\nFirst, BigQuery enforces a 10MB minimum charge for every query against a table. While this seems trivial, it accumulates rapidly. A business intelligence dashboard with 20 charts that each trigger a query can result in 20 separate 10MB charges every time a user refreshes the page. For ad-hoc analysis and frequent dashboard use, these small charges can compound into a significant monthly expense.\n\nSecond, optimizing costs requires significant and continuous data engineering effort. To avoid full table scans, data must be strategically partitioned and clustered. Partitioning by a date column, for example, allows queries with a `WHERE`\n\nclause on that date to scan only the relevant partitions, drastically reducing costs. However, this is not an automatic process. It requires forethought during schema design and consistent maintenance. If tables are not properly optimized for common query patterns, you pay a premium for \"lazy\" full table scans, effectively negating the potential cost benefits. This \"partitioning tax\" places an operational burden on lean teams who may lack the dedicated resources for constant performance\ntuning.\n\nFinally, the compute model, abstracted as \"slots,\" can be opaque. In the on-demand model, you share a large pool of slots with other customers, with performance varying based on contention. For predictable performance, you can purchase dedicated slots through a flat-rate plan, but this is a substantial fixed cost, often starting in the thousands of dollars per month, which is prohibitive for many smaller teams.\n\n## How MotherDuck's Hybrid Architecture Solves the BigQuery Cost Problem\n\nNow that we understand how BigQuery's architecture drives its cost model, let's explore a fundamentally different approach. The [MotherDuck](https://motherduck.com/) and [DuckDB](https://duckdb.org/) ecosystem is not a smaller version of a traditional data warehouse. It is a new architecture built on a different set of principles.\n\nThe core of this architecture is DuckDB, an open-source, in-process analytical database. \"In-process\" means the database engine runs inside the application that is querying the data (for example, your Python script or a local command-line interface). This eliminates network latency for local operations and simplifies the development experience to feel more like SQLite than a distributed system.\n\nMotherDuck extends this local-first experience with a serverless DuckDB service in the cloud. It provides persistent storage, scalable compute, and collaboration features, but its power comes from its unique relationship with the client-side DuckDB engine.\n\n### Compute Locality: MotherDuck\u2019s Key Architectural Advantage\n\nThis architecture introduces a powerful mental model: **compute locality**. Traditional data warehouses like BigQuery operate on a principle of high \"data gravity.\" You must first move all your data into the warehouse's centralized storage. Then, to query it, your client sends a request over the network to a massive, centralized compute cluster that processes the data and sends results back.\n\nThe data flow looks like this:\n`Your Laptop -> BQ Client -> Network -> BQ Compute Cluster -> BQ Storage`\n\n\nThe MotherDuck and DuckDB model flips this. It pushes compute to where the data lives. If you have a Parquet file on your laptop, DuckDB processes it locally. If you have data in MotherDuck's cloud storage, the query is routed to MotherDuck's serverless backend.\n\nThe real power emerges with the [hybrid execution model](https://motherduck.com/docs/key-tasks/running-hybrid-queries/), which allows a single query to join data across these locations. For example, you can join a local CSV file with a multi-gigabyte table stored in MotherDuck. The query optimizer intelligently decides the\nmost efficient way to execute the join, often by shipping the smaller, local dataset to the cloud for processing next to the larger remote dataset.\n\nThis hybrid path looks fundamentally different:\n\n**Local Query**:`Your Laptop (DuckDB Compute) -> Local File`\n\n**Hybrid Query**:`Your Laptop (DuckDB Compute) -> Network -> MotherDuck Storage`\n\n(joining local and remote data)\n\nThis principle of compute locality gives developers immense flexibility. You can analyze production data in the cloud while enriching it with new data that only exists on your machine, all within a single SQL statement.\n\n### How This Differs from a Scaled-Up PostgreSQL\n\nFor many teams working with medium data, the default choice is not a massive cloud warehouse but an oversized PostgreSQL instance on a service like Amazon RDS. While familiar, this approach comes with its own set of challenges for analytics. PostgreSQL is a row-oriented, transactional (OLTP) database. Its storage format is optimized for quickly retrieving or updating entire rows, which is ideal for application backends.\n\nAnalytical queries, however, typically read a few columns from many rows (for example, calculating the average of a `sales_amount`\n\ncolumn). In a row-oriented system, the database must read the entire row for every record in the table, even though it only needs data from\none column. This leads to significant I/O inefficiency when using PostgreSQL for analytics.\n\nDuckDB, by contrast, is a columnar database. It stores all values for a single column together on disk. When you run an analytical query, it only reads the data for the columns referenced in that query. This dramatically reduces the amount of data read from disk, leading to orders-of-magnitude performance improvements for scans and aggregations. Furthermore, columnar storage enables better data compression, reducing the storage footprint.\n\nOperationally, the MotherDuck and DuckDB model is also simpler. There is no server to provision or manage, no extensions to install for analytical functions, and no need to run `VACUUM`\n\ncommands to reclaim storage and prevent performance degradation. It is an architecture designed specifically for analytics, avoiding the overhead of retrofitting a transactional database for analytical workloads.\n\n## Head-to-Head: MotherDuck vs. BigQuery on Key Features\n\nThis hybrid architecture leads to a completely different set of trade-offs in performance, cost, and complexity. The following table provides a direct comparison of the core concepts between the two platforms.\n\n| Feature | Google BigQuery | MotherDuck |\n|---|---|---|\nCore Architecture | Massively Parallel Processing (MPP), distributed system. Centralized storage and compute. | Hybrid model. In-process client-side engine (DuckDB) with a serverless cloud backend. Compute follows the data. |\nIdeal Data Scale | 10s of terabytes to petabytes | Gigabytes to 10s of terabytes |\nPrimary Workload | Large-scale batch ETL/ELT, infrequent reporting on huge datasets, enterprise-wide data warehousing. | Interactive BI dashboards, ad-hoc exploratory analysis, embedded analytics, and data-intensive applications. |\nLatency Profile | Seconds to minutes. Optimized for high throughput and scanning massive volumes of data. | Sub-second to seconds. Optimized for low-latency, interactive queries. |\nCompute Model | Serverless, abstracted into \"slots.\" On-demand (shared pool) or Flat-Rate (dedicated capacity). | Serverless cloud compute combined with local compute on the client machine. |\nData Eng. Overhead | High. Requires careful partitioning and clustering to manage costs and performance. | Low. No servers, clusters, or partitions to manage. Simplified data loading and schema management. |\nKey Differentiator | Ability to query petabyte-scale datasets. Deep integration with the Google Cloud Platform ecosystem. | Hybrid execution, joining local and remote data. Fast, interactive query performance on medium-sized data. |\n\nThe developer experience also differs. While both platforms use SQL, DuckDB's dialect is largely compatible with PostgreSQL, which is familiar to many developers. BigQuery uses its own [Standard SQL dialect](https://cloud.google.com/bigquery/docs/introduction-sql). Getting started with MotherDuck is as simple as installing DuckDB and using a connection string in your preferred client, whereas setting up BigQuery often involves navigating GCP's IAM permissions, projects, and billing accounts.\n\n## A Real-World Cost Breakdown: MotherDuck vs. BigQuery\n\nThe architectural differences naturally lead to very different pricing models. Understanding these is key to choosing the right tool for your workload.\n\nBigQuery primarily offers two models:\n\n**On-Demand**: You pay per terabyte of data scanned by your queries (for example, $6.25 per TB in`us-central1`\n\nas of late 2023). This is simple to start with but can lead to unpredictable and high costs for workloads with many exploratory or unoptimized queries.**Flat-Rate**: You pay a fixed monthly fee for a dedicated amount of compute capacity (slots). This provides predictable costs and performance but comes with a high price tag, making it suitable only for large organizations with heavy, consistent workloads.\n\nMotherDuck's pricing model is designed for flexibility and cost-efficiency at a smaller scale. It has three main components:\n\n**Storage**: A simple, low-cost fee per gigabyte per month for data stored in MotherDuck.**Compute**: A usage-based model where you pay for query execution time. This aligns costs directly with usage, so you do not pay for idle compute.**Egress**: A standard fee for data transferred out of the service.\n\nYou can view the full details on our [pricing page](https://motherduck.com/learn-more/data-warehouse-tco/). The most significant difference is the idle cost. With BigQuery's on-demand model, the idle cost is\nlow (just storage), but any query activity, no matter how small, can trigger large scan costs. With MotherDuck, the idle cost is also just storage, but the usage-based compute ensures that costs scale smoothly with actual work performed, not with the size of the underlying tables.\n\n### Scenario-Based Comparison\n\nLet's model a realistic workload for a lean, 10-person team with a 200GB dataset. Their primary use case is a BI tool that runs 100 complex queries per day. In BigQuery, these queries are not perfectly optimized and scan an average of 10GB of data each.\n\n**BigQuery On-Demand Calculation**:- Data scanned per day: 100 queries * 10 GB/query = 1,000 GB = 1 TB\n- Data scanned per month: 1 TB/day * 30 days = 30 TB\n- Estimated monthly query cost: 30 TB * $6.25/TB =\n**$187.50** - Storage cost (200GB): ~\n**$4.60** **Total: ~$192.10/month**\n\n\nThis calculation assumes every query is reasonably optimized. A single poorly written query that scans the entire 200GB table would cost $1.25. If 10 such queries are run by analysts in a day, that adds $12.50 to the daily bill, or an extra $375 per month. The cost is volatile and sensitive to user behavior.\n\n**MotherDuck Calculation**:- Modeling the exact compute cost is more complex as it depends on query runtime, but for interactive workloads on a 200GB dataset, queries typically complete in seconds.\n- A comparable workload would likely fall within MotherDuck's standard usage tiers, which are designed to be significantly more cost-effective for this scale than BigQuery's scan-based pricing.\n- Storage cost (200GB): ~\n**$5.00** - The key benefit is cost predictability. The compute cost is tied to actual processing, not the size of data on disk, insulating the budget from the effects of unoptimized analytical queries. For bursty, interactive workloads, this model provides a much lower and more predictable monthly bill.\n\n\n## When to Choose MotherDuck (And When to Stick with BigQuery)\n\nWith a clear understanding of the technology and costs, the crucial question remains: which one is right for your use case? The choice is about right-sizing your data stack to your specific scale and workload.\n\n**Choose MotherDuck if:**\n\n- Your total data size is in the gigabytes to low tens of terabytes range.\n- Your primary need is low-latency, interactive queries for dashboards, such as powering the CEO's daily metrics dashboard that joins sales data from Stripe with product usage data from S3.\n- Your team values simplicity, a fast development cycle, and a Postgres-like developer experience.\n- Your workload is \"bursty\" with periods of inactivity, and you want to avoid paying for idle compute.\n- You need to analyze data from multiple sources, including local files or object storage, in a single query.\n\n**Stick with BigQuery if:**\n\n- Your data is in the hundreds of terabytes to petabytes range.\n- Your primary workload is large-scale, batch ETL/ELT that can take minutes or hours to run.\n- You are deeply integrated into the Google Cloud Platform ecosystem and rely on services like Vertex AI or Dataflow.\n- You have a dedicated data platform team to manage schemas, optimize query costs, and administer the platform.\n\n### Benefits by Role\n\nDifferent members of a data team will experience the benefits of this architectural choice differently.\n\n**For the Data Engineer:**You can simplify your stack. Replace complex Airflow DAGs that shuttle data between systems with simple SQL queries that read directly from object storage. You can also test dbt models locally with DuckDB against production data in MotherDuck without incurring high scan costs for every test run.**For the Data Analyst:**You can achieve sub-second query times on your Metabase or Tableau dashboards. You can stop waiting minutes for queries to return and explore data interactively without asking an engineer to partition a table first.**For the Application Developer:**You can build snappy, customer-facing analytics features, like a \"Your Year in Review\" page, with a simple Python or Node.js client and no new infrastructure to manage. The ability to use the same DuckDB engine in development and production simplifies testing and deployment.\n\n### Known Trade-offs and When MotherDuck Isn't the Right Fit\n\nTo make an informed decision, it is critical to understand the limitations of the MotherDuck and DuckDB architecture. Acknowledging trade-offs is a hallmark of technical authority, and no single tool is perfect for every job.\n\n**High-Throughput Transactional Workloads (OLTP):**MotherDuck and DuckDB are analytical databases (OLAP). They are not designed to be the primary backend for an application that requires thousands of concurrent, low-latency writes and updates per second. For that, a traditional OLTP database like PostgreSQL or MySQL remains the best choice.**Fine-Grained Row-Level Security:**At present, the platform's security model is based on database-level permissions. Organizations that require complex, fine-grained access controls, such as restricting user access to specific rows within a table based on their role, may find the current capabilities insufficient.**Massive User Concurrency:**While excellent for a team of analysts or for powering an embedded analytics feature for a moderate number of users, the architecture is not currently designed to serve thousands of simultaneous, public-facing analytical queries, such as on a major e-commerce website. Workloads requiring that level of concurrency are better suited for platforms built specifically for that scale.\n\n## Migration Playbook: How to Move Data from BigQuery to MotherDuck\n\nIf you have been convinced by the argument but are left wondering \"what now?\", this section provides a high-level, actionable playbook for migrating your data. The process is straightforward and demonstrates the platform's focus on simplicity.\n\n**Step 1: Export Data from BigQuery to Google Cloud Storage (GCS)**\n\nThe most efficient way to get data out of BigQuery is to export it to a columnar format like [Parquet](https://parquet.apache.org/) in an object storage bucket. Parquet is highly compressed and performs exceptionally well with DuckDB. You can do this with a single SQL command in the BigQuery console.\n\nCopy code\n\n```\nEXPORT DATA\nOPTIONS(\nuri='gs://your-gcs-bucket/path/to/export/data_*.parquet',\nformat='PARQUET',\noverwrite=true\n) AS\nSELECT * FROM your_project.your_dataset.your_table;\n```\n\n\nThis command will export the contents of `your_table`\n\ninto one or more Parquet files in the specified GCS bucket.\n\n**Step 2: Transfer Data to a Compatible Object Store**\n\nWhile DuckDB can read directly from GCS, for loading data into MotherDuck's managed storage, it is often easiest to use a cloud object store like Amazon S3 or Cloudflare R2. You can use a tool like [ rclone](https://rclone.org/) or cloud-native transfer services to move the Parquet files from your GCS bucket to an S3 bucket.\n\n**Step 3: Load Data into MotherDuck**\n\nOnce your data is in S3, loading it into a MotherDuck table is a simple `CREATE TABLE AS`\n\nstatement. From your local DuckDB CLI or Python script connected to MotherDuck, you can run the following command. You will first need to configure DuckDB with your AWS credentials to access the S3 bucket. For more details, see our documentation on [loading data from S3](https://motherduck.com/docs/integrations/cloud-storage/amazon-s3/).\n\nCopy code\n\n```\nCREATE TABLE my_new_table AS\nSELECT * FROM 's3://your-s3-bucket/path/to/export/data_*.parquet';\n```\n\n\nDuckDB will automatically infer the schema from the Parquet files, parallelize the download from S3, and load the data efficiently into your new table in MotherDuck's managed storage. This simple, SQL-based approach avoids complex ingestion pipelines and allows you to move terabytes of data with just a few commands.\n\n## The Right Tool for Your Data Scale\n\nBigQuery is a powerful and impressive technology, but its architecture and pricing are optimized for true \"big data.\" For the vast and growing world of \"medium data,\" from gigabytes to tens of terabytes, its model can introduce unnecessary cost and complexity. MotherDuck, built on the fast-growing DuckDB ecosystem, offers an architecture designed specifically for this scale. Its focus on compute locality, developer experience, and interactive performance provides a simpler, faster, and more cost-effective solution for many common analytical workloads.\n\nThe choice is not about finding a universal replacement, but about right-sizing your tools. Use the platform whose cost model and performance profile align with your data's scale and your application's requirements. The flexibility of the DuckDB ecosystem is its portability, speed, and analytical power that runs anywhere from a laptop to the cloud, giving lean teams the ability to build powerful data applications without the overhead of a massive data platform.\n\nTo see the difference for yourself:\n\n[Sign up for a free MotherDuck account](https://app.motherduck.com/?auth_flow=signup).- Use the playbook above to load one of your medium-sized Parquet files.\n- Connect your favorite BI tool and experience the interactive performance firsthand.\n\nStart using MotherDuck now!\n\n## FAQS\n\n### Is MotherDuck a direct replacement for BigQuery?\n\nMotherDuck can be a powerful and cost-effective replacement for BigQuery for workloads in the gigabyte-to-tens-of-terabytes range, especially those focused on interactive analytics and business intelligence. It is not a replacement for petabyte-scale batch processing, where BigQuery's MPP architecture excels.\n\n### How does MotherDuck handle concurrency compared to BigQuery?\n\nBigQuery is designed for high concurrency on massive datasets and can handle thousands of simultaneous queries through its slot-based architecture. MotherDuck is optimized for the concurrency needs of data teams and embedded analytics applications, delivering low-latency responses for dozens to hundreds of concurrent users. It is not currently designed for massive public-facing applications with thousands of simultaneous queries.\n\n### Why is my BigQuery bill so high when my data isn\u2019t that big?\n\nYour BigQuery bill is likely high due to the ['big data tax'](https://motherduck.com/learn-more/modern-data-warehouse-playbook/) inherent in its pricing model, which is based on the volume of data scanned, not the size of your result. A simple query can trigger a full table scan, billing you for hundreds of gigabytes, and a minimum 10 MB charge applies to every query, which adds up quickly on dashboards. Modern cloud data warehouse solutions like MotherDuck are built on a different architecture to avoid these cost traps for medium-sized data.\n\n### Is there a simpler, cheaper alternative to a full-blown data warehouse for a startup or small team?\n\nAbsolutely. For teams working with gigabytes to terabytes of data, a new architectural pattern is emerging that avoids the cost and complexity of massive warehouses like BigQuery. A modern cloud data warehouse solution like MotherDuck, built on the fast, in-process DuckDB engine, provides a serverless, cost-effective platform tailored for the scale of most startups and small teams.\n\n### How does pricing for modern analytics platforms compare to BigQuery\u2019s on-demand cost model?\n\nUnlike BigQuery\u2019s model, which charges for the total data scanned from disk, many modern platforms are designed for greater cost-efficiency on medium data. By leveraging principles like compute locality, a platform like MotherDuck can process data where it lives\u2014either locally or in the cloud\u2014minimizing costly data scans. This results in a more predictable and often significantly lower bill for analytical workloads.\n\n### How can I get analytics without hiring a dedicated team to manage infrastructure?\n\nServerless analytics platforms are the answer, as they handle all infrastructure management for you. While BigQuery is serverless, optimizing its cost often requires significant data engineering effort like partitioning and clustering. A modern cloud data warehouse solution like MotherDuck simplifies this further, offering a serverless experience that is cost-effective out of the box without needing constant tuning.",
  [2m  â”‚[0m     "numLines": 258,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 258
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_nsdf4vt5/.kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_nsdf4vt5/.kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_nsdf4vt5/.kurt/sources/motherduck.com/learn-more/hybrid-analytics-guide.md",
  [2m  â”‚[0m     "content": "---\ntitle: hybrid-analytics-guide\ncontent_type: tutorial\nsource_url: https://motherduck.com/learn-more/hybrid-analytics-guide\nindexed_at: '2025-11-25T09:57:23.352209'\ncontent_hash: 91344eaa7ee54322\nhas_step_by_step: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO LEARN](https://motherduck.com/learn-more/)\n\n# Hybrid Analytics: Query Local & Cloud Data Instantly\n\n12 min readBY\n\n[Aditya Somani](https://motherduck.com/authors/aditya-aomani/)\n\n![Hybrid Analytics: Query Local & Cloud Data Instantly](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FZero_latency_e8da68f055.png&w=3840&q=75)\n\nAre your analytics queries timing out in Postgres? Does your laptop grind to a halt when you try to analyze a large CSV file? If you feel caught between the memory limits of local tools and the spiraling costs of cloud data warehouses, you're not alone. The traditional divide between local development and cloud-scale analytics creates friction, slows down projects, and drains budgets.\n\nBut there is a modern, more efficient way. Imagine running complex SQL queries on a 20 GB Parquet file directly on your laptop, without a flicker of memory strain. Picture joining a local spreadsheet with a massive table in Amazon S3 using a single, elegant command. This is the reality of the **hybrid analytics workflow**, a powerful approach that unifies your local machine and the cloud into a single, unified data environment.\n\nThis guide is your definitive resource for understanding and implementing this new workflow. We'll break down the core concepts, provide practical examples, and show you how to solve the most common data bottlenecks for good. You will learn why traditional databases fail for analytics, how to analyze datasets larger than your RAM, and how the power of \"Dual Execution\" lets you instantly join local files with cloud data, all without the cost and complexity of managing a server cluster.\n\n## Why Are My Analytics Queries on Postgres & MySQL Timing Out?\n\nIf your analytical queries on a database like PostgreSQL or MySQL are consistently slow or timing out, you've hit a fundamental architectural limit. These databases are masterpieces of engineering for **[Online Transaction Processing (OLTP)](https://estuary.dev/blog/postgres-to-motherduck/)**. These are the small, fast operations that power applications, like creating a user or updating an order. However, this same design becomes a major bottleneck for **[Online Analytical Processing (OLAP)](https://en.wikipedia.org/wiki/Online_analytical_processing)**, which involves complex queries that scan and aggregate vast amounts of data.\n\nThe root of the problem is their **[row-oriented](https://en.wikipedia.org/wiki/Data_orientation)** storage model. A row-store keeps all data for a single record together on disk, which is efficient for retrieving an entire user profile. But for an analytical query that only needs to sum the `sale_amount` column across millions of rows, a row-store is forced to read every single column for every single row. This wastes an enormous amount of I/O and CPU cycles on data it doesn't need, which is the primary reason your dashboards are slow and your queries fail.\n\nThe next logical step is to move analytical workloads to a system built on a **[columnar architecture](https://motherduck.com/learn-more/columnar-storage-guide/)**. A columnar database organizes data by column, storing all values for `sale_amount` together. When your query asks for the sum of `sale_amount`, the database reads _only_ that column, greatly reducing I/O and speeding up performance by orders of magnitude. Modern analytical engines like DuckDB couple this with **[vectorized query execution](https://15721.courses.cs.cmu.edu/spring2024/notes/06-vectorization.pdf)**, a technique that processes data in large batches or \"vectors\" instead of row-by-row. This method fully utilizes modern CPU capabilities to perform a single instruction on multiple data points simultaneously, leading to large gains in computational efficiency.\n\n| Architecture | Row-Oriented (OLTP) | Column-Oriented (OLAP) |\n| --- | --- | --- |\n| **Primary Use Case** | Application backends, frequent small updates. | Business intelligence, data analytics, large scans. |\n| **Data Storage** | All data for a single record is stored together. | All data for a single column is stored together. |\n| **Query Example** | `SELECT * FROM users WHERE user_id = 123;` (Fast) | `SELECT SUM(sale_amount) FROM sales;` (Slow) |\n| **Best For** | Fast writes and single-record lookups. | Fast aggregations and filtering on specific columns. |\n\nBy offloading analytics to a purpose-built columnar engine, you let your transactional database continue to excel at what it does best while your analytical queries run in a [high-speed, optimized environment](https://motherduck.com/blog/postgres-duckdb-options/).\n\n## How Can I Analyze a 20 GB Parquet File on My Laptop Without It Crashing?\n\nThe second major bottleneck for data professionals is local memory. Trying to load a 20 GB file into a traditional in-memory library like Pandas on a 16 GB laptop will almost certainly result in a `MemoryError`. This happens because these tools must load the entire dataset into your computer's RAM before they can begin processing.\n\nThe modern solution is **out-of-core processing**, a strategy where data is processed in manageable chunks directly from disk without ever being fully loaded into RAM. This is the key feature of engines like DuckDB, which uses a [streaming execution model to handle datasets far larger than your available memory](https://duckdb.org/2024/07/09/memory-management.html). If a complex operation requires more memory than is available (by default, 80% of your system's RAM), DuckDB can \"spill\" intermediate results to a temporary file on disk, ensuring your query completes without crashing.\n\nThis capability turns your laptop into a surprisingly powerful analytics workstation. With an out-of-core engine, analyzing that 20 GB file becomes straightforward. You can simply use SQL to query it directly from a Python script or Jupyter notebook.\n\n```python\nCopy code\n\nimport duckdb\n\n# This query runs out-of-core, never loading the full 20GB file into RAM.\n# DuckDB's streaming engine processes the file in chunks.\nresult = duckdb.sql(\"\"\"\n  SELECT\n      product_category,\n      AVG(sale_price) as avg_price\n  FROM read_parquet('large_sales_dataset.parquet')\n  WHERE region = 'North America'\n  GROUP BY product_category\n  ORDER BY avg_price DESC;\n\"\"\").arrow()\n\nprint(result)\n```\n\nThis simple, powerful approach allows you to perform heavy-duty data analysis on your local machine, iterating quickly without waiting for a cloud cluster to spin up or worrying about memory crashes.\n\n## How Can I Join Local CSVs with Cloud Data in a Single Query?\n\nThis is where the hybrid workflow really shows its strength. Joining a local CSV of fresh sales data with a historical customer table in an S3 bucket has traditionally been a major challenge. The problem is \"data gravity,\" since it's [slow and expensive to move large amounts of data](https://motherduck.com/learn-more/no-etl-query-raw-files/). A naive query would have to either upload your entire local file to the cloud or download the entire cloud table to your laptop, both of which are very inefficient.\n\nMotherDuck solves this with an innovative query planner called **Dual Execution**. It treats your laptop and the MotherDuck cloud as two nodes in a single distributed system. When you run a hybrid query, the planner intelligently breaks it down and pushes computation to where the data lives, minimizing data transfer.\n\n> \"The most innovative feature of MotherDuck's architecture is its dual execution model. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\" - [Bringing DuckDB to the Cloud: Dual Execution Explained](https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained/)\n\nImagine you want to join a local CSV of new product pricing with a massive sales table in MotherDuck, but only for a specific product. Instead of moving entire tables, the Dual Execution planner does the following:\n\n1. **Local Scan:** It scans the small pricing CSV on your laptop.\n2. **Filter Locally:** It applies the filter for the specific product on your machine.\n3. **Transfer Minimal Data:** It sends only the single, filtered pricing row (a few bytes of data) to the cloud.\n4. **Join in the Cloud:** It performs the final join against the massive sales table in the MotherDuck cloud.\n\nThis process can reduce network traffic by orders of magnitude compared to traditional methods, turning a difficult query into an interactive one. You can see this in action with a simple `EXPLAIN` statement, which [shows which parts of the query run locally `(L)` and which run remotely `(R)`](https://motherduck.com/docs/key-tasks/running-hybrid-queries/).\n\n## Why Are Data Engineers So Excited About Dual-Execution Engines?\n\nData engineers are excited because dual-execution engines solve one of their biggest challenges: the \"it worked on my machine\" problem. In traditional setups, the tools and data used for local development are often completely different from the production cloud environment, leading to bugs and deployment failures.\n\nMotherDuck eliminates this by using the **exact same DuckDB engine** both locally and in the cloud. A query that works on your laptop is guaranteed to work in the cloud, creating a direct path from development to production. This greatly improves developer experience (DX) and accelerates iteration cycles.\n\nThis architecture provides three key benefits:\n\n1. **Faster Development:** Engineers can build and test pipelines with the zero-latency feedback of local development before scaling to the cloud.\n2. **Lower Costs:** By using the free, powerful compute on users' laptops and minimizing data transfer, this model significantly reduces cloud bills.\n3. **Better Collaboration:** It transforms the traditionally \"single-player\" DuckDB into a [\"multiplayer\" platform](https://motherduck.com/videos/339/bringing-duckdb-to-the-cloud-dual-execution-explained/) where teams can share databases and work from a single source of truth.\n\nThis powerful combination of local speed and cloud scale is backed by real-world success.\n\n> **[Case Study: Finqore's 60x Pipeline Acceleration](https://motherduck.com/case-studies/)**\n> Finqore, a financial technology company, was struggling with an 8-hour data pipeline built on Postgres. By migrating to a hybrid workflow with MotherDuck, they were able to use DuckDB's performance and MotherDuck's serverless scale to transform that pipeline into an **8-minute workflow**, a 60x improvement that unlocked real-time data exploration for their team.\n\n## How Can I Run SQL Analytics Without Managing Clusters?\n\nThe operational overhead of provisioning, scaling, and maintaining server clusters is a major drain on data teams. **[Serverless SQL analytics](https://motherduck.com/docs/concepts/architecture-and-capabilities/)** removes this burden entirely. Instead of managing infrastructure, you simply write and run your queries, and the platform handles the rest.\n\nThis model is not only simpler but also much more cost-effective for the \"spiky\" workloads typical of analytics. You pay only for the compute you actually use, avoiding the massive costs of idle clusters.\n\n| Feature / Scenario | Traditional Cloud Warehouse (e.g., Snowflake/BigQuery) | Hybrid Approach (Local + Serverless) |\n| --- | --- | --- |\n| **Development & Prototyping** | Billed for active compute time, even for small test queries. | **Free.** Uses local machine resources with no cloud costs. |\n| **Pricing Model** | Complex credits or per-TB scanned, which is hard to predict. | Simple, usage-based billing; pay only for queries you run. |\n| **Idle Compute** | Billed for provisioned clusters, even when idle, which can create a costly [idle tax on short-running queries](https://motherduck.com/learn-more/reduce-snowflake-costs-duckdb). | **No cost.** Serverless architecture has no idle compute. |\n| **Hardware Requirement** | Entirely reliant on expensive, provisioned cloud hardware. | Leverages powerful, existing local hardware (laptops, servers). |\n\nBy adopting a serverless, hybrid approach, you can reduce your cloud data warehouse costs while empowering your team with a faster, more flexible workflow.\n\n## What Lightweight SQL Solutions Can Be Integrated with Jupyter Notebooks?\n\nFor data scientists who work primarily in Jupyter, several excellent tools bring the power of SQL directly into the notebook environment. The most powerful option for serious analytics is the **DuckDB engine** itself. It can query Pandas DataFrames, Arrow tables, and large local files directly with full SQL support. Its performance on analytical queries is excellent in the embedded space.\n\nFor comparison, while Python's standard library includes **SQLite**, it's a row-based OLTP engine and is significantly slower than DuckDB for analytical queries. An independent benchmark found DuckDB to be **[12-35 times faster](https://motherduck.com/learn-more/duckdb-vs-sqlite-databases/)** for aggregations on a 100-million-row dataset. For any analytical task in a notebook, DuckDB is the clear choice for performance and scalability.\n\nAnother popular tool is **[JupySQL](https://jupysql.ploomber.io/)**, which provides convenient SQL \"magics\" (`%sql`, `%%sql`) that turn a notebook cell into a SQL editor. It connects to various backends, including DuckDB, making it great for quick, ad-hoc exploration.\n\n## How Can I Keep Queries on My Laptop in Sync With the Cloud?\n\nThe key to keeping local and cloud queries synchronized is **single-engine semantics**. Because MotherDuck uses the same DuckDB engine on your laptop and in the cloud, a query validated locally is guaranteed to behave identically in production. This eliminates the most common source of dev-prod drift.\n\nA modern dev-to-prod workflow combines this with tools like dbt (data build tool). An analyst can develop and test a dbt model on their laptop against local data for rapid iteration. Once validated, they can promote the model to run in the cloud against the full production dataset with a [single configuration change](https://motherduck.com/blog/dual-execution-dbt/), ensuring complete consistency. This workflow combines the speed of local development with the scale and reliability of a production cloud environment.\n\n## When Should I Not Use This Hybrid Approach?\n\nWhile the hybrid, in-process model is very useful for analytics, it's not a solution for every problem. It's crucial to understand its limitations to know when a different architecture is needed.\n\n1. **High-Concurrency Applications:** An in-process engine like DuckDB is not designed to be a client-server database serving hundreds of simultaneous application users. For powering a public-facing web application, a traditional OLTP database like PostgreSQL or a distributed SQL database is the right choice.\n2. **Real-Time, High-Volume Ingestion:** If your use case involves ingesting thousands of events per second in real-time, a specialized streaming platform or real-time OLAP database would be more suitable.\n3. **Truly Massive, Multi-Terabyte Workloads:** While DuckDB can handle surprisingly large datasets on a single node, if your active working set is in the tens or hundreds of terabytes, you've reached the scale where a distributed cloud data warehouse becomes necessary to parallelize work across a large cluster.\n\nThe beauty of the MotherDuck ecosystem is that it provides a smooth way to scale up. You can start with a local-first, hybrid workflow and, as your concurrency or data scale needs grow, easily push more of the workload to the MotherDuck cloud without changing your core tools or SQL logic.\n\n### TABLE OF CONTENTS\n\n[Why Are My Analytics Queries on Postgres & MySQL Timing Out?](https://motherduck.com/learn-more/hybrid-analytics-guide/#why-are-my-analytics-queries-on-postgres-mysql-timing-out)\n\n[How Can I Analyze a 20 GB Parquet File on My Laptop Without It Crashing?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-analyze-a-20-gb-parquet-file-on-my-laptop-without-it-crashing)\n\n[How Can I Join Local CSVs with Cloud Data in a Single Query?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-join-local-csvs-with-cloud-data-in-a-single-query)\n\n[Why Are Data Engineers So Excited About Dual-Execution Engines?](https://motherduck.com/learn-more/hybrid-analytics-guide/#why-are-data-engineers-so-excited-about-dual-execution-engines)\n\n[How Can I Run SQL Analytics Without Managing Clusters?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-run-sql-analytics-without-managing-clusters)\n\n[What Lightweight SQL Solutions Can Be Integrated with Jupyter Notebooks?](https://motherduck.com/learn-more/hybrid-analytics-guide/#what-lightweight-sql-solutions-can-be-integrated-with-jupyter-notebooks)\n\n[How Can I Keep Queries on My Laptop in Sync With the Cloud?](https://motherduck.com/learn-more/hybrid-analytics-guide/#how-can-i-keep-queries-on-my-laptop-in-sync-with-the-cloud)\n\n[When Should I Not Use This Hybrid Approach?](https://motherduck.com/learn-more/hybrid-analytics-guide/#when-should-i-not-use-this-hybrid-approach)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\n## FAQS\n\n### My analytics queries on Postgres / MySQL are timing out. What\u2019s the next step up?\n\nThe next step is to move your analytical workloads from a row-oriented (OLTP) database like Postgres to a purpose-built columnar (OLAP) engine like DuckDB. Columnar engines are designed for analytics and are orders of magnitude faster for large scans and aggregations because they only read the specific columns needed for a query.\n\n### How can I analyze a 20 GB Parquet file on my laptop without it crashing?\n\nUse a query engine that supports \"out-of-core\" processing, like DuckDB. It processes data in manageable chunks directly from your disk instead of loading the entire file into RAM. This allows you to analyze datasets that are significantly larger than your computer's available memory.\n\n### Why is it so hard to join local CSVs with cloud data?\n\nThe main challenge is \"data gravity\"\u2014it's slow and expensive to move large datasets across a network. Traditional tools force you to either upload your entire local file or download the entire cloud table. A modern hybrid platform with a dual-execution engine solves this by intelligently minimizing data transfer.\n\n### Can I mix local datasets and S3 data in a single query?\n\nYes. With a hybrid analytics platform like MotherDuck, you can write a single SQL query that joins local files (e.g., CSVs) with data in cloud storage (e.g., Parquet files in S3). The system treats them as if they exist in one unified environment, abstracting away their physical location.\n\n### How can I run SQL analytics without managing clusters?\n\nAdopt a serverless SQL analytics platform. These services handle all infrastructure provisioning, scaling, and maintenance for you. You simply run your queries and pay only for the compute you use, which eliminates the high cost and operational overhead of managing idle clusters.\n\n### Why are data engineers excited about dual-execution engines?\n\nDual-execution engines solve the \"it worked on my machine\" problem by using the exact same query engine on the developer's laptop and in the cloud. This guarantees consistency, speeds up development cycles, lowers costs by leveraging local compute, and enables better team collaboration on a single source of truth.\n\n### What lightweight SQL solutions can be integrated with Jupyter notebooks?\n\nDuckDB is the leading lightweight SQL solution for Jupyter. It can be installed via pip and can directly query Pandas DataFrames, Arrow tables, and large local files with full SQL support. It is significantly faster for analytical queries than other embedded options like SQLite.\n\n### How can I keep queries on my laptop in sync with the cloud?\n\nUse a platform that offers single-engine semantics, like MotherDuck. Because it runs the same DuckDB engine locally and in the cloud, a query validated on your laptop is guaranteed to behave identically in production. This eliminates dev-prod drift and ensures consistency.\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 212,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 212
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_nsdf4vt5/.kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     "content": "---\ntitle: Why Your Snowflake Bill is High and How to Fix It with a Hybrid Approach\ncontent_type: guide\ndescription: Your Snowflake bill is high due to the 60s idle-compute tax. Learn how\n  a hybrid analytics model with DuckDB & MotherDuck can cut costs by 70-90%. Read\n  the guide.\npublished_date: '2025-11-14T00:00:00'\nsource_url: https://motherduck.com/learn-more/reduce-snowflake-costs-duckdb\nindexed_at: '2025-11-25T10:52:43.885585'\ncontent_hash: 64adc347264a0b7a\nhas_step_by_step: true\nhas_narrative: true\n---\n\n# Why Your Snowflake Bill is High and How to Fix It with a Hybrid Approach\n\n17 min readBYYour Snowflake bill is high primarily because of its compute billing model, which enforces a [60-second minimum charge](https://docs.snowflake.com/en/user-guide/cost-understanding-compute) each time a warehouse resumes. This creates a significant \"idle tax\" on the frequent, short-running queries common in BI dashboards and ad-hoc analysis. You're often paying for compute you don't actually use.\n\nA surprisingly high bill for a modest amount of data is frustrating. We see it all the time. The immediate question is, \"Why is my bill so high when my data isn't that big?\" The cost isn't driven by data at rest, it's driven by data in motion, specifically by compute patterns. For many modern analytical workflows, the bill inflates from thousands of frequent queries accumulating disproportionately high compute charges.\n\nIf you don't address this, you'll face budget overruns, throttled innovation, or pressure to undertake a costly and risky platform migration. The solution isn't always abandoning a powerful platform like Snowflake. You can augment it intelligently instead.\n\nThis guide provides a practical playbook for understanding the root causes of high Snowflake costs and a strategy for reducing them using internal optimizations and a [modern hybrid architecture](https://motherduck.com/docs/concepts/architecture-and-capabilities/).\n\n## The Real Reason Your Snowflake Bill is So High\n\nTo control costs effectively, you need to diagnose the problem first. The primary driver of inflated Snowflake bills for bursty, interactive workloads is the platform's billing model for compute. It creates a significant hidden idle tax.\n\nSnowflake bills for compute per-second, but only after a 60-second minimum is met each time a virtual warehouse resumes from a suspended state. A query that takes only five seconds to execute gets billed for a full minute of compute time. In this common scenario, you're paying for 55 seconds (over 91%) of compute resources that sit idle.\n\nHere's what this looks like on a timeline. For a 5-second query, the billed duration on Snowflake versus a usage-based platform like MotherDuck is stark.\n\n**Snowflake (X-Small Warehouse):**\n\n**MotherDuck (Pulse Compute):**\n\nWhen a BI dashboard executes 20 quick queries upon loading, each taking three seconds, this single page view could trigger 1,200 seconds (20 minutes) of billed compute time. The actual work took only one minute.\n\nThis problem gets worse with warehouse sizing. Each incremental size increase in a Snowflake warehouse [doubles its credit consumption rate](https://docs.snowflake.com/en/user-guide/cost-understanding-compute). We often see teams defaulting to 'Medium' or 'Large' warehouses for all tasks. That creates a 4x to 8x cost premium for workloads that could easily run on an 'X-Small' warehouse.\n\nThis combination of minimum billing increments and oversized compute creates exponential cost leak. Serverless features like [Automatic Clustering](https://docs.snowflake.com/en/user-guide/cost-understanding-overall) and [Materialized Views](https://docs.snowflake.com/en/user-guide/cost-understanding-overall) consume credits in the background too, contributing to credit creep that's difficult to trace without diligent monitoring.\n\n| Warehouse Size | Credits per Hour | Relative Cost |\n|---|---|---|\n| X-Small | 1 | 1x |\n| Small | 2 | 2x |\n| Medium | 4 | 4x |\n| Large | 8 | 8x |\n| X-Large | 16 | 16x |\n\n## First Aid: A Playbook to Immediately Optimize Snowflake\n\nBefore considering architectural changes, you can achieve significant savings by optimizing your existing Snowflake environment. These internal fixes are your first line of defense against cost overruns. They can often reduce spend by 20-40%.\n\n### 1. Master Warehouse Management (Set AUTO_SUSPEND to 60s)\n\nSet aggressive yet intelligent warehouse timeouts. For most workloads, set the [ AUTO_SUSPEND parameter](https://docs.snowflake.com/en/user-guide/cost-controlling-controls#use-auto-suspension) to exactly 60 seconds. This ensures the warehouse suspends after one minute of inactivity, stopping credit consumption. Setting it lower than 60 seconds is counterproductive. A new query arriving within that first minute could trigger a second 60-second minimum charge.\n\nRight-size warehouses by defaulting to smaller configurations. Use 'X-Small' warehouses by default and only scale up when a specific workload fails to meet its performance SLA. Consolidate workloads onto fewer, appropriately sized warehouses to prevent warehouse sprawl. Multiple underutilized compute clusters add up on your bill.\n\nWe helped one analytics team save approximately $38,000 annually by moving its BI queries from a Medium to a Small warehouse. They accepted a marginal 4-second increase in query time.\n\n### 2. Leverage Snowflake's Caching Layers (Result & Warehouse)\n\nSnowflake's multi-layered cache is one of its most powerful cost-saving features. Not using it leaves money on the table.\n\n**Result Cache:** If you run the exact same query as one run previously (by anyone in the account) and the underlying data hasn't changed, Snowflake returns the results instantly from a global result cache. No warehouse starts. That's free compute. It's especially effective for BI dashboards where multiple users view the same default state.\n\n**Warehouse Cache (Local Disk Cache):** When a query runs, the required data from storage gets cached on the SSDs of the active virtual warehouse. Subsequent queries that need the same data read it from this much faster local cache instead of remote storage. This dramatically speeds up queries and reduces I/O. Keeping a warehouse warm for related analytical queries can be beneficial.\n\nDesign workloads to maximize cache hits through consistent query patterns.\n\n### 3. Optimize Inefficient Queries (Prune Partitions & Avoid SELECT *)\n\nPoorly written queries burn credits unnecessarily. While comprehensive query tuning is a deep topic, these practices provide immediate savings:\n\n**Avoid SELECT *:** Select only the columns you need. This reduces the amount of data processed and moved, improving caching and query performance.\n\n**Filter Early and Prune Partitions:** Apply `WHERE`\n\nclauses that filter on a table's clustering key as early as possible. This lets Snowflake prune massive amounts of data from being scanned. It's the single most effective way to speed up queries on large tables.\n\n**Use QUALIFY for Complex Window Function Filtering:** Instead of using a subquery or CTE to filter window function results, use the\n\n[. It's more readable and often more performant.](https://docs.snowflake.com/en/sql-reference/constructs/qualify)\n\n`QUALIFY`\n\nclause### 4. Implement Cost Guardrails with Resource Monitors\n\nImplement [resource monitors](https://docs.snowflake.com/en/user-guide/resource-monitors) as a critical safety net. Resource monitors track credit consumption and trigger actions like sending notifications or automatically suspending compute when usage hits predefined thresholds. They're the most effective tool for preventing budget overruns from runaway queries or misconfigured pipelines.\n\nCopy code\n\n```\n-- Create a monitor that notifies at 75% and suspends at 100%\nCREATE OR REPLACE RESOURCE MONITOR monthly_etl_monitor\nWITH CREDIT_QUOTA = 5000\nTRIGGERS ON 75 PERCENT DO NOTIFY\nON 100 PERCENT DO SUSPEND;\n-- Assign the monitor to a warehouse\nALTER WAREHOUSE etl_heavy_wh SET RESOURCE_MONITOR = monthly_etl_monitor;\n```\n\n\nActively monitor serverless feature costs too. Query the [ serverless_task_history](https://docs.snowflake.com/en/sql-reference/functions/serverless_task_history) view to track credits consumed by Automatic Clustering, Search Optimization, and other background tasks. This helps you understand your hidden costs and tune these features appropriately.\n\nThese internal fixes can significantly lower your Snowflake bill. To eliminate entire categories of spend, particularly from non-production workloads, you need a different approach to compute location.\n\n## Go Local: Slashing Dev & Test Costs with DuckDB\n\nA substantial portion of cloud data warehouse spend gets consumed by non-production workloads. Every [ dbt run](https://github.com/duckdb/dbt-duckdb), data validation script, and ad-hoc analysis performed by engineers during development consumes expensive cloud compute credits. By adopting a local-first development workflow, you can shift this entire category of work off the cloud and reduce these costs to zero.\n\nDuckDB makes this shift possible. It's a fast, in-process analytical database designed to run complex SQL queries directly on your laptop or within a CI/CD runner. DuckDB queries data files like [Parquet](https://motherduck.com/learn-more/why-choose-parquet-table-file-format/) and [CSV](https://duckdb.org/docs/data/csv/) directly. You don't need to load data into a separate database for local development. Engineers can build, test, and iterate on data models and pipelines locally before incurring any cloud costs.\n\nThis workflow saves money and dramatically improves developer velocity. You shorten the feedback loop from minutes (waiting for a cloud warehouse to provision and run) to seconds.\n\nA typical local development pattern in Python is straightforward. You can prototype rapidly without any cloud interaction.\n\nCopy code\n\n```\nimport duckdb\nimport pandas as pd\n# Analyze a local Parquet file instantly\n# No cloud warehouse, no compute credits consumed\ndf = duckdb.sql(\"\"\"\nSELECT\nproduct_category,\nCOUNT(DISTINCT order_id) as total_orders,\nAVG(order_value) as average_value\nFROM 'local_ecommerce_data.parquet'\nWHERE order_date >= '2024-01-01'\nGROUP BY ALL\nORDER BY total_orders DESC;\n\"\"\").df()\nprint(df)\n```\n\n\nRunning analytics locally is powerful for development. For sharing insights and powering production dashboards, this local-first approach extends into a hybrid architecture.\n\n## The Hybrid Solution: MotherDuck for Cost-Effective Interactive Analytics\n\nMotherDuck is a serverless data warehouse built on DuckDB. It provides a simpler, more cost-effective solution for workloads that are inefficient on traditional cloud data warehouses. It directly solves the idle tax problem by replacing the provisioned warehouse model with per-query, usage-based compute that bills in [one-second increments](https://motherduck.com/product/pricing/).\n\nThis billing model profoundly impacts the cost of interactive analytics. Let's quantify the savings with a realistic scenario.\n\n### Breaking Down the Costs: A Tale of Two Queries\n\nConsider a common BI dashboard used by an operations team. It refreshes every 5 minutes during an 8-hour workday to provide timely updates. Each refresh executes 10 small queries to populate various charts. Each query takes 4 seconds to run.\n\n**Workload Parameters:**\n\n**Queries per refresh:**10**Execution time per query:**4 seconds**Refresh frequency:**Every 5 minutes (12 refreshes per hour)**Operational hours:**8 hours/day, 22 days/month\n\n**Snowflake Cost Calculation (X-Small Warehouse):**\n\nBecause of the high frequency, the team can't let the warehouse suspend between refreshes without incurring repeated 60-second minimums. Their most cost-effective option is running an X-Small warehouse continuously during the workday.\n\n**Total active hours per month:**8 hours/day * 22 days/month = 176 hours**Credits consumed per hour (X-Small):**1**Total credits per month:**176 hours * 1 credit/hour = 176 credits**Estimated Monthly Cost (@ $3.00/credit):**176 credits * $3.00/credit = **$528**\n\nThis assumes perfect management. A more common scenario where the warehouse runs 24/7 would cost **$2,160** (720 hours * 1 credit/hr * $3.00/credit).\n\n**MotherDuck Cost Calculation ( Pulse Compute):**\n\nMotherDuck bills only for the actual compute time used by queries.\n\n**Total queries per month:**10 queries/refresh * 12 refreshes/hr * 8 hrs/day * 22 days/month = 21,120 queries**Total execution time per month:**21,120 queries * 4 seconds/query = 84,480 seconds**Total execution hours:**84,480 seconds / 3,600 s/hr = 23.47 hours**Estimated Monthly Cost (@ $0.25/CU-hour, assuming 1 CU):**23.47 CU-hours * $0.25/CU-hour = **$5.87**\n\nEven assuming a more complex query consuming 4 Compute Units, the cost would only be **$23.48**. This example shows how a usage-based model eliminates waste for bursty workloads, reducing costs by over 95% in this scenario.\n\nThis calculation focuses on compute cost, the primary driver. While negligible for this interactive pattern, a full TCO analysis would include data storage and egress, where MotherDuck also offers competitive pricing.\n\nMotherDuck's architecture introduces [ \"dual execution.\"](https://motherduck.com/docs/concepts/architecture-and-capabilities/) Its query planner intelligently splits work between the local DuckDB client and the MotherDuck cloud service. This minimizes data transfer and latency by performing filters and aggregations locally before sending smaller, pre-processed datasets to the cloud. This hybrid model works ideal for interactive analytics, BI dashboards, and ad-hoc queries on sub-terabyte hot data.\n\nCopy code\n\n```\n-- Connect to MotherDuck from any DuckDB-compatible client\n[ATTACH 'md:';](https://motherduck.com/docs/getting-started/)\n-- This query joins a large cloud table with a small local file.\n-- The filter on the local file is pushed down, so only matching\n-- user_ids are ever requested from the cloud, minimizing data transfer.\nSELECT\ncloud_events.event_name,\ncloud_events.event_timestamp,\nlocal_users.user_department\nFROM my_db.main.cloud_events\nJOIN read_csv_auto('local_user_enrichment.csv') AS local_users\nON cloud_events.user_id = local_users.user_id\nWHERE local_users.is_priority_user = TRUE;\n```\n\n\n### Proven in Production: Real-World Case Studies of Significant Cost Savings\n\nThe savings from this new architecture aren't just theoretical. Companies are already using this model to achieve significant results.\n\n\nCase Study:The SaaS company Definite migrated its entire data warehouse from Snowflake to a DuckDB-based solution. The results were quick and significant, achieving an[Definite Slashes Costs by 70%]over 70% reductionin their data warehousing expenses. In their detailed write-up, the engineering team noted that even after accounting for the migration effort, the savings freed up a significant portion of their budget for core product development.\n\n\nCase Study:Okta's security engineering team needed to process trillions of log records for threat detection, with data volumes spiking daily. Their Snowflake solution was costing approximately[Okta Eliminates a $60,000 Monthly Snowflake Bill]$2,000 per day ($60,000 monthly). By building a clever system that used thousands of small DuckDB instances running in parallel on serverless functions, they significantly reduced their processing costs. This case shows that even at a large scale, the DuckDB ecosystem can be much cheaper than traditional cloud warehouses.\n\n\nCase Study:A data engineering team shared their story of implementing a smart caching layer for their BI tool. Instead of having every dashboard query hit Snowflake directly, they routed smaller, frequent queries to a DuckDB instance that served cached results. Large, complex queries were still sent to Snowflake. The impact was a[A 79% BI Spend Reduction with a Simple Caching Layer]79% immediate reductionin their Snowflake BI spend, and average query times sped up by 7x. This highlights the power of a hybrid \"best tool for the job\" approach.\n\n## A Framework for Workload Triage\n\nUnderstanding the tool landscape is one thing. Systematically deciding which of your workloads belong where requires a data-driven approach. By analyzing query history, you can classify every workload and route it to the most efficient engine.\n\nThe two most important axes for classification are **Execution Time** and **Query Frequency**. Consider a third axis too: **data freshness requirements**. A dashboard needing near real-time data has different constraints than one running on a nightly batch refresh.\n\nA simple 2x2 matrix provides a clear framework for triage:\n\n**Low Execution Time, High Frequency:**Short, bursty queries that run often.**Low Execution Time, Low Frequency:**Quick, sporadic, ad-hoc queries.**High Execution Time, Low Frequency:**Long-running, scheduled batch jobs.**High Execution Time, High Frequency:**Often an anti-pattern indicating a need for data modeling or architectural redesign. It can occur in complex, near-real-time operational analytics.\n\nYou can analyze Snowflake's [ query_history](https://docs.snowflake.com/en/sql-reference/account-usage/query_history) using SQL to categorize your workloads. This query provides a starting point. We use\n\n`MEDIAN`\n\ninstead of `AVG`\n\nfor execution time because it's more robust to outliers and gives a better sense of typical query duration.Copy code\n\n```\n-- Analyze query patterns over the last 30 days\nWITH query_stats AS (\nSELECT\nwarehouse_name,\nuser_name,\nquery_id,\nexecution_time / 1000 AS execution_seconds\nFROM\nsnowflake.account_usage.query_history\nWHERE\nstart_time >= DATEADD('day', -30, CURRENT_TIMESTAMP())\nAND warehouse_name IS NOT NULL\nAND execution_status = 'SUCCESS'\n)\nSELECT\nwarehouse_name,\nuser_name,\nCOUNT(query_id) AS query_count,\nMEDIAN(execution_seconds) AS median_execution_seconds, -- More robust than AVG\nCASE\nWHEN query_count > 1000 AND median_execution_seconds < 30 THEN 'Interactive BI / High Frequency'\nWHEN query_count <= 1000 AND median_execution_seconds < 60 THEN 'Ad-Hoc Exploration'\nWHEN median_execution_seconds >= 300 THEN 'Batch ETL / Heavy Analytics'\nELSE 'General Purpose'\nEND AS workload_category\nFROM\nquery_stats\nGROUP BY\nwarehouse_name, user_name\nORDER BY\nquery_count DESC;\n```\n\n\nOnce categorized, map these workloads to the optimal tool:\n\n-\n**Interactive BI / High Frequency (Short & Bursty):**Prime candidates for migration to**MotherDuck**. The per-second, usage-based billing model eliminates the idle tax, offering dramatic cost savings for dashboards and embedded analytics. -\n**Ad-Hoc Exploration (Short & Sporadic):**This category fits well with**MotherDuck**or local**DuckDB**. For queries on smaller datasets or local files, DuckDB provides instant, free execution. For shared datasets, MotherDuck offers a cost-effective cloud backend. -\n**Batch ETL / Heavy Analytics (Long & Scheduled):**These large, resource-intensive jobs often work best on**Snowflake**. Its provisioned warehouses provide predictable performance for multi-terabyte transformations. Its mature ecosystem simplifies complex data pipelines. -\n**Development & CI/CD:**Move all non-production workloads to local**DuckDB**, regardless of their characteristics. This completely eliminates cloud compute costs during development and testing.\n\n## When the Hybrid Approach Isn't the Right Fit: Sticking with Snowflake\n\nTo build an effective architecture, you need to know a tool's limitations. The hybrid approach isn't a universal solution. Certain workloads are best suited for a mature, large-scale data warehouse like Snowflake. Acknowledging this builds trust and leads to better technical decisions.\n\n**Massive Batch ETL/ELT:** For scheduled jobs processing many terabytes of data, Snowflake's provisioned compute model provides predictable power and performance. The 60-second minimum doesn't matter for jobs that run for hours.\n\n** Enterprise-Grade Governance and Security:** Organizations with complex data masking requirements, deep Active Directory integrations, or strict regional data residency rules often rely on Snowflake's mature and comprehensive features.\n\n**Highly Optimized, Long-Running Workloads:** If you have a workload that already runs consistently on a warehouse and maximizes its uptime (like a data science cluster running for 8 hours straight), the idle tax isn't a problem. There's little cost benefit to moving it.\n\nThe goal of a hybrid architecture is using the right tool for the right job, not replacing a tool that's already performing efficiently.\n\n## The Modern Alternatives Landscape: Where Does MotherDuck Fit?\n\nWhile the Snowflake-and-MotherDuck hybrid model effectively addresses many common workloads, the broader data platform market offers other specialized solutions. Understanding where they fit provides a complete picture for architectural decisions.\n\nData lake query engines like [Starburst](https://www.starburst.io/) and [Dremio](https://www.dremio.com/) are powerful for organizations wanting to query data directly in object storage like S3. They offer flexibility but often come with significant operational overhead.\n\nFor use cases demanding sub-second latency at very high concurrency (like real-time observability), specialized engines like [ClickHouse](https://clickhouse.com) often provide superior price-performance.\n\nWithin classic cloud data warehouses, [Google BigQuery](https://cloud.google.com/bigquery/) presents a different pricing model. Its on-demand, per-terabyte-scanned pricing can be cost-effective for sporadic forensic queries. But it carries the risk of a runaway query where a single mistake leads to a massive bill.\n\nMotherDuck carves a unique niche. It combines the serverless simplicity of BigQuery with the efficiency of a local-first workflow powered by DuckDB. This makes it highly cost-effective and productive for teams focused on speed, iteration, and interactive analytics. You don't get the cost penalty of a traditional warehouse or the operational complexity of a data lake.\n\n| Workload Type | Recommended Primary Tool | Rationale |\n|---|---|---|\nLocal Dev/Testing | DuckDB | Eliminates cloud compute cost for non-production work. |\nInteractive Dashboards (<5TB) | MotherDuck | Per-second billing avoids idle tax on bursty query patterns. |\nLarge Batch ETL (>10TB) | Snowflake | Predictable performance and mature features for heavy jobs. |\nReal-Time Observability (High QPS) | ClickHouse | Optimized architecture for sub-second latency at high concurrency. |\nSporadic Forensic Queries | BigQuery (On-Demand) / MotherDuck | Pay-per-use model is efficient for unpredictable, infrequent queries. |\n\n## Conclusion and Path Forward\n\nThe path to a more efficient and cost-effective analytics stack doesn't require abandoning existing investments. You augment them intelligently. By adopting a three-tiered strategy, organizations gain control over their cloud data warehouse spending while empowering teams with better tools.\n\nThe strategy is simple:\n\n-\n**Tune:**Implement Snowflake-native optimizations like 60-second auto-suspend timers, right-sized warehouses, and resource monitors to immediately reduce waste. -\n**Go Local:**Shift all development and testing workloads to a local-first workflow with DuckDB. This eliminates an entire category of cloud compute spend. -\n**Go Hybrid:**Use the workload triage framework to identify bursty, interactive workloads. Offload them to MotherDuck, replacing the idle tax with fair, usage-based billing.\n\nThis hybrid architecture uses each platform's strengths. Snowflake handles massive, scheduled batch processing and enterprise governance. The DuckDB/MotherDuck ecosystem handles cost-effective development, ad-hoc exploration, and interactive analytics.\n\nStart with your own data. Analyze your Snowflake `query_history`\n\nusing the provided script. If you see a high volume of queries with median execution times under 30 seconds, that workload is a prime candidate for migration.\n\nFrom there:\n\n**Audit:**Use the provided SQL scripts to identify your most expensive and inefficient warehouses.**Experiment:**[Download DuckDB](https://duckdb.org/docs/installation/)and run your next data model test locally.**Prototype:**[Sign up for MotherDuck's free tier](https://app.motherduck.com/signup), upload a dataset, and connect a BI tool to experience the performance and simplicity firsthand.\n\nBy taking these steps, teams transform their analytics budget from a source of stress into a driver of innovation.\n\nStart using MotherDuck now!\n\n## FAQS\n\n### Why is my Snowflake bill so high when my data isn\u2019t that big?\n\nYour bill is likely high due to compute costs, which often account for over 80% of the total. Snowflake's pricing includes a 60-second minimum charge every time a warehouse activates, creating an \"idle-compute tax\" on the short, frequent queries common in development and BI.\n\n### What are cost-effective alternatives to Snowflake for data warehousing?\n\nFor many modern analytical workloads, a hybrid architecture using DuckDB for local processing and MotherDuck for a serverless cloud backend is a highly cost-effective alternative. This model is designed to eliminate idle compute costs and can reduce data warehousing bills by 70-90%.\n\n### Can any tools reduce my Snowflake spend by handling queries locally?\n\nYes. The open-source DuckDB Snowflake Extension allows you to query data from your Snowflake warehouse directly within a local DuckDB instance. This lets you handle development, testing, and iterative analysis on your laptop for free, significantly reducing Snowflake credit consumption.\n\n### How does a hybrid local-cloud analytics model optimize costs?\n\nIt shifts the bulk of analytics work\u2014especially development and ad-hoc queries\u2014from an expensive, minute-metered cloud warehouse to your local machine, where it's free. You only use the serverless cloud backend (like MotherDuck) for collaboration or larger queries, paying only for the actual seconds of compute used.\n\n### How can we optimize Snowflake resource allocation without hurting performance?\n\nStart by right-sizing warehouses (default to XS/S), setting aggressive auto-suspend policies (30-120 seconds), and consolidating workloads. For a bigger impact, offload development and BI workloads to a hybrid DuckDB/MotherDuck architecture to isolate and reduce the most inefficient costs.\n\n### Is MotherDuck a full replacement for Snowflake?\n\nFor many startups and teams with data under a few terabytes, it can be a full replacement. For enterprises with petabyte-scale batch processing needs, it serves as a powerful complement to offload expensive interactive and development workloads that Snowflake handles inefficiently.\n\n### Can MotherDuck connect to my existing BI tools like Tableau or Power BI?\n\nYes. MotherDuck and DuckDB support standard connection protocols like JDBC and ODBC, allowing them to integrate with most major BI and data visualization tools. You can power dashboards from either a local DuckDB instance or the MotherDuck serverless backend.",
  [2m  â”‚[0m     "numLines": 355,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 355
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ UDF|user.defined|python function|custom function

  [96mğŸ”§ TOOL:[0m Grep â†’ ATTACH.*postgres|attach.*other|server-side
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": "<15151 chars>",
  [2m  â”‚[0m   "numLines": 66
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "content",
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "content": "<14954 chars>",
  [2m  â”‚[0m   "numLines": 67
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_nsdf4vt5/.kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_nsdf4vt5/.kurt/sources/motherduck.com/learn-more/duckdb-python-quickstart-part2.md",
  [2m  â”‚[0m     "content": "Country: [[\"American Samoa \"]]\nRegion: [[\"OCEANIA \"]]\n```\n\n\nThis interoperability makes DuckDB a fantastic glue layer for data pipelines involving various Python libraries.\n\n## Extending DuckDB with Python: User-Defined Functions (UDFs)\n\nWARNING: UDFs and MotherDuck Please note that User-Defined Functions (UDFs), as described in this section, are a feature of the local, embedded DuckDB Python library and**do not work on MotherDuck**. The reason is that UDFs require a Python runtime to execute the function's code. MotherDuck is a serverless platform that provides SQL execution but does not run user-provided Python code on its servers. The examples below are for local DuckDB usage within a Python environment.\n\nSometimes you need to perform an operation within your SQL query that's simply not available in standard SQL or DuckDB's built-in functions, but it's easy to do in Python. This is where User-Defined Functions (UDFs) come in. DuckDB lets you define Python functions and call them directly from your SQL queries.\n\nLet's revisit the population data. Looking at the `Region`\n\ncolumn, there seem to be some extra spaces (padding) that make grouping or filtering tricky.\n\nCopy code\n\n```\nimport duckdb\n# Connect to DuckDB and load population data\ncon = duckdb.connect(database=':memory:') # Or use the persistent DB file\ncon.sql(\"INSTALL httpfs\")\ncon.sql(\"LOAD httpfs\")\n# Load the data into a table if it doesn't exist in this session\ntry:\ncon.sql(\"SELECT COUNT(*) FROM population\")\nexcept duckdb.CatalogException:\nprint(\"Loading population data from URL...\")\ncon.sql(\"SELECT * FROM read_csv_auto('https://bit.ly/3KoiZR0')\").to_table(\"population\")\nprint(\"Population data loaded.\")\ncon.sql(\"\"\"\nSELECT DISTINCT Region, length(Region) AS numChars\nFROM population\n\"\"\").show()\n```\n\n\nCopy code\n\n```\nLoading population data from URL...\nPopulation data loaded.\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Region \u2502 numChars \u2502\n\u2502 varchar \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 WESTERN EUROPE \u2502 35 \u2502\n\u2502 SUB-SAHARAN AFRICA \u2502 35 \u2502\n\u2502 NEAR EAST \u2502 35 \u2502\n\u2502 C.W. OF IND. STATES \u2502 20 \u2502\n\u2502 BALTICS \u2502 35 \u2502\n\u2502 ASIA (EX. NEAR EAST) \u2502 29 \u2502\n\u2502 OCEANIA \u2502 35 \u2502\n\u2502 NORTHERN AMERICA \u2502 35 \u2502\n\u2502 NORTHERN AFRICA \u2502 35 \u2502\n\u2502 EASTERN EUROPE \u2502 35 \u2502\n\u2502 LATIN AMER. & CARIB \u2502 23 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 11 rows 2 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n\nSee those character counts? 23 for \"LATIN AMER. & CARIB\" looks about right, but 35 for \"BALTICS\" (which is 7 characters long)? Definitely trailing spaces.\n\nDuckDB has a built-in `trim()`\n\nfunction, but for the sake of demonstration, you can write a Python UDF to remove leading/trailing spaces.\n\nCopy code\n\n```\ndef remove_spaces_py(field: str) -> str:\n\"\"\"Removes leading/trailing spaces from a string.\"\"\"\nif field is not None:\n# Use Python's strip()\nreturn field.strip() # Python's strip() removes both leading/trailing\n# Or use lstrip() and rstrip() specifically\n# return field.lstrip().rstrip()\nreturn field\n# Register the Python function as a SQL function in DuckDB\ncon.create_function('remove_spaces_py', remove_spaces_py)\n```\n\n\nYou defined a simple Python function `remove_spaces_py`\n\n. You used type hints (`str`\n\nfor input and output), which helps DuckDB infer the SQL types (VARCHAR). Then, `con.create_function()`\n\nregisters this Python function under a name you can use in SQL (`remove_spaces_py`\n\n).\n\n### Introspecting Registered Functions\n\nAfter registering a UDF, you can query DuckDB's built-in `duckdb_functions()`",
  [2m  â”‚[0m     "numLines": 100,
  [2m  â”‚[0m     "startLine": 260,
  [2m  â”‚[0m     "totalLines": 636
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now I have enough information to write a comprehensive answer. Let me write the answer file:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_3.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "create",
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_3.md",
  [2m  â”‚[0m   "content": "<4444 chars>",
  [2m  â”‚[0m   "structuredPatch": [],
  [2m  â”‚[0m   "originalFile": null
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I've completed the research and written the answer to `/tmp/answer_without_kg_3.md`. 
  [94mâ”‚[0m 
  [94mâ”‚[0m **Summary of findings:** MotherDuck does not yet support several DuckDB SQL features:
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. **Python/Native User-Defined Functions (UDFs)** - Because MotherDuck is a serverless platform that doesn't run user-provided Python code on its servers
  [94mâ”‚[0m 
  [94mâ”‚[0m 2. **Server-side ATTACH to other databases** (like PostgreSQL) - Federated queries to external databases from the cloud backend are not supported
  [94mâ”‚[0m 
  [94mâ”‚[0m 3. **Custom extensions** - Users cannot install custom DuckDB extensions on MotherDuck
  [94mâ”‚[0m 
  [94mâ”‚[0m The recommended workaround for all of these is to perform the operations locally in DuckDB or transform data upstream before loading into MotherDuck.
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 10,617 | Cost: $0.8664
     Cumulative: 10,617 tokens | $0.8664
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent has clearly indicated task completion by stating it has written the answer and provided a summary of findings without asking for user input.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 18 | Duration: 95.4s
â•‘    Tokens: 10,617 | Cost: $0.8664
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  Running LLM judge evaluation...
   ğŸ§  LLM Judge score: 0.98
