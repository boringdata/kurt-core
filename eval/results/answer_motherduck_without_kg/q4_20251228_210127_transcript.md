# Scenario: answer_motherduck_without_kg

**Status**: âœ… PASSED
**Timestamp**: 20251228_210127

**Duration**: 118.07 seconds
**Tokens Used**: 0

---

## Conversation


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ SCENARIO: answer_motherduck_without_kg
   Answer MotherDuck questions by searching sources directly (no KG)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


ğŸ” Verifying .claude installation...
   âœ“ .claude folder exists at /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_cxkixykg/.claude
   âœ“ Skills: 0 found
   âœ“ Commands: 0 found

ğŸ” Filtering to questions: [4]

ğŸ§ª Running 1 of 10 question(s) defined in questions_motherduck.yaml


======================================================================
â“ Question 4/10
...Why might a query run slower on MotherDuck than locally?
======================================================================


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¬ QUESTION 4
â”‚ Answer this question by searching through the .kurt/sources/ directory:

...Why might a query run slower on MotherDuck than locally?

Required Steps:
1. Search for relevant markdown files in .kurt/sources/ using glob and grep
2. Read the most relevant files (aim for 3-5 most relevant sources)
3. As you read each file, keep track of:
   - The file path
   - The file title/name
   - Key information extracted from that file
4. Synthesize a comprehensive answer based on the information found
5. Write your answer to: /tmp/answer_without_kg_4.md

IMPORTANT - Your answer MUST have this structure:
```markdown
# Answer

[Your comprehensive answer here, drawing from the sources you read]

## Sources

List each source file you actually read and used, showing the EXACT LOCAL FILE PATH:
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
- .kurt/sources/motherduck.com/[path/to/file.md] - Brief description of key information used from this file
[etc.]

DO NOT use URLs or document titles as links. Show the actual file path you read with the Read tool.
```

The Sources section is MANDATORY - you must list every file you read to answer the question.

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§¹ Clearing Claude Code context for clean scenario start...
   âœ“ Context cleared


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”„ TURN 1
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [96mğŸ”§ TOOL:[0m Grep â†’ slower|slow|performance|latency

  [96mğŸ”§ TOOL:[0m Grep â†’ query.*local|local.*query|MotherDuck.*local

  [96mğŸ”§ TOOL:[0m Grep â†’ network|roundtrip|round-trip|cloud
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-csvs-still-matter-the-indispensable-file-format.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-did-we-raise-now-ceo-backstage-call.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-dont-data-producers-pay-attention-to-how-their-data-is-used-downstream.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-should-you-care-about-duckdb-ft-mihai-bojin.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-use-duckdb-in-your-data-pipelines-ft-niels-claeys.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/take-flight-with-the-modern-duck-stack.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-the-power-of-duckdb-for-interactive-sql-notebooks.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-can-postgres-learn-from-duckdb-pgconfdev-2025.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/what-if-sql-queries-returned-results-instantly.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/where-data-science-meets-shrek-how-buzzfeed-uses-ai.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/speed-querying-stackoverflow-data-with-duckdb-ft-michael-hunger.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-bad-the-good-and-the-ugly-navigating-the-new-waves-of-machine-learning-ft-adi-polak.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-surprising-birth-of-duckdb-ft-co-creator-hannes-muhleisen.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/why-web-developers-should-care-about-analytical-databases.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/data-discoverability-secoda-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/webinar/ducking-easy-insights-motherduck-dbt.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/understanding-ducklake-a-table-format-with-a-modern-architecture.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/taming-file-zoos-data-science-with-duckdb-database-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/simple-way-to-convert-csv-and-parquet-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/is-bi-too-big-for-small-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/making-pyspark-code-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-ga-produck-tour.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/faster-data-pipelines-development-with-mcp-and-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-bootstrap-a-data-warehouse-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-vs-pandas-vs-polars-for-python-devs.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unleashing-duckdb-dbt-for-local-analytics-triumphs.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-dbt-accelerating-the-developer-experience-with-local-power.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/build-bigger-with-small-ai-running-small-models-locally.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-duck-in-the-hand-is-worth-two-in-the-cloud.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-new-paradigm-for-data-visualization-with-just-sql-markdown.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/trust-and-security.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ dual.*execution|hybrid.*execution

  [96mğŸ”§ TOOL:[0m Grep â†’ cold start|cold-start|startup

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_cxkixykg/.kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_cxkixykg/.kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     "content": "---\ntitle: bringing-duckdb-to-the-cloud-dual-execution-explained\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained\nindexed_at: '2025-11-25T20:45:20.295187'\ncontent_hash: ab4030806c409492\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nBringing DuckDB to the Cloud: Dual Execution Explained - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Bringing DuckDB to the Cloud: Dual Execution Explained](https://www.youtube.com/watch?v=2eVNm4A8sLw)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=2eVNm4A8sLw&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 1:08:11\n\n\u2022Live\n\n\u2022\n\nYouTubeQuack & CodeMotherDuck Features\n\n# Bringing DuckDB to the Cloud: Dual Execution Explained\n\n2024/06/28\n\nBringing DuckDB's analytical power to the cloud requires more than just running it on a server. While DuckDB excels at processing data on a local machine, the dynamics of data analysis change when dealing with cloud-scale data, collaboration, and shared resources. This shift introduces challenges around [security](https://motherduck.com/learn-more/secure-startup-data-warehouse/), concurrent access, and performance, which is where a purpose-built cloud architecture becomes essential.\n\nMotherDuck, a modern cloud data warehouse, is built on DuckDB to solve these challenges. It extends DuckDB\u2019s capabilities without forking the open source project, creating a hybrid system that intelligently balances local and cloud computing. In a conversation with MotherDuck founding engineer Stephanie, she explained the architecture and the innovative query model, known as **dual execution**, that makes this possible.\n\n* * *\n\n## The MotherDuck Architecture: Beyond Hosted DuckDB\n\nA common misconception is that MotherDuck is simply DuckDB hosted in the cloud. The reality is a more sophisticated system designed to overcome the limitations of using a locally-optimized engine in a distributed environment. MotherDuck\u2019s architecture is built on three key components and a core philosophy of not forking DuckDB.\n\nInstead of maintaining a separate version, MotherDuck leverages DuckDB\u2019s powerful **extension system**. This allows MotherDuck to add new capabilities at multiple layers, including the SQL parser, optimizer, and storage interface, while staying current with the latest open source DuckDB releases. This tight integration means users benefit from the rapid innovation of the DuckDB community almost immediately.\n\nThe architecture can be broken down into three main layers:\n\n1. **The Client Extension**: This is how MotherDuck integrates with the DuckDB ecosystem. Whether a user is working in the CLI, a Python script, or a JDBC connection, a simple `ATTACH` command connects their local DuckDB instance to the MotherDuck cloud. This extension is even used to run DuckDB in the browser via WASM for the MotherDuck UI, enabling client-side processing to reduce latency and cloud compute costs.\n2. **The Compute Layer**: In the cloud, queries are processed by containerized DuckDB instances, fondly called \"ducklings.\" These compute resources are scaled based on user needs, providing the necessary CPU and memory to handle complex analytical queries on large datasets.\n3. **The Storage Layer**: DuckDB's native file format is optimized for a single writer on a local file system. This model is not well-suited for the cloud, where multiple users need to read and write to the same database concurrently. To solve this, MotherDuck implemented a differential storage system that maps a logical database file to append-only snapshot layers in cloud object storage. This design is cloud-friendly, enabling efficient in-place updates and forming the foundation for features like database sharing and time travel.\n\n* * *\n\n## Enabling Collaboration with Database Sharing and Secure Credentials\n\nThis unique architecture transforms DuckDB from a \"single-player\" tool into a collaborative \"multiplayer\" platform. One of the most significant advantages is **database sharing**. Instead of emailing SQL scripts or passing around large database files, team members within the same organization can grant query access to their databases. This streamlines collaboration, ensuring everyone works from a consistent and up-to-date version of the data.\n\nSecurity is another critical aspect of any cloud data platform. MotherDuck provides a **centralized Secret Manager**, co-designed with the DuckDB team. Users can create persistent, encrypted secrets for accessing external data sources like AWS S3, Google Cloud Storage, or Delta Lake. Once a secret is created in MotherDuck, it can be reused across different clients and sessions without needing to expose or reconfigure credentials on each local machine.\n\n* * *\n\n## Understanding Dual Execution\n\nThe most innovative feature of MotherDuck's architecture is its **dual execution model**. This is a hybrid query execution strategy where the optimizer intelligently decides whether to run parts of a query locally on the client or remotely in the MotherDuck cloud. The primary goal is to minimize data movement and leverage compute where it makes the most sense.\n\nThe process was demonstrated with a query joining two tables. When both tables reside in a MotherDuck database, the query is executed entirely in the cloud. An `EXPLAIN` plan for this query reveals that all operators, from the table scans to the final join, are marked as **remote**. The only local operation is downloading the final result set to the client.\n\nThe real power of dual execution becomes apparent when a query involves both local and cloud data. In the demonstration, one of the remote tables was copied to a local Parquet file. The query was then modified to join this local file with the table that remained in the cloud.\n\nThe `EXPLAIN` plan for this new query showed a mix of local and remote operations. The scan of the Parquet file and its associated processing happened **locally**, leveraging the user's machine. The scan of the cloud table happened **remotely** on MotherDuck's compute. The system then efficiently transferred only the necessary intermediate data to complete the join. This hybrid approach avoids needlessly uploading the local file to the cloud or downloading the entire remote table, resulting in faster and more efficient queries.\n\n* * *\n\n## Taking Control with Manual Execution Overrides\n\nWhile MotherDuck's optimizer is designed to make the most efficient choice automatically, some situations require manual control. For this, users can use the `md_run` parameter within scan functions like `read_parquet()` or `read_csv()`.\n\nBy setting `md_run = 'local'`, a user can force the scan of a remote S3 file to be executed on their local client. This involves downloading the data, which might be desirable if the data needs to be processed near the user's location or if they prefer using local credentials. Conversely, setting `md_run = 'remote'` forces the operation to run in the MotherDuck cloud.\n\nThis control is especially useful for performance tuning. A direct comparison showed that querying a 2GB Parquet file on S3 was significantly faster when executed remotely in MotherDuck (25 seconds) compared to a local DuckDB client that had to first download the data (36 seconds). By pushing the computation to the data's location, MotherDuck minimizes network I/O and delivers results more quickly.\n\n* * *\n\n## A Symbiotic Future for Local and Cloud Analytics\n\nMotherDuck's architecture is a thoughtful extension of DuckDB's core principles, adapting its local-first power for the demands of the cloud. The dual execution model provides a flexible and efficient bridge between a user's laptop and cloud data, optimizing workloads in a way that pure-cloud or pure-local systems cannot.\n\nThis relationship is symbiotic. As the largest production user of DuckDB, MotherDuck continuously pushes the engine to its limits, uncovering opportunities for improvement and contributing enhancements back to the open source project. This collaboration ensures that both DuckDB and MotherDuck will continue to evolve, offering a powerful and seamless analytical experience that spans from local exploration to cloud-scale production.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Ficeberg_video_thumbnail_adfdabe3d6.jpg&w=3840&q=75)\\\\\n\\\\\n46:26](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n[2025-06-12](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse/)\n\n### [Escaping Catalog Hell: A Guide to Iceberg, DuckDB & the Data Lakehouse](https://motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse)\n\nBuilding a data stack means choosing between easy SaaS and complex open-source. Apache Iceberg is a middle ground, but its catalog is a hurdle. New tools now simplify using Iceberg with DuckDB to create a flexible, local data lakehouse.\n\nMotherDuck Features\n\n[![\"Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FG_Se_B_Sox_AW_Fg_HD_f4fddaa9ab.jpg&w=3840&q=75)](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n[2025-04-23](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly/)\n\n### [Instant SQL Mode - Real Time Feedback to Make SQL Data Exploration Fly](https://motherduck.com/videos/instant-sql-mode-real-time-feedback-to-make-sql-data-exploration-fly)\n\nHamilton Ulmer shares insights from MotherDuck's Instant SQL Mode, exploring how real-time query result previews eliminate the traditional write-run-debug cycle through client-side parsing and DuckDB-WASM caching.\n\nSQL\n\nTalk\n\nMotherDuck Features\n\n[![\"DuckDB: Run dbt build with sub-second execution times\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_03_26_at_3_40_16_PM_c6e3a8096f.png&w=3840&q=75)\\\\\n\\\\\n24:06](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n[2025-03-13](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n### [DuckDB: Run dbt build with sub-second execution times](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times)\n\nWhether you're new to DuckDB or looking to optimize your workflows, this session will provide practical insights to help you leverage its full potential.\n\nYouTube\n\nData Pipelines\n\ndbt\n\nMotherDuck Features\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 184,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 184
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/escaping-catalog-hell-a-guide-to-iceberg-duckdb-the-data-lakehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/react-components-testing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/what-is-a-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/modern-data-warehouse-playbook.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/no-etl-query-raw-files.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/data-warehouse-tco.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/bigquery-alternative-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/cloud-data-warehouse-startup-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/forms/finance-onboarding.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/case-studies/reflex-careers-gig-economy-retail.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/motherduck-ga-produck-tour.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckfooding-at-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/data-teams.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/duckdb-users.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/pricing.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/product/app-developers.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/product-analytics-motherduck-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/reduce-snowflake-costs-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/secure-startup-data-warehouse.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/self-service-analytics-startups.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/learn-more/web-assembly.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 20,
  [2m  â”‚[0m   "appliedLimit": 20
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m This is very helpful. Now let me search for more specific content about why queries might be slower on MotherDuck:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ data movement|network latency|upload|download|transfer

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_cxkixykg/.kurt/sources/motherduck.com/videos/a-duck-in-the-hand-is-worth-two-in-the-cloud.md

  [96mğŸ”§ TOOL:[0m Glob â†’ **/*performance*
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_cxkixykg/.kurt/sources/motherduck.com/videos/a-duck-in-the-hand-is-worth-two-in-the-cloud.md",
  [2m  â”‚[0m     "content": "---\ntitle: a-duck-in-the-hand-is-worth-two-in-the-cloud\ncontent_type: event\nsource_url: https://motherduck.com/videos/a-duck-in-the-hand-is-worth-two-in-the-cloud\nindexed_at: '2025-11-25T20:43:57.610535'\ncontent_hash: f012bc1f4514dcb3\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nMatson & Prawiroatmodjo- Data preparation and analytics on your laptop with DuckDB \\| PyData NYC 2024 - YouTube\n\n[Photo image of PyData](https://www.youtube.com/channel/UCOjD18EJYcsBog4IozkF_7w?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nPyData\n\n170K subscribers\n\n[Matson & Prawiroatmodjo- Data preparation and analytics on your laptop with DuckDB \\| PyData NYC 2024](https://www.youtube.com/watch?v=yi2zgenIZm4)\n\nPyData\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=yi2zgenIZm4&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 33:22\n\n\u2022Live\n\n\u2022\n\nYouTubeBI & VisualizationAI, ML and LLMsSQLPythonTalk\n\n# A duck in the hand is worth two in the cloud\n\n2024/11/08\n\nWhat if I told you that you could complete a JSON parse and extract task on your laptop before a distributed compute cluster even finishes booting up? DuckDB is a lightweight, in-process analytical database that runs on your laptop inside of Python and can wrangle large datasets efficiently, both from local and remote data sources. In this talk, we will show you how to query a dataset with DuckDB to extract, load and transform data right on your laptop. We'll then show you how to move your workloads to the Cloud, so you can run them at scale. By developing locally and pushing to the Cloud it's not only easy to develop, debug and iterate, but also makes it easy to quickly switch back and forth between workloads that do and don't require Cloud compute resources, cutting both cost and time.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Data-based: Going Beyond the Dataframe\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FData_based_f32745b461.png&w=3840&q=75)](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n[2025-11-20](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n### [Data-based: Going Beyond the Dataframe](https://motherduck.com/videos/going-beyond-the-dataframe)\n\nLearn how to turbocharge your Python data work using DuckDB and MotherDuck with Pandas. We walk through performance comparisons, exploratory data analysis on bigger datasets, and an end-to-end ML feature engineering pipeline.\n\nWebinar\n\nPython\n\nAI, ML and LLMs\n\n[![\"Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FHex_Webinar_778e3959e4.png&w=3840&q=75)](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n[2025-11-14](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n### [Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck)\n\nAI isn't here to replace data work, it's here to make it better. Watch this webinar to see how Hex and MotherDuck build AI workflows that prioritize context, iteration, and real-world impact.\n\nWebinar\n\nAI, ML and LLMs\n\n[![\"Can DuckDB replace your data stack?\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FCan_Duck_DB_Replace_Your_Data_Stack_Mother_Duck_Co_Founder_Ryan_Boyd_3_56_screenshot_70e18322ec.png&w=3840&q=75)\\\\\n\\\\\n60:00](https://motherduck.com/videos/can-duckdb-replace-your-data-stack/)\n\n[2025-10-23](https://motherduck.com/videos/can-duckdb-replace-your-data-stack/)\n\n### [Can DuckDB replace your data stack?](https://motherduck.com/videos/can-duckdb-replace-your-data-stack)\n\nMotherDuck co-founder Ryan Boyd joins the Super Data Brothers show to talk about all things DuckDB, MotherDuck, AI agents/LLMs, hypertenancy and more.\n\nYouTube\n\nBI & Visualization\n\nAI, ML and LLMs\n\nInterview\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 129,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 129
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "filenames": [],
  [2m  â”‚[0m   "durationMs": 409,
  [2m  â”‚[0m   "numFiles": 0,
  [2m  â”‚[0m   "truncated": false
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/pg_duckdb-postgres-analytics-just-got-faster-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-core-to-custom-unlocking-new-possibilities-with-duckdb-extensions.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/how-to-efficiently-load-data-into-ducklake-with-estuary.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-tutorial-for-beginners.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-motherduck-for-beginners-your-ultimate-guide.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/bringing-duckdb-to-the-cloud-dual-execution-explained.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/a-new-paradigm-for-data-visualization-with-just-sql-markdown.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-service.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/terms-of-use.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 15,
  [2m  â”‚[0m   "appliedLimit": 15
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_cxkixykg/.kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_cxkixykg/.kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_cxkixykg/.kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_cxkixykg/.kurt/sources/motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql.md",
  [2m  â”‚[0m     "content": "---\ntitle: stay-in-flow-with-motherducks-instant-sql\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/stay-in-flow-with-motherducks-instant-sql\nindexed_at: '2025-11-25T20:44:49.950311'\ncontent_hash: e6211ffb372b0740\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nStay in Flow with MotherDuck's Instant SQL - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Stay in Flow with MotherDuck's Instant SQL](https://www.youtube.com/watch?v=T3gmsbohn48)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=T3gmsbohn48&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 57:59\n\n\u2022Live\n\n\u2022\n\nYouTube\n\n# Stay in Flow with MotherDuck's Instant SQL\n\n2025/05/21\n\n## What is MotherDuck?\n\nMotherDuck is a cloud data warehouse built on top of and powered by DuckDB, focused on making big data feel small. Unlike traditional big data systems that emerged in the early 2000s with technologies like Hadoop and Spark, MotherDuck takes a different approach by recognizing two key changes in the modern data landscape:\n\n- **Modern hardware is significantly more powerful**: Today's laptops have multiple cores and substantial RAM, with some EC2 instances offering hundreds of cores and terabytes of memory\n- **Most queries aren't actually \"big data\"**: Analysis shows that 99% of queries on systems like Redshift and Snowflake can fit on a single large node, with most under a gigabyte\n\nMotherDuck's architecture is built on three core pillars:\n\n1. **Effortless and serverless**: Pay only for what you use without managing resources or clusters\n2. **Dedicated instances**: Each user gets their own \"duckling\" (DuckDB instance) to avoid noisy neighbor problems\n3. **Dual execution query engine**: Run queries locally, in the cloud, or combine both for optimal performance\n\n## The Problem with Traditional SQL Workflows\n\nTraditional SQL development follows a frustrating pattern that breaks flow state:\n\n1. Write your query\n2. Hit the run button\n3. Wait for results\n4. Debug errors\n5. Repeat\n\nThis write-wait-debug cycle can happen hundreds or thousands of times when developing new queries. Each wait forces a context switch that exhausts mental energy and disrupts concentration. As Hamilton, a front-end engineer at MotherDuck, explains, this interaction model hasn't fundamentally changed since the 1970s.\n\nThe lack of observability tools in SQL makes debugging particularly challenging:\n\n- **CTE debugging**: Common Table Expressions are notoriously difficult to debug, requiring manual commenting and isolation\n- **Complex expressions**: Breaking apart column expressions to identify issues requires writing more SQL\n- **No immediate feedback**: Unlike modern development environments, SQL lacks real-time validation and preview capabilities\n\n## Introducing Instant SQL\n\nInstant SQL transforms SQL development by providing immediate feedback on every keystroke, similar to how digital audio workstations (DAWs) work in music production. The system is guided by Brett Victor's principle: \"Creators need an immediate connection to what they create.\"\n\n### Key Features\n\n**Real-time Query Execution**\n\n- Results update with every keystroke (50-100ms latency)\n- Powered by DuckDB's local execution capabilities\n- Smart caching and query rewriting for performance\n\n**Advanced Observability**\n\n- Click on any column to decompose complex expressions\n- Navigate through CTEs with instant result previews\n- Parser-aware syntax highlighting showing query structure\n\n**AI Integration**\n\n- Context-aware suggestions based on cursor position\n- Real-time preview of AI-generated changes\n- Semantic understanding of query intent\n\n## Technical Implementation\n\nInstant SQL leverages several technical innovations:\n\n### Parser-Powered Intelligence\n\nDuckDB exposes its query parser through SQL, allowing Instant SQL to:\n\n- Generate abstract syntax trees (AST)\n- Identify expression boundaries and table references\n- Create a semantic path through the query based on cursor position\n\n### Intelligent Caching Strategy\n\nThe system automatically:\n\n- Parses queries to identify table references\n- Builds a directed acyclic graph (DAG) of dependencies\n- Creates optimized caches for interactive modeling\n- Rewrites queries to use cached data\n\n### Dual Execution with MotherDuck\n\nFor large datasets, MotherDuck's architecture enables:\n\n- Server-side scanning and filtering of massive tables\n- Local caching of relevant subsets\n- Seamless coordination between cloud and local execution\n\n## Practical Applications\n\n### Local File Exploration\n\nQuery local files directly without uploading to the cloud:\n\n```sql\nCopy code\n\nSELECT * FROM 'path/to/file.parquet'\n```\n\n### Cross-Database Queries\n\nCombine data from multiple sources:\n\n- MotherDuck tables\n- PostgreSQL replicas\n- Local files\n- S3 object storage\n\n### Interactive Data Modeling\n\n- Modify CTEs and see downstream impacts immediately\n- Test transformations without full query execution\n- Debug complex joins and aggregations in real-time\n\n## Performance at Scale\n\nInstant SQL scales effectively through:\n\n- **Smart sampling**: For terabyte-scale data, it samples intelligently\n- **Filtered caching**: WHERE clauses execute server-side to minimize data transfer\n- **Incremental updates**: Only affected parts of the query are re-executed\n\nThe system works with datasets ranging from local CSV files to 100+ billion row tables in MotherDuck, adapting its caching strategy based on data size and query complexity.\n\n## Getting Started\n\nTo try Instant SQL:\n\n- Sign up for a MotherDuck account at motherduck.com\n- Run `duckdb -i` for local-only exploration without an account\n- Join the community at slack.motherduck.com for support and feedback\n\nInstant SQL represents a fundamental shift in SQL development, transforming a traditionally passive, wait-heavy process into an active, immediate experience that keeps developers in flow state while writing complex analytical queries.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"Data-based: Going Beyond the Dataframe\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FData_based_f32745b461.png&w=3840&q=75)](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n[2025-11-20](https://motherduck.com/videos/going-beyond-the-dataframe/)\n\n### [Data-based: Going Beyond the Dataframe](https://motherduck.com/videos/going-beyond-the-dataframe)\n\nLearn how to turbocharge your Python data work using DuckDB and MotherDuck with Pandas. We walk through performance comparisons, exploratory data analysis on bigger datasets, and an end-to-end ML feature engineering pipeline.\n\nWebinar\n\nPython\n\nAI, ML and LLMs\n\n[![\"Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FHex_Webinar_778e3959e4.png&w=3840&q=75)](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n[2025-11-14](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck/)\n\n### [Empowering Data Teams: Smarter AI Workflows with Hex & MotherDuck](https://motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck)\n\nAI isn't here to replace data work, it's here to make it better. Watch this webinar to see how Hex and MotherDuck build AI workflows that prioritize context, iteration, and real-world impact.\n\nWebinar\n\nAI, ML and LLMs\n\n[![\"Lies, Damn Lies, and Benchmarks\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FLies_Damn_Lies_and_Benchmarks_Thumbnail_404db1bf46.png&w=3840&q=75)](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n[2025-10-31](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n### [Lies, Damn Lies, and Benchmarks](https://motherduck.com/videos/lies-damn-lies-and-benchmarks)\n\nWhy do database benchmarks so often mislead? MotherDuck CEO Jordan Tigani discusses the pitfalls of performance benchmarking, lessons from BigQuery, and why your own workload is the only benchmark that truly matters.\n\nStream\n\nInterview\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 253,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 253
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_cxkixykg/.kurt/sources/motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck.md",
  [2m  â”‚[0m     "content": "---\ntitle: local-dev-cloud-prod-with-dagster-and-motherduck\ncontent_type: tutorial\nsource_url: https://motherduck.com/videos/local-dev-cloud-prod-with-dagster-and-motherduck\nindexed_at: '2025-11-25T20:44:38.997867'\ncontent_hash: 93135d51ac34dc01\nhas_narrative: true\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nLocal Dev, Cloud Prod with Dagster and MotherDuck - YouTube\n\n[Photo image of MotherDuck](https://www.youtube.com/channel/UCC0AT6XjO_ebWIifTDp5REg?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nMotherDuck\n\n10.8K subscribers\n\n[Local Dev, Cloud Prod with Dagster and MotherDuck](https://www.youtube.com/watch?v=cOSiMMb_rjk)\n\nMotherDuck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=cOSiMMb_rjk&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 52:48\n\n\u2022Live\n\n\u2022\n\nYouTubeData Pipelines\n\n# Local Dev, Cloud Prod with Dagster and MotherDuck\n\n2024/04/22\n\nHave you ever wondered how the seamless transition from a local development environment to production in data engineering can significantly boost efficiency and innovation? A staggering number of data engineers and developers grapple with this challenge, often hindered by the complexities of ensuring consistency, scalability, and reliability across different stages of their data pipelines. This article sheds light on a game-changing strategy that leverages the synergies between MotherDuck and Dagster, two pioneering platforms that are redefining the landscape of data engineering.\n\nBy exploring the transformative approach of integrating MotherDuck and Dagster, readers will gain invaluable insights into streamlining their data pipelines from development to production. Witness firsthand the journey of Colton Padden, who made a pivotal shift from Airflow to Dagster, driven by the platform's unmatched ease of use and efficiency. Moreover, delve into Alex's transition from an industrial engineer to a data scientist at Intel, where his fascination with DuckDB catalyzed his current pioneering role. This exploration not only highlights the personal and professional transformations brought about by these platforms but also addresses the common hurdles faced in data engineering, particularly the daunting task of moving from local development to production environments.\n\nHow do MotherDuck and Dagster collectively propose to navigate these challenges, offering a beacon of hope for data engineers and developers seeking to elevate their workflow? Engage with this comprehensive guide to uncover the strategies that could revolutionize your data engineering projects, empowering you with the tools to thrive in the ever-evolving digital landscape.\n\n## Introduction to MotherDuck and Dagster - A Comprehensive Guide to Streamlining Data Pipelines from Development to Production\n\nIn the realm of data engineering, the leap from a local development environment to a fully-fledged production system often presents a daunting array of challenges. From ensuring data integrity and consistency to optimizing performance and scalability, data engineers and developers are constantly in search of more efficient, reliable solutions. Enter MotherDuck and Dagster, two innovative platforms that have emerged as game-changers in the way data pipelines are managed and executed.\n\nMotherDuck, building on the prowess of DuckDB, reimagines cloud data warehousing by prioritizing developer experience and efficiency, while Dagster presents itself as a modern data orchestrator focused on enhancing developer workflow and productivity. The synergy between these platforms is not just about technology; it's about transforming the approach to data engineering from the ground up.\n\nColton Padden's journey from being an avid Airflow user to becoming an advocate for Dagster encapsulates the transformative impact of embracing new technologies in data engineering. His experience highlights not just the ease of use but the profound efficiency gains that come with adopting Dagster. Similarly, Alex's path from industrial engineering to data science, propelled by his intrigue for DuckDB, underscores the importance of innovative tools in career evolution and the execution of data projects.\n\nThe integration of MotherDuck and Dagster offers a compelling solution to the common problems faced by data engineers, especially the intricate process of transitioning from local development to production. This guide aims to explore the intricacies of this integration, providing insights into how data engineers and developers can leverage these platforms to streamline their data pipelines, enhance productivity, and ultimately, transform their data engineering practices for the better.\n\nWhat specific challenges do these platforms address, and how do they pave the way for a smoother, more efficient transition from development to production?\n\n## The Problem Statement and Proposed Solution: Navigating the Challenges of Data Engineering with Innovative Tools\n\nIn the intricate world of data engineering, professionals often encounter a myriad of obstacles that can hinder the development and deployment of efficient data pipelines. These challenges range from **mocking data sources** for testing environments, **writing unit tests for data pipelines** to ensure reliability and accuracy, to the complexities involved in **integrating with external systems**\u2014each presenting a unique set of difficulties in the transition from development to production. Colton Padden's insights into these common hindrances underscore the necessity for tools that not only address these issues but do so in a manner that augments developer productivity.\n\nThe introduction of **Dagster, MotherDuck, and Evidence** marks a significant leap forward in the quest for solutions that embody the principles of software engineering within the realm of data engineering. These tools collectively offer a paradigm shift in how data pipelines are constructed, tested, and deployed:\n\n- **Dagster** emerges as a beacon of modern data orchestration, emphasizing a workflow-centric approach that enhances visibility and control over data pipeline operations. Its asset-centric model facilitates a clear visualization of data lineage and dependencies, ensuring an organized and maintainable codebase.\n\n- **MotherDuck** takes the stage as a revolutionary cloud data warehouse solution, leveraging DuckDB's prowess to offer unparalleled consistency between local development and cloud deployment. Its serverless architecture and Git-like operations for databases pave the way for efficient resource utilization and effortless version control, respectively.\n\n- The integration with **Evidence** introduces an innovative method for building data dashboards, wherein SQL queries can be embedded directly within markdown files. This simplicity in dashboard creation democratizes data visualization, allowing developers and analysts alike to craft dynamic data stories without the need for extensive technical expertise in data science.\n\n\nThe significance of these developments cannot be overstated. By applying software engineering principles to data engineering, these tools collectively enhance efficiency and developer experience across the board. One of the most groundbreaking aspects of this integration is the seamless transition it facilitates from local development to production environments. This transition, characterized by a lack of code changes when moving from using DuckDB locally to leveraging MotherDuck in the cloud, epitomizes the efficiency and ease of scalability that modern data projects require.\n\nConsider the specific use case of building dashboards with Evidence. The ability to embed SQL queries in markdown for dynamic data visualization not only simplifies the process but also accelerates the development cycle, enabling rapid iteration and deployment of insightful data visualizations. This approach not only saves time but also ensures that data insights are accessible and actionable.\n\nThrough the lens of these innovative solutions, it becomes clear that the future of data engineering lies in embracing tools and methodologies that streamline the pipeline from development to production. By fostering an environment where efficiency and developer productivity are paramount, Dagster, MotherDuck, and Evidence are setting a new standard for how data engineering challenges are addressed. As these tools continue to evolve and gain traction, the data engineering landscape is poised for a transformation that prioritizes agility, reliability, and accessibility in data operations.\n\n## Deep Dive into Dagster: A Modern Data Orchestration Framework\n\nAt the heart of effective data engineering lies the orchestration of complex data pipelines, a task that requires precision, foresight, and the right set of tools. **Dagster**, as introduced by Colton Padden, emerges not just as a tool but as a comprehensive framework designed to refine and enhance the way data engineers and developers manage their pipelines. What sets Dagster apart is its foundational approach to orchestrating data pipelines, emphasizing **developer workflow and productivity** above all.\n\nUnlike traditional orchestrators that focus on tasks as discrete units of work, Dagster introduces a **paradigm shift towards assets**. This shift is more than a mere change in terminology; it represents a fundamental rethinking of how data pipelines are constructed and visualized. Assets, in the Dagster universe, are tangible elements\u2014be it a table, a report, or a machine learning model\u2014that provide a clearer visualization of data lineage and dependencies. This approach not only simplifies the understanding of complex data flows but also enhances the manageability of dependencies within the pipeline.\n\nOne of the standout features of Dagster is its **auto materialization policies**. In traditional setups, pipelines are often triggered on a schedule, without regard to whether the upstream data has changed. Dagster, however, employs a more reactive model. These policies enable pipelines to trigger based on changes in upstream data, thereby ensuring that data flows are not just efficient but also relevant. This responsiveness to data changes underscores Dagster's commitment to efficiency and resource optimization.\n\nDagster's prowess is not limited to orchestrating workflows and managing assets. Its **extensive support for integrations** with a wide array of tools and platforms makes it a versatile player in the modern data stack. Whether it's integrating with data warehouses like Snowflake, computation platforms like Dask, or visualization tools like Evidence, Dagster serves as the linchpin that unites various components of the data stack, facilitating a seamless flow of data across tools and teams.\n\nUnderstanding how assets are defined and managed in Dagster offers insights into its structural and operational efficiency. **Assets in Dagster** are not just static entities but are defined with rich context, including metadata that describes their lineage, parameters, and dependencies. This structured approach to handling data assets within pipelines not only enhances transparency but also bolsters the reliability of the entire data ecosystem.\n\nAs developers and data engineers delve into Dagster, they discover a framework that is not just about executing data tasks but about creating a cohesive and efficient data operation environment. Dagster's emphasis on assets, coupled with its reactive triggering mechanisms and robust integration capabilities, positions it as a critical tool in the arsenal of modern data professionals seeking to navigate the complexities of data pipeline orchestration with ease and efficiency.\n\n## Introduction to MotherDuck: Rethinking the Cloud Data Warehouse\n\nIn a landscape dominated by scale-centric cloud data warehouses, the inception of **MotherDuck** signals a pivotal shift, emphasizing the **developer experience** as the cornerstone of modern data warehousing. Alex, a pivotal figure behind MotherDuck, articulates the necessity for this paradigm shift, driven by the limitations of traditional data warehousing architectures that often sideline the agility and productivity of developers. MotherDuck emerges as a beacon of innovation, seamlessly blending the robustness of cloud warehousing with the nimbleness required for agile development.\n\n**Git-like operations for databases** stand out as one of MotherDuck's most groundbreaking features. This functionality ushers in a new era for database versioning and deployment, offering:\n\n- **Zero-copy cloning**: Instantly clone databases for development or testing without the data movement overhead.\n- **Branching and merging**: Manage database changes with the same flexibility as code changes, facilitating smoother transitions from development to production environments.\n\nAt the core of MotherDuck's philosophy is its seamless integration with **DuckDB**, ensuring a uniform experience from local development to cloud deployment. This integration eliminates the common friction points encountered when moving workloads to the cloud, fostering an environment where developers can focus on innovation rather than infrastructure nuances. The synergy between MotherDuck and DuckDB ensures:\n\n- Consistent SQL dialects and functions, regardless of the environment.\n- A streamlined path from prototype to production, without the need to rewrite or adjust code for cloud deployment.\n\nThe **serverless architecture** of MotherDuck introduces a dynamic approach to resource allocation, where compute resources are tailored to the workload's demands, significantly reducing operational costs. This elasticity allows organizations to scale their data operations without the burden of over-provisioning or managing complex scaling policies.\n\nFurthermore, the **dual-engine execution model** represents a breakthrough in query processing, elegantly balancing the decision of where to process queries\u2014locally or in the cloud\u2014based on:\n\n- **Data locality**: Optimize query performance by processing data closest to its source, reducing latency and transmission costs.\n- **Query complexity**: Intelligent routing of queries to the most appropriate execution environment, ensuring optimal use of resources.\n\nMotherDuck's innovative approach not only redefines the cloud data warehousing landscape but also aligns with the evolving needs of data engineers and developers. By prioritizing developer experience and operational efficiency, MotherDuck stands as a testament to the belief that the future of data warehousing lies in flexibility, scalability, and, most importantly, empowering those who harness data to drive insights and innovation.\n\n## Evidence: Revolutionizing Data Dashboards with Markdown and DuckDB\n\n**Evidence** emerges as a transformative platform in the realm of data applications and dashboards, offering an unparalleled mix of simplicity and power. At the heart of its innovation is the integration of DuckDB WebAssembly, which propels Evidence into the forefront of responsive and interactive user experiences. This integration is not just about leveraging DuckDB's capabilities in a new environment; it's about redefining how developers and data analysts approach the creation and dissemination of data visualizations.\n\nColton Padden's firsthand experience with building data applications using Evidence showcases the platform's unique approach to dashboard creation: writing in markdown. This methodology isn't just about simplicity; it's about accessibility. By allowing SQL queries to be embedded directly into markdown files, Evidence lowers the barrier to dynamic data visualization, making it a powerful tool for those without extensive coding skills. The implications of this approach are significant, offering:\n\n- **Ease of Use**: Users can create complex, interactive dashboards with minimal coding, focusing on the storytelling aspect of data rather than the technicalities of implementation.\n- **Flexibility**: The markdown format is universally recognized and can be easily edited and versioned, facilitating collaboration and continuous improvement.\n\nThe seamless integration of Evidence with MotherDuck and DuckDB ensures a coherent and efficient workflow from data processing to visualization. This trio of technologies harmonizes to create a unified environment where data can be effortlessly processed, analyzed, and visualized without the need for transitioning between different platforms or languages. This integration demonstrates the platforms' commitment to developer and analyst productivity, underscoring the potential of these tools to streamline the data analysis process.\n\nPractical examples of dashboards created during the presentation illuminate Evidence's effectiveness in delivering insightful data visualizations. These examples not only illustrate the platform's capability to render complex data sets into comprehensible and visually appealing formats but also highlight the ease with which these visualizations can be customized and enriched with interactive elements.\n\nThe democratization of data visualization represents one of Evidence's most compelling contributions to the field of data science. By making sophisticated data analysis and visualization accessible to a broader audience, Evidence empowers organizations to harness the full potential of their data. This accessibility is pivotal for:\n\n- **Enhancing Decision-Making**: Empowering more team members with the ability to analyze and visualize data fosters a data-driven culture.\n- **Promoting Innovation**: When barriers to data visualization are removed, it opens the door for innovative solutions to emerge from across an organization.\n\nIn essence, Evidence's innovative platform, built on the robust capabilities of DuckDB WebAssembly and seamlessly integrated with MotherDuck, signifies a leap forward in data visualization and application development. Its approach not only simplifies the creation process but also extends the power of data visualization to those without specialized expertise in data science. This paradigm shift has the potential to transform how organizations leverage data, making informed decisions more accessible than ever before.\n\n## The Future of Data Engineering with Dagster, MotherDuck, and Evidence\n\nAs we stand on the brink of a new era in data engineering, the integration of **Dagster**, **MotherDuck**, and **Evidence** presents an unprecedented opportunity to reshape how data pipelines are developed, managed, and scaled. This trio of tools ushers in a transformative approach to handling data, from local development environments to production, emphasizing efficiency, scalability, and an enhanced developer experience.\n\nThe implications for data engineering are profound:\n\n- **Efficiency**: By streamlining the development process and minimizing the need for code changes when transitioning between environments, these tools significantly reduce the time and effort required to deploy data pipelines.\n- **Scalability**: The seamless integration of MotherDuck and DuckDB, coupled with Dagster's orchestration capabilities, allows for dynamic scaling of data pipelines, ensuring that they can handle increasing loads without compromising performance.\n- **Developer Experience**: The focus on developer productivity tools, such as the ease of writing unit tests for data pipelines and the application of software engineering principles to data engineering, enhances the overall experience and productivity of data professionals.\n\nLooking ahead, the future developments and enhancements of these platforms hold great promise. With the continuous evolution of technology and the increasing complexity of data workflows, the adaptability and innovation demonstrated by Dagster, MotherDuck, and Evidence will be critical. As hinted by the speakers, we can anticipate further advancements in these platforms that will push the boundaries of what is possible in data engineering.\n\nFor data professionals seeking to tackle the challenges of data engineering, exploring these tools further is not just an option\u2014it's a necessity. The resources provided offer a starting point for diving into the capabilities and potential applications of these innovative solutions. Whether it's Dagster's Python data orchestrator that enhances workflow and productivity, MotherDuck's rethinking of cloud data warehousing, or Evidence's revolutionizing of data dashboards with markdown and DuckDB, each tool offers unique advantages that can be leveraged to meet and exceed data engineering objectives.\n\n**A call to action** for data professionals: Embrace the innovative solutions offered by Dagster, MotherDuck, and Evidence. By integrating these tools into your data pipelines, you can build more efficient, scalable, and developer-friendly workflows. The transformative potential of these technologies is not just in their individual capabilities but in their combined power to revolutionize data engineering. As you embark on this journey, remember that the future of data engineering is not just about managing data\u2014it's about unlocking its potential to drive innovation and success.\n\n...SHOW MORE\n\n## Related Videos\n\n[![\"DuckDB: Run dbt build with sub-second execution times\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2025_03_26_at_3_40_16_PM_c6e3a8096f.png&w=3840&q=75)\\\\\n\\\\\n24:06](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n[2025-03-13](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times/)\n\n### [DuckDB: Run dbt build with sub-second execution times](https://motherduck.com/videos/duckdb-run-dbt-build-with-sub-second-execution-times)\n\nWhether you're new to DuckDB or looking to optimize your workflows, this session will provide practical insights to help you leverage its full potential.\n\nYouTube\n\nData Pipelines\n\ndbt\n\nMotherDuck Features\n\n[![\"Can DuckDB revolutionize the data lake experience?\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmaxresdefault_2074c1c09e.jpg&w=3840&q=75)\\\\\n\\\\\n16:37](https://motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience/)\n\n[2024-11-22](https://motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience/)\n\n### [Can DuckDB revolutionize the data lake experience?](https://motherduck.com/videos/can-duckdb-revolutionize-the-data-lake-experience)\n\nMehdi explores DuckDB as a catalog for Data Lake and Lakehouse pattern. He'll define what we mean by \"data catalog\", gives clear examples on how they work and dive into a pragmatic use case with DuckDB & MotherDuck.\n\nYouTube\n\nData Pipelines\n\nSources\n\n[![\"Duckfooding at MotherDuck\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fmaxresdefault_3_289eafcbac.jpg&w=3840&q=75)\\\\\n\\\\\n40:09](https://motherduck.com/videos/duckfooding-at-motherduck/)\n\n[2024-10-14](https://motherduck.com/videos/duckfooding-at-motherduck/)\n\n### [Duckfooding at MotherDuck](https://motherduck.com/videos/duckfooding-at-motherduck)\n\nIn this talk at our MotherDuck NYC meetup \\[warning: bad sound\\], Nicholas Ursa talked about the paradigm shift from distributed computing and the Big data era to MotherDuck. He then explains how we build our internal data warehouse.\n\nYouTube\n\nData Pipelines\n\nMeetup\n\nTalk\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 246,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 246
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_cxkixykg/.kurt/sources/motherduck.com/learn-more/fix-slow-bi-dashboards.md",
  [2m  â”‚[0m     "content": "---\ntitle: 'Fix Slow BI Dashboards: A Guide to Sub-Second Analytics'\ncontent_type: guide\ndescription: Frustrated by slow BI dashboards? Learn the causes of dashboard latency,\n  from architectural bottlenecks to lakehouse issues. See how MotherDuck helps startups\n  scale.\npublished_date: '2025-10-17T00:00:00'\nsource_url: https://motherduck.com/learn-more/fix-slow-bi-dashboards\nindexed_at: '2025-11-25T10:52:12.236559'\ncontent_hash: 0e692003fc4f1f77\nhas_step_by_step: true\nhas_narrative: true\n---\n\nAs a startup, your data is your compass. But as you scale, that compass begins to spin. Business Intelligence (BI) dashboards that were once snappy now take minutes to load, turning quick checks into coffee breaks. Queries time out. Your team hesitates to ask new questions of the data because the feedback loop is agonizingly slow. You're facing a critical growth inflection point: your analytics stack can no longer keep up with your ambition.\n\nYou know you need something more powerful, but the thought of migrating to a traditional cloud data warehouse, with its complex setup, opaque pricing, and multi-year contracts, is daunting. What if there was a better way? A path that gives you the power of a massive data warehouse without the overhead, designed specifically for teams who move fast?\n\nThis guide is for the technical founders, first data hires, and lean analytics teams feeling this pain. We'll provide a clear framework for evaluating if MotherDuck is the right solution to solve your dashboard latency issues, scale your analytics, and empower your team to make faster, data-driven decisions.\n\n### What You'll Learn in This Guide\n\n**What Causes Dashboard Latency?**We'll uncover why your current database struggles with analytical queries and identify the key architectural bottlenecks.**How Do You Scale Beyond a Single Node?**Discover the limitations of single-node engines for large datasets and how MotherDuck's hybrid architecture provides a straightforward path to scale.**How Can You Achieve Real-Time Insights?**Learn how to move from stale, batch-updated data to sub-second freshness without overloading your production systems.**What Does a Migration Look Like?**Follow a practical, week-by-week plan to migrate a critical workload to MotherDuck and start seeing results in under 30 days.**Is MotherDuck the Right Fit for Your Startup?**Get a clear comparison of when MotherDuck excels and what limitations to consider for your specific use case.\n\n## Why Are My Startup's BI Dashboards So Slow?\n\nIf your dashboards are grinding to a halt, the problem usually isn't a single query but an architectural mismatch. Most startups begin by running analytics on a replica of their production database, often a system like PostgreSQL or MySQL. These are [ Online Transaction Processing (OLTP)](https://motherduck.com/learn-more/what-is-OLAP/) databases, brilliant for handling thousands of small, fast transactions like creating a user or processing an order.\n\nHowever, analytical queries are a completely different beast. They are **Online Analytical Processing (OLAP)** workloads, which involve scanning millions or billions of rows across a few columns to calculate aggregations. Asking an OLTP database to perform heavy OLAP queries is like asking a race car to haul lumber. It wasn't built for the job.\n\nThe core issue lies in how the data is stored on disk. OLTP databases are **row-oriented**, meaning they store all the data for a single record together. When your BI tool asks for the total number of users by `plan_type`\n\nfrom a table with 50 columns, a row-oriented database is forced to read all 50 columns for every single row, even though it only needs one. This wastes an enormous amount of I/O.\n\nThis is where [ columnar storage](https://motherduck.com/learn-more/columnar-storage-guide/), the foundation of modern analytical databases like DuckDB, makes a big difference. A columnar database stores all values from a single column together. When you run the same query, it reads\n\n*only*the\n\n`plan_type`\n\ncolumn, dramatically reducing the amount of data scanned. This fundamental difference is often the key to unlocking sub-second query performance.### How Do Row and Columnar Databases Compare for Analytics?\n\n| Feature | Row-Oriented (e.g., PostgreSQL) | Column-Oriented (e.g., DuckDB/MotherDuck) |\n|---|---|---|\nPrimary Use Case | OLTP: Fast reads and writes of individual records. | OLAP: Fast scans and aggregations over large datasets. |\nData Layout | Stores all values for a single record contiguously. | Stores all values for a single column contiguously. |\nQuery Performance | Slow for analytical queries that only need a few columns. | Extremely fast for analytical queries. It only reads the required columns. |\nCompression | Less effective, as it stores mixed data types in each row. | Highly effective, leading to smaller storage footprint and faster scans. |\n\n## What Happens When My Analytics Queries Need to Scan Billions of Rows?\n\nAs your startup succeeds, your data volume explodes. The single-node analytics database that was once a perfect solution starts to show its limits. While an engine like DuckDB is incredibly fast and can even process datasets larger than RAM by [\"spilling\" intermediate results to disk](https://duckdb.org/2024/07/09/memory-management.html), it is ultimately constrained by the resources of a single machine.\n\nWhen you run a complex query with multiple joins and aggregations on billions of rows, the intermediate data generated can overwhelm the system's memory. This can lead to slow queries as the engine constantly writes to and reads from disk, or worse, an [ OutOfMemoryException](https://duckdb.org/docs/stable/guides/troubleshooting/oom_errors.html) that kills the query entirely.\n\nFurthermore, a single node has a finite capacity for **concurrency**. As more team members connect with BI tools, the CPU and I/O resources get saturated, and everyone's queries slow down. This is the practical ceiling of a single-node engine. This is precisely the problem MotherDuck was built to solve. It extends the lightning-fast local experience of DuckDB with a serverless cloud backend, giving you a straightforward path to scale.\n\n## How Does MotherDuck's Hybrid Architecture Deliver Speed and Scale?\n\nMotherDuck introduces a novel architecture that gives you the best of both worlds: the zero-latency feel of local development and the on-demand power of the cloud. This is achieved through a few key concepts.\n\n### What is Dual Execution and Why Does it Matter?\n\nThe magic behind MotherDuck is its [ Dual Execution query planner](https://www.cidrdb.org/cidr2024/papers/p46-atwal.pdf). Instead of forcing you to move all your data to the cloud, it intelligently pushes the computation to where the data lives, minimizing network latency and data transfer costs.\n\n**Local Query:**If you query a CSV file on your laptop, the query runs entirely on your local DuckDB instance. The result is instantaneous.**Cloud Query:**If you query a large table stored in MotherDuck, the work is routed to a dedicated, serverless compute instance (a \"Duckling\") in the cloud.**Hybrid Query:**This is where it gets powerful. If you join a local file with a large cloud table, the planner is smart enough to push filters down to your local machine first. It processes the local file, sends only the small, filtered result to the cloud, and then performs the final join. This makes complex queries incredibly efficient.\n\n### How Do You Handle High Concurrency from BI Tools?\n\nA common and dangerous blind spot for startups is how BI tools handle concurrency. Tools like Tableau or Looker often use a single service account, funneling queries from dozens of users through one connection. This can quickly overwhelm a database.\n\n## Case Study: How Layers Solved Its Concurrency Bottleneck\n\nThe SaaS company\n\n[faced this exact problem. Their analytics, running on PostgreSQL, were overwhelmed when their BI tool masked 73 simultaneous users behind a single service account. This exhausted their connection pool and caused a high rate of query timeouts.]LayersAfter migrating to MotherDuck, they used the\n\nRead Scalingfeature. By connecting their BI tool with a special Read Scaling Token, user queries were automatically distributed across a pool of 16 dedicated, read-only DuckDB replicas. This instantly parallelized the workload, providing each user session with isolated compute. The result was a dramatic improvement in stability, with BI query timeouts dropping to virtually zero.\n\nBy issuing a unique, read-only token for each BI integration and using the `session_hint`\n\nparameter, you can ensure user queries are [intelligently load-balanced](https://motherduck.com/docs/key-tasks/authenticating-and-connecting-to-motherduck/read-scaling/), providing a consistently fast experience for everyone.\n\n## Why Might Lakehouse Architectures Have High Latency Issues?\n\nThe lakehouse architecture, which combines a data lake's flexibility with a data warehouse's performance, is a popular choice. However, startups often find that it comes with a significant \"latency tax,\" especially for interactive queries.\n\nThe primary culprit is the [ \"small files problem.\"](https://dev.to/thedanicafine/the-apache-iceberg-small-file-problem-1k2m) Streaming ingestion jobs often write thousands of tiny files to cloud object storage like Amazon S3. To run a query, the engine must make thousands of separate, high-latency HTTP requests just to read the metadata and find the right files. Each request can take 100-200 milliseconds, adding up to seconds of delay before a single byte of data is even processed.\n\nA benchmark demonstrated this vividly: a query scanning a 5 GB Apache Iceberg table with thousands of small files took **5.093 seconds** to run. The same query on MotherDuck's **DuckLake** format, which uses a database-backed index to avoid scanning S3 for metadata, returned in just **0.146 seconds**, a [ 34x speedup](https://motherduck.com/blog/open-lakehouse-stack-duckdb-table-formats/).\n\n### How Does DuckLake Compare to Apache Iceberg?\n\n| Feature | Apache Iceberg (Standard) | MotherDuck DuckLake |\n|---|---|---|\nMetadata Storage | Stored as files (manifests) in the object store (e.g., S3). | Stored in a fast, transactional database, separate from data files. |\nQuery Planning | Requires multiple sequential S3 reads to discover which data files to scan. | A single SQL query to the catalog database returns the exact list of files. |\nSmall File Handling | Creates many small data and metadata files, requiring costly maintenance jobs (compaction). | Dramatically reduces metadata overhead and the need for frequent compaction. |\nPerformance | High latency for interactive queries due to metadata discovery overhead. |\n34x faster |\n\n## How Can I Get Real-Time Data Without Overloading My Production Database?\n\nIs your sales dashboard always 30 minutes out of date? This frustrating lag is almost always due to a reliance on traditional, batch-based ETL (Extract, Transform, Load) jobs that run on a schedule. For a startup that needs to react instantly to user behavior, this is no longer acceptable.\n\nThe modern solution is a streaming architecture built on **Change Data Capture (CDC)**. Instead of querying your production database tables (which adds heavy load), CDC tools like Estuary or [Streamkap](https://motherduck.com/blog/streamkap-mysql-to-motherduck/) monitor the database's internal transaction log. They capture every insert, update, and delete as it happens and stream these events to MotherDuck in near real-time, often with [ less than 100ms of latency](https://estuary.dev/destination/motherduck/).\n\nThis approach provides two critical benefits:\n\n**Sub-Minute Freshness:**Your analytics are always synchronized with reality.**Zero Production Impact:**It completely isolates your analytical workload, ensuring that heavy queries never slow down your customer-facing application.\n\n## What Are the Signs My Startup Has Outgrown Its Current Analytics Stack?\n\nThe signs that you've outgrown your analytics stack are both technical and cultural. Recognizing them early can save you from months of frustration and slow decision-making.\n\n**Technical Red Flags:**\n\n**High Query Latency:**Your p95 query latency for interactive dashboards consistently exceeds 2-5 seconds. Research shows that[beyond 1 second, users lose their flow of thought, and beyond 10 seconds, you lose their attention entirely](https://www.nngroup.com/articles/powers-of-10-time-scales-in-ux/).**Rising Error Rates:**You see an increase in query timeouts, connection failures, or application errors related to database load.**Stale Data:**Business teams complain that dashboards are always out of date, indicating that nightly batch jobs are no longer sufficient.\n\n**Business and Cultural Red Flags:**\n\n**Dashboard Abandonment:**Analysts and business users stop using the BI tool because it's \"too slow.\"**Rising Costs:**Your infrastructure bill is growing faster than the value you're getting from your data, often due to over-provisioning to compensate for poor performance.**Slow Product Velocity:**Engineers spend more time optimizing the database than building features, and product managers can't get timely data to inform their roadmap.\n\nCompanies that see these signs can achieve significant improvements by migrating. **Finqore** reduced 8-hour financial data pipelines to just 8 minutes, while **uDisc** cut query times from minutes to seconds, leading to [a significant lift in daily active analysts](https://motherduck.com/case-studies/).\n\n## How Do I Plan a Migration to MotherDuck?\n\nMigrating to MotherDuck doesn't have to be a massive, disruptive project. You can see value in under 30 days by following a straightforward, phased approach focused on a single, high-pain workload.\n\n### Your 30-Day Migration Roadmap\n\n| Week | Key Tasks | Success Criteria |\n|---|---|---|\nWeek 1: Connect & Ingest | - Create your MotherDuck account and generate service tokens. - Install the DuckDB SDK and connect via `ATTACH 'md:';` . - Perform an initial bulk load of a target dataset (e.g., one large, slow table) into a new MotherDuck database. | - You can successfully query your data in MotherDuck from your local machine and BI tool. - Historical data for one target workload is fully loaded. |\nWeek 2: Mirror a Pilot Workload | - Select one high-pain, low-risk dashboard to migrate. - Set up an incremental CDC pipeline (e.g., using Estuary) to keep MotherDuck in sync with the source. - Re-create the dashboard's data model to point to MotherDuck. | - The mirrored dashboard in MotherDuck is live and updating in near real-time. - p95 query latency is under 2 seconds. |\nWeek 3: Validate & Optimize | - Share the new dashboard with a small group of business users for feedback. - Monitor query performance and cost. - Use optimizations like Read Scaling tokens for the BI tool connection. | - Users confirm the new dashboard is significantly faster and accurate. - The projected cost for the pilot workload is lower than the legacy system. |\nWeek 4: Cut-Over & Expand | - Officially switch all users to the new MotherDuck-powered dashboard. - Decommission the old data models and pipelines for the migrated workload. - Plan the migration of the next set of analytical workloads. | - 100% of users for the pilot workload are using the new dashboard. - A prioritized backlog for the next migration is created. |\n\n## What Are the Limitations of MotherDuck I Should Consider?\n\nNo tool is perfect for every situation. Being honest about limitations is key to making the right choice. MotherDuck is rapidly evolving, but startups should be aware of a few current constraints:\n\n**Regional Availability:**MotherDuck currently operates in the AWS[us-east-1](https://motherduck.com/docs/concepts/architecture-and-capabilities/)and recently started in[eu-central-1](https://motherduck.com/blog/motherduck-in-europe/), hosted in Frankfurt. Teams with strict data residency requirements outside this region will need to consider this.**Partial DuckDB SQL Coverage:**MotherDuck does not yet support the full range of DuckDB's SQL features. Notably, custom Python/native User-Defined Functions (UDFs), server-side`ATTACH`\n\nto other databases (like Postgres), and custom extensions are not yet available. The common workaround is to perform these specific transformations upstream before loading data into MotherDuck.\n\n## Conclusion: Stop Waiting, Start Analyzing\n\nFor a startup, speed is everything: speed to market, speed to insight, and speed of execution. A slow analytics stack is a direct drag on all three. You don't need the complexity and cost of a legacy data warehouse, nor can you afford the performance bottlenecks of an overloaded production database.\n\nMotherDuck offers a third way, purpose-built for the scale and agility of a modern startup. By combining the raw speed of a local-first engine with the on-demand scale of the cloud, it eliminates the trade-offs that have held analytics teams back for years. If your dashboards are slow and your team is frustrated, it's time to evaluate a new approach.\n\n[Ready to experience sub-second analytics? Start building on MotherDuck's free tier today.](https://motherduck.com/startups/)\n\nStart using MotherDuck now!\n\n## FAQS\n\n### How much does MotherDuck cost for a startup?\n\nMotherDuck's pricing is designed for startups. It includes a generous free tier for smaller projects and two pay-as-you-go plans that charge only for storage used and compute-seconds executed. There are no idle cluster charges or minimums, which can lead to [ 70-90% cost savings](https://motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck/) compared to traditional data warehouses.\n\n### Can MotherDuck handle our data volume?\n\nYes. Analysis shows that [over 95% of startup databases are smaller than 1 TB](https://hemantkgupta.medium.com/insight-from-paper-motherduck-duckdb-in-the-cloud-and-in-the-client-e4a73da9dbec), a size range where MotherDuck excels. The hybrid architecture is designed to scale from megabytes on your laptop to tens of terabytes in the cloud, ensuring you have a growth path.\n\n### How does MotherDuck compare to Snowflake or BigQuery for a startup?\n\nWhile large warehouses are powerful, they often come with significant operational complexity and cost overhead that can be burdensome for a startup. MotherDuck offers [a simpler, more cost-effective path](https://motherduck.com/learn-more/modern-data-warehouse-playbook/). Its key differentiators are the serverless, per-second billing model and the unique \"local-first\" hybrid architecture, which provides an excellent development experience and eliminates network latency for many common analytical tasks.\n\n### How do startups decide if MotherDuck fits their analytics stack?\n\nA startup should consider MotherDuck if they experience slow BI dashboards, rising query timeouts, and stale data. It's a strong fit for teams that have outgrown a single database (like PostgreSQL) but want to avoid the cost and complexity of a traditional data warehouse. MotherDuck excels with data volumes from gigabytes to tens of terabytes and is ideal for building fast, interactive analytics without a large data engineering team.\n\n### Why do dashboards struggle when scanning billions of rows?\n\nWhen scanning billions of rows, single-node engines like DuckDB can be constrained by the memory and I/O of a single machine. Complex queries generate large intermediate results that can exceed available RAM, forcing the engine to \"spill\" to a slower disk, which increases latency. High user concurrency can also saturate the CPU, causing all queries to slow down.\n\n### Why do lakehouse architectures often have high query latency?\n\nLakehouse latency is often caused by the **\"small files problem.\"** Streaming jobs write thousands of tiny files to object storage like S3. To run a query, the engine must make many high-latency network requests just to read the metadata and find the right files, adding seconds of delay before the query even starts processing data.\n\n### What are the main causes of slow BI dashboards?\n\nThe primary cause is an architectural mismatch. Startups often run heavy analytical (OLAP) queries on their production (OLTP) database, like PostgreSQL. These row-oriented databases are inefficient for analytics, as they must read entire records instead of just the needed columns. As data grows, this leads to high I/O, slow queries, and dashboard latency.\n\n### How does data modeling impact dashboard performance?\n\nIt has a massive impact. Using a [ star schema](https://motherduck.com/learn-more/star-schema-data-warehouse-guide/), which organizes data into a central \"fact\" table and surrounding \"dimension\" tables, is a proven technique for accelerating analytical queries in columnar databases.",
  [2m  â”‚[0m     "numLines": 189,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 189
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Excellent! This is very helpful. Let me search for a couple more specific sources on query performance differences:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Grep â†’ small data|single node|local.*fast|faster.*local

  [96mğŸ”§ TOOL:[0m Read â†’ /var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_cxkixykg/.kurt/sources/motherduck.com/videos/the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "text",
  [2m  â”‚[0m   "file": {
  [2m  â”‚[0m     "filePath": "/var/folders/yr/lv_0z6m15fg8llcm02_kw4m80000gn/T/kurt_eval_cxkixykg/.kurt/sources/motherduck.com/videos/the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck.md",
  [2m  â”‚[0m     "content": "---\ntitle: the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck\ncontent_type: event\nsource_url: https://motherduck.com/videos/the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck\nindexed_at: '2025-11-25T20:43:51.796116'\ncontent_hash: ece5599d4fe0d66d\n---\n\nHands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)\n\n[Motherduck home](https://motherduck.com/)\n\n[START FREE](https://app.motherduck.com/?auth_flow=signup)\n\n[BACK TO VIDEOS](https://motherduck.com/videos/)\n\nThe Death of Big Data and Why It\u2019s Time To Think Small \\| Jordan Tigani, CEO, MotherDuck - YouTube\n\n[Photo image of The MAD Podcast with Matt Turck](https://www.youtube.com/channel/UCQID78IY6EOojr5RUdD47MQ?embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\nThe MAD Podcast with Matt Turck\n\n21.3K subscribers\n\n[The Death of Big Data and Why It\u2019s Time To Think Small \\| Jordan Tigani, CEO, MotherDuck](https://www.youtube.com/watch?v=MEQu3Sgwjr8)\n\nThe MAD Podcast with Matt Turck\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=MEQu3Sgwjr8&embeds_referring_euri=https%3A%2F%2Fmotherduck.com%2F)\n\n0:00\n\n0:00 / 59:08\n\n\u2022Live\n\n\u2022\n\nYouTubeInterview\n\n# The Death of Big Data and Why It\u2019s Time To Think Small \\| Jordan Tigani, CEO, MotherDuck\n\n2024/10/24\n\nA founding engineer on Google BigQuery and now at the helm of MotherDuck, Jordan Tigani challenges the decade-long dominance of Big Data and introduces a compelling alternative that could change how companies handle data.\n\n## Related Videos\n\n[![\"Lies, Damn Lies, and Benchmarks\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FLies_Damn_Lies_and_Benchmarks_Thumbnail_404db1bf46.png&w=3840&q=75)](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n[2025-10-31](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/)\n\n### [Lies, Damn Lies, and Benchmarks](https://motherduck.com/videos/lies-damn-lies-and-benchmarks)\n\nWhy do database benchmarks so often mislead? MotherDuck CEO Jordan Tigani discusses the pitfalls of performance benchmarking, lessons from BigQuery, and why your own workload is the only benchmark that truly matters.\n\nStream\n\nInterview\n\n[![\"Can DuckDB replace your data stack?\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.us-east-1.amazonaws.com%2Fassets%2Fimg%2FCan_Duck_DB_Replace_Your_Data_Stack_Mother_Duck_Co_Founder_Ryan_Boyd_3_56_screenshot_70e18322ec.png&w=3840&q=75)\\\\\n\\\\\n60:00](https://motherduck.com/videos/can-duckdb-replace-your-data-stack/)\n\n[2025-10-23](https://motherduck.com/videos/can-duckdb-replace-your-data-stack/)\n\n### [Can DuckDB replace your data stack?](https://motherduck.com/videos/can-duckdb-replace-your-data-stack)\n\nMotherDuck co-founder Ryan Boyd joins the Super Data Brothers show to talk about all things DuckDB, MotherDuck, AI agents/LLMs, hypertenancy and more.\n\nYouTube\n\nBI & Visualization\n\nAI, ML and LLMs\n\nInterview\n\n[![\"Big data is dead, analytics is alive.\" video thumbnail](https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FScreenshot_2024_10_25_at_11_39_36_AM_8217aaed02.png&w=3840&q=75)\\\\\n\\\\\n50:21](https://changelog.com/practicalai/292)\n\n[2024-10-09](https://changelog.com/practicalai/292)\n\n### [Big data is dead, analytics is alive.](https://changelog.com/practicalai/292)\n\nTill and Adithya from MotherDuck discuss DuckDB\u2019s impact on analytics and AI, showcasing its fast, versatile in-process SQL and AI features like text-to-SQL, vector search, and query correction\u2014powerful analytics even on your laptop.\n\nAI, ML and LLMs\n\nInterview\n\n[View all](https://motherduck.com/videos/)\n\nAuthorization Response",
  [2m  â”‚[0m     "numLines": 127,
  [2m  â”‚[0m     "startLine": 1,
  [2m  â”‚[0m     "totalLines": 127
  [2m  â”‚[0m   }
  [2m  â”‚[0m }
  [2m  â””â”€[0m
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "mode": "files_with_matches",
  [2m  â”‚[0m   "filenames": [
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/unfinished-business-re-inventing-modern-data-tools.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/using-sql-in-your-data-lake-with-duckdb-iceberg-dbt-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/whats-new-in-data-small-data-big-impact.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-future-of-bi-exploring-the-impact-of-bi-as-code-tools-with-duckdb.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/the-power-of-wasm-for-analytics-duckdb-in-the-browser.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/moving-forward-from-ad-hoc-reports-with-duckdb-and-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/smarter-ai-workflows-with-hex-motherduck.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/is-bi-too-big-for-small-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/from-curiosity-to-impact-how-dosomething-democratized-data.md",
  [2m  â”‚[0m     ".kurt/sources/motherduck.com/videos/duckdb-streamlit-crafting-dynamic-dashboards-and-data-apps.md"
  [2m  â”‚[0m   ],
  [2m  â”‚[0m   "numFiles": 10,
  [2m  â”‚[0m   "appliedLimit": 10
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m Now I have enough information. Let me write the comprehensive answer:
  [94mâ””â”€[0m

  [96mğŸ”§ TOOL:[0m Write â†’ /tmp/answer_without_kg_4.md
  [92m  âœ“ RESULT:[0m
  [2m  â”‚[0m {
  [2m  â”‚[0m   "type": "create",
  [2m  â”‚[0m   "filePath": "/tmp/answer_without_kg_4.md",
  [2m  â”‚[0m   "content": "<4635 chars>",
  [2m  â”‚[0m   "structuredPatch": [],
  [2m  â”‚[0m   "originalFile": null
  [2m  â”‚[0m }
  [2m  â””â”€[0m

  [94mâ”Œâ”€ ğŸ¤– AGENT MESSAGE[0m
  [94mâ”‚[0m I've written a comprehensive answer to `/tmp/answer_without_kg_4.md`. The answer explains why queries might run slower on MotherDuck than locally, covering:
  [94mâ”‚[0m 
  [94mâ”‚[0m 1. **Network latency and data transfer overhead** - cloud execution adds round-trip time
  [94mâ”‚[0m 2. **Data locality** - local files are faster to query locally; cloud data benefits from remote execution
  [94mâ”‚[0m 3. **Small dataset optimization** - modern laptops handle small data very efficiently without cloud overhead
  [94mâ”‚[0m 4. **Cold start issues** - local DuckDB is always "warm" while cloud resources may need allocation
  [94mâ”‚[0m 5. **Use case optimization** - MotherDuck excels at scale and collaboration, but adds overhead for simple local scenarios
  [94mâ”‚[0m 
  [94mâ”‚[0m The answer is grounded in 5 source files from the MotherDuck documentation that I read and analyzed, including specific performance comparisons (e.g., 25 seconds remote vs 36 seconds local for S3 data queries).
  [94mâ””â”€[0m

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š TURN 1 METRICS
     Tokens: 12,603 | Cost: $0.5260
     Cumulative: 12,603 tokens | $0.5260
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âœ… TASK COMPLETE
     Reason: llm: The agent is not asking for user input but rather presenting a completed analysis based on its findings.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SESSION COMPLETE
â•‘    Turns: 1 | Tools: 15 | Duration: 101.0s
â•‘    Tokens: 12,603 | Cost: $0.5260
â•‘    Stop reason: Task completed (no follow-up questions)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ§  Running LLM judge evaluation...
   ğŸ§  LLM Judge score: 1.00
