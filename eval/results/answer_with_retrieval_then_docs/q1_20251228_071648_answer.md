# Answer

**Parquet** is the most efficient file format for loading data into MotherDuck due to its:
- Columnar storage layout optimized for analytical queries
- Impressive compression capabilities
- Support for predicate pushdown and column pruning
- Native handling of large datasets with high performance

MotherDuck natively supports ingesting from **CSV, Parquet, JSON, Iceberg, and Delta** file formats. However, for analytical workloads where you're typically reading a subset of columns from large datasets, Parquet significantly outperforms row-based formats like CSV.

**Recommendation:** Use Parquet when possible for best performance; CSV works for simpler use cases or when data arrives in that format.

---
Sources:
- Claim: "MotherDuck can natively ingest from CSV, Parquet, JSON, Iceberg, & Delta file formats" [MotherDuck]
- Claim: "Parquet has emerged as a popular choice for its columnar storage layout and impressive compression capabilities" [4aa515a5]
- Claim: "Parquet effectively supports techniques like predicate pushdown and column pruning" [4aa515a5]
- Claim: "Parquet excels at analytical queries that involve reading a subset of columns from a large dataset" [4aa515a5]
- Entities: Parquet, CSV, JSON, Iceberg, Delta, Apache Parquet
