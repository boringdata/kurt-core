"""LLM-as-judge evaluation using DSPy.

Scores generated answers against canonical answers using multiple criteria.
"""

import os
from typing import Any, Dict, List, Optional

import dspy


class AnswerScorer(dspy.Signature):
    """Score a generated answer against a canonical answer.

    Evaluates the answer on multiple dimensions:
    - Accuracy: Factual correctness compared to canonical answer
    - Completeness: Coverage of key topics and required information
    - Relevance: How well the answer addresses the question
    - Clarity: Writing quality and readability
    """

    question = dspy.InputField(desc="The question that was asked")
    canonical_answer = dspy.InputField(desc="The canonical/reference answer")
    required_topics = dspy.InputField(desc="List of topics that must be covered")
    generated_answer = dspy.InputField(desc="The answer generated by the system")

    # Score components (0.0-1.0)
    accuracy_score = dspy.OutputField(
        desc="Accuracy score (0.0-1.0): How factually correct is the answer compared to the canonical answer?"
    )
    completeness_score = dspy.OutputField(
        desc="Completeness score (0.0-1.0): Does the answer cover all required topics and key information?"
    )
    relevance_score = dspy.OutputField(
        desc="Relevance score (0.0-1.0): How well does the answer address the specific question asked?"
    )
    clarity_score = dspy.OutputField(
        desc="Clarity score (0.0-1.0): Is the answer well-written, clear, and easy to understand?"
    )
    feedback = dspy.OutputField(
        desc="Brief explanation of the scores (2-3 sentences)"
    )


def configure_dspy_llm(provider: str = "anthropic", model: Optional[str] = None):
    """Configure DSPy with the specified LLM provider.

    Args:
        provider: LLM provider ('anthropic' or 'openai')
        model: Optional model name (uses defaults if not specified)
    """
    if provider == "anthropic":
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY environment variable not set")

        model = model or "claude-3-5-sonnet-20241022"
        lm = dspy.Claude(
            model=model,
            api_key=api_key,
            max_tokens=2048,
        )

    elif provider == "openai":
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable not set")

        model = model or "gpt-4o-mini"
        lm = dspy.LM(
            model=f"openai/{model}",
            api_key=api_key,
            max_tokens=2048,
        )

    else:
        raise ValueError(f"Unknown LLM provider: {provider}")

    dspy.configure(lm=lm)
    return lm


def score_single_answer(
    question: str,
    canonical_answer: str,
    generated_answer: str,
    required_topics: List[str],
    score_weights: Dict[str, float],
    llm_provider: str = "anthropic",
) -> Dict[str, Any]:
    """Score a single answer using LLM-as-judge.

    Args:
        question: The question text
        canonical_answer: The reference answer
        generated_answer: The system-generated answer
        required_topics: List of topics that should be covered
        score_weights: Weights for each score component
        llm_provider: LLM provider to use

    Returns:
        Dictionary with scores and feedback:
        {
            "overall_score": 0.85,
            "component_scores": {
                "accuracy": 0.9,
                "completeness": 0.8,
                "relevance": 0.9,
                "clarity": 0.8
            },
            "feedback": "The answer correctly identifies..."
        }
    """
    # Configure DSPy (if not already configured)
    configure_dspy_llm(provider=llm_provider)

    # Create scorer module
    scorer = dspy.ChainOfThought(AnswerScorer)

    # Score the answer
    try:
        result = scorer(
            question=question,
            canonical_answer=canonical_answer,
            required_topics=", ".join(required_topics),
            generated_answer=generated_answer,
        )

        # Extract scores (convert string to float)
        component_scores = {
            "accuracy": _parse_score(result.accuracy_score),
            "completeness": _parse_score(result.completeness_score),
            "relevance": _parse_score(result.relevance_score),
            "clarity": _parse_score(result.clarity_score),
        }

        # Calculate weighted overall score
        overall_score = sum(
            component_scores[component] * weight
            for component, weight in score_weights.items()
        )

        return {
            "overall_score": round(overall_score, 2),
            "component_scores": component_scores,
            "feedback": result.feedback,
        }

    except Exception as e:
        # Return zero scores on error
        return {
            "overall_score": 0.0,
            "component_scores": {
                "accuracy": 0.0,
                "completeness": 0.0,
                "relevance": 0.0,
                "clarity": 0.0,
            },
            "feedback": f"Scoring failed: {str(e)}",
        }


def score_batch_answers(
    questions: List[Dict[str, Any]],
    answers: List[Dict[str, str]],
    score_weights: Dict[str, float],
    llm_provider: str = "anthropic",
) -> Dict[str, Any]:
    """Score multiple answers in batch.

    Args:
        questions: List of question dictionaries with:
            - id: Question ID
            - question: Question text
            - canonical_answer: Reference answer
            - required_topics: List of required topics
        answers: List of answer dictionaries with:
            - question_id: Question ID
            - answer: Generated answer text
        score_weights: Weights for score components
        llm_provider: LLM provider to use

    Returns:
        Dictionary with scored questions and summary:
        {
            "questions": [
                {
                    "id": "q1_file_formats",
                    "question": "What file formats...",
                    "canonical_answer": "Parquet is...",
                    "generated_answer": "Parquet is...",
                    "llm_judge": {
                        "overall_score": 0.85,
                        "component_scores": {...},
                        "feedback": "..."
                    }
                }
            ],
            "summary": {
                "average_score": 0.78,
                "passed": true,
                "num_questions": 10
            }
        }
    """
    # Create lookup for answers
    answers_by_id = {a["question_id"]: a["answer"] for a in answers}

    scored_questions = []
    all_scores = []

    # Score each question
    for question in questions:
        question_id = question["id"]
        generated_answer = answers_by_id.get(question_id)

        if not generated_answer:
            # No answer found - assign zero scores
            llm_judge_result = {
                "overall_score": 0.0,
                "component_scores": {
                    "accuracy": 0.0,
                    "completeness": 0.0,
                    "relevance": 0.0,
                    "clarity": 0.0,
                },
                "feedback": "No answer was generated for this question",
            }
        else:
            # Score the answer
            llm_judge_result = score_single_answer(
                question=question["question"],
                canonical_answer=question["canonical_answer"],
                generated_answer=generated_answer,
                required_topics=question.get("required_topics", []),
                score_weights=score_weights,
                llm_provider=llm_provider,
            )

        # Build scored question entry
        scored_question = {
            "id": question_id,
            "question": question["question"],
            "canonical_answer": question["canonical_answer"],
            "generated_answer": generated_answer or "",
            "llm_judge": llm_judge_result,
        }

        scored_questions.append(scored_question)

        # Track scores for summary
        score = llm_judge_result["overall_score"]
        all_scores.append(score)

    # Calculate summary statistics
    average_score = sum(all_scores) / len(all_scores) if all_scores else 0.0

    summary = {
        "average_score": round(average_score, 2),
        "passed": average_score >= 0.7,  # Default threshold
        "num_questions": len(questions),
    }

    return {
        "questions": scored_questions,
        "summary": summary,
    }


def _parse_score(score_str: str) -> float:
    """Parse a score string to float (0.0-1.0).

    Handles various formats:
    - "0.85" -> 0.85
    - "85%" -> 0.85
    - "8.5/10" -> 0.85
    - "good" -> 0.8 (fallback heuristic)

    Args:
        score_str: Score string from LLM

    Returns:
        Float score between 0.0 and 1.0
    """
    try:
        # Remove whitespace
        score_str = str(score_str).strip()

        # Handle percentage format
        if "%" in score_str:
            score_str = score_str.replace("%", "")
            return float(score_str) / 100.0

        # Handle X/Y format
        if "/" in score_str:
            numerator, denominator = score_str.split("/")
            return float(numerator) / float(denominator)

        # Try direct float conversion
        score = float(score_str)

        # If score > 1, assume it's out of 10 or 100
        if score > 1.0:
            if score <= 10.0:
                return score / 10.0
            else:
                return score / 100.0

        # Clamp to [0.0, 1.0]
        return max(0.0, min(1.0, score))

    except (ValueError, TypeError):
        # Fallback: try to infer from text
        score_str_lower = score_str.lower()

        if "excellent" in score_str_lower or "perfect" in score_str_lower:
            return 1.0
        elif "good" in score_str_lower or "high" in score_str_lower:
            return 0.8
        elif "fair" in score_str_lower or "medium" in score_str_lower:
            return 0.6
        elif "poor" in score_str_lower or "low" in score_str_lower:
            return 0.4
        else:
            # Default to middle score
            return 0.5
