---
# Kurt Evaluation Scenarios - MotherDuck Question Answering
#
# Simple scenario for testing the answer command with MotherDuck documentation

scenarios:
  - name: answer_motherduck
    description: Answer question about MotherDuck file formats using knowledge graph
    project: motherduck
    conversational: false  # Command-based scenario - no agent interaction

    notes: |
      Tests the answer command with MotherDuck documentation:
      - Load pre-built knowledge graph from motherduck dump (874 docs, 371 entities)
      - Run kurt answer command and save output to file
      - Verify the answer contains relevant content

      This is a command-based scenario that runs kurt answer directly via setup_commands
      rather than using a conversational agent.

    setup_commands:
      # Run the answer command and save to markdown file
      - KURT_TELEMETRY_DISABLED=1 uv run kurt answer "What file formats are most efficient for loading data into MotherDuck?" > cmd_answer.md 2>&1

    post_scenario_commands:
      # Gather all command outputs into a final result file
      - |
        cat > results.md <<'EOF'
        # Answer Results - MotherDuck File Formats

        ## Question
        What file formats are most efficient for loading data into MotherDuck?

        ## Answer
        EOF
        cat cmd_answer.md >> results.md

    assertions:
      # Verify knowledge graph is populated
      - type: SQLQueryAssertion
        query: SELECT COUNT(*) >= 100 FROM entities

      - type: SQLQueryAssertion
        query: SELECT COUNT(*) >= 500 FROM documents

      # Verify answer command output file exists
      - type: FileExists
        path: cmd_answer.md

      - type: FileContains
        path: cmd_answer.md
        content: "(?i)Parquet"
        is_regex: true

      - type: FileContains
        path: cmd_answer.md
        content: "(?i)file format"
        is_regex: true
